pub fn sub_832B66B8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B66B8 size=184
	// 832B66B8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B66BC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B66C0: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B66C4: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B66C8: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B66CC: 3BEB3880  addi r31, r11, 0x3880
	ctx.r[31].s64 = ctx.r[11].s64 + 14464;
	// 832B66D0: 387F0028  addi r3, r31, 0x28
	ctx.r[3].s64 = ctx.r[31].s64 + 40;
	// 832B66D4: 4B041625  bl 0x822f7cf8
	ctx.lr = 0x832B66D8;
	crate::recompiler::externs::call(&mut ctx, base, 0x822F7CF8);
	// 832B66D8: 387F001C  addi r3, r31, 0x1c
	ctx.r[3].s64 = ctx.r[31].s64 + 28;
	// 832B66DC: 4B05D0C5  bl 0x823137a0
	ctx.lr = 0x832B66E0;
	crate::recompiler::externs::call(&mut ctx, base, 0x823137A0);
	// 832B66E0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B66E4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B66E8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B66EC: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B66F0: 4E800020  blr
	return;
}

pub fn sub_832B6770(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6770 size=96
	// 832B6770: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6774: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6778: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B677C: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6780: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6784: 3FE08332  lis r31, -0x7cce
	ctx.r[31].s64 = -2093875200;
	// 832B6788: 3D60820F  lis r11, -0x7df1
	ctx.r[11].s64 = -2112946176;
	// 832B678C: 3BDFB0BC  addi r30, r31, -0x4f44
	ctx.r[30].s64 = ctx.r[31].s64 + -20292;
	// 832B6790: 396B2A30  addi r11, r11, 0x2a30
	ctx.r[11].s64 = ctx.r[11].s64 + 10800;
	// 832B6794: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832B6798: 917FB0BC  stw r11, -0x4f44(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-20292 as u32), ctx.r[11].u32 ) };
	// 832B679C: 4AF4586D  bl 0x821fc008
	ctx.lr = 0x832B67A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC008);
	// 832B67A0: 3D60820F  lis r11, -0x7df1
	ctx.r[11].s64 = -2112946176;
	// 832B67A4: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832B67A8: 396B2A40  addi r11, r11, 0x2a40
	ctx.r[11].s64 = ctx.r[11].s64 + 10816;
	// 832B67AC: 917FB0BC  stw r11, -0x4f44(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-20292 as u32), ctx.r[11].u32 ) };
	// 832B67B0: 4AF45859  bl 0x821fc008
	ctx.lr = 0x832B67B4;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC008);
	// 832B67B4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B67B8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B67BC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B67C0: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B67C4: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B67C8: 4E800020  blr
	return;
}

pub fn sub_832B67D0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B67D0 size=96
	// 832B67D0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B67D4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B67D8: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B67DC: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B67E0: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B67E4: 3FE08332  lis r31, -0x7cce
	ctx.r[31].s64 = -2093875200;
	// 832B67E8: 3D608200  lis r11, -0x7e00
	ctx.r[11].s64 = -2113929216;
	// 832B67EC: 3BDFB0C4  addi r30, r31, -0x4f3c
	ctx.r[30].s64 = ctx.r[31].s64 + -20284;
	// 832B67F0: 396B1460  addi r11, r11, 0x1460
	ctx.r[11].s64 = ctx.r[11].s64 + 5216;
	// 832B67F4: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832B67F8: 917FB0C4  stw r11, -0x4f3c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-20284 as u32), ctx.r[11].u32 ) };
	// 832B67FC: 4AF4580D  bl 0x821fc008
	ctx.lr = 0x832B6800;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC008);
	// 832B6800: 3D60820F  lis r11, -0x7df1
	ctx.r[11].s64 = -2112946176;
	// 832B6804: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832B6808: 396B2A40  addi r11, r11, 0x2a40
	ctx.r[11].s64 = ctx.r[11].s64 + 10816;
	// 832B680C: 917FB0C4  stw r11, -0x4f3c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-20284 as u32), ctx.r[11].u32 ) };
	// 832B6810: 4AF457F9  bl 0x821fc008
	ctx.lr = 0x832B6814;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC008);
	// 832B6814: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B6818: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B681C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6820: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B6824: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6828: 4E800020  blr
	return;
}

pub fn sub_832B6830(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6830 size=200
	// 832B6830: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6834: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6838: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B683C: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6840: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6844: 3FE08332  lis r31, -0x7cce
	ctx.r[31].s64 = -2093875200;
	// 832B6848: 3D608210  lis r11, -0x7df0
	ctx.r[11].s64 = -2112880640;
	// 832B684C: 3BDFB0CC  addi r30, r31, -0x4f34
	ctx.r[30].s64 = ctx.r[31].s64 + -20276;
	// 832B6850: 396BADE4  addi r11, r11, -0x521c
	ctx.r[11].s64 = ctx.r[11].s64 + -21020;
	// 832B6854: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832B6858: 917FB0CC  stw r11, -0x4f34(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-20276 as u32), ctx.r[11].u32 ) };
	// 832B685C: 4AF457AD  bl 0x821fc008
	ctx.lr = 0x832B6860;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC008);
	// 832B6860: 3D60820F  lis r11, -0x7df1
	ctx.r[11].s64 = -2112946176;
	// 832B6864: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832B6868: 396B2A40  addi r11, r11, 0x2a40
	ctx.r[11].s64 = ctx.r[11].s64 + 10816;
	// 832B686C: 917FB0CC  stw r11, -0x4f34(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-20276 as u32), ctx.r[11].u32 ) };
	// 832B6870: 4AF45799  bl 0x821fc008
	ctx.lr = 0x832B6874;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC008);
	// 832B6874: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B6878: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B687C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6880: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B6884: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6888: 4E800020  blr
	return;
}

pub fn sub_832B68F8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B68F8 size=168
	// 832B68F8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B68FC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6900: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B6904: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6908: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B690C: 3FC08332  lis r30, -0x7cce
	ctx.r[30].s64 = -2093875200;
	// 832B6910: 3BFEB0E8  addi r31, r30, -0x4f18
	ctx.r[31].s64 = ctx.r[30].s64 + -20248;
	// 832B6914: 387F0008  addi r3, r31, 8
	ctx.r[3].s64 = ctx.r[31].s64 + 8;
	// 832B6918: 4B8AABF9  bl 0x82b61510
	ctx.lr = 0x832B691C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82B61510);
	// 832B691C: 807F000C  lwz r3, 0xc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832B6920: 4AF65419  bl 0x8221bd38
	ctx.lr = 0x832B6924;
	crate::recompiler::externs::call(&mut ctx, base, 0x8221BD38);
	// 832B6924: 3D40820C  lis r10, -0x7df4
	ctx.r[10].s64 = -2113142784;
	// 832B6928: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B692C: 394A0FD0  addi r10, r10, 0xfd0
	ctx.r[10].s64 = ctx.r[10].s64 + 4048;
	// 832B6930: 917F000C  stw r11, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[11].u32 ) };
	// 832B6934: 915EB0E8  stw r10, -0x4f18(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(-20248 as u32), ctx.r[10].u32 ) };
	// 832B6938: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B693C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6940: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6944: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B6948: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B694C: 4E800020  blr
	return;
	// 832B6950: 4E800020  blr
	return;
}

pub fn sub_832B69A0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B69A0 size=88
	// 832B69A0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B69A4: 4B9F2A65  bl 0x82ca9408
	ctx.lr = 0x832B69A8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9408);
	// 832B69A8: 9421FF80  stwu r1, -0x80(r1)
	ea = ctx.r[1].u32.wrapping_add(-128 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B69AC: 3D60834A  lis r11, -0x7cb6
	ctx.r[11].s64 = -2092302336;
	// 832B69B0: 3B800003  li r28, 3
	ctx.r[28].s64 = 3;
	// 832B69B4: 396B6088  addi r11, r11, 0x6088
	ctx.r[11].s64 = ctx.r[11].s64 + 24712;
	// 832B69B8: 3BC00000  li r30, 0
	ctx.r[30].s64 = 0;
	// 832B69BC: 3BEB0034  addi r31, r11, 0x34
	ctx.r[31].s64 = ctx.r[11].s64 + 52;
	// 832B69C0: 3D60820F  lis r11, -0x7df1
	ctx.r[11].s64 = -2112946176;
	// 832B69C4: 3BAB2A2C  addi r29, r11, 0x2a2c
	ctx.r[29].s64 = ctx.r[11].s64 + 10796;
	// 832B69C8: 3BFFFFF4  addi r31, r31, -0xc
	ctx.r[31].s64 = ctx.r[31].s64 + -12;
	// 832B69CC: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B69D0: 93BFFFFC  stw r29, -4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-4 as u32), ctx.r[29].u32 ) };
	// 832B69D4: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B69D8: 419A0010  beq cr6, 0x832b69e8
	if ctx.cr[6].eq {
	pc = 0x832B69E8; continue 'dispatch;
	}
	// 832B69DC: 4AF45945  bl 0x821fc320
	ctx.lr = 0x832B69E0;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC320);
	// 832B69E0: 93DF0000  stw r30, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832B69E4: 93DF0004  stw r30, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[30].u32 ) };
	// 832B69E8: 379CFFFF  addic. r28, r28, -1
	ctx.xer.ca = (ctx.r[28].u32 > (!(-1 as u32)));
	ctx.r[28].s64 = ctx.r[28].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[28].s32, 0, &mut ctx.xer);
	// 832B69EC: 4080FFDC  bge 0x832b69c8
	if !ctx.cr[0].lt {
	pc = 0x832B69C8; continue 'dispatch;
	}
	// 832B69F0: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832B69F4: 4B9F2A64  b 0x82ca9458
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9458);
	return;
}

pub fn sub_832B69F8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B69F8 size=88
	// 832B69F8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B69FC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6A00: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6A04: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6A08: 3D608332  lis r11, -0x7cce
	ctx.r[11].s64 = -2093875200;
	// 832B6A0C: 3D40820F  lis r10, -0x7df1
	ctx.r[10].s64 = -2112946176;
	// 832B6A10: 3BEBB12C  addi r31, r11, -0x4ed4
	ctx.r[31].s64 = ctx.r[11].s64 + -20180;
	// 832B6A14: 396A2A2C  addi r11, r10, 0x2a2c
	ctx.r[11].s64 = ctx.r[10].s64 + 10796;
	// 832B6A18: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B6A1C: 917F0000  stw r11, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832B6A20: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B6A24: 419A0018  beq cr6, 0x832b6a3c
	if ctx.cr[6].eq {
	pc = 0x832B6A3C; continue 'dispatch;
	}
	// 832B6A28: 4AF458F9  bl 0x821fc320
	ctx.lr = 0x832B6A2C;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC320);
	// 832B6A2C: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B6A30: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B6A34: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B6A38: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B6A3C: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B6A40: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6A44: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6A48: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6A4C: 4E800020  blr
	return;
}

pub fn sub_832B6A50(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6A50 size=80
	// 832B6A50: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6A54: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6A58: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B6A5C: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6A60: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6A64: 3D60834A  lis r11, -0x7cb6
	ctx.r[11].s64 = -2092302336;
	// 832B6A68: 3BC00007  li r30, 7
	ctx.r[30].s64 = 7;
	// 832B6A6C: 396B60C8  addi r11, r11, 0x60c8
	ctx.r[11].s64 = ctx.r[11].s64 + 24776;
	// 832B6A70: 3BEB0260  addi r31, r11, 0x260
	ctx.r[31].s64 = ctx.r[11].s64 + 608;
	// 832B6A74: 3BFFFFB4  addi r31, r31, -0x4c
	ctx.r[31].s64 = ctx.r[31].s64 + -76;
	// 832B6A78: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B6A7C: 4B8B7F15  bl 0x82b6e990
	ctx.lr = 0x832B6A80;
	crate::recompiler::externs::call(&mut ctx, base, 0x82B6E990);
	// 832B6A80: 37DEFFFF  addic. r30, r30, -1
	ctx.xer.ca = (ctx.r[30].u32 > (!(-1 as u32)));
	ctx.r[30].s64 = ctx.r[30].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832B6A84: 4080FFF0  bge 0x832b6a74
	if !ctx.cr[0].lt {
	pc = 0x832B6A74; continue 'dispatch;
	}
	// 832B6A88: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B6A8C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6A90: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6A94: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B6A98: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6A9C: 4E800020  blr
	return;
}

pub fn sub_832B6AA0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6AA0 size=96
	// 832B6AA0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6AA4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6AA8: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B6AAC: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6AB0: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6AB4: 3FE08332  lis r31, -0x7cce
	ctx.r[31].s64 = -2093875200;
	// 832B6AB8: 3D60820F  lis r11, -0x7df1
	ctx.r[11].s64 = -2112946176;
	// 832B6ABC: 3BDFB138  addi r30, r31, -0x4ec8
	ctx.r[30].s64 = ctx.r[31].s64 + -20168;
	// 832B6AC0: 396B2A30  addi r11, r11, 0x2a30
	ctx.r[11].s64 = ctx.r[11].s64 + 10800;
	// 832B6AC4: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832B6AC8: 917FB138  stw r11, -0x4ec8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-20168 as u32), ctx.r[11].u32 ) };
	// 832B6ACC: 4AF4553D  bl 0x821fc008
	ctx.lr = 0x832B6AD0;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC008);
	// 832B6AD0: 3D60820F  lis r11, -0x7df1
	ctx.r[11].s64 = -2112946176;
	// 832B6AD4: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832B6AD8: 396B2A40  addi r11, r11, 0x2a40
	ctx.r[11].s64 = ctx.r[11].s64 + 10816;
	// 832B6ADC: 917FB138  stw r11, -0x4ec8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-20168 as u32), ctx.r[11].u32 ) };
	// 832B6AE0: 4AF45529  bl 0x821fc008
	ctx.lr = 0x832B6AE4;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC008);
	// 832B6AE4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B6AE8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6AEC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6AF0: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B6AF4: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6AF8: 4E800020  blr
	return;
}

pub fn sub_832B6B00(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6B00 size=88
	// 832B6B00: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6B04: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6B08: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6B0C: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6B10: 3D608332  lis r11, -0x7cce
	ctx.r[11].s64 = -2093875200;
	// 832B6B14: 3D40820F  lis r10, -0x7df1
	ctx.r[10].s64 = -2112946176;
	// 832B6B18: 3BEBB140  addi r31, r11, -0x4ec0
	ctx.r[31].s64 = ctx.r[11].s64 + -20160;
	// 832B6B1C: 396A2A2C  addi r11, r10, 0x2a2c
	ctx.r[11].s64 = ctx.r[10].s64 + 10796;
	// 832B6B20: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B6B24: 917F0000  stw r11, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832B6B28: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B6B2C: 419A0018  beq cr6, 0x832b6b44
	if ctx.cr[6].eq {
	pc = 0x832B6B44; continue 'dispatch;
	}
	// 832B6B30: 4AF457F1  bl 0x821fc320
	ctx.lr = 0x832B6B34;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC320);
	// 832B6B34: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B6B38: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B6B3C: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B6B40: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B6B44: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B6B48: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6B4C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6B50: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6B54: 4E800020  blr
	return;
}

pub fn sub_832B6B58(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6B58 size=224
	// 832B6B58: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6B5C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6B60: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6B64: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6B68: 3D608332  lis r11, -0x7cce
	ctx.r[11].s64 = -2093875200;
	// 832B6B6C: 3D40820F  lis r10, -0x7df1
	ctx.r[10].s64 = -2112946176;
	// 832B6B70: 3BEBB14C  addi r31, r11, -0x4eb4
	ctx.r[31].s64 = ctx.r[11].s64 + -20148;
	// 832B6B74: 396A2A2C  addi r11, r10, 0x2a2c
	ctx.r[11].s64 = ctx.r[10].s64 + 10796;
	// 832B6B78: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B6B7C: 917F0000  stw r11, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832B6B80: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B6B84: 419A0018  beq cr6, 0x832b6b9c
	if ctx.cr[6].eq {
	pc = 0x832B6B9C; continue 'dispatch;
	}
	// 832B6B88: 4AF45799  bl 0x821fc320
	ctx.lr = 0x832B6B8C;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC320);
	// 832B6B8C: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B6B90: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B6B94: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B6B98: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B6B9C: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B6BA0: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6BA4: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6BA8: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6BAC: 4E800020  blr
	return;
	// 832B6BB0: 3D60834A  lis r11, -0x7cb6
	ctx.r[11].s64 = -2092302336;
	// 832B6BB4: 386B6328  addi r3, r11, 0x6328
	ctx.r[3].s64 = ctx.r[11].s64 + 25384;
	// 832B6BB8: 4AF7B100  b 0x82231cb8
	crate::recompiler::externs::call(&mut ctx, base, 0x82231CB8);
	return;
}

pub fn sub_832B6C38(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6C38 size=104
	// 832B6C38: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6C3C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6C40: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6C44: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6C48: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B6C4C: 3BEBDED4  addi r31, r11, -0x212c
	ctx.r[31].s64 = ctx.r[11].s64 + -8492;
	// 832B6C50: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B6C54: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B6C58: 419A0008  beq cr6, 0x832b6c60
	if ctx.cr[6].eq {
	pc = 0x832B6C60; continue 'dispatch;
	}
	// 832B6C5C: 4AF650DD  bl 0x8221bd38
	ctx.lr = 0x832B6C60;
	crate::recompiler::externs::call(&mut ctx, base, 0x8221BD38);
	// 832B6C60: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B6C64: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B6C68: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B6C6C: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B6C70: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B6C74: 913F000C  stw r9, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[9].u32 ) };
	// 832B6C78: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B6C7C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6C80: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6C84: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6C88: 4E800020  blr
	return;
}

pub fn sub_832B6CA0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6CA0 size=152
	// 832B6CA0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6CA4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6CA8: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6CAC: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6CB0: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B6CB4: 3BEBDEE8  addi r31, r11, -0x2118
	ctx.r[31].s64 = ctx.r[11].s64 + -8472;
	// 832B6CB8: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B6CBC: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B6CC0: 419A0008  beq cr6, 0x832b6cc8
	if ctx.cr[6].eq {
	pc = 0x832B6CC8; continue 'dispatch;
	}
	// 832B6CC4: 4AF65075  bl 0x8221bd38
	ctx.lr = 0x832B6CC8;
	crate::recompiler::externs::call(&mut ctx, base, 0x8221BD38);
	// 832B6CC8: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B6CCC: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B6CD0: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B6CD4: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B6CD8: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B6CDC: 913F000C  stw r9, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[9].u32 ) };
	// 832B6CE0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B6CE4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6CE8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6CEC: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6CF0: 4E800020  blr
	return;
}

pub fn sub_832B6D38(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6D38 size=88
	// 832B6D38: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6D3C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6D40: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6D44: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6D48: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B6D4C: 3BEBDEF8  addi r31, r11, -0x2108
	ctx.r[31].s64 = ctx.r[11].s64 + -8456;
	// 832B6D50: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B6D54: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B6D58: 419A0008  beq cr6, 0x832b6d60
	if ctx.cr[6].eq {
	pc = 0x832B6D60; continue 'dispatch;
	}
	// 832B6D5C: 4AF64FDD  bl 0x8221bd38
	ctx.lr = 0x832B6D60;
	crate::recompiler::externs::call(&mut ctx, base, 0x8221BD38);
	// 832B6D60: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B6D64: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B6D68: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B6D6C: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B6D70: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B6D74: 913F000C  stw r9, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[9].u32 ) };
	// 832B6D78: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B6D7C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6D80: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6D84: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6D88: 4E800020  blr
	return;
}

pub fn sub_832B6D90(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6D90 size=88
	// 832B6D90: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6D94: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6D98: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6D9C: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6DA0: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B6DA4: 3BEBDF08  addi r31, r11, -0x20f8
	ctx.r[31].s64 = ctx.r[11].s64 + -8440;
	// 832B6DA8: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B6DAC: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B6DB0: 419A0008  beq cr6, 0x832b6db8
	if ctx.cr[6].eq {
	pc = 0x832B6DB8; continue 'dispatch;
	}
	// 832B6DB4: 4AF64F85  bl 0x8221bd38
	ctx.lr = 0x832B6DB8;
	crate::recompiler::externs::call(&mut ctx, base, 0x8221BD38);
	// 832B6DB8: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B6DBC: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B6DC0: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B6DC4: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B6DC8: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B6DCC: 913F000C  stw r9, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[9].u32 ) };
	// 832B6DD0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B6DD4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6DD8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6DDC: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6DE0: 4E800020  blr
	return;
}

pub fn sub_832B6DE8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6DE8 size=88
	// 832B6DE8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6DEC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6DF0: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6DF4: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6DF8: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B6DFC: 3BEBDF18  addi r31, r11, -0x20e8
	ctx.r[31].s64 = ctx.r[11].s64 + -8424;
	// 832B6E00: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B6E04: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B6E08: 419A0008  beq cr6, 0x832b6e10
	if ctx.cr[6].eq {
	pc = 0x832B6E10; continue 'dispatch;
	}
	// 832B6E0C: 4AF64F2D  bl 0x8221bd38
	ctx.lr = 0x832B6E10;
	crate::recompiler::externs::call(&mut ctx, base, 0x8221BD38);
	// 832B6E10: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B6E14: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B6E18: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B6E1C: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B6E20: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B6E24: 913F000C  stw r9, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[9].u32 ) };
	// 832B6E28: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B6E2C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6E30: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6E34: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6E38: 4E800020  blr
	return;
}

pub fn sub_832B6E40(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6E40 size=104
	// 832B6E40: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6E44: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6E48: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6E4C: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6E50: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B6E54: 3BEBDF28  addi r31, r11, -0x20d8
	ctx.r[31].s64 = ctx.r[11].s64 + -8408;
	// 832B6E58: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B6E5C: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B6E60: 419A0008  beq cr6, 0x832b6e68
	if ctx.cr[6].eq {
	pc = 0x832B6E68; continue 'dispatch;
	}
	// 832B6E64: 4AF64ED5  bl 0x8221bd38
	ctx.lr = 0x832B6E68;
	crate::recompiler::externs::call(&mut ctx, base, 0x8221BD38);
	// 832B6E68: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B6E6C: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B6E70: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B6E74: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B6E78: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B6E7C: 913F000C  stw r9, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[9].u32 ) };
	// 832B6E80: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B6E84: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6E88: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6E8C: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6E90: 4E800020  blr
	return;
}

pub fn sub_832B6EA8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6EA8 size=104
	// 832B6EA8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6EAC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B6EB0: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B6EB4: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B6EB8: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6EBC: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B6EC0: 3BC00009  li r30, 9
	ctx.r[30].s64 = 9;
	// 832B6EC4: 396BDF40  addi r11, r11, -0x20c0
	ctx.r[11].s64 = ctx.r[11].s64 + -8384;
	// 832B6EC8: 3BEB00C8  addi r31, r11, 0xc8
	ctx.r[31].s64 = ctx.r[11].s64 + 200;
	// 832B6ECC: 3BFFFFEC  addi r31, r31, -0x14
	ctx.r[31].s64 = ctx.r[31].s64 + -20;
	// 832B6ED0: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B6ED4: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B6ED8: 419A0008  beq cr6, 0x832b6ee0
	if ctx.cr[6].eq {
	pc = 0x832B6EE0; continue 'dispatch;
	}
	// 832B6EDC: 4AF45445  bl 0x821fc320
	ctx.lr = 0x832B6EE0;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FC320);
	// 832B6EE0: 37DEFFFF  addic. r30, r30, -1
	ctx.xer.ca = (ctx.r[30].u32 > (!(-1 as u32)));
	ctx.r[30].s64 = ctx.r[30].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832B6EE4: 4080FFE8  bge 0x832b6ecc
	if !ctx.cr[0].lt {
	pc = 0x832B6ECC; continue 'dispatch;
	}
	// 832B6EE8: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B6EEC: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B6EF0: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B6EF4: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B6EF8: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B6EFC: 4E800020  blr
	return;
	// 832B6F00: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B6F04: 386BE008  addi r3, r11, -0x1ff8
	ctx.r[3].s64 = ctx.r[11].s64 + -8184;
	// 832B6F08: 4AF5DED0  b 0x82214dd8
	crate::recompiler::externs::call(&mut ctx, base, 0x82214DD8);
	return;
}

pub fn sub_832B6F10(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B6F10 size=536
	// 832B6F10: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B6F14: 4B9F24F5  bl 0x82ca9408
	ctx.lr = 0x832B6F18;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9408);
	// 832B6F18: 9421FF80  stwu r1, -0x80(r1)
	ea = ctx.r[1].u32.wrapping_add(-128 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B6F1C: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B6F20: 3BC00009  li r30, 9
	ctx.r[30].s64 = 9;
	// 832B6F24: 396BE010  addi r11, r11, -0x1ff0
	ctx.r[11].s64 = ctx.r[11].s64 + -8176;
	// 832B6F28: 3B800000  li r28, 0
	ctx.r[28].s64 = 0;
	// 832B6F2C: 3BEB0438  addi r31, r11, 0x438
	ctx.r[31].s64 = ctx.r[11].s64 + 1080;
	// 832B6F30: 3D608349  lis r11, -0x7cb7
	ctx.r[11].s64 = -2092367872;
	// 832B6F34: 3BAB7088  addi r29, r11, 0x7088
	ctx.r[29].s64 = ctx.r[11].s64 + 28808;
	// 832B6F38: 3BFFFF94  addi r31, r31, -0x6c
	ctx.r[31].s64 = ctx.r[31].s64 + -108;
	// 832B6F3C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B6F40: 4AF0F829  bl 0x821c6768
	ctx.lr = 0x832B6F44;
	crate::recompiler::externs::call(&mut ctx, base, 0x821C6768);
	// 832B6F44: 7FA9EB78  mr r9, r29
	ctx.r[9].u64 = ctx.r[29].u64;
	// 832B6F48: 7D4000A6  mfmsr r10
	ctx.r[10].u64 = ctx.msr;
	// 832B6F4C: 7DA10164  mtmsrd r13, 1
	ctx.msr = (ctx.r[13].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832B6F50: 7D604828  lwarx r11, 0, r9
	// lwarx
	let ea = ctx.r[9].u32;
	ctx.reserved.u32 = unsafe { crate::rt::load_u32(base as *const u8, ea) };
	ctx.r[11].u64 = ctx.reserved.u32 as u64;
	// 832B6F54: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832B6F58: 7D60492D  stwcx. r11, 0, r9
	// stwcx.
	let addr = ctx.r[9].u32;
	ctx.cr[0].lt = false;
	ctx.cr[0].gt = false;
	let ok = unsafe { crate::rt::stwcx32(base as *mut u8, addr, ctx.reserved.u32, ctx.r[11].u32) };
	ctx.cr[0].eq = ok;
	ctx.cr[0].so = ctx.xer.so;
	// 832B6F5C: 7D410164  mtmsrd r10, 1
	ctx.msr = (ctx.r[10].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832B6F60: 4082FFE8  bne 0x832b6f48
	if !ctx.cr[0].eq {
	pc = 0x832B6F48; continue 'dispatch;
	}
	// 832B6F64: 37DEFFFF  addic. r30, r30, -1
	ctx.xer.ca = (ctx.r[30].u32 > (!(-1 as u32)));
	ctx.r[30].s64 = ctx.r[30].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832B6F68: 939F0000  stw r28, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[28].u32 ) };
	// 832B6F6C: 4080FFCC  bge 0x832b6f38
	if !ctx.cr[0].lt {
	pc = 0x832B6F38; continue 'dispatch;
	}
	// 832B6F70: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832B6F74: 4B9F24E4  b 0x82ca9458
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9458);
	return;
	// 832B6F78: 3D608332  lis r11, -0x7cce
	ctx.r[11].s64 = -2093875200;
	// 832B6F7C: 3D408210  lis r10, -0x7df0
	ctx.r[10].s64 = -2112880640;
	// 832B6F80: 396BB1AC  addi r11, r11, -0x4e54
	ctx.r[11].s64 = ctx.r[11].s64 + -20052;
	// 832B6F84: 392A9BF8  addi r9, r10, -0x6408
	ctx.r[9].s64 = ctx.r[10].s64 + -25608;
	// 832B6F88: 814B0004  lwz r10, 4(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B6F8C: 912B0000  stw r9, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[9].u32 ) };
	// 832B6F90: 2F0A0001  cmpwi cr6, r10, 1
	ctx.cr[6].compare_i32(ctx.r[10].s32, 1, &mut ctx.xer);
	// 832B6F94: 419A0014  beq cr6, 0x832b6fa8
	if ctx.cr[6].eq {
	pc = 0x832B6FA8; continue 'dispatch;
	}
	// 832B6F98: 2F0A0002  cmpwi cr6, r10, 2
	ctx.cr[6].compare_i32(ctx.r[10].s32, 2, &mut ctx.xer);
	// 832B6F9C: 4C9A0020  bnelr cr6
	if !ctx.cr[6].eq { return; }
	// 832B6FA0: 386B0008  addi r3, r11, 8
	ctx.r[3].s64 = ctx.r[11].s64 + 8;
	// 832B6FA4: 4B8A3964  b 0x82b5a908
	crate::recompiler::externs::call(&mut ctx, base, 0x82B5A908);
	return;
	// 832B6FA8: 386B0008  addi r3, r11, 8
	ctx.r[3].s64 = ctx.r[11].s64 + 8;
	// 832B6FAC: 4B8A0D34  b 0x82b57ce0
	crate::recompiler::externs::call(&mut ctx, base, 0x82B57CE0);
	return;
	// 832B6FB0: 4E800020  blr
	return;
}

pub fn sub_832B7128(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7128 size=136
	// 832B7128: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B712C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7130: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7134: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7138: 3D60834C  lis r11, -0x7cb4
	ctx.r[11].s64 = -2092171264;
	// 832B713C: 3BEBE8D8  addi r31, r11, -0x1728
	ctx.r[31].s64 = ctx.r[11].s64 + -5928;
	// 832B7140: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7144: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B7148: 419A0008  beq cr6, 0x832b7150
	if ctx.cr[6].eq {
	pc = 0x832B7150; continue 'dispatch;
	}
	// 832B714C: 4AF64BED  bl 0x8221bd38
	ctx.lr = 0x832B7150;
	crate::recompiler::externs::call(&mut ctx, base, 0x8221BD38);
	// 832B7150: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7154: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B7158: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B715C: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B7160: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B7164: 913F000C  stw r9, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[9].u32 ) };
	// 832B7168: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B716C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7170: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7174: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7178: 4E800020  blr
	return;
}

pub fn sub_832B71B0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B71B0 size=120
	// 832B71B0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B71B4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B71B8: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B71BC: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B71C0: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B71C4: 3BEB49E4  addi r31, r11, 0x49e4
	ctx.r[31].s64 = ctx.r[11].s64 + 18916;
	// 832B71C8: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B71CC: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B71D0: 419A0008  beq cr6, 0x832b71d8
	if ctx.cr[6].eq {
	pc = 0x832B71D8; continue 'dispatch;
	}
	// 832B71D4: 4B9414AD  bl 0x82bf8680
	ctx.lr = 0x832B71D8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82BF8680);
	// 832B71D8: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B71DC: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B71E0: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B71E4: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B71E8: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B71EC: 913F000C  stw r9, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[9].u32 ) };
	// 832B71F0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B71F4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B71F8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B71FC: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7200: 4E800020  blr
	return;
}

pub fn sub_832B7228(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7228 size=88
	// 832B7228: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B722C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7230: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7234: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7238: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B723C: 3BEB4A0C  addi r31, r11, 0x4a0c
	ctx.r[31].s64 = ctx.r[11].s64 + 18956;
	// 832B7240: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7244: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B7248: 419A0008  beq cr6, 0x832b7250
	if ctx.cr[6].eq {
	pc = 0x832B7250; continue 'dispatch;
	}
	// 832B724C: 4B941435  bl 0x82bf8680
	ctx.lr = 0x832B7250;
	crate::recompiler::externs::call(&mut ctx, base, 0x82BF8680);
	// 832B7250: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7254: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B7258: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B725C: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B7260: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B7264: 913F000C  stw r9, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[9].u32 ) };
	// 832B7268: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B726C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7270: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7274: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7278: 4E800020  blr
	return;
}

pub fn sub_832B7280(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7280 size=200
	// 832B7280: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B7284: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7288: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B728C: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7290: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B7294: 38610058  addi r3, r1, 0x58
	ctx.r[3].s64 = ctx.r[1].s64 + 88;
	// 832B7298: 3BEB4B94  addi r31, r11, 0x4b94
	ctx.r[31].s64 = ctx.r[11].s64 + 19348;
	// 832B729C: 93E10058  stw r31, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[31].u32 ) };
	// 832B72A0: 7FE4FB78  mr r4, r31
	ctx.r[4].u64 = ctx.r[31].u64;
	// 832B72A4: 93E10050  stw r31, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[31].u32 ) };
	// 832B72A8: 817F0004  lwz r11, 4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B72AC: 814B0000  lwz r10, 0(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B72B0: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832B72B4: E8C10050  ld r6, 0x50(r1)
	ctx.r[6].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832B72B8: 9141005C  stw r10, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832B72BC: E8A10058  ld r5, 0x58(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) };
	// 832B72C0: 4B9CD6A1  bl 0x82c84960
	ctx.lr = 0x832B72C4;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C84960);
	// 832B72C4: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B72C8: 4B58E4E9  bl 0x828457b0
	ctx.lr = 0x832B72CC;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B72CC: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B72D0: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B72D4: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B72D8: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B72DC: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B72E0: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B72E4: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B72E8: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B72EC: 4E800020  blr
	return;
	// 832B72F0: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B72F4: 386B4BAC  addi r3, r11, 0x4bac
	ctx.r[3].s64 = ctx.r[11].s64 + 19372;
	// 832B72F8: 4AF5DAE0  b 0x82214dd8
	crate::recompiler::externs::call(&mut ctx, base, 0x82214DD8);
	return;
}

pub fn sub_832B7348(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7348 size=128
	// 832B7348: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B734C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7350: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7354: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7358: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B735C: 38610058  addi r3, r1, 0x58
	ctx.r[3].s64 = ctx.r[1].s64 + 88;
	// 832B7360: 3BEB4BB0  addi r31, r11, 0x4bb0
	ctx.r[31].s64 = ctx.r[11].s64 + 19376;
	// 832B7364: 93E10058  stw r31, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[31].u32 ) };
	// 832B7368: 7FE4FB78  mr r4, r31
	ctx.r[4].u64 = ctx.r[31].u64;
	// 832B736C: 93E10050  stw r31, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[31].u32 ) };
	// 832B7370: 817F0004  lwz r11, 4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7374: 814B0000  lwz r10, 0(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B7378: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832B737C: E8C10050  ld r6, 0x50(r1)
	ctx.r[6].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832B7380: 9141005C  stw r10, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832B7384: E8A10058  ld r5, 0x58(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) };
	// 832B7388: 4B06AB39  bl 0x82321ec0
	ctx.lr = 0x832B738C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82321EC0);
	// 832B738C: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7390: 4B58E421  bl 0x828457b0
	ctx.lr = 0x832B7394;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B7394: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7398: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B739C: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B73A0: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B73A4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B73A8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B73AC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B73B0: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B73B4: 4E800020  blr
	return;
	// 832B73B8: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B73BC: 386B4BD8  addi r3, r11, 0x4bd8
	ctx.r[3].s64 = ctx.r[11].s64 + 19416;
	// 832B73C0: 4B952850  b 0x82c09c10
	crate::recompiler::externs::call(&mut ctx, base, 0x82C09C10);
	return;
}

pub fn sub_832B73C8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B73C8 size=16
	// 832B73C8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B73CC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B73D0: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B73D4: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
}

pub fn sub_832B7428(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7428 size=112
	// 832B7428: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B742C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7430: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7434: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7438: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B743C: 38610058  addi r3, r1, 0x58
	ctx.r[3].s64 = ctx.r[1].s64 + 88;
	// 832B7440: 3BEB4CBC  addi r31, r11, 0x4cbc
	ctx.r[31].s64 = ctx.r[11].s64 + 19644;
	// 832B7444: 93E10058  stw r31, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[31].u32 ) };
	// 832B7448: 7FE4FB78  mr r4, r31
	ctx.r[4].u64 = ctx.r[31].u64;
	// 832B744C: 93E10050  stw r31, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[31].u32 ) };
	// 832B7450: 817F0004  lwz r11, 4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7454: 814B0000  lwz r10, 0(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B7458: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832B745C: E8C10050  ld r6, 0x50(r1)
	ctx.r[6].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832B7460: 9141005C  stw r10, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832B7464: E8A10058  ld r5, 0x58(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) };
	// 832B7468: 4B0BE991  bl 0x82375df8
	ctx.lr = 0x832B746C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82375DF8);
	// 832B746C: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7470: 4B58E341  bl 0x828457b0
	ctx.lr = 0x832B7474;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B7474: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7478: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B747C: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B7480: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B7484: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B7488: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B748C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7490: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7494: 4E800020  blr
	return;
}

pub fn sub_832B7498(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7498 size=112
	// 832B7498: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B749C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B74A0: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B74A4: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B74A8: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B74AC: 38610058  addi r3, r1, 0x58
	ctx.r[3].s64 = ctx.r[1].s64 + 88;
	// 832B74B0: 3BEB4CC8  addi r31, r11, 0x4cc8
	ctx.r[31].s64 = ctx.r[11].s64 + 19656;
	// 832B74B4: 93E10058  stw r31, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[31].u32 ) };
	// 832B74B8: 7FE4FB78  mr r4, r31
	ctx.r[4].u64 = ctx.r[31].u64;
	// 832B74BC: 93E10050  stw r31, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[31].u32 ) };
	// 832B74C0: 817F0004  lwz r11, 4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B74C4: 814B0000  lwz r10, 0(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B74C8: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832B74CC: E8C10050  ld r6, 0x50(r1)
	ctx.r[6].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832B74D0: 9141005C  stw r10, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832B74D4: E8A10058  ld r5, 0x58(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) };
	// 832B74D8: 4B9553F9  bl 0x82c0c8d0
	ctx.lr = 0x832B74DC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C0C8D0);
	// 832B74DC: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B74E0: 4B58E2D1  bl 0x828457b0
	ctx.lr = 0x832B74E4;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B74E4: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B74E8: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B74EC: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B74F0: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B74F4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B74F8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B74FC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7500: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7504: 4E800020  blr
	return;
}

pub fn sub_832B7508(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7508 size=112
	// 832B7508: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B750C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7510: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7514: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7518: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B751C: 38610058  addi r3, r1, 0x58
	ctx.r[3].s64 = ctx.r[1].s64 + 88;
	// 832B7520: 3BEB4CD4  addi r31, r11, 0x4cd4
	ctx.r[31].s64 = ctx.r[11].s64 + 19668;
	// 832B7524: 93E10058  stw r31, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[31].u32 ) };
	// 832B7528: 7FE4FB78  mr r4, r31
	ctx.r[4].u64 = ctx.r[31].u64;
	// 832B752C: 93E10050  stw r31, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[31].u32 ) };
	// 832B7530: 817F0004  lwz r11, 4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7534: 814B0000  lwz r10, 0(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B7538: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832B753C: E8C10050  ld r6, 0x50(r1)
	ctx.r[6].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832B7540: 9141005C  stw r10, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832B7544: E8A10058  ld r5, 0x58(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) };
	// 832B7548: 4B955471  bl 0x82c0c9b8
	ctx.lr = 0x832B754C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C0C9B8);
	// 832B754C: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7550: 4B58E261  bl 0x828457b0
	ctx.lr = 0x832B7554;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B7554: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7558: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B755C: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B7560: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B7564: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B7568: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B756C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7570: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7574: 4E800020  blr
	return;
}

pub fn sub_832B7578(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7578 size=112
	// 832B7578: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B757C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7580: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7584: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7588: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B758C: 38610058  addi r3, r1, 0x58
	ctx.r[3].s64 = ctx.r[1].s64 + 88;
	// 832B7590: 3BEB4CE0  addi r31, r11, 0x4ce0
	ctx.r[31].s64 = ctx.r[11].s64 + 19680;
	// 832B7594: 93E10058  stw r31, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[31].u32 ) };
	// 832B7598: 7FE4FB78  mr r4, r31
	ctx.r[4].u64 = ctx.r[31].u64;
	// 832B759C: 93E10050  stw r31, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[31].u32 ) };
	// 832B75A0: 817F0004  lwz r11, 4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B75A4: 814B0000  lwz r10, 0(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B75A8: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832B75AC: E8C10050  ld r6, 0x50(r1)
	ctx.r[6].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832B75B0: 9141005C  stw r10, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832B75B4: E8A10058  ld r5, 0x58(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) };
	// 832B75B8: 4B9554E9  bl 0x82c0caa0
	ctx.lr = 0x832B75BC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C0CAA0);
	// 832B75BC: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B75C0: 4B58E1F1  bl 0x828457b0
	ctx.lr = 0x832B75C4;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B75C4: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B75C8: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B75CC: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B75D0: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B75D4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B75D8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B75DC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B75E0: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B75E4: 4E800020  blr
	return;
}

pub fn sub_832B75E8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B75E8 size=80
	// 832B75E8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B75EC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B75F0: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B75F4: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B75F8: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B75FC: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B7600: 3BC00004  li r30, 4
	ctx.r[30].s64 = 4;
	// 832B7604: 396B4D30  addi r11, r11, 0x4d30
	ctx.r[11].s64 = ctx.r[11].s64 + 19760;
	// 832B7608: 3BEB008C  addi r31, r11, 0x8c
	ctx.r[31].s64 = ctx.r[11].s64 + 140;
	// 832B760C: 3BFFFFE4  addi r31, r31, -0x1c
	ctx.r[31].s64 = ctx.r[31].s64 + -28;
	// 832B7610: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B7614: 4B8DF865  bl 0x82b96e78
	ctx.lr = 0x832B7618;
	crate::recompiler::externs::call(&mut ctx, base, 0x82B96E78);
	// 832B7618: 37DEFFFF  addic. r30, r30, -1
	ctx.xer.ca = (ctx.r[30].u32 > (!(-1 as u32)));
	ctx.r[30].s64 = ctx.r[30].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832B761C: 4080FFF0  bge 0x832b760c
	if !ctx.cr[0].lt {
	pc = 0x832B760C; continue 'dispatch;
	}
	// 832B7620: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B7624: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7628: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B762C: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B7630: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7634: 4E800020  blr
	return;
}

pub fn sub_832B7638(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7638 size=80
	// 832B7638: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B763C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7640: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B7644: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7648: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B764C: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B7650: 3BC00004  li r30, 4
	ctx.r[30].s64 = 4;
	// 832B7654: 396B4DBC  addi r11, r11, 0x4dbc
	ctx.r[11].s64 = ctx.r[11].s64 + 19900;
	// 832B7658: 3BEB00B4  addi r31, r11, 0xb4
	ctx.r[31].s64 = ctx.r[11].s64 + 180;
	// 832B765C: 3BFFFFDC  addi r31, r31, -0x24
	ctx.r[31].s64 = ctx.r[31].s64 + -36;
	// 832B7660: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B7664: 4B886375  bl 0x82b3d9d8
	ctx.lr = 0x832B7668;
	crate::recompiler::externs::call(&mut ctx, base, 0x82B3D9D8);
	// 832B7668: 37DEFFFF  addic. r30, r30, -1
	ctx.xer.ca = (ctx.r[30].u32 > (!(-1 as u32)));
	ctx.r[30].s64 = ctx.r[30].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832B766C: 4080FFF0  bge 0x832b765c
	if !ctx.cr[0].lt {
	pc = 0x832B765C; continue 'dispatch;
	}
	// 832B7670: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B7674: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7678: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B767C: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B7680: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7684: 4E800020  blr
	return;
}

pub fn sub_832B7688(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7688 size=1040
	// 832B7688: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B768C: 4B9F1D7D  bl 0x82ca9408
	ctx.lr = 0x832B7690;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9408);
	// 832B7690: 9421FF80  stwu r1, -0x80(r1)
	ea = ctx.r[1].u32.wrapping_add(-128 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7694: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B7698: 3BA00004  li r29, 4
	ctx.r[29].s64 = 4;
	// 832B769C: 396B4CF4  addi r11, r11, 0x4cf4
	ctx.r[11].s64 = ctx.r[11].s64 + 19700;
	// 832B76A0: 3B800000  li r28, 0
	ctx.r[28].s64 = 0;
	// 832B76A4: 3BEB0040  addi r31, r11, 0x40
	ctx.r[31].s64 = ctx.r[11].s64 + 64;
	// 832B76A8: 3BFFFFF4  addi r31, r31, -0xc
	ctx.r[31].s64 = ctx.r[31].s64 + -12;
	// 832B76AC: 817F0000  lwz r11, 0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B76B0: 806B0000  lwz r3, 0(r11)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B76B4: 916B0000  stw r11, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832B76B8: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B76BC: 914A0004  stw r10, 4(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), ctx.r[10].u32 ) };
	// 832B76C0: 813F0000  lwz r9, 0(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B76C4: 7F034840  cmplw cr6, r3, r9
	ctx.cr[6].compare_u32(ctx.r[3].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832B76C8: 939F0004  stw r28, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[28].u32 ) };
	// 832B76CC: 419A001C  beq cr6, 0x832b76e8
	if ctx.cr[6].eq {
	pc = 0x832B76E8; continue 'dispatch;
	}
	// 832B76D0: 83C30000  lwz r30, 0(r3)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B76D4: 4B58E0DD  bl 0x828457b0
	ctx.lr = 0x832B76D8;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B76D8: 817F0000  lwz r11, 0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B76DC: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832B76E0: 7F1E5840  cmplw cr6, r30, r11
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832B76E4: 409AFFEC  bne cr6, 0x832b76d0
	if !ctx.cr[6].eq {
	pc = 0x832B76D0; continue 'dispatch;
	}
	// 832B76E8: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B76EC: 4B58E0C5  bl 0x828457b0
	ctx.lr = 0x832B76F0;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B76F0: 37BDFFFF  addic. r29, r29, -1
	ctx.xer.ca = (ctx.r[29].u32 > (!(-1 as u32)));
	ctx.r[29].s64 = ctx.r[29].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[29].s32, 0, &mut ctx.xer);
	// 832B76F4: 939F0000  stw r28, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[28].u32 ) };
	// 832B76F8: 4080FFB0  bge 0x832b76a8
	if !ctx.cr[0].lt {
	pc = 0x832B76A8; continue 'dispatch;
	}
	// 832B76FC: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832B7700: 4B9F1D58  b 0x82ca9458
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9458);
	return;
}

pub fn sub_832B7A98(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7A98 size=88
	// 832B7A98: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B7A9C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7AA0: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7AA4: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7AA8: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B7AAC: 3BEB5010  addi r31, r11, 0x5010
	ctx.r[31].s64 = ctx.r[11].s64 + 20496;
	// 832B7AB0: 387F0038  addi r3, r31, 0x38
	ctx.r[3].s64 = ctx.r[31].s64 + 56;
	// 832B7AB4: 4B89152D  bl 0x82b48fe0
	ctx.lr = 0x832B7AB8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82B48FE0);
	// 832B7AB8: 387F0010  addi r3, r31, 0x10
	ctx.r[3].s64 = ctx.r[31].s64 + 16;
	// 832B7ABC: 4B891525  bl 0x82b48fe0
	ctx.lr = 0x832B7AC0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82B48FE0);
	// 832B7AC0: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B7AC4: 4B952C05  bl 0x82c0a6c8
	ctx.lr = 0x832B7AC8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C0A6C8);
	// 832B7AC8: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7ACC: 4B58DCE5  bl 0x828457b0
	ctx.lr = 0x832B7AD0;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B7AD0: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7AD4: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B7AD8: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B7ADC: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7AE0: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7AE4: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7AE8: 4E800020  blr
	return;
}

pub fn sub_832B7AF0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7AF0 size=104
	// 832B7AF0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B7AF4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7AF8: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7AFC: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7B00: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B7B04: 3BEB5088  addi r31, r11, 0x5088
	ctx.r[31].s64 = ctx.r[11].s64 + 20616;
	// 832B7B08: 387F0038  addi r3, r31, 0x38
	ctx.r[3].s64 = ctx.r[31].s64 + 56;
	// 832B7B0C: 4B8914D5  bl 0x82b48fe0
	ctx.lr = 0x832B7B10;
	crate::recompiler::externs::call(&mut ctx, base, 0x82B48FE0);
	// 832B7B10: 387F0010  addi r3, r31, 0x10
	ctx.r[3].s64 = ctx.r[31].s64 + 16;
	// 832B7B14: 4B8914CD  bl 0x82b48fe0
	ctx.lr = 0x832B7B18;
	crate::recompiler::externs::call(&mut ctx, base, 0x82B48FE0);
	// 832B7B18: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B7B1C: 4B952BAD  bl 0x82c0a6c8
	ctx.lr = 0x832B7B20;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C0A6C8);
	// 832B7B20: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7B24: 4B58DC8D  bl 0x828457b0
	ctx.lr = 0x832B7B28;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B7B28: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7B2C: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B7B30: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B7B34: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7B38: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7B3C: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7B40: 4E800020  blr
	return;
}

pub fn sub_832B7B58(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7B58 size=392
	// 832B7B58: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B7B5C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7B60: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7B64: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7B68: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B7B6C: 38610058  addi r3, r1, 0x58
	ctx.r[3].s64 = ctx.r[1].s64 + 88;
	// 832B7B70: 3BEB5184  addi r31, r11, 0x5184
	ctx.r[31].s64 = ctx.r[11].s64 + 20868;
	// 832B7B74: 93E10058  stw r31, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[31].u32 ) };
	// 832B7B78: 7FE4FB78  mr r4, r31
	ctx.r[4].u64 = ctx.r[31].u64;
	// 832B7B7C: 93E10050  stw r31, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[31].u32 ) };
	// 832B7B80: 817F0004  lwz r11, 4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7B84: 814B0000  lwz r10, 0(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B7B88: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832B7B8C: E8C10050  ld r6, 0x50(r1)
	ctx.r[6].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832B7B90: 9141005C  stw r10, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832B7B94: E8A10058  ld r5, 0x58(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) };
	// 832B7B98: 4B973E99  bl 0x82c2ba30
	ctx.lr = 0x832B7B9C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C2BA30);
	// 832B7B9C: 807F0004  lwz r3, 4(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832B7BA0: 4B58DC11  bl 0x828457b0
	ctx.lr = 0x832B7BA4;
	crate::recompiler::externs::call(&mut ctx, base, 0x828457B0);
	// 832B7BA4: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7BA8: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B7BAC: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832B7BB0: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832B7BB4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B7BB8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7BBC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7BC0: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7BC4: 4E800020  blr
	return;
	// 832B7BC8: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B7BCC: 806B5974  lwz r3, 0x5974(r11)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(22900 as u32) ) } as u64;
	// 832B7BD0: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B7BD4: 4D9A0020  beqlr cr6
	if ctx.cr[6].eq { return; }
	// 832B7BD8: 81630000  lwz r11, 0(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B7BDC: 814B0008  lwz r10, 8(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(8 as u32) ) } as u64;
	// 832B7BE0: 7D4903A6  mtctr r10
	ctx.ctr.u64 = ctx.r[10].u64;
	// 832B7BE4: 4E800420  bctr
	crate::rt::call_indirect(ctx.ctr.u32);
	return;
	// 832B7BE8: 4E800020  blr
	return;
}

pub fn sub_832B7CE0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7CE0 size=88
	// 832B7CE0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B7CE4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7CE8: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B7CEC: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7CF0: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7CF4: 3FE08333  lis r31, -0x7ccd
	ctx.r[31].s64 = -2093809664;
	// 832B7CF8: 3BDF5A28  addi r30, r31, 0x5a28
	ctx.r[30].s64 = ctx.r[31].s64 + 23080;
	// 832B7CFC: 807F5A28  lwz r3, 0x5a28(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(23080 as u32) ) } as u64;
	// 832B7D00: 4B9D6591  bl 0x82c8e290
	ctx.lr = 0x832B7D04;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C8E290);
	// 832B7D04: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7D08: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B7D0C: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B7D10: 917F5A28  stw r11, 0x5a28(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(23080 as u32), ctx.r[11].u32 ) };
	// 832B7D14: 915E0004  stw r10, 4(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(4 as u32), ctx.r[10].u32 ) };
	// 832B7D18: 913E0008  stw r9, 8(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(8 as u32), ctx.r[9].u32 ) };
	// 832B7D1C: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B7D20: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7D24: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7D28: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B7D2C: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7D30: 4E800020  blr
	return;
}

pub fn sub_832B7D38(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7D38 size=88
	// 832B7D38: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B7D3C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7D40: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B7D44: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7D48: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7D4C: 3FE08333  lis r31, -0x7ccd
	ctx.r[31].s64 = -2093809664;
	// 832B7D50: 3BDF5A1C  addi r30, r31, 0x5a1c
	ctx.r[30].s64 = ctx.r[31].s64 + 23068;
	// 832B7D54: 807F5A1C  lwz r3, 0x5a1c(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(23068 as u32) ) } as u64;
	// 832B7D58: 4B9D6539  bl 0x82c8e290
	ctx.lr = 0x832B7D5C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C8E290);
	// 832B7D5C: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7D60: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B7D64: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B7D68: 917F5A1C  stw r11, 0x5a1c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(23068 as u32), ctx.r[11].u32 ) };
	// 832B7D6C: 915E0004  stw r10, 4(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(4 as u32), ctx.r[10].u32 ) };
	// 832B7D70: 913E0008  stw r9, 8(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(8 as u32), ctx.r[9].u32 ) };
	// 832B7D74: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B7D78: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7D7C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7D80: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B7D84: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7D88: 4E800020  blr
	return;
}

pub fn sub_832B7D90(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7D90 size=120
	// 832B7D90: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B7D94: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7D98: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B7D9C: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7DA0: 3D60832F  lis r11, -0x7cd1
	ctx.r[11].s64 = -2094071808;
	// 832B7DA4: 3BEBE9A8  addi r31, r11, -0x1658
	ctx.r[31].s64 = ctx.r[11].s64 + -5720;
	// 832B7DA8: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B7DAC: 4B9D0B8D  bl 0x82c88938
	ctx.lr = 0x832B7DB0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C88938);
	// 832B7DB0: 807F0010  lwz r3, 0x10(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(16 as u32) ) } as u64;
	// 832B7DB4: 4B9D64DD  bl 0x82c8e290
	ctx.lr = 0x832B7DB8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82C8E290);
	// 832B7DB8: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832B7DBC: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832B7DC0: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832B7DC4: 917F0010  stw r11, 0x10(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(16 as u32), ctx.r[11].u32 ) };
	// 832B7DC8: 915F0014  stw r10, 0x14(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(20 as u32), ctx.r[10].u32 ) };
	// 832B7DCC: 913F0018  stw r9, 0x18(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(24 as u32), ctx.r[9].u32 ) };
	// 832B7DD0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B7DD4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7DD8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7DDC: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B7DE0: 4E800020  blr
	return;
}

pub fn sub_832B7E08(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B7E08 size=3084
	// 832B7E08: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B7E0C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B7E10: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B7E14: 3D608333  lis r11, -0x7ccd
	ctx.r[11].s64 = -2093809664;
	// 832B7E18: 806B7150  lwz r3, 0x7150(r11)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(29008 as u32) ) } as u64;
	// 832B7E1C: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832B7E20: 419A0024  beq cr6, 0x832b7e44
	if ctx.cr[6].eq {
	pc = 0x832B7E44; continue 'dispatch;
	}
	// 832B7E24: 4B3C4545  bl 0x8267c368
	ctx.lr = 0x832B7E28;
	crate::recompiler::externs::call(&mut ctx, base, 0x8267C368);
	// 832B7E28: 28030000  cmplwi r3, 0
	ctx.cr[0].compare_u32(ctx.r[0].u32, 0 as u32, &mut ctx.xer);
	// 832B7E2C: 41820018  beq 0x832b7e44
	if ctx.cr[0].eq {
	pc = 0x832B7E44; continue 'dispatch;
	}
	// 832B7E30: 81630000  lwz r11, 0(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B7E34: 38800001  li r4, 1
	ctx.r[4].s64 = 1;
	// 832B7E38: 816B0000  lwz r11, 0(r11)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B7E3C: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832B7E40: 4E800421  bctrl
	ctx.lr = 0x832B7E44;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832B7E44: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B7E48: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B7E4C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B7E50: 4E800020  blr
	return;
}

pub fn sub_832B8FC8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B8FC8 size=16
	// 832B8FC8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B8FCC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B8FD0: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B8FD4: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
}

pub fn sub_832B9100(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9100 size=96
	// 832B9100: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9104: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9108: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B910C: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B9110: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9114: 3D608330  lis r11, -0x7cd0
	ctx.r[11].s64 = -2094006272;
	// 832B9118: 3BC00003  li r30, 3
	ctx.r[30].s64 = 3;
	// 832B911C: 396BF050  addi r11, r11, -0xfb0
	ctx.r[11].s64 = ctx.r[11].s64 + -4016;
	// 832B9120: 3BEB0040  addi r31, r11, 0x40
	ctx.r[31].s64 = ctx.r[11].s64 + 64;
	// 832B9124: 3BFFFFF0  addi r31, r31, -0x10
	ctx.r[31].s64 = ctx.r[31].s64 + -16;
	// 832B9128: 897F0000  lbz r11, 0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B912C: 2B0B0004  cmplwi cr6, r11, 4
	ctx.cr[6].compare_u32(ctx.r[11].u32, 4 as u32, &mut ctx.xer);
	// 832B9130: 4198000C  blt cr6, 0x832b913c
	if ctx.cr[6].lt {
	pc = 0x832B913C; continue 'dispatch;
	}
	// 832B9134: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B9138: 4BCB91E9  bl 0x82f72320
	ctx.lr = 0x832B913C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82F72320);
	// 832B913C: 37DEFFFF  addic. r30, r30, -1
	ctx.xer.ca = (ctx.r[30].u32 > (!(-1 as u32)));
	ctx.r[30].s64 = ctx.r[30].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832B9140: 4080FFE4  bge 0x832b9124
	if !ctx.cr[0].lt {
	pc = 0x832B9124; continue 'dispatch;
	}
	// 832B9144: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B9148: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B914C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9150: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B9154: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B9158: 4E800020  blr
	return;
}

pub fn sub_832B9160(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9160 size=96
	// 832B9160: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9164: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9168: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B916C: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B9170: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9174: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9178: 3BC00003  li r30, 3
	ctx.r[30].s64 = 3;
	// 832B917C: 396B9628  addi r11, r11, -0x69d8
	ctx.r[11].s64 = ctx.r[11].s64 + -27096;
	// 832B9180: 3BEB0040  addi r31, r11, 0x40
	ctx.r[31].s64 = ctx.r[11].s64 + 64;
	// 832B9184: 3BFFFFF0  addi r31, r31, -0x10
	ctx.r[31].s64 = ctx.r[31].s64 + -16;
	// 832B9188: 897F0000  lbz r11, 0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B918C: 2B0B0004  cmplwi cr6, r11, 4
	ctx.cr[6].compare_u32(ctx.r[11].u32, 4 as u32, &mut ctx.xer);
	// 832B9190: 4198000C  blt cr6, 0x832b919c
	if ctx.cr[6].lt {
	pc = 0x832B919C; continue 'dispatch;
	}
	// 832B9194: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B9198: 4BCB9189  bl 0x82f72320
	ctx.lr = 0x832B919C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82F72320);
	// 832B919C: 37DEFFFF  addic. r30, r30, -1
	ctx.xer.ca = (ctx.r[30].u32 > (!(-1 as u32)));
	ctx.r[30].s64 = ctx.r[30].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832B91A0: 4080FFE4  bge 0x832b9184
	if !ctx.cr[0].lt {
	pc = 0x832B9184; continue 'dispatch;
	}
	// 832B91A4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B91A8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B91AC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B91B0: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B91B4: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B91B8: 4E800020  blr
	return;
}

pub fn sub_832B91C0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B91C0 size=96
	// 832B91C0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B91C4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B91C8: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B91CC: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B91D0: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B91D4: 3D608330  lis r11, -0x7cd0
	ctx.r[11].s64 = -2094006272;
	// 832B91D8: 3BC00001  li r30, 1
	ctx.r[30].s64 = 1;
	// 832B91DC: 396BF378  addi r11, r11, -0xc88
	ctx.r[11].s64 = ctx.r[11].s64 + -3208;
	// 832B91E0: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832B91E4: 3BFFFFF0  addi r31, r31, -0x10
	ctx.r[31].s64 = ctx.r[31].s64 + -16;
	// 832B91E8: 897F0000  lbz r11, 0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B91EC: 2B0B0004  cmplwi cr6, r11, 4
	ctx.cr[6].compare_u32(ctx.r[11].u32, 4 as u32, &mut ctx.xer);
	// 832B91F0: 4198000C  blt cr6, 0x832b91fc
	if ctx.cr[6].lt {
	pc = 0x832B91FC; continue 'dispatch;
	}
	// 832B91F4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B91F8: 4BCB9129  bl 0x82f72320
	ctx.lr = 0x832B91FC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82F72320);
	// 832B91FC: 37DEFFFF  addic. r30, r30, -1
	ctx.xer.ca = (ctx.r[30].u32 > (!(-1 as u32)));
	ctx.r[30].s64 = ctx.r[30].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832B9200: 4080FFE4  bge 0x832b91e4
	if !ctx.cr[0].lt {
	pc = 0x832B91E4; continue 'dispatch;
	}
	// 832B9204: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B9208: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B920C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9210: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B9214: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B9218: 4E800020  blr
	return;
}

pub fn sub_832B9220(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9220 size=336
	// 832B9220: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9224: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9228: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832B922C: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832B9230: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9234: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9238: 3BC00001  li r30, 1
	ctx.r[30].s64 = 1;
	// 832B923C: 396B9698  addi r11, r11, -0x6968
	ctx.r[11].s64 = ctx.r[11].s64 + -26984;
	// 832B9240: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832B9244: 3BFFFFF0  addi r31, r31, -0x10
	ctx.r[31].s64 = ctx.r[31].s64 + -16;
	// 832B9248: 897F0000  lbz r11, 0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832B924C: 2B0B0004  cmplwi cr6, r11, 4
	ctx.cr[6].compare_u32(ctx.r[11].u32, 4 as u32, &mut ctx.xer);
	// 832B9250: 4198000C  blt cr6, 0x832b925c
	if ctx.cr[6].lt {
	pc = 0x832B925C; continue 'dispatch;
	}
	// 832B9254: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832B9258: 4BCB90C9  bl 0x82f72320
	ctx.lr = 0x832B925C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82F72320);
	// 832B925C: 37DEFFFF  addic. r30, r30, -1
	ctx.xer.ca = (ctx.r[30].u32 > (!(-1 as u32)));
	ctx.r[30].s64 = ctx.r[30].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832B9260: 4080FFE4  bge 0x832b9244
	if !ctx.cr[0].lt {
	pc = 0x832B9244; continue 'dispatch;
	}
	// 832B9264: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832B9268: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B926C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9270: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832B9274: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832B9278: 4E800020  blr
	return;
}

pub fn sub_832B9370(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9370 size=40
	// 832B9370: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9374: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9378: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B937C: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9380: 386B983C  addi r3, r11, -0x67c4
	ctx.r[3].s64 = ctx.r[11].s64 + -26564;
	// 832B9384: 4BD4BDED  bl 0x83005170
	ctx.lr = 0x832B9388;
	crate::recompiler::externs::call(&mut ctx, base, 0x83005170);
	// 832B9388: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B938C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9390: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9394: 4E800020  blr
	return;
}

pub fn sub_832B9398(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9398 size=40
	// 832B9398: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B939C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B93A0: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B93A4: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B93A8: 386B9870  addi r3, r11, -0x6790
	ctx.r[3].s64 = ctx.r[11].s64 + -26512;
	// 832B93AC: 4BD4A21D  bl 0x830035c8
	ctx.lr = 0x832B93B0;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B93B0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B93B4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B93B8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B93BC: 4E800020  blr
	return;
}

pub fn sub_832B93C0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B93C0 size=40
	// 832B93C0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B93C4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B93C8: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B93CC: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B93D0: 386B9864  addi r3, r11, -0x679c
	ctx.r[3].s64 = ctx.r[11].s64 + -26524;
	// 832B93D4: 4BD4A2C5  bl 0x83003698
	ctx.lr = 0x832B93D8;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B93D8: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B93DC: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B93E0: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B93E4: 4E800020  blr
	return;
}

pub fn sub_832B93E8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B93E8 size=40
	// 832B93E8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B93EC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B93F0: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B93F4: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B93F8: 386B98A0  addi r3, r11, -0x6760
	ctx.r[3].s64 = ctx.r[11].s64 + -26464;
	// 832B93FC: 4BD4A1CD  bl 0x830035c8
	ctx.lr = 0x832B9400;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B9400: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B9404: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9408: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B940C: 4E800020  blr
	return;
}

pub fn sub_832B9410(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9410 size=40
	// 832B9410: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9414: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9418: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B941C: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9420: 386B9894  addi r3, r11, -0x676c
	ctx.r[3].s64 = ctx.r[11].s64 + -26476;
	// 832B9424: 4BD4A275  bl 0x83003698
	ctx.lr = 0x832B9428;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B9428: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B942C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9430: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9434: 4E800020  blr
	return;
}

pub fn sub_832B9438(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9438 size=40
	// 832B9438: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B943C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9440: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9444: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9448: 386B98D0  addi r3, r11, -0x6730
	ctx.r[3].s64 = ctx.r[11].s64 + -26416;
	// 832B944C: 4BD4A17D  bl 0x830035c8
	ctx.lr = 0x832B9450;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B9450: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B9454: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9458: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B945C: 4E800020  blr
	return;
}

pub fn sub_832B9460(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9460 size=40
	// 832B9460: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9464: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9468: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B946C: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9470: 386B98C4  addi r3, r11, -0x673c
	ctx.r[3].s64 = ctx.r[11].s64 + -26428;
	// 832B9474: 4BD4A225  bl 0x83003698
	ctx.lr = 0x832B9478;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B9478: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B947C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9480: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9484: 4E800020  blr
	return;
}

pub fn sub_832B9488(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9488 size=40
	// 832B9488: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B948C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9490: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9494: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9498: 386B9940  addi r3, r11, -0x66c0
	ctx.r[3].s64 = ctx.r[11].s64 + -26304;
	// 832B949C: 4BD4A12D  bl 0x830035c8
	ctx.lr = 0x832B94A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B94A0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B94A4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B94A8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B94AC: 4E800020  blr
	return;
}

pub fn sub_832B94B0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B94B0 size=40
	// 832B94B0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B94B4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B94B8: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B94BC: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B94C0: 386B9934  addi r3, r11, -0x66cc
	ctx.r[3].s64 = ctx.r[11].s64 + -26316;
	// 832B94C4: 4BD4A1D5  bl 0x83003698
	ctx.lr = 0x832B94C8;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B94C8: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B94CC: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B94D0: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B94D4: 4E800020  blr
	return;
}

pub fn sub_832B94D8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B94D8 size=40
	// 832B94D8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B94DC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B94E0: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B94E4: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B94E8: 386B9A00  addi r3, r11, -0x6600
	ctx.r[3].s64 = ctx.r[11].s64 + -26112;
	// 832B94EC: 4BD4A0DD  bl 0x830035c8
	ctx.lr = 0x832B94F0;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B94F0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B94F4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B94F8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B94FC: 4E800020  blr
	return;
}

pub fn sub_832B9500(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9500 size=40
	// 832B9500: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9504: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9508: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B950C: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9510: 386B99F4  addi r3, r11, -0x660c
	ctx.r[3].s64 = ctx.r[11].s64 + -26124;
	// 832B9514: 4BD4A185  bl 0x83003698
	ctx.lr = 0x832B9518;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B9518: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B951C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9520: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9524: 4E800020  blr
	return;
}

pub fn sub_832B9528(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9528 size=40
	// 832B9528: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B952C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9530: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9534: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9538: 386B99D0  addi r3, r11, -0x6630
	ctx.r[3].s64 = ctx.r[11].s64 + -26160;
	// 832B953C: 4BD4A08D  bl 0x830035c8
	ctx.lr = 0x832B9540;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B9540: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B9544: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9548: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B954C: 4E800020  blr
	return;
}

pub fn sub_832B9550(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9550 size=40
	// 832B9550: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9554: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9558: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B955C: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9560: 386B99C4  addi r3, r11, -0x663c
	ctx.r[3].s64 = ctx.r[11].s64 + -26172;
	// 832B9564: 4BD4A135  bl 0x83003698
	ctx.lr = 0x832B9568;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B9568: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B956C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9570: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9574: 4E800020  blr
	return;
}

pub fn sub_832B9578(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9578 size=40
	// 832B9578: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B957C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9580: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9584: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9588: 386B99A0  addi r3, r11, -0x6660
	ctx.r[3].s64 = ctx.r[11].s64 + -26208;
	// 832B958C: 4BD4A03D  bl 0x830035c8
	ctx.lr = 0x832B9590;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B9590: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B9594: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9598: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B959C: 4E800020  blr
	return;
}

pub fn sub_832B95A0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B95A0 size=40
	// 832B95A0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B95A4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B95A8: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B95AC: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B95B0: 386B9994  addi r3, r11, -0x666c
	ctx.r[3].s64 = ctx.r[11].s64 + -26220;
	// 832B95B4: 4BD4A0E5  bl 0x83003698
	ctx.lr = 0x832B95B8;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B95B8: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B95BC: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B95C0: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B95C4: 4E800020  blr
	return;
}

pub fn sub_832B95C8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B95C8 size=40
	// 832B95C8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B95CC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B95D0: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B95D4: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B95D8: 386B9970  addi r3, r11, -0x6690
	ctx.r[3].s64 = ctx.r[11].s64 + -26256;
	// 832B95DC: 4BD49FED  bl 0x830035c8
	ctx.lr = 0x832B95E0;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B95E0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B95E4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B95E8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B95EC: 4E800020  blr
	return;
}

pub fn sub_832B95F0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B95F0 size=40
	// 832B95F0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B95F4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B95F8: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B95FC: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9600: 386B9964  addi r3, r11, -0x669c
	ctx.r[3].s64 = ctx.r[11].s64 + -26268;
	// 832B9604: 4BD4A095  bl 0x83003698
	ctx.lr = 0x832B9608;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B9608: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B960C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9610: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9614: 4E800020  blr
	return;
}

pub fn sub_832B9618(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9618 size=40
	// 832B9618: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B961C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9620: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9624: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9628: 386B9A34  addi r3, r11, -0x65cc
	ctx.r[3].s64 = ctx.r[11].s64 + -26060;
	// 832B962C: 4BD49F9D  bl 0x830035c8
	ctx.lr = 0x832B9630;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B9630: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B9634: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9638: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B963C: 4E800020  blr
	return;
}

pub fn sub_832B9640(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9640 size=40
	// 832B9640: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9644: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9648: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B964C: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9650: 386B9A28  addi r3, r11, -0x65d8
	ctx.r[3].s64 = ctx.r[11].s64 + -26072;
	// 832B9654: 4BD4A045  bl 0x83003698
	ctx.lr = 0x832B9658;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B9658: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B965C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9660: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9664: 4E800020  blr
	return;
}

pub fn sub_832B9668(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9668 size=40
	// 832B9668: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B966C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9670: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9674: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9678: 386B9A64  addi r3, r11, -0x659c
	ctx.r[3].s64 = ctx.r[11].s64 + -26012;
	// 832B967C: 4BD49F4D  bl 0x830035c8
	ctx.lr = 0x832B9680;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B9680: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B9684: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9688: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B968C: 4E800020  blr
	return;
}

pub fn sub_832B9690(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9690 size=40
	// 832B9690: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9694: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9698: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B969C: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B96A0: 386B9A58  addi r3, r11, -0x65a8
	ctx.r[3].s64 = ctx.r[11].s64 + -26024;
	// 832B96A4: 4BD49FF5  bl 0x83003698
	ctx.lr = 0x832B96A8;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B96A8: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B96AC: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B96B0: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B96B4: 4E800020  blr
	return;
}

pub fn sub_832B96B8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B96B8 size=40
	// 832B96B8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B96BC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B96C0: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B96C4: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B96C8: 386B9AC4  addi r3, r11, -0x653c
	ctx.r[3].s64 = ctx.r[11].s64 + -25916;
	// 832B96CC: 4BD49EFD  bl 0x830035c8
	ctx.lr = 0x832B96D0;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B96D0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B96D4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B96D8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B96DC: 4E800020  blr
	return;
}

pub fn sub_832B96E0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B96E0 size=40
	// 832B96E0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B96E4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B96E8: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B96EC: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B96F0: 386B9AB8  addi r3, r11, -0x6548
	ctx.r[3].s64 = ctx.r[11].s64 + -25928;
	// 832B96F4: 4BD49FA5  bl 0x83003698
	ctx.lr = 0x832B96F8;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B96F8: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B96FC: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9700: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9704: 4E800020  blr
	return;
}

pub fn sub_832B9708(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9708 size=40
	// 832B9708: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B970C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9710: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9714: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9718: 386B9A94  addi r3, r11, -0x656c
	ctx.r[3].s64 = ctx.r[11].s64 + -25964;
	// 832B971C: 4BD49EAD  bl 0x830035c8
	ctx.lr = 0x832B9720;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B9720: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B9724: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9728: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B972C: 4E800020  blr
	return;
}

pub fn sub_832B9730(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9730 size=40
	// 832B9730: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9734: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9738: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B973C: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9740: 386B9A88  addi r3, r11, -0x6578
	ctx.r[3].s64 = ctx.r[11].s64 + -25976;
	// 832B9744: 4BD49F55  bl 0x83003698
	ctx.lr = 0x832B9748;
	crate::recompiler::externs::call(&mut ctx, base, 0x83003698);
	// 832B9748: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B974C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9750: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B9754: 4E800020  blr
	return;
}

pub fn sub_832B9758(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9758 size=40
	// 832B9758: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B975C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9760: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B9764: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9768: 386B9B1C  addi r3, r11, -0x64e4
	ctx.r[3].s64 = ctx.r[11].s64 + -25828;
	// 832B976C: 4BD4BA05  bl 0x83005170
	ctx.lr = 0x832B9770;
	crate::recompiler::externs::call(&mut ctx, base, 0x83005170);
	// 832B9770: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B9774: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B9778: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B977C: 4E800020  blr
	return;
}

pub fn sub_832B9780(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B9780 size=40
	// 832B9780: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B9784: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B9788: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B978C: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B9790: 386B9B44  addi r3, r11, -0x64bc
	ctx.r[3].s64 = ctx.r[11].s64 + -25788;
	// 832B9794: 4BD49E35  bl 0x830035c8
	ctx.lr = 0x832B9798;
	crate::recompiler::externs::call(&mut ctx, base, 0x830035C8);
	// 832B9798: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B979C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B97A0: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B97A4: 4E800020  blr
	return;
}

pub fn sub_832B97A8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832B97A8 size=2060
	// 832B97A8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832B97AC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832B97B0: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832B97B4: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B97B8: 386B9B68  addi r3, r11, -0x6498
	ctx.r[3].s64 = ctx.r[11].s64 + -25752;
	// 832B97BC: 4BD4CA7D  bl 0x83006238
	ctx.lr = 0x832B97C0;
	crate::recompiler::externs::call(&mut ctx, base, 0x83006238);
	// 832B97C0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832B97C4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832B97C8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832B97CC: 4E800020  blr
	return;
	// 832B97D0: 3D608336  lis r11, -0x7cca
	ctx.r[11].s64 = -2093613056;
	// 832B97D4: 396BB0A0  addi r11, r11, -0x4f60
	ctx.r[11].s64 = ctx.r[11].s64 + -20320;
	// 832B97D8: 386B0028  addi r3, r11, 0x28
	ctx.r[3].s64 = ctx.r[11].s64 + 40;
	// 832B97DC: 4BAC5F2C  b 0x82d7f708
	crate::recompiler::externs::call(&mut ctx, base, 0x82D7F708);
	return;
	// 832B97E0: 3D60820D  lis r11, -0x7df3
	ctx.r[11].s64 = -2113077248;
	// 832B97E4: 3D408330  lis r10, -0x7cd0
	ctx.r[10].s64 = -2094006272;
	// 832B97E8: 396B9128  addi r11, r11, -0x6ed8
	ctx.r[11].s64 = ctx.r[11].s64 + -28376;
	// 832B97EC: 916A1F68  stw r11, 0x1f68(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(8040 as u32), ctx.r[11].u32 ) };
	// 832B97F0: 4E800020  blr
	return;
}

pub fn sub_832BAC00(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BAC00 size=256
	// 832BAC00: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BAC04: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BAC08: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832BAC0C: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BAC10: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BAC14: 3D608350  lis r11, -0x7cb0
	ctx.r[11].s64 = -2091909120;
	// 832BAC18: 7C681B78  mr r8, r3
	ctx.r[8].u64 = ctx.r[3].u64;
	// 832BAC1C: 3BEBC46C  addi r31, r11, -0x3b94
	ctx.r[31].s64 = ctx.r[11].s64 + -15252;
	// 832BAC20: 38E4001F  addi r7, r4, 0x1f
	ctx.r[7].s64 = ctx.r[4].s64 + 31;
	// 832BAC24: 54FE0034  rlwinm r30, r7, 0, 0, 0x1a
	ctx.r[30].u64 = ctx.r[7].u32 as u64 & 0xFFFFFFFFu64;
	// 832BAC28: 812802DC  lwz r9, 0x2dc(r8)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(732 as u32) ) } as u64;
	// 832BAC2C: 817FFDB0  lwz r11, -0x250(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(-592 as u32) ) } as u64;
	// 832BAC30: 7D4B2214  add r10, r11, r4
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[4].u64;
	// 832BAC34: 7C6BF214  add r3, r11, r30
	ctx.r[3].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832BAC38: 7CCA4A14  add r6, r10, r9
	ctx.r[6].u64 = ctx.r[10].u64 + ctx.r[9].u64;
	// 832BAC3C: 90C802DC  stw r6, 0x2dc(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(732 as u32), ctx.r[6].u32 ) };
	// 832BAC40: 4B92F151  bl 0x82be9d90
	ctx.lr = 0x832BAC44;
	crate::recompiler::externs::call(&mut ctx, base, 0x82BE9D90);
	// 832BAC44: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832BAC48: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BAC4C: 917FFDB0  stw r11, -0x250(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-592 as u32), ctx.r[11].u32 ) };
	// 832BAC50: 419A003C  beq cr6, 0x832bac8c
	if ctx.cr[6].eq {
	pc = 0x832BAC8C; continue 'dispatch;
	}
	// 832BAC54: 815FFE18  lwz r10, -0x1e8(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(-488 as u32) ) } as u64;
	// 832BAC58: 7D23F214  add r9, r3, r30
	ctx.r[9].u64 = ctx.r[3].u64 + ctx.r[30].u64;
	// 832BAC5C: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BAC60: 419A002C  beq cr6, 0x832bac8c
	if ctx.cr[6].eq {
	pc = 0x832BAC8C; continue 'dispatch;
	}
	// 832BAC64: 817F0000  lwz r11, 0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BAC68: 811FFFFC  lwz r8, -4(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(-4 as u32) ) } as u64;
	// 832BAC6C: 7CEB4050  subf r7, r11, r8
	ctx.r[7].s64 = ctx.r[8].s64 - ctx.r[11].s64;
	// 832BAC70: 7D07582E  lwzx r8, r7, r11
	ctx.r[8].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[11].u32)) } as u64;
	// 832BAC74: 354AFFFF  addic. r10, r10, -1
	ctx.xer.ca = (ctx.r[10].u32 > (!(-1 as u32)));
	ctx.r[10].s64 = ctx.r[10].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832BAC78: 91280000  stw r9, 0(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[9].u32 ) };
	// 832BAC7C: 810B0000  lwz r8, 0(r11)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BAC80: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832BAC84: 7D284A14  add r9, r8, r9
	ctx.r[9].u64 = ctx.r[8].u64 + ctx.r[9].u64;
	// 832BAC88: 4082FFE8  bne 0x832bac70
	if !ctx.cr[0].eq {
	pc = 0x832BAC70; continue 'dispatch;
	}
	// 832BAC8C: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832BAC90: 917FFE18  stw r11, -0x1e8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-488 as u32), ctx.r[11].u32 ) };
	// 832BAC94: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BAC98: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BAC9C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BACA0: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832BACA4: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BACA8: 4E800020  blr
	return;
}

pub fn sub_832BAD00(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BAD00 size=760
	// 832BAD00: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BAD04: 4B9EE6FD  bl 0x82ca9400
	ctx.lr = 0x832BAD08;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9400);
	// 832BAD08: 9421FF70  stwu r1, -0x90(r1)
	ea = ctx.r[1].u32.wrapping_add(-144 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BAD0C: 7C7D1B78  mr r29, r3
	ctx.r[29].u64 = ctx.r[3].u64;
	// 832BAD10: 817D0114  lwz r11, 0x114(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BAD14: 815D0118  lwz r10, 0x118(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BAD18: 1D6B0184  mulli r11, r11, 0x184
	ctx.r[11].s64 = ctx.r[11].s64 * 388;
	// 832BAD1C: 7D4B5214  add r10, r11, r10
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832BAD20: 812AFFD8  lwz r9, -0x28(r10)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(-40 as u32) ) } as u64;
	// 832BAD24: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832BAD28: 419A02C8  beq cr6, 0x832baff0
	if ctx.cr[6].eq {
	pc = 0x832BAFF0; continue 'dispatch;
	}
	// 832BAD2C: 817D02B4  lwz r11, 0x2b4(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(692 as u32) ) } as u64;
	// 832BAD30: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BAD34: 419A02BC  beq cr6, 0x832baff0
	if ctx.cr[6].eq {
	pc = 0x832BAFF0; continue 'dispatch;
	}
	// 832BAD38: 3B5D0354  addi r26, r29, 0x354
	ctx.r[26].s64 = ctx.r[29].s64 + 852;
	// 832BAD3C: 2B1A0000  cmplwi cr6, r26, 0
	ctx.cr[6].compare_u32(ctx.r[26].u32, 0 as u32, &mut ctx.xer);
	// 832BAD40: 419A0020  beq cr6, 0x832bad60
	if ctx.cr[6].eq {
	pc = 0x832BAD60; continue 'dispatch;
	}
	// 832BAD44: 807A0004  lwz r3, 4(r26)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[26].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BAD48: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BAD4C: 419A0014  beq cr6, 0x832bad60
	if ctx.cr[6].eq {
	pc = 0x832BAD60; continue 'dispatch;
	}
	// 832BAD50: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BAD54: 4AEDBC75  bl 0x821969c8
	ctx.lr = 0x832BAD58;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BAD58: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BAD5C: 409A0294  bne cr6, 0x832baff0
	if !ctx.cr[6].eq {
	pc = 0x832BAFF0; continue 'dispatch;
	}
	// 832BAD60: 817D0114  lwz r11, 0x114(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BAD64: 3B600000  li r27, 0
	ctx.r[27].s64 = 0;
	// 832BAD68: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BAD6C: 4099026C  ble cr6, 0x832bafd8
	if !ctx.cr[6].gt {
	pc = 0x832BAFD8; continue 'dispatch;
	}
	// 832BAD70: 3B800000  li r28, 0
	ctx.r[28].s64 = 0;
	// 832BAD74: 817D0118  lwz r11, 0x118(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BAD78: 7FDC5A14  add r30, r28, r11
	ctx.r[30].u64 = ctx.r[28].u64 + ctx.r[11].u64;
	// 832BAD7C: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BAD80: 817E015C  lwz r11, 0x15c(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(348 as u32) ) } as u64;
	// 832BAD84: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832BAD88: 4E800421  bctrl
	ctx.lr = 0x832BAD8C;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BAD8C: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832BAD90: 419A0234  beq cr6, 0x832bafc4
	if ctx.cr[6].eq {
	pc = 0x832BAFC4; continue 'dispatch;
	}
	// 832BAD94: 817E0000  lwz r11, 0(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BAD98: 815E0018  lwz r10, 0x18(r30)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BAD9C: 7FEA5851  subf. r31, r10, r11
	ctx.r[31].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	ctx.cr[0].compare_i32(ctx.r[31].s32, 0, &mut ctx.xer);
	// 832BADA0: 4080000C  bge 0x832badac
	if !ctx.cr[0].lt {
	pc = 0x832BADAC; continue 'dispatch;
	}
	// 832BADA4: 817E0008  lwz r11, 8(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BADA8: 7FEBFA14  add r31, r11, r31
	ctx.r[31].u64 = ctx.r[11].u64 + ctx.r[31].u64;
	// 832BADAC: 817E0030  lwz r11, 0x30(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(48 as u32) ) } as u64;
	// 832BADB0: 7F1F5800  cmpw cr6, r31, r11
	ctx.cr[6].compare_i32(ctx.r[31].s32, ctx.r[11].s32, &mut ctx.xer);
	// 832BADB4: 4199001C  bgt cr6, 0x832badd0
	if ctx.cr[6].gt {
	pc = 0x832BADD0; continue 'dispatch;
	}
	// 832BADB8: 817D000C  lwz r11, 0xc(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BADBC: 815E0044  lwz r10, 0x44(r30)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(68 as u32) ) } as u64;
	// 832BADC0: 7F0B5000  cmpw cr6, r11, r10
	ctx.cr[6].compare_i32(ctx.r[11].s32, ctx.r[10].s32, &mut ctx.xer);
	// 832BADC4: 40990200  ble cr6, 0x832bafc4
	if !ctx.cr[6].gt {
	pc = 0x832BAFC4; continue 'dispatch;
	}
	// 832BADC8: 2F1F0000  cmpwi cr6, r31, 0
	ctx.cr[6].compare_i32(ctx.r[31].s32, 0, &mut ctx.xer);
	// 832BADCC: 419A01F8  beq cr6, 0x832bafc4
	if ctx.cr[6].eq {
	pc = 0x832BAFC4; continue 'dispatch;
	}
	// 832BADD0: 817E0160  lwz r11, 0x160(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(352 as u32) ) } as u64;
	// 832BADD4: 38A10050  addi r5, r1, 0x50
	ctx.r[5].s64 = ctx.r[1].s64 + 80;
	// 832BADD8: 38810054  addi r4, r1, 0x54
	ctx.r[4].s64 = ctx.r[1].s64 + 84;
	// 832BADDC: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BADE0: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832BADE4: 4E800421  bctrl
	ctx.lr = 0x832BADE8;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BADE8: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832BADEC: 419A01D8  beq cr6, 0x832bafc4
	if ctx.cr[6].eq {
	pc = 0x832BAFC4; continue 'dispatch;
	}
	// 832BADF0: 817E0058  lwz r11, 0x58(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(88 as u32) ) } as u64;
	// 832BADF4: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BADF8: 419A0014  beq cr6, 0x832bae0c
	if ctx.cr[6].eq {
	pc = 0x832BAE0C; continue 'dispatch;
	}
	// 832BADFC: 81410050  lwz r10, 0x50(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BAE00: 5545083C  slwi r5, r10, 1
	// 832BAE04: 90A10050  stw r5, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[5].u32 ) };
	// 832BAE08: 48000008  b 0x832bae10
	pc = 0x832BAE10; continue 'dispatch;
	// 832BAE0C: 80A10050  lwz r5, 0x50(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BAE10: 7F05F840  cmplw cr6, r5, r31
	ctx.cr[6].compare_u32(ctx.r[5].u32, ctx.r[31].u32, &mut ctx.xer);
	// 832BAE14: 4099000C  ble cr6, 0x832bae20
	if !ctx.cr[6].gt {
	pc = 0x832BAE20; continue 'dispatch;
	}
	// 832BAE18: 7FE5FB78  mr r5, r31
	ctx.r[5].u64 = ctx.r[31].u64;
	// 832BAE1C: 90A10050  stw r5, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[5].u32 ) };
	// 832BAE20: 815D000C  lwz r10, 0xc(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BAE24: 813E0044  lwz r9, 0x44(r30)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(68 as u32) ) } as u64;
	// 832BAE28: 7F0A4800  cmpw cr6, r10, r9
	ctx.cr[6].compare_i32(ctx.r[10].s32, ctx.r[9].s32, &mut ctx.xer);
	// 832BAE2C: 41990014  bgt cr6, 0x832bae40
	if ctx.cr[6].gt {
	pc = 0x832BAE40; continue 'dispatch;
	}
	// 832BAE30: 815E0034  lwz r10, 0x34(r30)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(52 as u32) ) } as u64;
	// 832BAE34: 7D495830  slw r9, r10, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[10].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832BAE38: 7D252838  and r5, r9, r5
	ctx.r[5].u64 = ctx.r[9].u64 & ctx.r[5].u64;
	// 832BAE3C: 90A10050  stw r5, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[5].u32 ) };
	// 832BAE40: 809E0018  lwz r4, 0x18(r30)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BAE44: 815E0010  lwz r10, 0x10(r30)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(16 as u32) ) } as u64;
	// 832BAE48: 7FE45050  subf r31, r4, r10
	ctx.r[31].s64 = ctx.r[10].s64 - ctx.r[4].s64;
	// 832BAE4C: 7F1F2840  cmplw cr6, r31, r5
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[5].u32, &mut ctx.xer);
	// 832BAE50: 409800E0  bge cr6, 0x832baf30
	if !ctx.cr[6].lt {
	pc = 0x832BAF30; continue 'dispatch;
	}
	// 832BAE54: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BAE58: 419A0068  beq cr6, 0x832baec0
	if ctx.cr[6].eq {
	pc = 0x832BAEC0; continue 'dispatch;
	}
	// 832BAE5C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BAE60: 419A0040  beq cr6, 0x832baea0
	if ctx.cr[6].eq {
	pc = 0x832BAEA0; continue 'dispatch;
	}
	// 832BAE64: 57EBF87E  srwi r11, r31, 1
	// 832BAE68: 81410054  lwz r10, 0x54(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BAE6C: 7C892378  mr r9, r4
	ctx.r[9].u64 = ctx.r[4].u64;
	// 832BAE70: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BAE74: 419A0024  beq cr6, 0x832bae98
	if ctx.cr[6].eq {
	pc = 0x832BAE98; continue 'dispatch;
	}
	// 832BAE78: 89090000  lbz r8, 0(r9)
	ctx.r[8].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BAE7C: 356BFFFF  addic. r11, r11, -1
	ctx.xer.ca = (ctx.r[11].u32 > (!(-1 as u32)));
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BAE80: 39290002  addi r9, r9, 2
	ctx.r[9].s64 = ctx.r[9].s64 + 2;
	// 832BAE84: 39080080  addi r8, r8, 0x80
	ctx.r[8].s64 = ctx.r[8].s64 + 128;
	// 832BAE88: 990A0000  stb r8, 0(r10)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), ctx.r[8].u8 ) };
	// 832BAE8C: 394A0001  addi r10, r10, 1
	ctx.r[10].s64 = ctx.r[10].s64 + 1;
	// 832BAE90: 4082FFE8  bne 0x832bae78
	if !ctx.cr[0].eq {
	pc = 0x832BAE78; continue 'dispatch;
	}
	// 832BAE94: 80A10050  lwz r5, 0x50(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BAE98: 91410054  stw r10, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[10].u32 ) };
	// 832BAE9C: 48000028  b 0x832baec4
	pc = 0x832BAEC4; continue 'dispatch;
	// 832BAEA0: 7FE5FB78  mr r5, r31
	ctx.r[5].u64 = ctx.r[31].u64;
	// 832BAEA4: 80610054  lwz r3, 0x54(r1)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BAEA8: 4B9EE5D9  bl 0x82ca9480
	ctx.lr = 0x832BAEAC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832BAEAC: 81610054  lwz r11, 0x54(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BAEB0: 80A10050  lwz r5, 0x50(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BAEB4: 7D5F5A14  add r10, r31, r11
	ctx.r[10].u64 = ctx.r[31].u64 + ctx.r[11].u64;
	// 832BAEB8: 91410054  stw r10, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[10].u32 ) };
	// 832BAEBC: 48000008  b 0x832baec4
	pc = 0x832BAEC4; continue 'dispatch;
	// 832BAEC0: 81410054  lwz r10, 0x54(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BAEC4: 817E0058  lwz r11, 0x58(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(88 as u32) ) } as u64;
	// 832BAEC8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BAECC: 419A003C  beq cr6, 0x832baf08
	if ctx.cr[6].eq {
	pc = 0x832BAF08; continue 'dispatch;
	}
	// 832BAED0: 7D7F2850  subf r11, r31, r5
	ctx.r[11].s64 = ctx.r[5].s64 - ctx.r[31].s64;
	// 832BAED4: 7D495378  mr r9, r10
	ctx.r[9].u64 = ctx.r[10].u64;
	// 832BAED8: 815E000C  lwz r10, 0xc(r30)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BAEDC: 556BF87E  srwi r11, r11, 1
	// 832BAEE0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BAEE4: 419A0038  beq cr6, 0x832baf1c
	if ctx.cr[6].eq {
	pc = 0x832BAF1C; continue 'dispatch;
	}
	// 832BAEE8: 890A0000  lbz r8, 0(r10)
	ctx.r[8].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BAEEC: 356BFFFF  addic. r11, r11, -1
	ctx.xer.ca = (ctx.r[11].u32 > (!(-1 as u32)));
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BAEF0: 394A0002  addi r10, r10, 2
	ctx.r[10].s64 = ctx.r[10].s64 + 2;
	// 832BAEF4: 39080080  addi r8, r8, 0x80
	ctx.r[8].s64 = ctx.r[8].s64 + 128;
	// 832BAEF8: 99090000  stb r8, 0(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.r[8].u8 ) };
	// 832BAEFC: 39290001  addi r9, r9, 1
	ctx.r[9].s64 = ctx.r[9].s64 + 1;
	// 832BAF00: 4082FFE8  bne 0x832baee8
	if !ctx.cr[0].eq {
	pc = 0x832BAEE8; continue 'dispatch;
	}
	// 832BAF04: 48000014  b 0x832baf18
	pc = 0x832BAF18; continue 'dispatch;
	// 832BAF08: 7CBF2850  subf r5, r31, r5
	ctx.r[5].s64 = ctx.r[5].s64 - ctx.r[31].s64;
	// 832BAF0C: 809E000C  lwz r4, 0xc(r30)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BAF10: 7D435378  mr r3, r10
	ctx.r[3].u64 = ctx.r[10].u64;
	// 832BAF14: 4B9EE56D  bl 0x82ca9480
	ctx.lr = 0x832BAF18;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832BAF18: 80A10050  lwz r5, 0x50(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BAF1C: 817E000C  lwz r11, 0xc(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BAF20: 7D7F5850  subf r11, r31, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[31].s64;
	// 832BAF24: 7D4B2A14  add r10, r11, r5
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[5].u64;
	// 832BAF28: 915E0018  stw r10, 0x18(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(24 as u32), ctx.r[10].u32 ) };
	// 832BAF2C: 48000058  b 0x832baf84
	pc = 0x832BAF84; continue 'dispatch;
	// 832BAF30: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BAF34: 419A0038  beq cr6, 0x832baf6c
	if ctx.cr[6].eq {
	pc = 0x832BAF6C; continue 'dispatch;
	}
	// 832BAF38: 54ABF87E  srwi r11, r5, 1
	// 832BAF3C: 81210054  lwz r9, 0x54(r1)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BAF40: 7C8A2378  mr r10, r4
	ctx.r[10].u64 = ctx.r[4].u64;
	// 832BAF44: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BAF48: 419A0030  beq cr6, 0x832baf78
	if ctx.cr[6].eq {
	pc = 0x832BAF78; continue 'dispatch;
	}
	// 832BAF4C: 890A0000  lbz r8, 0(r10)
	ctx.r[8].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BAF50: 356BFFFF  addic. r11, r11, -1
	ctx.xer.ca = (ctx.r[11].u32 > (!(-1 as u32)));
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BAF54: 394A0002  addi r10, r10, 2
	ctx.r[10].s64 = ctx.r[10].s64 + 2;
	// 832BAF58: 39080080  addi r8, r8, 0x80
	ctx.r[8].s64 = ctx.r[8].s64 + 128;
	// 832BAF5C: 99090000  stb r8, 0(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.r[8].u8 ) };
	// 832BAF60: 39290001  addi r9, r9, 1
	ctx.r[9].s64 = ctx.r[9].s64 + 1;
	// 832BAF64: 4082FFE8  bne 0x832baf4c
	if !ctx.cr[0].eq {
	pc = 0x832BAF4C; continue 'dispatch;
	}
	// 832BAF68: 4800000C  b 0x832baf74
	pc = 0x832BAF74; continue 'dispatch;
	// 832BAF6C: 80610054  lwz r3, 0x54(r1)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BAF70: 4B9EE511  bl 0x82ca9480
	ctx.lr = 0x832BAF74;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832BAF74: 80A10050  lwz r5, 0x50(r1)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BAF78: 817E0018  lwz r11, 0x18(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BAF7C: 7D6B2A14  add r11, r11, r5
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[5].u64;
	// 832BAF80: 917E0018  stw r11, 0x18(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(24 as u32), ctx.r[11].u32 ) };
	// 832BAF84: 817E0058  lwz r11, 0x58(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(88 as u32) ) } as u64;
	// 832BAF88: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BAF8C: 419A000C  beq cr6, 0x832baf98
	if ctx.cr[6].eq {
	pc = 0x832BAF98; continue 'dispatch;
	}
	// 832BAF90: 54A5F87E  srwi r5, r5, 1
	// 832BAF94: 90A10050  stw r5, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[5].u32 ) };
	// 832BAF98: 817E0164  lwz r11, 0x164(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(356 as u32) ) } as u64;
	// 832BAF9C: 7CA42B78  mr r4, r5
	ctx.r[4].u64 = ctx.r[5].u64;
	// 832BAFA0: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BAFA4: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832BAFA8: 4E800421  bctrl
	ctx.lr = 0x832BAFAC;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BAFAC: 815E015C  lwz r10, 0x15c(r30)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(348 as u32) ) } as u64;
	// 832BAFB0: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BAFB4: 7D4903A6  mtctr r10
	ctx.ctr.u64 = ctx.r[10].u64;
	// 832BAFB8: 4E800421  bctrl
	ctx.lr = 0x832BAFBC;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BAFBC: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832BAFC0: 409AFDD4  bne cr6, 0x832bad94
	if !ctx.cr[6].eq {
	pc = 0x832BAD94; continue 'dispatch;
	}
	// 832BAFC4: 817D0114  lwz r11, 0x114(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BAFC8: 3B7B0001  addi r27, r27, 1
	ctx.r[27].s64 = ctx.r[27].s64 + 1;
	// 832BAFCC: 3B9C0184  addi r28, r28, 0x184
	ctx.r[28].s64 = ctx.r[28].s64 + 388;
	// 832BAFD0: 7F1B5840  cmplw cr6, r27, r11
	ctx.cr[6].compare_u32(ctx.r[27].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832BAFD4: 4198FDA0  blt cr6, 0x832bad74
	if ctx.cr[6].lt {
	pc = 0x832BAD74; continue 'dispatch;
	}
	// 832BAFD8: 2B1A0000  cmplwi cr6, r26, 0
	ctx.cr[6].compare_u32(ctx.r[26].u32, 0 as u32, &mut ctx.xer);
	// 832BAFDC: 419A0014  beq cr6, 0x832baff0
	if ctx.cr[6].eq {
	pc = 0x832BAFF0; continue 'dispatch;
	}
	// 832BAFE0: 807A0004  lwz r3, 4(r26)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[26].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BAFE4: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BAFE8: 419A0008  beq cr6, 0x832baff0
	if ctx.cr[6].eq {
	pc = 0x832BAFF0; continue 'dispatch;
	}
	// 832BAFEC: 4BD509D5  bl 0x8300b9c0
	ctx.lr = 0x832BAFF0;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BAFF0: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832BAFF4: 4B9EE45C  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
}

pub fn sub_832BAFF8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BAFF8 size=312
	// 832BAFF8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BAFFC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BB000: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832BB004: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BB008: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BB00C: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832BB010: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BB014: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832BB018: 419A0008  beq cr6, 0x832bb020
	if ctx.cr[6].eq {
	pc = 0x832BB020; continue 'dispatch;
	}
	// 832BB01C: 3BDEFFFF  addi r30, r30, -1
	ctx.r[30].s64 = ctx.r[30].s64 + -1;
	// 832BB020: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BB024: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832BB028: 915F0334  stw r10, 0x334(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(820 as u32), ctx.r[10].u32 ) };
	// 832BB02C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BB030: 419A000C  beq cr6, 0x832bb03c
	if ctx.cr[6].eq {
	pc = 0x832BB03C; continue 'dispatch;
	}
	// 832BB034: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BB038: 4BFFFCC9  bl 0x832bad00
	ctx.lr = 0x832BB03C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BAD00);
	// 832BB03C: 817F000C  lwz r11, 0xc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BB040: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832BB044: 917F0010  stw r11, 0x10(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(16 as u32), ctx.r[11].u32 ) };
	// 832BB048: 409A003C  bne cr6, 0x832bb084
	if !ctx.cr[6].eq {
	pc = 0x832BB084; continue 'dispatch;
	}
	// 832BB04C: 48005A6D  bl 0x832c0ab8
	ctx.lr = 0x832BB050;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BB050: 817F0328  lwz r11, 0x328(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(808 as u32) ) } as u64;
	// 832BB054: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BB058: 409A0008  bne cr6, 0x832bb060
	if !ctx.cr[6].eq {
	pc = 0x832BB060; continue 'dispatch;
	}
	// 832BB05C: 907F0328  stw r3, 0x328(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(808 as u32), ctx.r[3].u32 ) };
	// 832BB060: 817F0328  lwz r11, 0x328(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(808 as u32) ) } as u64;
	// 832BB064: 3D400004  lis r10, 4
	ctx.r[10].s64 = 262144;
	// 832BB068: 614993E0  ori r9, r10, 0x93e0
	ctx.r[9].u64 = ctx.r[10].u64 | 37856;
	// 832BB06C: 7D0B1850  subf r8, r11, r3
	ctx.r[8].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BB070: 7F084840  cmplw cr6, r8, r9
	ctx.cr[6].compare_u32(ctx.r[8].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BB074: 40990010  ble cr6, 0x832bb084
	if !ctx.cr[6].gt {
	pc = 0x832BB084; continue 'dispatch;
	}
	// 832BB078: 39600001  li r11, 1
	ctx.r[11].s64 = 1;
	// 832BB07C: 907F0328  stw r3, 0x328(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(808 as u32), ctx.r[3].u32 ) };
	// 832BB080: 917F032C  stw r11, 0x32c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(812 as u32), ctx.r[11].u32 ) };
	// 832BB084: 815F014C  lwz r10, 0x14c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(332 as u32) ) } as u64;
	// 832BB088: 57CB103A  slwi r11, r30, 2
	// 832BB08C: 813F0148  lwz r9, 0x148(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(328 as u32) ) } as u64;
	// 832BB090: 7D6B5214  add r11, r11, r10
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832BB094: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832BB098: 810B0000  lwz r8, 0(r11)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BB09C: 5507003C  rlwinm r7, r8, 0, 0, 0x1e
	ctx.r[7].u64 = ctx.r[8].u32 as u64 & 0xFFFFFFFFu64;
	// 832BB0A0: 90FF0108  stw r7, 0x108(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(264 as u32), ctx.r[7].u32 ) };
	// 832BB0A4: 54E5003E  slwi r5, r7, 0
	// 832BB0A8: 80CB0000  lwz r6, 0(r11)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BB0AC: 54C407FE  clrlwi r4, r6, 0x1f
	ctx.r[4].u64 = ctx.r[6].u32 as u64 & 0x00000001u64;
	// 832BB0B0: 909F010C  stw r4, 0x10c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(268 as u32), ctx.r[4].u32 ) };
	// 832BB0B4: 806B0004  lwz r3, 4(r11)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BB0B8: 546B003C  rlwinm r11, r3, 0, 0, 0x1e
	ctx.r[11].u64 = ctx.r[3].u32 as u64 & 0xFFFFFFFFu64;
	// 832BB0BC: 7CE55850  subf r7, r5, r11
	ctx.r[7].s64 = ctx.r[11].s64 - ctx.r[5].s64;
	// 832BB0C0: 90FF0104  stw r7, 0x104(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(260 as u32), ctx.r[7].u32 ) };
	// 832BB0C4: 419A001C  beq cr6, 0x832bb0e0
	if ctx.cr[6].eq {
	pc = 0x832BB0E0; continue 'dispatch;
	}
	// 832BB0C8: 816A0000  lwz r11, 0(r10)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BB0CC: 556A003C  rlwinm r10, r11, 0, 0, 0x1e
	ctx.r[10].u64 = ctx.r[11].u32 as u64 & 0xFFFFFFFFu64;
	// 832BB0D0: 7D6A4850  subf r11, r10, r9
	ctx.r[11].s64 = ctx.r[9].s64 - ctx.r[10].s64;
	// 832BB0D4: 7D2B2A14  add r9, r11, r5
	ctx.r[9].u64 = ctx.r[11].u64 + ctx.r[5].u64;
	// 832BB0D8: 913F0100  stw r9, 0x100(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(256 as u32), ctx.r[9].u32 ) };
	// 832BB0DC: 4800001C  b 0x832bb0f8
	pc = 0x832BB0F8; continue 'dispatch;
	// 832BB0E0: 80DF0100  lwz r6, 0x100(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(256 as u32) ) } as u64;
	// 832BB0E4: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832BB0E8: 817F0154  lwz r11, 0x154(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(340 as u32) ) } as u64;
	// 832BB0EC: 387F0150  addi r3, r31, 0x150
	ctx.r[3].s64 = ctx.r[31].s64 + 336;
	// 832BB0F0: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832BB0F4: 4E800421  bctrl
	ctx.lr = 0x832BB0F8;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BB0F8: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BB0FC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BB100: 419A000C  beq cr6, 0x832bb10c
	if ctx.cr[6].eq {
	pc = 0x832BB10C; continue 'dispatch;
	}
	// 832BB104: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BB108: 4BFFFBF9  bl 0x832bad00
	ctx.lr = 0x832BB10C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BAD00);
	// 832BB10C: 397E0001  addi r11, r30, 1
	ctx.r[11].s64 = ctx.r[30].s64 + 1;
	// 832BB110: 917F000C  stw r11, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[11].u32 ) };
	// 832BB114: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BB118: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BB11C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BB120: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832BB124: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BB128: 4E800020  blr
	return;
}

pub fn sub_832BB130(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    // ---- function 0x832BB130 size=344
	// 832BB130: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BB134: 4B9EE2CD  bl 0x82ca9400
	ctx.lr = 0x832BB138;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9400);
	// 832BB138: 7FE51851  subf. r31, r5, r3
	ctx.r[31].s64 = ctx.r[3].s64 - ctx.r[5].s64;
	ctx.cr[0].compare_i32(ctx.r[31].s32, 0, &mut ctx.xer);
	// 832BB13C: 3BA00000  li r29, 0
	ctx.r[29].s64 = 0;
	// 832BB140: 3B800000  li r28, 0
	ctx.r[28].s64 = 0;
	// 832BB144: 3BC00001  li r30, 1
	ctx.r[30].s64 = 1;
	// 832BB148: 40800008  bge 0x832bb150
	if !ctx.cr[0].lt {
	pc = 0x832BB150; continue 'dispatch;
	}
	// 832BB14C: 3BE00000  li r31, 0
	ctx.r[31].s64 = 0;
	// 832BB150: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832BB154: 2F1F0000  cmpwi cr6, r31, 0
	ctx.cr[6].compare_i32(ctx.r[31].s32, 0, &mut ctx.xer);
	// 832BB158: 4099005C  ble cr6, 0x832bb1b4
	if !ctx.cr[6].gt {
	pc = 0x832BB1B4; continue 'dispatch;
	}
	// 832BB15C: 54AA103A  slwi r10, r5, 2
	// 832BB160: 7C8B2378  mr r11, r4
	ctx.r[11].u64 = ctx.r[4].u64;
	// 832BB164: 7D0A2214  add r8, r10, r4
	ctx.r[8].u64 = ctx.r[10].u64 + ctx.r[4].u64;
	// 832BB168: 836B0000  lwz r27, 0(r11)
	ctx.r[27].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BB16C: 81480000  lwz r10, 0(r8)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BB170: 834B0004  lwz r26, 4(r11)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BB174: 7F7EF038  and r30, r27, r30
	ctx.r[30].u64 = ctx.r[27].u64 & ctx.r[30].u64;
	// 832BB178: 7D5B5050  subf r10, r27, r10
	ctx.r[10].s64 = ctx.r[10].s64 - ctx.r[27].s64;
	// 832BB17C: 57DE07FE  clrlwi r30, r30, 0x1f
	ctx.r[30].u64 = ctx.r[30].u32 as u64 & 0x00000001u64;
	// 832BB180: 7F1AD840  cmplw cr6, r26, r27
	ctx.cr[6].compare_u32(ctx.r[26].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832BB184: 409900A0  ble cr6, 0x832bb224
	if !ctx.cr[6].gt {
	pc = 0x832BB224; continue 'dispatch;
	}
	// 832BB188: 7F0AE840  cmplw cr6, r10, r29
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[29].u32, &mut ctx.xer);
	// 832BB18C: 4099000C  ble cr6, 0x832bb198
	if !ctx.cr[6].gt {
	pc = 0x832BB198; continue 'dispatch;
	}
	// 832BB190: 7D5D5378  mr r29, r10
	ctx.r[29].u64 = ctx.r[10].u64;
	// 832BB194: 7D3C4B78  mr r28, r9
	ctx.r[28].u64 = ctx.r[9].u64;
	// 832BB198: 39290001  addi r9, r9, 1
	ctx.r[9].s64 = ctx.r[9].s64 + 1;
	// 832BB19C: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832BB1A0: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832BB1A4: 7F09F800  cmpw cr6, r9, r31
	ctx.cr[6].compare_i32(ctx.r[9].s32, ctx.r[31].s32, &mut ctx.xer);
	// 832BB1A8: 4198FFC0  blt cr6, 0x832bb168
	if ctx.cr[6].lt {
	pc = 0x832BB168; continue 'dispatch;
	}
	// 832BB1AC: 2B1D0000  cmplwi cr6, r29, 0
	ctx.cr[6].compare_u32(ctx.r[29].u32, 0 as u32, &mut ctx.xer);
	// 832BB1B0: 409A0028  bne cr6, 0x832bb1d8
	if !ctx.cr[6].eq {
	pc = 0x832BB1D8; continue 'dispatch;
	}
	// 832BB1B4: 546B103A  slwi r11, r3, 2
	// 832BB1B8: 81440000  lwz r10, 0(r4)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BB1BC: 78A90020  clrldi r9, r5, 0x20
	ctx.r[9].u64 = ctx.r[5].u64 & 0x00000000FFFFFFFFu64;
	// 832BB1C0: 78680020  clrldi r8, r3, 0x20
	ctx.r[8].u64 = ctx.r[3].u64 & 0x00000000FFFFFFFFu64;
	// 832BB1C4: 7CAB202E  lwzx r5, r11, r4
	ctx.r[5].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[4].u32)) } as u64;
	// 832BB1C8: 7D6A2850  subf r11, r10, r5
	ctx.r[11].s64 = ctx.r[5].s64 - ctx.r[10].s64;
	// 832BB1CC: 7D4B49D2  mulld r10, r11, r9
	ctx.r[10].s64 = ctx.r[11].s64 * ctx.r[9].s64;
	// 832BB1D0: 7D2A4392  divdu r9, r10, r8
	ctx.r[9].u64 = ctx.r[10].u64 / ctx.r[8].u64;
	// 832BB1D4: 553D003E  slwi r29, r9, 0
	// 832BB1D8: 7FEAFB78  mr r10, r31
	ctx.r[10].u64 = ctx.r[31].u64;
	// 832BB1DC: 7F1F1800  cmpw cr6, r31, r3
	ctx.cr[6].compare_i32(ctx.r[31].s32, ctx.r[3].s32, &mut ctx.xer);
	// 832BB1E0: 40980034  bge cr6, 0x832bb214
	if !ctx.cr[6].lt {
	pc = 0x832BB214; continue 'dispatch;
	}
	// 832BB1E4: 57EB103A  slwi r11, r31, 2
	// 832BB1E8: 7D6B2214  add r11, r11, r4
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[4].u64;
	// 832BB1EC: 812B0000  lwz r9, 0(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BB1F0: 810B0004  lwz r8, 4(r11)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BB1F4: 7D25F038  and r5, r9, r30
	ctx.r[5].u64 = ctx.r[9].u64 & ctx.r[30].u64;
	// 832BB1F8: 7F084840  cmplw cr6, r8, r9
	ctx.cr[6].compare_u32(ctx.r[8].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BB1FC: 54BE07FE  clrlwi r30, r5, 0x1f
	ctx.r[30].u64 = ctx.r[5].u32 as u64 & 0x00000001u64;
	// 832BB200: 40990024  ble cr6, 0x832bb224
	if !ctx.cr[6].gt {
	pc = 0x832BB224; continue 'dispatch;
	}
	// 832BB204: 394A0001  addi r10, r10, 1
	ctx.r[10].s64 = ctx.r[10].s64 + 1;
	// 832BB208: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832BB20C: 7F0A1800  cmpw cr6, r10, r3
	ctx.cr[6].compare_i32(ctx.r[10].s32, ctx.r[3].s32, &mut ctx.xer);
	// 832BB210: 4198FFDC  blt cr6, 0x832bb1ec
	if ctx.cr[6].lt {
	pc = 0x832BB1EC; continue 'dispatch;
	}
	// 832BB214: 93C70000  stw r30, 0(r7)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832BB218: 7FA3EB78  mr r3, r29
	ctx.r[3].u64 = ctx.r[29].u64;
	// 832BB21C: 93860000  stw r28, 0(r6)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[6].u32.wrapping_add(0 as u32), ctx.r[28].u32 ) };
	// 832BB220: 4B9EE230  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832BB224: 3860FFFF  li r3, -1
	ctx.r[3].s64 = -1;
	// 832BB228: 4B9EE228  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
}

pub fn sub_832BB288(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BB288 size=112
	// 832BB288: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BB28C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BB290: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832BB294: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BB298: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BB29C: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BB2A0: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832BB2A4: 397FFDAC  addi r11, r31, -0x254
	ctx.r[11].s64 = ctx.r[31].s64 + -596;
	// 832BB2A8: 386B0150  addi r3, r11, 0x150
	ctx.r[3].s64 = ctx.r[11].s64 + 336;
	// 832BB2AC: 815FFF0C  lwz r10, -0xf4(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(-244 as u32) ) } as u64;
	// 832BB2B0: 7D4903A6  mtctr r10
	ctx.ctr.u64 = ctx.r[10].u64;
	// 832BB2B4: 4E800421  bctrl
	ctx.lr = 0x832BB2B8;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BB2B8: 93DF00F8  stw r30, 0xf8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(248 as u32), ctx.r[30].u32 ) };
	// 832BB2BC: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BB2C0: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BB2C4: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BB2C8: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832BB2CC: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BB2D0: 4E800020  blr
	return;
}

pub fn sub_832BB2F8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BB2F8 size=152
	// 832BB2F8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BB2FC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BB300: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BB304: 35630104  addic. r11, r3, 0x104
	ctx.xer.ca = (ctx.r[3].u32 > (!(260 as u32)));
	ctx.r[11].s64 = ctx.r[3].s64 + 260;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BB308: 41820030  beq 0x832bb338
	if ctx.cr[0].eq {
	pc = 0x832BB338; continue 'dispatch;
	}
	// 832BB30C: 806B0004  lwz r3, 4(r11)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BB310: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BB314: 419A0024  beq cr6, 0x832bb338
	if ctx.cr[6].eq {
	pc = 0x832BB338; continue 'dispatch;
	}
	// 832BB318: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BB31C: 4AEDB6AD  bl 0x821969c8
	ctx.lr = 0x832BB320;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BB320: 7C6B0034  cntlzw r11, r3
	ctx.r[11].u64 = if ctx.r[3].u32 == 0 { 32 } else { ctx.r[3].u32.leading_zeros() as u64 };
	// 832BB324: 5563DFFE  rlwinm r3, r11, 0x1b, 0x1f, 0x1f
	ctx.r[3].u64 = ctx.r[11].u32 as u64 & 0x0000001Fu64;
	// 832BB328: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832BB32C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BB330: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BB334: 4E800020  blr
	return;
	// 832BB338: 38600001  li r3, 1
	ctx.r[3].s64 = 1;
	// 832BB33C: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832BB340: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BB344: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BB348: 4E800020  blr
	return;
}

pub fn sub_832BB390(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BB390 size=296
	// 832BB390: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BB394: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BB398: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832BB39C: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BB3A0: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BB3A4: 8163FDC0  lwz r11, -0x240(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(-576 as u32) ) } as u64;
	// 832BB3A8: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832BB3AC: 3BE3FCAC  addi r31, r3, -0x354
	ctx.r[31].s64 = ctx.r[3].s64 + -852;
	// 832BB3B0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BB3B4: 419A000C  beq cr6, 0x832bb3c0
	if ctx.cr[6].eq {
	pc = 0x832BB3C0; continue 'dispatch;
	}
	// 832BB3B8: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BB3BC: 4BFFF945  bl 0x832bad00
	ctx.lr = 0x832BB3C0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BAD00);
	// 832BB3C0: 93DF0350  stw r30, 0x350(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(848 as u32), ctx.r[30].u32 ) };
	// 832BB3C4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BB3C8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BB3CC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BB3D0: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832BB3D4: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BB3D8: 4E800020  blr
	return;
}

pub fn sub_832BB4B8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BB4B8 size=12
	// 832BB4B8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BB4BC: 4B9EDF19  bl 0x82ca93d4
	ctx.lr = 0x832BB4C0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93D4);
	// 832BB4C0: 9421FAA0  stwu r1, -0x560(r1)
	ea = ctx.r[1].u32.wrapping_add(-1376 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
}

pub fn sub_832BC598(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BC598 size=296
	// 832BC598: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BC59C: 4B9ECE71  bl 0x82ca940c
	ctx.lr = 0x832BC5A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA940C);
	// 832BC5A0: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BC5A4: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BC5A8: 3BC00000  li r30, 0
	ctx.r[30].s64 = 0;
	// 832BC5AC: 817F02C0  lwz r11, 0x2c0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(704 as u32) ) } as u64;
	// 832BC5B0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC5B4: 409A001C  bne cr6, 0x832bc5d0
	if !ctx.cr[6].eq {
	pc = 0x832BC5D0; continue 'dispatch;
	}
	// 832BC5B8: 48004501  bl 0x832c0ab8
	ctx.lr = 0x832BC5BC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BC5BC: 817F02B4  lwz r11, 0x2b4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(692 as u32) ) } as u64;
	// 832BC5C0: 907F02C0  stw r3, 0x2c0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(704 as u32), ctx.r[3].u32 ) };
	// 832BC5C4: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832BC5C8: 93DF0344  stw r30, 0x344(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(836 as u32), ctx.r[30].u32 ) };
	// 832BC5CC: 917F02C4  stw r11, 0x2c4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(708 as u32), ctx.r[11].u32 ) };
	// 832BC5D0: 817F0014  lwz r11, 0x14(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BC5D4: 3BA00001  li r29, 1
	ctx.r[29].s64 = 1;
	// 832BC5D8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC5DC: 419A007C  beq cr6, 0x832bc658
	if ctx.cr[6].eq {
	pc = 0x832BC658; continue 'dispatch;
	}
	// 832BC5E0: 480044D9  bl 0x832c0ab8
	ctx.lr = 0x832BC5E4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BC5E4: 817F02C4  lwz r11, 0x2c4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(708 as u32) ) } as u64;
	// 832BC5E8: 815F02B4  lwz r10, 0x2b4(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(692 as u32) ) } as u64;
	// 832BC5EC: 813F0018  lwz r9, 0x18(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BC5F0: 7D0B5050  subf r8, r11, r10
	ctx.r[8].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832BC5F4: 80DF0118  lwz r6, 0x118(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BC5F8: 80FF0014  lwz r7, 0x14(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BC5FC: 79050020  clrldi r5, r8, 0x20
	ctx.r[5].u64 = ctx.r[8].u64 & 0x00000000FFFFFFFFu64;
	// 832BC600: 809F02C0  lwz r4, 0x2c0(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(704 as u32) ) } as u64;
	// 832BC604: 7D6549D2  mulld r11, r5, r9
	ctx.r[11].s64 = ctx.r[5].s64 * ctx.r[9].s64;
	// 832BC608: 81260040  lwz r9, 0x40(r6)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(64 as u32) ) } as u64;
	// 832BC60C: 1D0B03E8  mulli r8, r11, 0x3e8
	ctx.r[8].s64 = ctx.r[11].s64 * 1000;
	// 832BC610: 7CE83B92  divdu r7, r8, r7
	ctx.r[7].u64 = ctx.r[8].u64 / ctx.r[7].u64;
	// 832BC614: 54E6003E  slwi r6, r7, 0
	// 832BC618: 7CA649D2  mulld r5, r6, r9
	ctx.r[5].s64 = ctx.r[6].s64 * ctx.r[9].s64;
	// 832BC61C: 78AB8402  rldicl r11, r5, 0x30, 0x10
	ctx.r[11].u64 = ctx.r[5].u64 & 0x000000000000FFFFu64;
	// 832BC620: 5569003E  slwi r9, r11, 0
	// 832BC624: 7D091850  subf r8, r9, r3
	ctx.r[8].s64 = ctx.r[3].s64 - ctx.r[9].s64;
	// 832BC628: 7D644051  subf. r11, r4, r8
	ctx.r[11].s64 = ctx.r[8].s64 - ctx.r[4].s64;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BC62C: 4180002C  blt 0x832bc658
	if ctx.cr[0].lt {
	pc = 0x832BC658; continue 'dispatch;
	}
	// 832BC630: 813F02C8  lwz r9, 0x2c8(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(712 as u32) ) } as u64;
	// 832BC634: 7F0B4800  cmpw cr6, r11, r9
	ctx.cr[6].compare_i32(ctx.r[11].s32, ctx.r[9].s32, &mut ctx.xer);
	// 832BC638: 40990020  ble cr6, 0x832bc658
	if !ctx.cr[6].gt {
	pc = 0x832BC658; continue 'dispatch;
	}
	// 832BC63C: 813F0114  lwz r9, 0x114(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BC640: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832BC644: 409A0024  bne cr6, 0x832bc668
	if !ctx.cr[6].eq {
	pc = 0x832BC668; continue 'dispatch;
	}
	// 832BC648: 396AFFFF  addi r11, r10, -1
	ctx.r[11].s64 = ctx.r[10].s64 + -1;
	// 832BC64C: 907F02C0  stw r3, 0x2c0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(704 as u32), ctx.r[3].u32 ) };
	// 832BC650: 93DF0344  stw r30, 0x344(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(836 as u32), ctx.r[30].u32 ) };
	// 832BC654: 917F02C4  stw r11, 0x2c4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(708 as u32), ctx.r[11].u32 ) };
	// 832BC658: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BC65C: 93BF0334  stw r29, 0x334(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(820 as u32), ctx.r[29].u32 ) };
	// 832BC660: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BC664: 4B9ECDF8  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
	// 832BC668: 2F0B02D5  cmpwi cr6, r11, 0x2d5
	ctx.cr[6].compare_i32(ctx.r[11].s32, 725, &mut ctx.xer);
	// 832BC66C: 40990008  ble cr6, 0x832bc674
	if !ctx.cr[6].gt {
	pc = 0x832BC674; continue 'dispatch;
	}
	// 832BC670: 93BF0338  stw r29, 0x338(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(824 as u32), ctx.r[29].u32 ) };
	// 832BC674: 817F0340  lwz r11, 0x340(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(832 as u32) ) } as u64;
	// 832BC678: 2B0B0003  cmplwi cr6, r11, 3
	ctx.cr[6].compare_u32(ctx.r[11].u32, 3 as u32, &mut ctx.xer);
	// 832BC67C: 41980018  blt cr6, 0x832bc694
	if ctx.cr[6].lt {
	pc = 0x832BC694; continue 'dispatch;
	}
	// 832BC680: 93DF0340  stw r30, 0x340(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(832 as u32), ctx.r[30].u32 ) };
	// 832BC684: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BC688: 93BF0334  stw r29, 0x334(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(820 as u32), ctx.r[29].u32 ) };
	// 832BC68C: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BC690: 4B9ECDCC  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
	// 832BC694: 815F033C  lwz r10, 0x33c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(828 as u32) ) } as u64;
	// 832BC698: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832BC69C: 39200002  li r9, 2
	ctx.r[9].s64 = 2;
	// 832BC6A0: 93BF0110  stw r29, 0x110(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(272 as u32), ctx.r[29].u32 ) };
	// 832BC6A4: 390A0001  addi r8, r10, 1
	ctx.r[8].s64 = ctx.r[10].s64 + 1;
	// 832BC6A8: 917F0340  stw r11, 0x340(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(832 as u32), ctx.r[11].u32 ) };
	// 832BC6AC: 913F0334  stw r9, 0x334(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(820 as u32), ctx.r[9].u32 ) };
	// 832BC6B0: 38600001  li r3, 1
	ctx.r[3].s64 = 1;
	// 832BC6B4: 911F033C  stw r8, 0x33c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(828 as u32), ctx.r[8].u32 ) };
	// 832BC6B8: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BC6BC: 4B9ECDA0  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
}

pub fn sub_832BC6C0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BC6C0 size=304
	// 832BC6C0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BC6C4: 4B9ECD49  bl 0x82ca940c
	ctx.lr = 0x832BC6C8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA940C);
	// 832BC6C8: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BC6CC: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BC6D0: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BC6D4: 409A0010  bne cr6, 0x832bc6e4
	if !ctx.cr[6].eq {
	pc = 0x832BC6E4; continue 'dispatch;
	}
	// 832BC6D8: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BC6DC: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BC6E0: 4B9ECD7C  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
	// 832BC6E4: 817F0320  lwz r11, 0x320(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(800 as u32) ) } as u64;
	// 832BC6E8: 815F000C  lwz r10, 0xc(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BC6EC: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BC6F0: 419AFFE8  beq cr6, 0x832bc6d8
	if ctx.cr[6].eq {
	pc = 0x832BC6D8; continue 'dispatch;
	}
	// 832BC6F4: 817F0170  lwz r11, 0x170(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(368 as u32) ) } as u64;
	// 832BC6F8: 3BA00001  li r29, 1
	ctx.r[29].s64 = 1;
	// 832BC6FC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC700: 419A0008  beq cr6, 0x832bc708
	if ctx.cr[6].eq {
	pc = 0x832BC708; continue 'dispatch;
	}
	// 832BC704: 93BF001C  stw r29, 0x1c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(28 as u32), ctx.r[29].u32 ) };
	// 832BC708: 817F001C  lwz r11, 0x1c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(28 as u32) ) } as u64;
	// 832BC70C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC710: 409AFFC8  bne cr6, 0x832bc6d8
	if !ctx.cr[6].eq {
	pc = 0x832BC6D8; continue 'dispatch;
	}
	// 832BC714: 817F02BC  lwz r11, 0x2bc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(700 as u32) ) } as u64;
	// 832BC718: 3BC00000  li r30, 0
	ctx.r[30].s64 = 0;
	// 832BC71C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC720: 419A0020  beq cr6, 0x832bc740
	if ctx.cr[6].eq {
	pc = 0x832BC740; continue 'dispatch;
	}
	// 832BC724: 2F0BFFFF  cmpwi cr6, r11, -1
	ctx.cr[6].compare_i32(ctx.r[11].s32, -1, &mut ctx.xer);
	// 832BC728: 419A0018  beq cr6, 0x832bc740
	if ctx.cr[6].eq {
	pc = 0x832BC740; continue 'dispatch;
	}
	// 832BC72C: 815F02E8  lwz r10, 0x2e8(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(744 as u32) ) } as u64;
	// 832BC730: 7D6B2050  subf r11, r11, r4
	ctx.r[11].s64 = ctx.r[4].s64 - ctx.r[11].s64;
	// 832BC734: 93DF02BC  stw r30, 0x2bc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(700 as u32), ctx.r[30].u32 ) };
	// 832BC738: 7D6B5214  add r11, r11, r10
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832BC73C: 917F02E8  stw r11, 0x2e8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(744 as u32), ctx.r[11].u32 ) };
	// 832BC740: 817F032C  lwz r11, 0x32c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(812 as u32) ) } as u64;
	// 832BC744: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC748: 419A0034  beq cr6, 0x832bc77c
	if ctx.cr[6].eq {
	pc = 0x832BC77C; continue 'dispatch;
	}
	// 832BC74C: 817F00F8  lwz r11, 0xf8(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(248 as u32) ) } as u64;
	// 832BC750: 93DF032C  stw r30, 0x32c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(812 as u32), ctx.r[30].u32 ) };
	// 832BC754: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC758: 419A001C  beq cr6, 0x832bc774
	if ctx.cr[6].eq {
	pc = 0x832BC774; continue 'dispatch;
	}
	// 832BC75C: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BC760: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BC764: 4800136D  bl 0x832bdad0
	ctx.lr = 0x832BC768;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BDAD0);
	// 832BC768: 38800001  li r4, 1
	ctx.r[4].s64 = 1;
	// 832BC76C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BC770: 48001361  bl 0x832bdad0
	ctx.lr = 0x832BC774;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BDAD0);
	// 832BC774: 93DF02C0  stw r30, 0x2c0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(704 as u32), ctx.r[30].u32 ) };
	// 832BC778: 93DF0344  stw r30, 0x344(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(836 as u32), ctx.r[30].u32 ) };
	// 832BC77C: 817F00EC  lwz r11, 0xec(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(236 as u32) ) } as u64;
	// 832BC780: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BC784: 419A000C  beq cr6, 0x832bc790
	if ctx.cr[6].eq {
	pc = 0x832BC790; continue 'dispatch;
	}
	// 832BC788: 93DF02C0  stw r30, 0x2c0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(704 as u32), ctx.r[30].u32 ) };
	// 832BC78C: 93DF0344  stw r30, 0x344(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(836 as u32), ctx.r[30].u32 ) };
	// 832BC790: 817F00FC  lwz r11, 0xfc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(252 as u32) ) } as u64;
	// 832BC794: 93DF0334  stw r30, 0x334(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(820 as u32), ctx.r[30].u32 ) };
	// 832BC798: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC79C: 93BF017C  stw r29, 0x17c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(380 as u32), ctx.r[29].u32 ) };
	// 832BC7A0: 419A0034  beq cr6, 0x832bc7d4
	if ctx.cr[6].eq {
	pc = 0x832BC7D4; continue 'dispatch;
	}
	// 832BC7A4: 817F00BC  lwz r11, 0xbc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(188 as u32) ) } as u64;
	// 832BC7A8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC7AC: 419A0028  beq cr6, 0x832bc7d4
	if ctx.cr[6].eq {
	pc = 0x832BC7D4; continue 'dispatch;
	}
	// 832BC7B0: 817F0110  lwz r11, 0x110(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(272 as u32) ) } as u64;
	// 832BC7B4: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC7B8: 409A001C  bne cr6, 0x832bc7d4
	if !ctx.cr[6].eq {
	pc = 0x832BC7D4; continue 'dispatch;
	}
	// 832BC7BC: 807F00CC  lwz r3, 0xcc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(204 as u32) ) } as u64;
	// 832BC7C0: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BC7C4: 419A0010  beq cr6, 0x832bc7d4
	if ctx.cr[6].eq {
	pc = 0x832BC7D4; continue 'dispatch;
	}
	// 832BC7C8: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BC7CC: 80BF00C8  lwz r5, 0xc8(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(200 as u32) ) } as u64;
	// 832BC7D0: 4B9ED1E1  bl 0x82ca99b0
	ctx.lr = 0x832BC7D4;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BC7D4: 817F00CC  lwz r11, 0xcc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(204 as u32) ) } as u64;
	// 832BC7D8: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BC7DC: 93DF0144  stw r30, 0x144(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(324 as u32), ctx.r[30].u32 ) };
	// 832BC7E0: 917F00D0  stw r11, 0xd0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(208 as u32), ctx.r[11].u32 ) };
	// 832BC7E4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BC7E8: 4B9ECC74  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
}

pub fn sub_832BC7F0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BC7F0 size=496
	// 832BC7F0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BC7F4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BC7F8: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832BC7FC: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BC800: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BC804: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BC808: 813F02FC  lwz r9, 0x2fc(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(764 as u32) ) } as u64;
	// 832BC80C: 3569FFFF  addic. r11, r9, -1
	ctx.xer.ca = (ctx.r[9].u32 > (!(-1 as u32)));
	ctx.r[11].s64 = ctx.r[9].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BC810: 917F02FC  stw r11, 0x2fc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(764 as u32), ctx.r[11].u32 ) };
	// 832BC814: 40800010  bge 0x832bc824
	if !ctx.cr[0].lt {
	pc = 0x832BC824; continue 'dispatch;
	}
	// 832BC818: 817F02F8  lwz r11, 0x2f8(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(760 as u32) ) } as u64;
	// 832BC81C: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832BC820: 917F02FC  stw r11, 0x2fc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(764 as u32), ctx.r[11].u32 ) };
	// 832BC824: 817F0324  lwz r11, 0x324(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(804 as u32) ) } as u64;
	// 832BC828: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC82C: 419A0048  beq cr6, 0x832bc874
	if ctx.cr[6].eq {
	pc = 0x832BC874; continue 'dispatch;
	}
	// 832BC830: 815F02CC  lwz r10, 0x2cc(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(716 as u32) ) } as u64;
	// 832BC834: 7D6B2050  subf r11, r11, r4
	ctx.r[11].s64 = ctx.r[4].s64 - ctx.r[11].s64;
	// 832BC838: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BC83C: 40990020  ble cr6, 0x832bc85c
	if !ctx.cr[6].gt {
	pc = 0x832BC85C; continue 'dispatch;
	}
	// 832BC840: 811F02D0  lwz r8, 0x2d0(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(720 as u32) ) } as u64;
	// 832BC844: 80FF0320  lwz r7, 0x320(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(800 as u32) ) } as u64;
	// 832BC848: 915F02D4  stw r10, 0x2d4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(724 as u32), ctx.r[10].u32 ) };
	// 832BC84C: 917F02CC  stw r11, 0x2cc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(716 as u32), ctx.r[11].u32 ) };
	// 832BC850: 911F02D8  stw r8, 0x2d8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(728 as u32), ctx.r[8].u32 ) };
	// 832BC854: 90FF02D0  stw r7, 0x2d0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(720 as u32), ctx.r[7].u32 ) };
	// 832BC858: 4800001C  b 0x832bc874
	pc = 0x832BC874; continue 'dispatch;
	// 832BC85C: 815F02D4  lwz r10, 0x2d4(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(724 as u32) ) } as u64;
	// 832BC860: 7F045040  cmplw cr6, r4, r10
	ctx.cr[6].compare_u32(ctx.r[4].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BC864: 40990010  ble cr6, 0x832bc874
	if !ctx.cr[6].gt {
	pc = 0x832BC874; continue 'dispatch;
	}
	// 832BC868: 815F0320  lwz r10, 0x320(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(800 as u32) ) } as u64;
	// 832BC86C: 917F02D4  stw r11, 0x2d4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(724 as u32), ctx.r[11].u32 ) };
	// 832BC870: 915F02D8  stw r10, 0x2d8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(728 as u32), ctx.r[10].u32 ) };
	// 832BC874: 817F02FC  lwz r11, 0x2fc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(764 as u32) ) } as u64;
	// 832BC878: 3BC00000  li r30, 0
	ctx.r[30].s64 = 0;
	// 832BC87C: 815F0300  lwz r10, 0x300(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(768 as u32) ) } as u64;
	// 832BC880: 5568103A  slwi r8, r11, 2
	// 832BC884: 909F0324  stw r4, 0x324(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(804 as u32), ctx.r[4].u32 ) };
	// 832BC888: 7C88512E  stwx r4, r8, r10
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[8].u32.wrapping_add(ctx.r[10].u32), ctx.r[4].u32) };
	// 832BC88C: 80FF0308  lwz r7, 0x308(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(776 as u32) ) } as u64;
	// 832BC890: 80DF02E0  lwz r6, 0x2e0(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(736 as u32) ) } as u64;
	// 832BC894: 80BF02FC  lwz r5, 0x2fc(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(764 as u32) ) } as u64;
	// 832BC898: 54A3103A  slwi r3, r5, 2
	// 832BC89C: 7CC7192E  stwx r6, r7, r3
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[7].u32.wrapping_add(ctx.r[3].u32), ctx.r[6].u32) };
	// 832BC8A0: 817F0304  lwz r11, 0x304(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(772 as u32) ) } as u64;
	// 832BC8A4: 815F02E4  lwz r10, 0x2e4(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(740 as u32) ) } as u64;
	// 832BC8A8: 811F02FC  lwz r8, 0x2fc(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(764 as u32) ) } as u64;
	// 832BC8AC: 5507103A  slwi r7, r8, 2
	// 832BC8B0: 7D4B392E  stwx r10, r11, r7
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[11].u32.wrapping_add(ctx.r[7].u32), ctx.r[10].u32) };
	// 832BC8B4: 80DF030C  lwz r6, 0x30c(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(780 as u32) ) } as u64;
	// 832BC8B8: 80BF02E8  lwz r5, 0x2e8(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(744 as u32) ) } as u64;
	// 832BC8BC: 807F02FC  lwz r3, 0x2fc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(764 as u32) ) } as u64;
	// 832BC8C0: 546B103A  slwi r11, r3, 2
	// 832BC8C4: 7CA6592E  stwx r5, r6, r11
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[6].u32.wrapping_add(ctx.r[11].u32), ctx.r[5].u32) };
	// 832BC8C8: 815F0184  lwz r10, 0x184(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(388 as u32) ) } as u64;
	// 832BC8CC: 811F0310  lwz r8, 0x310(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(784 as u32) ) } as u64;
	// 832BC8D0: 80FF02FC  lwz r7, 0x2fc(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(764 as u32) ) } as u64;
	// 832BC8D4: 54E6103A  slwi r6, r7, 2
	// 832BC8D8: 7D48312E  stwx r10, r8, r6
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[8].u32.wrapping_add(ctx.r[6].u32), ctx.r[10].u32) };
	// 832BC8DC: 80BF0188  lwz r5, 0x188(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(392 as u32) ) } as u64;
	// 832BC8E0: 807F0314  lwz r3, 0x314(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(788 as u32) ) } as u64;
	// 832BC8E4: 817F02FC  lwz r11, 0x2fc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(764 as u32) ) } as u64;
	// 832BC8E8: 556A103A  slwi r10, r11, 2
	// 832BC8EC: 7CA3512E  stwx r5, r3, r10
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[3].u32.wrapping_add(ctx.r[10].u32), ctx.r[5].u32) };
	// 832BC8F0: 811F0318  lwz r8, 0x318(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(792 as u32) ) } as u64;
	// 832BC8F4: 80FF018C  lwz r7, 0x18c(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(396 as u32) ) } as u64;
	// 832BC8F8: 80DF02FC  lwz r6, 0x2fc(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(764 as u32) ) } as u64;
	// 832BC8FC: 54C5103A  slwi r5, r6, 2
	// 832BC900: 7CE8292E  stwx r7, r8, r5
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[8].u32.wrapping_add(ctx.r[5].u32), ctx.r[7].u32) };
	// 832BC904: 807F02B8  lwz r3, 0x2b8(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(696 as u32) ) } as u64;
	// 832BC908: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BC90C: 409A0038  bne cr6, 0x832bc944
	if !ctx.cr[6].eq {
	pc = 0x832BC944; continue 'dispatch;
	}
	// 832BC910: 817F02F4  lwz r11, 0x2f4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(756 as u32) ) } as u64;
	// 832BC914: 552A103A  slwi r10, r9, 2
	// 832BC918: 813F02F0  lwz r9, 0x2f0(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(752 as u32) ) } as u64;
	// 832BC91C: 1D0B03E8  mulli r8, r11, 0x3e8
	ctx.r[8].s64 = ctx.r[11].s64 * 1000;
	// 832BC920: 80FF0300  lwz r7, 0x300(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(768 as u32) ) } as u64;
	// 832BC924: 7CC84B92  divdu r6, r8, r9
	ctx.r[6].u64 = ctx.r[8].u64 / ctx.r[9].u64;
	// 832BC928: 54C5003E  slwi r5, r6, 0
	// 832BC92C: 7C652050  subf r3, r5, r4
	ctx.r[3].s64 = ctx.r[4].s64 - ctx.r[5].s64;
	// 832BC930: 7C6A392E  stwx r3, r10, r7
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[10].u32.wrapping_add(ctx.r[7].u32), ctx.r[3].u32) };
	// 832BC934: 93DF018C  stw r30, 0x18c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(396 as u32), ctx.r[30].u32 ) };
	// 832BC938: 909F02B8  stw r4, 0x2b8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(696 as u32), ctx.r[4].u32 ) };
	// 832BC93C: 93DF0188  stw r30, 0x188(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(392 as u32), ctx.r[30].u32 ) };
	// 832BC940: 48000018  b 0x832bc958
	pc = 0x832BC958; continue 'dispatch;
	// 832BC944: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BC948: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC94C: 419A000C  beq cr6, 0x832bc958
	if ctx.cr[6].eq {
	pc = 0x832BC958; continue 'dispatch;
	}
	// 832BC950: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BC954: 4BFFE3AD  bl 0x832bad00
	ctx.lr = 0x832BC958;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BAD00);
	// 832BC958: 817F00D0  lwz r11, 0xd0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(208 as u32) ) } as u64;
	// 832BC95C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BC960: 917F00D4  stw r11, 0xd4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(212 as u32), ctx.r[11].u32 ) };
	// 832BC964: 419A000C  beq cr6, 0x832bc970
	if ctx.cr[6].eq {
	pc = 0x832BC970; continue 'dispatch;
	}
	// 832BC968: 3960FFFF  li r11, -1
	ctx.r[11].s64 = -1;
	// 832BC96C: 917F00B8  stw r11, 0xb8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(184 as u32), ctx.r[11].u32 ) };
	// 832BC970: 817F02B4  lwz r11, 0x2b4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(692 as u32) ) } as u64;
	// 832BC974: 815F02C0  lwz r10, 0x2c0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(704 as u32) ) } as u64;
	// 832BC978: 392B0001  addi r9, r11, 1
	ctx.r[9].s64 = ctx.r[11].s64 + 1;
	// 832BC97C: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BC980: 913F02B4  stw r9, 0x2b4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(692 as u32), ctx.r[9].u32 ) };
	// 832BC984: 409A001C  bne cr6, 0x832bc9a0
	if !ctx.cr[6].eq {
	pc = 0x832BC9A0; continue 'dispatch;
	}
	// 832BC988: 48004131  bl 0x832c0ab8
	ctx.lr = 0x832BC98C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BC98C: 817F02B4  lwz r11, 0x2b4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(692 as u32) ) } as u64;
	// 832BC990: 907F02C0  stw r3, 0x2c0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(704 as u32), ctx.r[3].u32 ) };
	// 832BC994: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832BC998: 93DF0344  stw r30, 0x344(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(836 as u32), ctx.r[30].u32 ) };
	// 832BC99C: 917F02C4  stw r11, 0x2c4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(708 as u32), ctx.r[11].u32 ) };
	// 832BC9A0: 817F000C  lwz r11, 0xc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BC9A4: 3940FFFF  li r10, -1
	ctx.r[10].s64 = -1;
	// 832BC9A8: 813F0144  lwz r9, 0x144(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(324 as u32) ) } as u64;
	// 832BC9AC: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BC9B0: 915F02BC  stw r10, 0x2bc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(700 as u32), ctx.r[10].u32 ) };
	// 832BC9B4: 93DF017C  stw r30, 0x17c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(380 as u32), ctx.r[30].u32 ) };
	// 832BC9B8: 917F0320  stw r11, 0x320(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(800 as u32), ctx.r[11].u32 ) };
	// 832BC9BC: 917F0010  stw r11, 0x10(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(16 as u32), ctx.r[11].u32 ) };
	// 832BC9C0: 913F0034  stw r9, 0x34(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(52 as u32), ctx.r[9].u32 ) };
	// 832BC9C4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BC9C8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BC9CC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BC9D0: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832BC9D4: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BC9D8: 4E800020  blr
	return;
}

pub fn sub_832BC9E0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BC9E0 size=336
	// 832BC9E0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BC9E4: 4B9ECA0D  bl 0x82ca93f0
	ctx.lr = 0x832BC9E8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93F0);
	// 832BC9E8: 9421FF40  stwu r1, -0xc0(r1)
	ea = ctx.r[1].u32.wrapping_add(-192 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BC9EC: 7CCB3378  mr r11, r6
	ctx.r[11].u64 = ctx.r[6].u64;
	// 832BC9F0: 7CF63B78  mr r22, r7
	ctx.r[22].u64 = ctx.r[7].u64;
	// 832BC9F4: 38CB0004  addi r6, r11, 4
	ctx.r[6].s64 = ctx.r[11].s64 + 4;
	// 832BC9F8: 7D174378  mr r23, r8
	ctx.r[23].u64 = ctx.r[8].u64;
	// 832BC9FC: 90C10058  stw r6, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[6].u32 ) };
	// 832BCA00: 7F262A14  add r25, r6, r5
	ctx.r[25].u64 = ctx.r[6].u64 + ctx.r[5].u64;
	// 832BCA04: 836B0000  lwz r27, 0(r11)
	ctx.r[27].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BCA08: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832BCA0C: 419A0118  beq cr6, 0x832bcb24
	if ctx.cr[6].eq {
	pc = 0x832BCB24; continue 'dispatch;
	}
	// 832BCA10: 81430118  lwz r10, 0x118(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BCA14: 1D640184  mulli r11, r4, 0x184
	ctx.r[11].s64 = ctx.r[4].s64 * 388;
	// 832BCA18: 7D2B502E  lwzx r9, r11, r10
	ctx.r[9].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[10].u32)) } as u64;
	// 832BCA1C: 7FAB5214  add r29, r11, r10
	ctx.r[29].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832BCA20: 811D0018  lwz r8, 0x18(r29)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BCA24: 831D0014  lwz r24, 0x14(r29)
	ctx.r[24].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BCA28: 7F484851  subf. r26, r8, r9
	ctx.r[26].s64 = ctx.r[9].s64 - ctx.r[8].s64;
	ctx.cr[0].compare_i32(ctx.r[26].s32, 0, &mut ctx.xer);
	// 832BCA2C: 40800014  bge 0x832bca40
	if !ctx.cr[0].lt {
	pc = 0x832BCA40; continue 'dispatch;
	}
	// 832BCA30: 817D0008  lwz r11, 8(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BCA34: 7F4BD214  add r26, r11, r26
	ctx.r[26].u64 = ctx.r[11].u64 + ctx.r[26].u64;
	// 832BCA38: 48000008  b 0x832bca40
	pc = 0x832BCA40; continue 'dispatch;
	// 832BCA3C: 80C10058  lwz r6, 0x58(r1)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) } as u64;
	// 832BCA40: 7F06B840  cmplw cr6, r6, r23
	ctx.cr[6].compare_u32(ctx.r[6].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BCA44: 419900E0  bgt cr6, 0x832bcb24
	if ctx.cr[6].gt {
	pc = 0x832BCB24; continue 'dispatch;
	}
	// 832BCA48: 7F06B040  cmplw cr6, r6, r22
	ctx.cr[6].compare_u32(ctx.r[6].u32, ctx.r[22].u32, &mut ctx.xer);
	// 832BCA4C: 419800D8  blt cr6, 0x832bcb24
	if ctx.cr[6].lt {
	pc = 0x832BCB24; continue 'dispatch;
	}
	// 832BCA50: 7F19B840  cmplw cr6, r25, r23
	ctx.cr[6].compare_u32(ctx.r[25].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BCA54: 419900D0  bgt cr6, 0x832bcb24
	if ctx.cr[6].gt {
	pc = 0x832BCB24; continue 'dispatch;
	}
	// 832BCA58: 817D0008  lwz r11, 8(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BCA5C: 7D5A5850  subf r10, r26, r11
	ctx.r[10].s64 = ctx.r[11].s64 - ctx.r[26].s64;
	// 832BCA60: 555E0036  rlwinm r30, r10, 0, 0, 0x1b
	ctx.r[30].u64 = ctx.r[10].u32 as u64 & 0xFFFFFFFFu64;
	// 832BCA64: 2B1E0010  cmplwi cr6, r30, 0x10
	ctx.cr[6].compare_u32(ctx.r[30].u32, 16 as u32, &mut ctx.xer);
	// 832BCA68: 40990008  ble cr6, 0x832bca70
	if !ctx.cr[6].gt {
	pc = 0x832BCA70; continue 'dispatch;
	}
	// 832BCA6C: 3BDEFFF0  addi r30, r30, -0x10
	ctx.r[30].s64 = ctx.r[30].s64 + -16;
	// 832BCA70: 7F28CB78  mr r8, r25
	ctx.r[8].u64 = ctx.r[25].u64;
	// 832BCA74: 38E10058  addi r7, r1, 0x58
	ctx.r[7].s64 = ctx.r[1].s64 + 88;
	// 832BCA78: 38A10050  addi r5, r1, 0x50
	ctx.r[5].s64 = ctx.r[1].s64 + 80;
	// 832BCA7C: 38810054  addi r4, r1, 0x54
	ctx.r[4].s64 = ctx.r[1].s64 + 84;
	// 832BCA80: 7F03C378  mr r3, r24
	ctx.r[3].u64 = ctx.r[24].u64;
	// 832BCA84: 48003445  bl 0x832bfec8
	ctx.lr = 0x832BCA88;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BFEC8);
	// 832BCA88: 83E10050  lwz r31, 0x50(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BCA8C: 7F1FD840  cmplw cr6, r31, r27
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832BCA90: 4099000C  ble cr6, 0x832bca9c
	if !ctx.cr[6].gt {
	pc = 0x832BCA9C; continue 'dispatch;
	}
	// 832BCA94: 7F7FDB78  mr r31, r27
	ctx.r[31].u64 = ctx.r[27].u64;
	// 832BCA98: 93E10050  stw r31, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[31].u32 ) };
	// 832BCA9C: 7F7FD850  subf r27, r31, r27
	ctx.r[27].s64 = ctx.r[27].s64 - ctx.r[31].s64;
	// 832BCAA0: 7F1FF040  cmplw cr6, r31, r30
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832BCAA4: 4099000C  ble cr6, 0x832bcab0
	if !ctx.cr[6].gt {
	pc = 0x832BCAB0; continue 'dispatch;
	}
	// 832BCAA8: 7FDFF378  mr r31, r30
	ctx.r[31].u64 = ctx.r[30].u64;
	// 832BCAAC: 93E10050  stw r31, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[31].u32 ) };
	// 832BCAB0: 807D0000  lwz r3, 0(r29)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BCAB4: 817D0010  lwz r11, 0x10(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(16 as u32) ) } as u64;
	// 832BCAB8: 7FC35850  subf r30, r3, r11
	ctx.r[30].s64 = ctx.r[11].s64 - ctx.r[3].s64;
	// 832BCABC: 7F1EF840  cmplw cr6, r30, r31
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[31].u32, &mut ctx.xer);
	// 832BCAC0: 40980044  bge cr6, 0x832bcb04
	if !ctx.cr[6].lt {
	pc = 0x832BCB04; continue 'dispatch;
	}
	// 832BCAC4: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832BCAC8: 419A0024  beq cr6, 0x832bcaec
	if ctx.cr[6].eq {
	pc = 0x832BCAEC; continue 'dispatch;
	}
	// 832BCACC: 83810054  lwz r28, 0x54(r1)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BCAD0: 7FC5F378  mr r5, r30
	ctx.r[5].u64 = ctx.r[30].u64;
	// 832BCAD4: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832BCAD8: 4B9EC9A9  bl 0x82ca9480
	ctx.lr = 0x832BCADC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832BCADC: 7FFEF850  subf r31, r30, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[30].s64;
	// 832BCAE0: 7D7CF214  add r11, r28, r30
	ctx.r[11].u64 = ctx.r[28].u64 + ctx.r[30].u64;
	// 832BCAE4: 93E10050  stw r31, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[31].u32 ) };
	// 832BCAE8: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832BCAEC: 7FE5FB78  mr r5, r31
	ctx.r[5].u64 = ctx.r[31].u64;
	// 832BCAF0: 807D000C  lwz r3, 0xc(r29)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BCAF4: 80810054  lwz r4, 0x54(r1)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BCAF8: 4B9EC989  bl 0x82ca9480
	ctx.lr = 0x832BCAFC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832BCAFC: 817D000C  lwz r11, 0xc(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BCB00: 48000014  b 0x832bcb14
	pc = 0x832BCB14; continue 'dispatch;
	// 832BCB04: 7FE5FB78  mr r5, r31
	ctx.r[5].u64 = ctx.r[31].u64;
	// 832BCB08: 80810054  lwz r4, 0x54(r1)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BCB0C: 4B9EC975  bl 0x82ca9480
	ctx.lr = 0x832BCB10;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832BCB10: 817D0000  lwz r11, 0(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BCB14: 7D6BFA14  add r11, r11, r31
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[31].u64;
	// 832BCB18: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832BCB1C: 917D0000  stw r11, 0(r29)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BCB20: 409AFF1C  bne cr6, 0x832bca3c
	if !ctx.cr[6].eq {
	pc = 0x832BCA3C; continue 'dispatch;
	}
	// 832BCB24: 382100C0  addi r1, r1, 0xc0
	ctx.r[1].s64 = ctx.r[1].s64 + 192;
	// 832BCB28: 4B9EC918  b 0x82ca9440
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9440);
	return;
}

pub fn sub_832BCB30(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BCB30 size=696
	// 832BCB30: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BCB34: 4B9EC8B9  bl 0x82ca93ec
	ctx.lr = 0x832BCB38;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93EC);
	// 832BCB38: 9421FF40  stwu r1, -0xc0(r1)
	ea = ctx.r[1].u32.wrapping_add(-192 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BCB3C: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BCB40: 48003F79  bl 0x832c0ab8
	ctx.lr = 0x832BCB44;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BCB44: 7C7B1B78  mr r27, r3
	ctx.r[27].u64 = ctx.r[3].u64;
	// 832BCB48: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BCB4C: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832BCB50: 4BFFFB71  bl 0x832bc6c0
	ctx.lr = 0x832BCB54;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BC6C0);
	// 832BCB54: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BCB58: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BCB5C: 409A000C  bne cr6, 0x832bcb68
	if !ctx.cr[6].eq {
	pc = 0x832BCB68; continue 'dispatch;
	}
	// 832BCB60: 382100C0  addi r1, r1, 0xc0
	ctx.r[1].s64 = ctx.r[1].s64 + 192;
	// 832BCB64: 4B9EC8D8  b 0x82ca943c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA943C);
	return;
	// 832BCB68: 82FF0100  lwz r23, 0x100(r31)
	ctx.r[23].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(256 as u32) ) } as u64;
	// 832BCB6C: 3B200000  li r25, 0
	ctx.r[25].s64 = 0;
	// 832BCB70: 817F0104  lwz r11, 0x104(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(260 as u32) ) } as u64;
	// 832BCB74: 815F00E0  lwz r10, 0xe0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(224 as u32) ) } as u64;
	// 832BCB78: 7EFEBB78  mr r30, r23
	ctx.r[30].u64 = ctx.r[23].u64;
	// 832BCB7C: 82BF00BC  lwz r21, 0xbc(r31)
	ctx.r[21].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(188 as u32) ) } as u64;
	// 832BCB80: 7ECBBA14  add r22, r11, r23
	ctx.r[22].u64 = ctx.r[11].u64 + ctx.r[23].u64;
	// 832BCB84: 839F011C  lwz r28, 0x11c(r31)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(284 as u32) ) } as u64;
	// 832BCB88: 2F0A0000  cmpwi cr6, r10, 0
	ctx.cr[6].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832BCB8C: 40990090  ble cr6, 0x832bcc1c
	if !ctx.cr[6].gt {
	pc = 0x832BCC1C; continue 'dispatch;
	}
	// 832BCB90: 7F1EB040  cmplw cr6, r30, r22
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[22].u32, &mut ctx.xer);
	// 832BCB94: 41990088  bgt cr6, 0x832bcc1c
	if ctx.cr[6].gt {
	pc = 0x832BCC1C; continue 'dispatch;
	}
	// 832BCB98: 7F1EB840  cmplw cr6, r30, r23
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BCB9C: 41980080  blt cr6, 0x832bcc1c
	if ctx.cr[6].lt {
	pc = 0x832BCC1C; continue 'dispatch;
	}
	// 832BCBA0: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BCBA4: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BCBA8: 83BE0000  lwz r29, 0(r30)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BCBAC: 3BDE0004  addi r30, r30, 4
	ctx.r[30].s64 = ctx.r[30].s64 + 4;
	// 832BCBB0: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BCBB4: 40990054  ble cr6, 0x832bcc08
	if !ctx.cr[6].gt {
	pc = 0x832BCC08; continue 'dispatch;
	}
	// 832BCBB8: 7F8BE378  mr r11, r28
	ctx.r[11].u64 = ctx.r[28].u64;
	// 832BCBBC: 814B0000  lwz r10, 0(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BCBC0: 7F0AC800  cmpw cr6, r10, r25
	ctx.cr[6].compare_i32(ctx.r[10].s32, ctx.r[25].s32, &mut ctx.xer);
	// 832BCBC4: 419A001C  beq cr6, 0x832bcbe0
	if ctx.cr[6].eq {
	pc = 0x832BCBE0; continue 'dispatch;
	}
	// 832BCBC8: 815F0114  lwz r10, 0x114(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BCBCC: 38840001  addi r4, r4, 1
	ctx.r[4].s64 = ctx.r[4].s64 + 1;
	// 832BCBD0: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832BCBD4: 7F045000  cmpw cr6, r4, r10
	ctx.cr[6].compare_i32(ctx.r[4].s32, ctx.r[10].s32, &mut ctx.xer);
	// 832BCBD8: 4198FFE4  blt cr6, 0x832bcbbc
	if ctx.cr[6].lt {
	pc = 0x832BCBBC; continue 'dispatch;
	}
	// 832BCBDC: 4800002C  b 0x832bcc08
	pc = 0x832BCC08; continue 'dispatch;
	// 832BCBE0: 2F04FFFF  cmpwi cr6, r4, -1
	ctx.cr[6].compare_i32(ctx.r[4].s32, -1, &mut ctx.xer);
	// 832BCBE4: 419A0024  beq cr6, 0x832bcc08
	if ctx.cr[6].eq {
	pc = 0x832BCC08; continue 'dispatch;
	}
	// 832BCBE8: 2B1D0000  cmplwi cr6, r29, 0
	ctx.cr[6].compare_u32(ctx.r[29].u32, 0 as u32, &mut ctx.xer);
	// 832BCBEC: 419A001C  beq cr6, 0x832bcc08
	if ctx.cr[6].eq {
	pc = 0x832BCC08; continue 'dispatch;
	}
	// 832BCBF0: 7EC8B378  mr r8, r22
	ctx.r[8].u64 = ctx.r[22].u64;
	// 832BCBF4: 7EE7BB78  mr r7, r23
	ctx.r[7].u64 = ctx.r[23].u64;
	// 832BCBF8: 7FC6F378  mr r6, r30
	ctx.r[6].u64 = ctx.r[30].u64;
	// 832BCBFC: 7FA5EB78  mr r5, r29
	ctx.r[5].u64 = ctx.r[29].u64;
	// 832BCC00: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BCC04: 4BFFFDDD  bl 0x832bc9e0
	ctx.lr = 0x832BCC08;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BC9E0);
	// 832BCC08: 817F00E0  lwz r11, 0xe0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(224 as u32) ) } as u64;
	// 832BCC0C: 3B390001  addi r25, r25, 1
	ctx.r[25].s64 = ctx.r[25].s64 + 1;
	// 832BCC10: 7FDDF214  add r30, r29, r30
	ctx.r[30].u64 = ctx.r[29].u64 + ctx.r[30].u64;
	// 832BCC14: 7F195800  cmpw cr6, r25, r11
	ctx.cr[6].compare_i32(ctx.r[25].s32, ctx.r[11].s32, &mut ctx.xer);
	// 832BCC18: 4198FF78  blt cr6, 0x832bcb90
	if ctx.cr[6].lt {
	pc = 0x832BCB90; continue 'dispatch;
	}
	// 832BCC1C: 4AFA9495  bl 0x822660b0
	ctx.lr = 0x832BCC20;
	crate::recompiler::externs::call(&mut ctx, base, 0x822660B0);
	// 832BCC20: 3D608350  lis r11, -0x7cb0
	ctx.r[11].s64 = -2091909120;
	// 832BCC24: 3B8BC200  addi r28, r11, -0x3e00
	ctx.r[28].s64 = ctx.r[11].s64 + -15872;
	// 832BCC28: 816BC200  lwz r11, -0x3e00(r11)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-15872 as u32) ) } as u64;
	// 832BCC2C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCC30: 409A000C  bne cr6, 0x832bcc3c
	if !ctx.cr[6].eq {
	pc = 0x832BCC3C; continue 'dispatch;
	}
	// 832BCC34: 7C6B1B78  mr r11, r3
	ctx.r[11].u64 = ctx.r[3].u64;
	// 832BCC38: 917C0000  stw r11, 0(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BCC3C: 815C0004  lwz r10, 4(r28)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BCC40: 7D6B1850  subf r11, r11, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BCC44: 3F40C000  lis r26, -0x4000
	ctx.r[26].s64 = -1073741824;
	// 832BCC48: 7D2A5850  subf r9, r10, r11
	ctx.r[9].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BCC4C: 7F09D040  cmplw cr6, r9, r26
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[26].u32, &mut ctx.xer);
	// 832BCC50: 4099000C  ble cr6, 0x832bcc5c
	if !ctx.cr[6].gt {
	pc = 0x832BCC5C; continue 'dispatch;
	}
	// 832BCC54: 7D585378  mr r24, r10
	ctx.r[24].u64 = ctx.r[10].u64;
	// 832BCC58: 4800000C  b 0x832bcc64
	pc = 0x832BCC64; continue 'dispatch;
	// 832BCC5C: 7D785B78  mr r24, r11
	ctx.r[24].u64 = ctx.r[11].u64;
	// 832BCC60: 917C0004  stw r11, 4(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832BCC64: 817F00FC  lwz r11, 0xfc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(252 as u32) ) } as u64;
	// 832BCC68: 7F7BC050  subf r27, r27, r24
	ctx.r[27].s64 = ctx.r[24].s64 - ctx.r[27].s64;
	// 832BCC6C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCC70: 419A0100  beq cr6, 0x832bcd70
	if ctx.cr[6].eq {
	pc = 0x832BCD70; continue 'dispatch;
	}
	// 832BCC74: 2B150000  cmplwi cr6, r21, 0
	ctx.cr[6].compare_u32(ctx.r[21].u32, 0 as u32, &mut ctx.xer);
	// 832BCC78: 419A00F8  beq cr6, 0x832bcd70
	if ctx.cr[6].eq {
	pc = 0x832BCD70; continue 'dispatch;
	}
	// 832BCC7C: 817F0148  lwz r11, 0x148(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(328 as u32) ) } as u64;
	// 832BCC80: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCC84: 409A00A8  bne cr6, 0x832bcd2c
	if !ctx.cr[6].eq {
	pc = 0x832BCD2C; continue 'dispatch;
	}
	// 832BCC88: 817F0174  lwz r11, 0x174(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(372 as u32) ) } as u64;
	// 832BCC8C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCC90: 409A009C  bne cr6, 0x832bcd2c
	if !ctx.cr[6].eq {
	pc = 0x832BCD2C; continue 'dispatch;
	}
	// 832BCC94: 817F00E4  lwz r11, 0xe4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(228 as u32) ) } as u64;
	// 832BCC98: 815F0198  lwz r10, 0x198(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(408 as u32) ) } as u64;
	// 832BCC9C: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BCCA0: 41980008  blt cr6, 0x832bcca8
	if ctx.cr[6].lt {
	pc = 0x832BCCA8; continue 'dispatch;
	}
	// 832BCCA4: 817F0198  lwz r11, 0x198(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(408 as u32) ) } as u64;
	// 832BCCA8: 556A083C  slwi r10, r11, 1
	// 832BCCAC: 813F019C  lwz r9, 0x19c(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(412 as u32) ) } as u64;
	// 832BCCB0: 7D0B5214  add r8, r11, r10
	ctx.r[8].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832BCCB4: 5507F0BE  srwi r7, r8, 2
	// 832BCCB8: 7F093840  cmplw cr6, r9, r7
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[7].u32, &mut ctx.xer);
	// 832BCCBC: 40980070  bge cr6, 0x832bcd2c
	if !ctx.cr[6].lt {
	pc = 0x832BCD2C; continue 'dispatch;
	}
	// 832BCCC0: 3BBF0254  addi r29, r31, 0x254
	ctx.r[29].s64 = ctx.r[31].s64 + 596;
	// 832BCCC4: 2B1D0000  cmplwi cr6, r29, 0
	ctx.cr[6].compare_u32(ctx.r[29].u32, 0 as u32, &mut ctx.xer);
	// 832BCCC8: 419A0020  beq cr6, 0x832bcce8
	if ctx.cr[6].eq {
	pc = 0x832BCCE8; continue 'dispatch;
	}
	// 832BCCCC: 807D0004  lwz r3, 4(r29)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BCCD0: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BCCD4: 419A0014  beq cr6, 0x832bcce8
	if ctx.cr[6].eq {
	pc = 0x832BCCE8; continue 'dispatch;
	}
	// 832BCCD8: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BCCDC: 4AED9CED  bl 0x821969c8
	ctx.lr = 0x832BCCE0;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BCCE0: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BCCE4: 409A0030  bne cr6, 0x832bcd14
	if !ctx.cr[6].eq {
	pc = 0x832BCD14; continue 'dispatch;
	}
	// 832BCCE8: 817F0160  lwz r11, 0x160(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(352 as u32) ) } as u64;
	// 832BCCEC: 387F0150  addi r3, r31, 0x150
	ctx.r[3].s64 = ctx.r[31].s64 + 336;
	// 832BCCF0: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832BCCF4: 4E800421  bctrl
	ctx.lr = 0x832BCCF8;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BCCF8: 2B1D0000  cmplwi cr6, r29, 0
	ctx.cr[6].compare_u32(ctx.r[29].u32, 0 as u32, &mut ctx.xer);
	// 832BCCFC: 419A0030  beq cr6, 0x832bcd2c
	if ctx.cr[6].eq {
	pc = 0x832BCD2C; continue 'dispatch;
	}
	// 832BCD00: 807D0004  lwz r3, 4(r29)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BCD04: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BCD08: 419A0024  beq cr6, 0x832bcd2c
	if ctx.cr[6].eq {
	pc = 0x832BCD2C; continue 'dispatch;
	}
	// 832BCD0C: 4BD4ECB5  bl 0x8300b9c0
	ctx.lr = 0x832BCD10;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BCD10: 4800001C  b 0x832bcd2c
	pc = 0x832BCD2C; continue 'dispatch;
	// 832BCD14: 817F0020  lwz r11, 0x20(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BCD18: 556A0108  rlwinm r10, r11, 0, 4, 4
	ctx.r[10].u64 = ctx.r[11].u32 as u64 & 0xFFFFFFFFu64;
	// 832BCD1C: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BCD20: 409A000C  bne cr6, 0x832bcd2c
	if !ctx.cr[6].eq {
	pc = 0x832BCD2C; continue 'dispatch;
	}
	// 832BCD24: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BCD28: 4BA06159  bl 0x82cc2e80
	ctx.lr = 0x832BCD2C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC2E80);
	// 832BCD2C: 833F0020  lwz r25, 0x20(r31)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BCD30: 7F1EB040  cmplw cr6, r30, r22
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[22].u32, &mut ctx.xer);
	// 832BCD34: 4199003C  bgt cr6, 0x832bcd70
	if ctx.cr[6].gt {
	pc = 0x832BCD70; continue 'dispatch;
	}
	// 832BCD38: 7F1EB840  cmplw cr6, r30, r23
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BCD3C: 41980034  blt cr6, 0x832bcd70
	if ctx.cr[6].lt {
	pc = 0x832BCD70; continue 'dispatch;
	}
	// 832BCD40: 817F0024  lwz r11, 0x24(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(36 as u32) ) } as u64;
	// 832BCD44: 7F2ACB78  mr r10, r25
	ctx.r[10].u64 = ctx.r[25].u64;
	// 832BCD48: 393F0120  addi r9, r31, 0x120
	ctx.r[9].s64 = ctx.r[31].s64 + 288;
	// 832BCD4C: 80FF010C  lwz r7, 0x10c(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(268 as u32) ) } as u64;
	// 832BCD50: 7EC8B378  mr r8, r22
	ctx.r[8].u64 = ctx.r[22].u64;
	// 832BCD54: 80BF00C4  lwz r5, 0xc4(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(196 as u32) ) } as u64;
	// 832BCD58: 7FC6F378  mr r6, r30
	ctx.r[6].u64 = ctx.r[30].u64;
	// 832BCD5C: 809F00D0  lwz r4, 0xd0(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(208 as u32) ) } as u64;
	// 832BCD60: 7EA3AB78  mr r3, r21
	ctx.r[3].u64 = ctx.r[21].u64;
	// 832BCD64: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832BCD68: 48007C81  bl 0x832c49e8
	ctx.lr = 0x832BCD6C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C49E8);
	// 832BCD6C: 7C791B78  mr r25, r3
	ctx.r[25].u64 = ctx.r[3].u64;
	// 832BCD70: 817F02E4  lwz r11, 0x2e4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(740 as u32) ) } as u64;
	// 832BCD74: 7D6BDA14  add r11, r11, r27
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[27].u64;
	// 832BCD78: 917F02E4  stw r11, 0x2e4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(740 as u32), ctx.r[11].u32 ) };
	// 832BCD7C: 4AFA9335  bl 0x822660b0
	ctx.lr = 0x832BCD80;
	crate::recompiler::externs::call(&mut ctx, base, 0x822660B0);
	// 832BCD80: 817C0000  lwz r11, 0(r28)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BCD84: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCD88: 409A000C  bne cr6, 0x832bcd94
	if !ctx.cr[6].eq {
	pc = 0x832BCD94; continue 'dispatch;
	}
	// 832BCD8C: 7C6B1B78  mr r11, r3
	ctx.r[11].u64 = ctx.r[3].u64;
	// 832BCD90: 917C0000  stw r11, 0(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BCD94: 815C0004  lwz r10, 4(r28)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BCD98: 7D6B1850  subf r11, r11, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BCD9C: 7D2A5850  subf r9, r10, r11
	ctx.r[9].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BCDA0: 7F09D040  cmplw cr6, r9, r26
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[26].u32, &mut ctx.xer);
	// 832BCDA4: 4199000C  bgt cr6, 0x832bcdb0
	if ctx.cr[6].gt {
	pc = 0x832BCDB0; continue 'dispatch;
	}
	// 832BCDA8: 7D6A5B78  mr r10, r11
	ctx.r[10].u64 = ctx.r[11].u64;
	// 832BCDAC: 917C0004  stw r11, 4(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832BCDB0: 817F02E0  lwz r11, 0x2e0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(736 as u32) ) } as u64;
	// 832BCDB4: 813F0144  lwz r9, 0x144(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(324 as u32) ) } as u64;
	// 832BCDB8: 7D785850  subf r11, r24, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[24].s64;
	// 832BCDBC: 7F194840  cmplw cr6, r25, r9
	ctx.cr[6].compare_u32(ctx.r[25].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BCDC0: 7D0B5214  add r8, r11, r10
	ctx.r[8].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832BCDC4: 911F02E0  stw r8, 0x2e0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(736 as u32), ctx.r[8].u32 ) };
	// 832BCDC8: 40990008  ble cr6, 0x832bcdd0
	if !ctx.cr[6].gt {
	pc = 0x832BCDD0; continue 'dispatch;
	}
	// 832BCDCC: 933F0144  stw r25, 0x144(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(324 as u32), ctx.r[25].u32 ) };
	// 832BCDD0: 7D445378  mr r4, r10
	ctx.r[4].u64 = ctx.r[10].u64;
	// 832BCDD4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BCDD8: 4BFFFA19  bl 0x832bc7f0
	ctx.lr = 0x832BCDDC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BC7F0);
	// 832BCDDC: 382100C0  addi r1, r1, 0xc0
	ctx.r[1].s64 = ctx.r[1].s64 + 192;
	// 832BCDE0: 4B9EC65C  b 0x82ca943c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA943C);
	return;
}

pub fn sub_832BCDE8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BCDE8 size=560
	// 832BCDE8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BCDEC: 4B9EC5FD  bl 0x82ca93e8
	ctx.lr = 0x832BCDF0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93E8);
	// 832BCDF0: 9421FF40  stwu r1, -0xc0(r1)
	ea = ctx.r[1].u32.wrapping_add(-192 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BCDF4: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BCDF8: 7C962378  mr r22, r4
	ctx.r[22].u64 = ctx.r[4].u64;
	// 832BCDFC: 48003CBD  bl 0x832c0ab8
	ctx.lr = 0x832BCE00;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BCE00: 831F0100  lwz r24, 0x100(r31)
	ctx.r[24].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(256 as u32) ) } as u64;
	// 832BCE04: 7C751B78  mr r21, r3
	ctx.r[21].u64 = ctx.r[3].u64;
	// 832BCE08: 817F0104  lwz r11, 0x104(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(260 as u32) ) } as u64;
	// 832BCE0C: 3B400000  li r26, 0
	ctx.r[26].s64 = 0;
	// 832BCE10: 815F00E0  lwz r10, 0xe0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(224 as u32) ) } as u64;
	// 832BCE14: 7F1EC378  mr r30, r24
	ctx.r[30].u64 = ctx.r[24].u64;
	// 832BCE18: 829F00BC  lwz r20, 0xbc(r31)
	ctx.r[20].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(188 as u32) ) } as u64;
	// 832BCE1C: 7EEBC214  add r23, r11, r24
	ctx.r[23].u64 = ctx.r[11].u64 + ctx.r[24].u64;
	// 832BCE20: 839F011C  lwz r28, 0x11c(r31)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(284 as u32) ) } as u64;
	// 832BCE24: 2F0A0000  cmpwi cr6, r10, 0
	ctx.cr[6].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832BCE28: 4099009C  ble cr6, 0x832bcec4
	if !ctx.cr[6].gt {
	pc = 0x832BCEC4; continue 'dispatch;
	}
	// 832BCE2C: 7F1EB840  cmplw cr6, r30, r23
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BCE30: 41990094  bgt cr6, 0x832bcec4
	if ctx.cr[6].gt {
	pc = 0x832BCEC4; continue 'dispatch;
	}
	// 832BCE34: 7F1EC040  cmplw cr6, r30, r24
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[24].u32, &mut ctx.xer);
	// 832BCE38: 4198008C  blt cr6, 0x832bcec4
	if ctx.cr[6].lt {
	pc = 0x832BCEC4; continue 'dispatch;
	}
	// 832BCE3C: 56CB07BC  rlwinm r11, r22, 0, 0x1e, 0x1e
	ctx.r[11].u64 = ctx.r[22].u32 as u64 & 0xFFFFFFFFu64;
	// 832BCE40: 83BE0000  lwz r29, 0(r30)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BCE44: 3BDE0004  addi r30, r30, 4
	ctx.r[30].s64 = ctx.r[30].s64 + 4;
	// 832BCE48: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCE4C: 419A0064  beq cr6, 0x832bceb0
	if ctx.cr[6].eq {
	pc = 0x832BCEB0; continue 'dispatch;
	}
	// 832BCE50: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BCE54: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BCE58: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BCE5C: 40990054  ble cr6, 0x832bceb0
	if !ctx.cr[6].gt {
	pc = 0x832BCEB0; continue 'dispatch;
	}
	// 832BCE60: 7F8BE378  mr r11, r28
	ctx.r[11].u64 = ctx.r[28].u64;
	// 832BCE64: 814B0000  lwz r10, 0(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BCE68: 7F0AD000  cmpw cr6, r10, r26
	ctx.cr[6].compare_i32(ctx.r[10].s32, ctx.r[26].s32, &mut ctx.xer);
	// 832BCE6C: 419A001C  beq cr6, 0x832bce88
	if ctx.cr[6].eq {
	pc = 0x832BCE88; continue 'dispatch;
	}
	// 832BCE70: 815F0114  lwz r10, 0x114(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BCE74: 38840001  addi r4, r4, 1
	ctx.r[4].s64 = ctx.r[4].s64 + 1;
	// 832BCE78: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832BCE7C: 7F045000  cmpw cr6, r4, r10
	ctx.cr[6].compare_i32(ctx.r[4].s32, ctx.r[10].s32, &mut ctx.xer);
	// 832BCE80: 4198FFE4  blt cr6, 0x832bce64
	if ctx.cr[6].lt {
	pc = 0x832BCE64; continue 'dispatch;
	}
	// 832BCE84: 4800002C  b 0x832bceb0
	pc = 0x832BCEB0; continue 'dispatch;
	// 832BCE88: 2F04FFFF  cmpwi cr6, r4, -1
	ctx.cr[6].compare_i32(ctx.r[4].s32, -1, &mut ctx.xer);
	// 832BCE8C: 419A0024  beq cr6, 0x832bceb0
	if ctx.cr[6].eq {
	pc = 0x832BCEB0; continue 'dispatch;
	}
	// 832BCE90: 2B1D0000  cmplwi cr6, r29, 0
	ctx.cr[6].compare_u32(ctx.r[29].u32, 0 as u32, &mut ctx.xer);
	// 832BCE94: 419A001C  beq cr6, 0x832bceb0
	if ctx.cr[6].eq {
	pc = 0x832BCEB0; continue 'dispatch;
	}
	// 832BCE98: 7EE8BB78  mr r8, r23
	ctx.r[8].u64 = ctx.r[23].u64;
	// 832BCE9C: 7F07C378  mr r7, r24
	ctx.r[7].u64 = ctx.r[24].u64;
	// 832BCEA0: 7FC6F378  mr r6, r30
	ctx.r[6].u64 = ctx.r[30].u64;
	// 832BCEA4: 7FA5EB78  mr r5, r29
	ctx.r[5].u64 = ctx.r[29].u64;
	// 832BCEA8: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BCEAC: 4BFFFB35  bl 0x832bc9e0
	ctx.lr = 0x832BCEB0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BC9E0);
	// 832BCEB0: 817F00E0  lwz r11, 0xe0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(224 as u32) ) } as u64;
	// 832BCEB4: 3B5A0001  addi r26, r26, 1
	ctx.r[26].s64 = ctx.r[26].s64 + 1;
	// 832BCEB8: 7FDDF214  add r30, r29, r30
	ctx.r[30].u64 = ctx.r[29].u64 + ctx.r[30].u64;
	// 832BCEBC: 7F1A5800  cmpw cr6, r26, r11
	ctx.cr[6].compare_i32(ctx.r[26].s32, ctx.r[11].s32, &mut ctx.xer);
	// 832BCEC0: 4198FF6C  blt cr6, 0x832bce2c
	if ctx.cr[6].lt {
	pc = 0x832BCE2C; continue 'dispatch;
	}
	// 832BCEC4: 4AFA91ED  bl 0x822660b0
	ctx.lr = 0x832BCEC8;
	crate::recompiler::externs::call(&mut ctx, base, 0x822660B0);
	// 832BCEC8: 3D608350  lis r11, -0x7cb0
	ctx.r[11].s64 = -2091909120;
	// 832BCECC: 3BABC200  addi r29, r11, -0x3e00
	ctx.r[29].s64 = ctx.r[11].s64 + -15872;
	// 832BCED0: 816BC200  lwz r11, -0x3e00(r11)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-15872 as u32) ) } as u64;
	// 832BCED4: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCED8: 409A000C  bne cr6, 0x832bcee4
	if !ctx.cr[6].eq {
	pc = 0x832BCEE4; continue 'dispatch;
	}
	// 832BCEDC: 7C6B1B78  mr r11, r3
	ctx.r[11].u64 = ctx.r[3].u64;
	// 832BCEE0: 917D0000  stw r11, 0(r29)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BCEE4: 815D0004  lwz r10, 4(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BCEE8: 7D6B1850  subf r11, r11, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BCEEC: 3F60C000  lis r27, -0x4000
	ctx.r[27].s64 = -1073741824;
	// 832BCEF0: 7D2A5850  subf r9, r10, r11
	ctx.r[9].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BCEF4: 7F09D840  cmplw cr6, r9, r27
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832BCEF8: 4099000C  ble cr6, 0x832bcf04
	if !ctx.cr[6].gt {
	pc = 0x832BCF04; continue 'dispatch;
	}
	// 832BCEFC: 7D595378  mr r25, r10
	ctx.r[25].u64 = ctx.r[10].u64;
	// 832BCF00: 4800000C  b 0x832bcf0c
	pc = 0x832BCF0C; continue 'dispatch;
	// 832BCF04: 7D795B78  mr r25, r11
	ctx.r[25].u64 = ctx.r[11].u64;
	// 832BCF08: 917D0004  stw r11, 4(r29)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832BCF0C: 817F00FC  lwz r11, 0xfc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(252 as u32) ) } as u64;
	// 832BCF10: 7F95C850  subf r28, r21, r25
	ctx.r[28].s64 = ctx.r[25].s64 - ctx.r[21].s64;
	// 832BCF14: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCF18: 419A0094  beq cr6, 0x832bcfac
	if ctx.cr[6].eq {
	pc = 0x832BCFAC; continue 'dispatch;
	}
	// 832BCF1C: 2B140000  cmplwi cr6, r20, 0
	ctx.cr[6].compare_u32(ctx.r[20].u32, 0 as u32, &mut ctx.xer);
	// 832BCF20: 419A008C  beq cr6, 0x832bcfac
	if ctx.cr[6].eq {
	pc = 0x832BCFAC; continue 'dispatch;
	}
	// 832BCF24: 835F0020  lwz r26, 0x20(r31)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BCF28: 574B0420  rlwinm r11, r26, 0, 0x10, 0x10
	ctx.r[11].u64 = ctx.r[26].u32 as u64 & 0xFFFFFFFFu64;
	// 832BCF2C: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BCF30: 419A0014  beq cr6, 0x832bcf44
	if ctx.cr[6].eq {
	pc = 0x832BCF44; continue 'dispatch;
	}
	// 832BCF34: 56CB07BC  rlwinm r11, r22, 0, 0x1e, 0x1e
	ctx.r[11].u64 = ctx.r[22].u32 as u64 & 0xFFFFFFFFu64;
	// 832BCF38: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCF3C: 409A0070  bne cr6, 0x832bcfac
	if !ctx.cr[6].eq {
	pc = 0x832BCFAC; continue 'dispatch;
	}
	// 832BCF40: 48000028  b 0x832bcf68
	pc = 0x832BCF68; continue 'dispatch;
	// 832BCF44: 56CB07FE  clrlwi r11, r22, 0x1f
	ctx.r[11].u64 = ctx.r[22].u32 as u64 & 0x00000001u64;
	// 832BCF48: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCF4C: 409A0008  bne cr6, 0x832bcf54
	if !ctx.cr[6].eq {
	pc = 0x832BCF54; continue 'dispatch;
	}
	// 832BCF50: 635A0200  ori r26, r26, 0x200
	ctx.r[26].u64 = ctx.r[26].u64 | 512;
	// 832BCF54: 56CB07BC  rlwinm r11, r22, 0, 0x1e, 0x1e
	ctx.r[11].u64 = ctx.r[22].u32 as u64 & 0xFFFFFFFFu64;
	// 832BCF58: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCF5C: 409A000C  bne cr6, 0x832bcf68
	if !ctx.cr[6].eq {
	pc = 0x832BCF68; continue 'dispatch;
	}
	// 832BCF60: 574B0314  rlwinm r11, r26, 0, 0xc, 0xa
	ctx.r[11].u64 = ctx.r[26].u32 as u64 & 0xFFFFFFFFu64;
	// 832BCF64: 657A0002  oris r26, r11, 2
	ctx.r[26].u64 = ctx.r[11].u64 | 131072;
	// 832BCF68: 635A0100  ori r26, r26, 0x100
	ctx.r[26].u64 = ctx.r[26].u64 | 256;
	// 832BCF6C: 7F1EB840  cmplw cr6, r30, r23
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BCF70: 4199003C  bgt cr6, 0x832bcfac
	if ctx.cr[6].gt {
	pc = 0x832BCFAC; continue 'dispatch;
	}
	// 832BCF74: 7F1EC040  cmplw cr6, r30, r24
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[24].u32, &mut ctx.xer);
	// 832BCF78: 41980034  blt cr6, 0x832bcfac
	if ctx.cr[6].lt {
	pc = 0x832BCFAC; continue 'dispatch;
	}
	// 832BCF7C: 817F0024  lwz r11, 0x24(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(36 as u32) ) } as u64;
	// 832BCF80: 7F4AD378  mr r10, r26
	ctx.r[10].u64 = ctx.r[26].u64;
	// 832BCF84: 393F0120  addi r9, r31, 0x120
	ctx.r[9].s64 = ctx.r[31].s64 + 288;
	// 832BCF88: 80FF010C  lwz r7, 0x10c(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(268 as u32) ) } as u64;
	// 832BCF8C: 7EE8BB78  mr r8, r23
	ctx.r[8].u64 = ctx.r[23].u64;
	// 832BCF90: 80BF00C4  lwz r5, 0xc4(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(196 as u32) ) } as u64;
	// 832BCF94: 7FC6F378  mr r6, r30
	ctx.r[6].u64 = ctx.r[30].u64;
	// 832BCF98: 809F00D0  lwz r4, 0xd0(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(208 as u32) ) } as u64;
	// 832BCF9C: 7E83A378  mr r3, r20
	ctx.r[3].u64 = ctx.r[20].u64;
	// 832BCFA0: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832BCFA4: 48007A45  bl 0x832c49e8
	ctx.lr = 0x832BCFA8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C49E8);
	// 832BCFA8: 7C7A1B78  mr r26, r3
	ctx.r[26].u64 = ctx.r[3].u64;
	// 832BCFAC: 817F02E4  lwz r11, 0x2e4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(740 as u32) ) } as u64;
	// 832BCFB0: 7D6BE214  add r11, r11, r28
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[28].u64;
	// 832BCFB4: 917F02E4  stw r11, 0x2e4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(740 as u32), ctx.r[11].u32 ) };
	// 832BCFB8: 4AFA90F9  bl 0x822660b0
	ctx.lr = 0x832BCFBC;
	crate::recompiler::externs::call(&mut ctx, base, 0x822660B0);
	// 832BCFBC: 817D0000  lwz r11, 0(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BCFC0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BCFC4: 409A000C  bne cr6, 0x832bcfd0
	if !ctx.cr[6].eq {
	pc = 0x832BCFD0; continue 'dispatch;
	}
	// 832BCFC8: 7C6B1B78  mr r11, r3
	ctx.r[11].u64 = ctx.r[3].u64;
	// 832BCFCC: 917D0000  stw r11, 0(r29)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BCFD0: 815D0004  lwz r10, 4(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BCFD4: 7D6B1850  subf r11, r11, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BCFD8: 7D2A5850  subf r9, r10, r11
	ctx.r[9].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BCFDC: 7F09D840  cmplw cr6, r9, r27
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832BCFE0: 4199000C  bgt cr6, 0x832bcfec
	if ctx.cr[6].gt {
	pc = 0x832BCFEC; continue 'dispatch;
	}
	// 832BCFE4: 7D6A5B78  mr r10, r11
	ctx.r[10].u64 = ctx.r[11].u64;
	// 832BCFE8: 917D0004  stw r11, 4(r29)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832BCFEC: 817F02E0  lwz r11, 0x2e0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(736 as u32) ) } as u64;
	// 832BCFF0: 38600001  li r3, 1
	ctx.r[3].s64 = 1;
	// 832BCFF4: 813F0144  lwz r9, 0x144(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(324 as u32) ) } as u64;
	// 832BCFF8: 7D795850  subf r11, r25, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[25].s64;
	// 832BCFFC: 7F1A4840  cmplw cr6, r26, r9
	ctx.cr[6].compare_u32(ctx.r[26].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BD000: 7D0B5214  add r8, r11, r10
	ctx.r[8].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832BD004: 911F02E0  stw r8, 0x2e0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(736 as u32), ctx.r[8].u32 ) };
	// 832BD008: 40990008  ble cr6, 0x832bd010
	if !ctx.cr[6].gt {
	pc = 0x832BD010; continue 'dispatch;
	}
	// 832BD00C: 935F0144  stw r26, 0x144(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(324 as u32), ctx.r[26].u32 ) };
	// 832BD010: 382100C0  addi r1, r1, 0xc0
	ctx.r[1].s64 = ctx.r[1].s64 + 192;
	// 832BD014: 4B9EC424  b 0x82ca9438
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9438);
	return;
}

pub fn sub_832BD018(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BD018 size=512
	// 832BD018: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BD01C: 4B9EC3F1  bl 0x82ca940c
	ctx.lr = 0x832BD020;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA940C);
	// 832BD020: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BD024: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BD028: 3BA00000  li r29, 0
	ctx.r[29].s64 = 0;
	// 832BD02C: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BD030: 7FBEEB78  mr r30, r29
	ctx.r[30].u64 = ctx.r[29].u64;
	// 832BD034: 419A01D8  beq cr6, 0x832bd20c
	if ctx.cr[6].eq {
	pc = 0x832BD20C; continue 'dispatch;
	}
	// 832BD038: 817F00F0  lwz r11, 0xf0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(240 as u32) ) } as u64;
	// 832BD03C: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BD040: 409A01CC  bne cr6, 0x832bd20c
	if !ctx.cr[6].eq {
	pc = 0x832BD20C; continue 'dispatch;
	}
	// 832BD044: 817F00F4  lwz r11, 0xf4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(244 as u32) ) } as u64;
	// 832BD048: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BD04C: 409A01C0  bne cr6, 0x832bd20c
	if !ctx.cr[6].eq {
	pc = 0x832BD20C; continue 'dispatch;
	}
	// 832BD050: 817F0334  lwz r11, 0x334(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(820 as u32) ) } as u64;
	// 832BD054: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD058: 409A000C  bne cr6, 0x832bd064
	if !ctx.cr[6].eq {
	pc = 0x832BD064; continue 'dispatch;
	}
	// 832BD05C: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BD060: 4BFFF539  bl 0x832bc598
	ctx.lr = 0x832BD064;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BC598);
	// 832BD064: 817F0338  lwz r11, 0x338(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(824 as u32) ) } as u64;
	// 832BD068: 93BF0334  stw r29, 0x334(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(820 as u32), ctx.r[29].u32 ) };
	// 832BD06C: 93BF017C  stw r29, 0x17c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(380 as u32), ctx.r[29].u32 ) };
	// 832BD070: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD074: 419A000C  beq cr6, 0x832bd080
	if ctx.cr[6].eq {
	pc = 0x832BD080; continue 'dispatch;
	}
	// 832BD078: 93BF0338  stw r29, 0x338(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(824 as u32), ctx.r[29].u32 ) };
	// 832BD07C: 48000070  b 0x832bd0ec
	pc = 0x832BD0EC; continue 'dispatch;
	// 832BD080: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD084: 7FA8EB78  mr r8, r29
	ctx.r[8].u64 = ctx.r[29].u64;
	// 832BD088: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD08C: 40990114  ble cr6, 0x832bd1a0
	if !ctx.cr[6].gt {
	pc = 0x832BD1A0; continue 'dispatch;
	}
	// 832BD090: 7FAAEB78  mr r10, r29
	ctx.r[10].u64 = ctx.r[29].u64;
	// 832BD094: 817F0118  lwz r11, 0x118(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BD098: 7D6A5A14  add r11, r10, r11
	ctx.r[11].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832BD09C: 812B0054  lwz r9, 0x54(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BD0A0: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832BD0A4: 419A002C  beq cr6, 0x832bd0d0
	if ctx.cr[6].eq {
	pc = 0x832BD0D0; continue 'dispatch;
	}
	// 832BD0A8: 93AB0054  stw r29, 0x54(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(84 as u32), ctx.r[29].u32 ) };
	// 832BD0AC: 817F000C  lwz r11, 0xc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BD0B0: 2B0B0001  cmplwi cr6, r11, 1
	ctx.cr[6].compare_u32(ctx.r[11].u32, 1 as u32, &mut ctx.xer);
	// 832BD0B4: 4099001C  ble cr6, 0x832bd0d0
	if !ctx.cr[6].gt {
	pc = 0x832BD0D0; continue 'dispatch;
	}
	// 832BD0B8: 813F0118  lwz r9, 0x118(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BD0BC: 7D2A4A14  add r9, r10, r9
	ctx.r[9].u64 = ctx.r[10].u64 + ctx.r[9].u64;
	// 832BD0C0: 80E90044  lwz r7, 0x44(r9)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(68 as u32) ) } as u64;
	// 832BD0C4: 7F0B3800  cmpw cr6, r11, r7
	ctx.cr[6].compare_i32(ctx.r[11].s32, ctx.r[7].s32, &mut ctx.xer);
	// 832BD0C8: 41990008  bgt cr6, 0x832bd0d0
	if ctx.cr[6].gt {
	pc = 0x832BD0D0; continue 'dispatch;
	}
	// 832BD0CC: 3BC00001  li r30, 1
	ctx.r[30].s64 = 1;
	// 832BD0D0: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD0D4: 39080001  addi r8, r8, 1
	ctx.r[8].s64 = ctx.r[8].s64 + 1;
	// 832BD0D8: 394A0184  addi r10, r10, 0x184
	ctx.r[10].s64 = ctx.r[10].s64 + 388;
	// 832BD0DC: 7F085840  cmplw cr6, r8, r11
	ctx.cr[6].compare_u32(ctx.r[8].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832BD0E0: 4198FFB4  blt cr6, 0x832bd094
	if ctx.cr[6].lt {
	pc = 0x832BD094; continue 'dispatch;
	}
	// 832BD0E4: 2F1E0000  cmpwi cr6, r30, 0
	ctx.cr[6].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832BD0E8: 419A00B8  beq cr6, 0x832bd1a0
	if ctx.cr[6].eq {
	pc = 0x832BD1A0; continue 'dispatch;
	}
	// 832BD0EC: 817F0330  lwz r11, 0x330(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(816 as u32) ) } as u64;
	// 832BD0F0: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832BD0F4: 917F0330  stw r11, 0x330(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(816 as u32), ctx.r[11].u32 ) };
	// 832BD0F8: 807F0358  lwz r3, 0x358(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(856 as u32) ) } as u64;
	// 832BD0FC: 809F0258  lwz r4, 0x258(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(600 as u32) ) } as u64;
	// 832BD100: 48001881  bl 0x832be980
	ctx.lr = 0x832BD104;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BE980);
	// 832BD104: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BD108: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD10C: 480009C5  bl 0x832bdad0
	ctx.lr = 0x832BD110;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BDAD0);
	// 832BD110: 815F0198  lwz r10, 0x198(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(408 as u32) ) } as u64;
	// 832BD114: 813F019C  lwz r9, 0x19c(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(412 as u32) ) } as u64;
	// 832BD118: 1D0A001D  mulli r8, r10, 0x1d
	ctx.r[8].s64 = ctx.r[10].s64 * 29;
	// 832BD11C: 5507D97E  srwi r7, r8, 5
	// 832BD120: 7F093840  cmplw cr6, r9, r7
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[7].u32, &mut ctx.xer);
	// 832BD124: 40980038  bge cr6, 0x832bd15c
	if !ctx.cr[6].lt {
	pc = 0x832BD15C; continue 'dispatch;
	}
	// 832BD128: 3BDF0150  addi r30, r31, 0x150
	ctx.r[30].s64 = ctx.r[31].s64 + 336;
	// 832BD12C: 817F0160  lwz r11, 0x160(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(352 as u32) ) } as u64;
	// 832BD130: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BD134: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832BD138: 4E800421  bctrl
	ctx.lr = 0x832BD13C;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD13C: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BD140: 419A001C  beq cr6, 0x832bd15c
	if ctx.cr[6].eq {
	pc = 0x832BD15C; continue 'dispatch;
	}
	// 832BD144: 817F0198  lwz r11, 0x198(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(408 as u32) ) } as u64;
	// 832BD148: 815F019C  lwz r10, 0x19c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(412 as u32) ) } as u64;
	// 832BD14C: 1D2B001D  mulli r9, r11, 0x1d
	ctx.r[9].s64 = ctx.r[11].s64 * 29;
	// 832BD150: 5528D97E  srwi r8, r9, 5
	// 832BD154: 7F0A4040  cmplw cr6, r10, r8
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[8].u32, &mut ctx.xer);
	// 832BD158: 4198FFD4  blt cr6, 0x832bd12c
	if ctx.cr[6].lt {
	pc = 0x832BD12C; continue 'dispatch;
	}
	// 832BD15C: 93BF02C0  stw r29, 0x2c0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(704 as u32), ctx.r[29].u32 ) };
	// 832BD160: 38800001  li r4, 1
	ctx.r[4].s64 = 1;
	// 832BD164: 93BF0344  stw r29, 0x344(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(836 as u32), ctx.r[29].u32 ) };
	// 832BD168: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD16C: 48000965  bl 0x832bdad0
	ctx.lr = 0x832BD170;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BDAD0);
	// 832BD170: 357F0354  addic. r11, r31, 0x354
	ctx.xer.ca = (ctx.r[31].u32 > (!(852 as u32)));
	ctx.r[11].s64 = ctx.r[31].s64 + 852;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BD174: 41820014  beq 0x832bd188
	if ctx.cr[0].eq {
	pc = 0x832BD188; continue 'dispatch;
	}
	// 832BD178: 806B0004  lwz r3, 4(r11)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BD17C: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BD180: 419A0008  beq cr6, 0x832bd188
	if ctx.cr[6].eq {
	pc = 0x832BD188; continue 'dispatch;
	}
	// 832BD184: 4BD4E83D  bl 0x8300b9c0
	ctx.lr = 0x832BD188;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BD188: 357F0254  addic. r11, r31, 0x254
	ctx.xer.ca = (ctx.r[31].u32 > (!(596 as u32)));
	ctx.r[11].s64 = ctx.r[31].s64 + 596;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BD18C: 41820014  beq 0x832bd1a0
	if ctx.cr[0].eq {
	pc = 0x832BD1A0; continue 'dispatch;
	}
	// 832BD190: 806B0004  lwz r3, 4(r11)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BD194: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BD198: 419A0008  beq cr6, 0x832bd1a0
	if ctx.cr[6].eq {
	pc = 0x832BD1A0; continue 'dispatch;
	}
	// 832BD19C: 4BD4E825  bl 0x8300b9c0
	ctx.lr = 0x832BD1A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BD1A0: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD1A4: 39400001  li r10, 1
	ctx.r[10].s64 = 1;
	// 832BD1A8: 915F017C  stw r10, 0x17c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(380 as u32), ctx.r[10].u32 ) };
	// 832BD1AC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD1B0: 419A000C  beq cr6, 0x832bd1bc
	if ctx.cr[6].eq {
	pc = 0x832BD1BC; continue 'dispatch;
	}
	// 832BD1B4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD1B8: 4BFFDB49  bl 0x832bad00
	ctx.lr = 0x832BD1BC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BAD00);
	// 832BD1BC: 480038FD  bl 0x832c0ab8
	ctx.lr = 0x832BD1C0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BD1C0: 817F02BC  lwz r11, 0x2bc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(700 as u32) ) } as u64;
	// 832BD1C4: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD1C8: 419A0020  beq cr6, 0x832bd1e8
	if ctx.cr[6].eq {
	pc = 0x832BD1E8; continue 'dispatch;
	}
	// 832BD1CC: 2F0BFFFF  cmpwi cr6, r11, -1
	ctx.cr[6].compare_i32(ctx.r[11].s32, -1, &mut ctx.xer);
	// 832BD1D0: 419A0018  beq cr6, 0x832bd1e8
	if ctx.cr[6].eq {
	pc = 0x832BD1E8; continue 'dispatch;
	}
	// 832BD1D4: 815F02E8  lwz r10, 0x2e8(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(744 as u32) ) } as u64;
	// 832BD1D8: 93BF02BC  stw r29, 0x2bc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(700 as u32), ctx.r[29].u32 ) };
	// 832BD1DC: 7D6B5050  subf r11, r11, r10
	ctx.r[11].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832BD1E0: 7D2B1A14  add r9, r11, r3
	ctx.r[9].u64 = ctx.r[11].u64 + ctx.r[3].u64;
	// 832BD1E4: 913F02E8  stw r9, 0x2e8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(744 as u32), ctx.r[9].u32 ) };
	// 832BD1E8: 817F000C  lwz r11, 0xc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BD1EC: 38800001  li r4, 1
	ctx.r[4].s64 = 1;
	// 832BD1F0: 815F0008  lwz r10, 8(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BD1F4: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BD1F8: 40980008  bge cr6, 0x832bd200
	if !ctx.cr[6].lt {
	pc = 0x832BD200; continue 'dispatch;
	}
	// 832BD1FC: 388B0001  addi r4, r11, 1
	ctx.r[4].s64 = ctx.r[11].s64 + 1;
	// 832BD200: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD204: 4BFFDDF5  bl 0x832baff8
	ctx.lr = 0x832BD208;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BAFF8);
	// 832BD208: 93BF017C  stw r29, 0x17c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(380 as u32), ctx.r[29].u32 ) };
	// 832BD20C: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BD210: 4B9EC24C  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
}

pub fn sub_832BD218(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BD218 size=552
	// 832BD218: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BD21C: 4B9EC1DD  bl 0x82ca93f8
	ctx.lr = 0x832BD220;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93F8);
	// 832BD220: 9421FF60  stwu r1, -0xa0(r1)
	ea = ctx.r[1].u32.wrapping_add(-160 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BD224: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BD228: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BD22C: 419A020C  beq cr6, 0x832bd438
	if ctx.cr[6].eq {
	pc = 0x832BD438; continue 'dispatch;
	}
	// 832BD230: 817F00F0  lwz r11, 0xf0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(240 as u32) ) } as u64;
	// 832BD234: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BD238: 409A0200  bne cr6, 0x832bd438
	if !ctx.cr[6].eq {
	pc = 0x832BD438; continue 'dispatch;
	}
	// 832BD23C: 817F00F4  lwz r11, 0xf4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(244 as u32) ) } as u64;
	// 832BD240: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BD244: 409A01F4  bne cr6, 0x832bd438
	if !ctx.cr[6].eq {
	pc = 0x832BD438; continue 'dispatch;
	}
	// 832BD248: 817F0008  lwz r11, 8(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BD24C: 3B200001  li r25, 1
	ctx.r[25].s64 = 1;
	// 832BD250: 3B000000  li r24, 0
	ctx.r[24].s64 = 0;
	// 832BD254: 7F3CCB78  mr r28, r25
	ctx.r[28].u64 = ctx.r[25].u64;
	// 832BD258: 933F017C  stw r25, 0x17c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(380 as u32), ctx.r[25].u32 ) };
	// 832BD25C: 931F0334  stw r24, 0x334(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(820 as u32), ctx.r[24].u32 ) };
	// 832BD260: 2B0B0001  cmplwi cr6, r11, 1
	ctx.cr[6].compare_u32(ctx.r[11].u32, 1 as u32, &mut ctx.xer);
	// 832BD264: 40980008  bge cr6, 0x832bd26c
	if !ctx.cr[6].lt {
	pc = 0x832BD26C; continue 'dispatch;
	}
	// 832BD268: 7D7C5B78  mr r28, r11
	ctx.r[28].u64 = ctx.r[11].u64;
	// 832BD26C: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD270: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD274: 419A0030  beq cr6, 0x832bd2a4
	if ctx.cr[6].eq {
	pc = 0x832BD2A4; continue 'dispatch;
	}
	// 832BD278: 817F0018  lwz r11, 0x18(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BD27C: 815F0014  lwz r10, 0x14(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BD280: 7D4A5A14  add r10, r10, r11
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832BD284: 394AFFFF  addi r10, r10, -1
	ctx.r[10].s64 = ctx.r[10].s64 + -1;
	// 832BD288: 7D6A5B96  divwu r11, r10, r11
	ctx.r[11].u32 = ctx.r[10].u32 / ctx.r[11].u32;
	// 832BD28C: 7F0BE040  cmplw cr6, r11, r28
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[28].u32, &mut ctx.xer);
	// 832BD290: 4198000C  blt cr6, 0x832bd29c
	if ctx.cr[6].lt {
	pc = 0x832BD29C; continue 'dispatch;
	}
	// 832BD294: 7F3ECB78  mr r30, r25
	ctx.r[30].u64 = ctx.r[25].u64;
	// 832BD298: 48000010  b 0x832bd2a8
	pc = 0x832BD2A8; continue 'dispatch;
	// 832BD29C: 7FCBE050  subf r30, r11, r28
	ctx.r[30].s64 = ctx.r[28].s64 - ctx.r[11].s64;
	// 832BD2A0: 48000008  b 0x832bd2a8
	pc = 0x832BD2A8; continue 'dispatch;
	// 832BD2A4: 7F9EE378  mr r30, r28
	ctx.r[30].u64 = ctx.r[28].u64;
	// 832BD2A8: 811F000C  lwz r8, 0xc(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BD2AC: 7F08E040  cmplw cr6, r8, r28
	ctx.cr[6].compare_u32(ctx.r[8].u32, ctx.r[28].u32, &mut ctx.xer);
	// 832BD2B0: 419A0184  beq cr6, 0x832bd434
	if ctx.cr[6].eq {
	pc = 0x832BD434; continue 'dispatch;
	}
	// 832BD2B4: 815F014C  lwz r10, 0x14c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(332 as u32) ) } as u64;
	// 832BD2B8: 578B103A  slwi r11, r28, 2
	// 832BD2BC: 7D6B5214  add r11, r11, r10
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832BD2C0: 812BFFFC  lwz r9, -4(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-4 as u32) ) } as u64;
	// 832BD2C4: 552707FE  clrlwi r7, r9, 0x1f
	ctx.r[7].u64 = ctx.r[9].u32 as u64 & 0x00000001u64;
	// 832BD2C8: 2B070000  cmplwi cr6, r7, 0
	ctx.cr[6].compare_u32(ctx.r[7].u32, 0 as u32, &mut ctx.xer);
	// 832BD2CC: 419A000C  beq cr6, 0x832bd2d8
	if ctx.cr[6].eq {
	pc = 0x832BD2D8; continue 'dispatch;
	}
	// 832BD2D0: 7F9BE378  mr r27, r28
	ctx.r[27].u64 = ctx.r[28].u64;
	// 832BD2D4: 4800003C  b 0x832bd310
	pc = 0x832BD310; continue 'dispatch;
	// 832BD2D8: 397CFFFE  addi r11, r28, -2
	ctx.r[11].s64 = ctx.r[28].s64 + -2;
	// 832BD2DC: 2F0B0001  cmpwi cr6, r11, 1
	ctx.cr[6].compare_i32(ctx.r[11].s32, 1, &mut ctx.xer);
	// 832BD2E0: 4198002C  blt cr6, 0x832bd30c
	if ctx.cr[6].lt {
	pc = 0x832BD30C; continue 'dispatch;
	}
	// 832BD2E4: 5569103A  slwi r9, r11, 2
	// 832BD2E8: 7D495214  add r10, r9, r10
	ctx.r[10].u64 = ctx.r[9].u64 + ctx.r[10].u64;
	// 832BD2EC: 812A0000  lwz r9, 0(r10)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BD2F0: 552707FE  clrlwi r7, r9, 0x1f
	ctx.r[7].u64 = ctx.r[9].u32 as u64 & 0x00000001u64;
	// 832BD2F4: 2B070000  cmplwi cr6, r7, 0
	ctx.cr[6].compare_u32(ctx.r[7].u32, 0 as u32, &mut ctx.xer);
	// 832BD2F8: 409A0014  bne cr6, 0x832bd30c
	if !ctx.cr[6].eq {
	pc = 0x832BD30C; continue 'dispatch;
	}
	// 832BD2FC: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832BD300: 394AFFFC  addi r10, r10, -4
	ctx.r[10].s64 = ctx.r[10].s64 + -4;
	// 832BD304: 2F0B0001  cmpwi cr6, r11, 1
	ctx.cr[6].compare_i32(ctx.r[11].s32, 1, &mut ctx.xer);
	// 832BD308: 4098FFE4  bge cr6, 0x832bd2ec
	if !ctx.cr[6].lt {
	pc = 0x832BD2EC; continue 'dispatch;
	}
	// 832BD30C: 3B6B0001  addi r27, r11, 1
	ctx.r[27].s64 = ctx.r[11].s64 + 1;
	// 832BD310: 7F1BF040  cmplw cr6, r27, r30
	ctx.cr[6].compare_u32(ctx.r[27].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832BD314: 40990010  ble cr6, 0x832bd324
	if !ctx.cr[6].gt {
	pc = 0x832BD324; continue 'dispatch;
	}
	// 832BD318: 83BF00FC  lwz r29, 0xfc(r31)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(252 as u32) ) } as u64;
	// 832BD31C: 931F00FC  stw r24, 0xfc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(252 as u32), ctx.r[24].u32 ) };
	// 832BD320: 4800000C  b 0x832bd32c
	pc = 0x832BD32C; continue 'dispatch;
	// 832BD324: 7F7EDB78  mr r30, r27
	ctx.r[30].u64 = ctx.r[27].u64;
	// 832BD328: 7F1DC378  mr r29, r24
	ctx.r[29].u64 = ctx.r[24].u64;
	// 832BD32C: 7F1C4040  cmplw cr6, r28, r8
	ctx.cr[6].compare_u32(ctx.r[28].u32, ctx.r[8].u32, &mut ctx.xer);
	// 832BD330: 4198000C  blt cr6, 0x832bd33c
	if ctx.cr[6].lt {
	pc = 0x832BD33C; continue 'dispatch;
	}
	// 832BD334: 7F1E4040  cmplw cr6, r30, r8
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[8].u32, &mut ctx.xer);
	// 832BD338: 40990018  ble cr6, 0x832bd350
	if !ctx.cr[6].gt {
	pc = 0x832BD350; continue 'dispatch;
	}
	// 832BD33C: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832BD340: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD344: 4BFFDCB5  bl 0x832baff8
	ctx.lr = 0x832BD348;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BAFF8);
	// 832BD348: 7F1EE040  cmplw cr6, r30, r28
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[28].u32, &mut ctx.xer);
	// 832BD34C: 419A00E8  beq cr6, 0x832bd434
	if ctx.cr[6].eq {
	pc = 0x832BD434; continue 'dispatch;
	}
	// 832BD350: 3BDF0354  addi r30, r31, 0x354
	ctx.r[30].s64 = ctx.r[31].s64 + 852;
	// 832BD354: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832BD358: 419A0018  beq cr6, 0x832bd370
	if ctx.cr[6].eq {
	pc = 0x832BD370; continue 'dispatch;
	}
	// 832BD35C: 807E0004  lwz r3, 4(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BD360: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BD364: 419A000C  beq cr6, 0x832bd370
	if ctx.cr[6].eq {
	pc = 0x832BD370; continue 'dispatch;
	}
	// 832BD368: 3880FFFF  li r4, -1
	ctx.r[4].s64 = -1;
	// 832BD36C: 4AED965D  bl 0x821969c8
	ctx.lr = 0x832BD370;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BD370: 835F00F8  lwz r26, 0xf8(r31)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(248 as u32) ) } as u64;
	// 832BD374: 2F1A0000  cmpwi cr6, r26, 0
	ctx.cr[6].compare_i32(ctx.r[26].s32, 0, &mut ctx.xer);
	// 832BD378: 419A0010  beq cr6, 0x832bd388
	if ctx.cr[6].eq {
	pc = 0x832BD388; continue 'dispatch;
	}
	// 832BD37C: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BD380: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD384: 4800074D  bl 0x832bdad0
	ctx.lr = 0x832BD388;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BDAD0);
	// 832BD388: 817F000C  lwz r11, 0xc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BD38C: 815F0320  lwz r10, 0x320(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(800 as u32) ) } as u64;
	// 832BD390: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BD394: 419A0010  beq cr6, 0x832bd3a4
	if ctx.cr[6].eq {
	pc = 0x832BD3A4; continue 'dispatch;
	}
	// 832BD398: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD39C: 4BFFF795  bl 0x832bcb30
	ctx.lr = 0x832BD3A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BCB30);
	// 832BD3A0: 933F0110  stw r25, 0x110(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(272 as u32), ctx.r[25].u32 ) };
	// 832BD3A4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD3A8: 4BFFFC71  bl 0x832bd018
	ctx.lr = 0x832BD3AC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BD018);
	// 832BD3AC: 817F000C  lwz r11, 0xc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BD3B0: 7F0BE040  cmplw cr6, r11, r28
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[28].u32, &mut ctx.xer);
	// 832BD3B4: 419A0040  beq cr6, 0x832bd3f4
	if ctx.cr[6].eq {
	pc = 0x832BD3F4; continue 'dispatch;
	}
	// 832BD3B8: 2F1D0000  cmpwi cr6, r29, 0
	ctx.cr[6].compare_i32(ctx.r[29].s32, 0, &mut ctx.xer);
	// 832BD3BC: 419A0018  beq cr6, 0x832bd3d4
	if ctx.cr[6].eq {
	pc = 0x832BD3D4; continue 'dispatch;
	}
	// 832BD3C0: 817F000C  lwz r11, 0xc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BD3C4: 7F0BD840  cmplw cr6, r11, r27
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832BD3C8: 409A000C  bne cr6, 0x832bd3d4
	if !ctx.cr[6].eq {
	pc = 0x832BD3D4; continue 'dispatch;
	}
	// 832BD3CC: 7F1DC378  mr r29, r24
	ctx.r[29].u64 = ctx.r[24].u64;
	// 832BD3D0: 933F00FC  stw r25, 0xfc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(252 as u32), ctx.r[25].u32 ) };
	// 832BD3D4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD3D8: 4BFFF759  bl 0x832bcb30
	ctx.lr = 0x832BD3DC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BCB30);
	// 832BD3DC: 933F0110  stw r25, 0x110(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(272 as u32), ctx.r[25].u32 ) };
	// 832BD3E0: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD3E4: 4BFFFC35  bl 0x832bd018
	ctx.lr = 0x832BD3E8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BD018);
	// 832BD3E8: 817F000C  lwz r11, 0xc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BD3EC: 7F0BE040  cmplw cr6, r11, r28
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[28].u32, &mut ctx.xer);
	// 832BD3F0: 409AFFC8  bne cr6, 0x832bd3b8
	if !ctx.cr[6].eq {
	pc = 0x832BD3B8; continue 'dispatch;
	}
	// 832BD3F4: 931F02C0  stw r24, 0x2c0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(704 as u32), ctx.r[24].u32 ) };
	// 832BD3F8: 2F1D0000  cmpwi cr6, r29, 0
	ctx.cr[6].compare_i32(ctx.r[29].s32, 0, &mut ctx.xer);
	// 832BD3FC: 931F0344  stw r24, 0x344(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(836 as u32), ctx.r[24].u32 ) };
	// 832BD400: 419A0008  beq cr6, 0x832bd408
	if ctx.cr[6].eq {
	pc = 0x832BD408; continue 'dispatch;
	}
	// 832BD404: 933F00FC  stw r25, 0xfc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(252 as u32), ctx.r[25].u32 ) };
	// 832BD408: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832BD40C: 419A0014  beq cr6, 0x832bd420
	if ctx.cr[6].eq {
	pc = 0x832BD420; continue 'dispatch;
	}
	// 832BD410: 807E0004  lwz r3, 4(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BD414: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BD418: 419A0008  beq cr6, 0x832bd420
	if ctx.cr[6].eq {
	pc = 0x832BD420; continue 'dispatch;
	}
	// 832BD41C: 4BD4E5A5  bl 0x8300b9c0
	ctx.lr = 0x832BD420;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BD420: 2F1A0000  cmpwi cr6, r26, 0
	ctx.cr[6].compare_i32(ctx.r[26].s32, 0, &mut ctx.xer);
	// 832BD424: 419A0010  beq cr6, 0x832bd434
	if ctx.cr[6].eq {
	pc = 0x832BD434; continue 'dispatch;
	}
	// 832BD428: 38800001  li r4, 1
	ctx.r[4].s64 = 1;
	// 832BD42C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD430: 480006A1  bl 0x832bdad0
	ctx.lr = 0x832BD434;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BDAD0);
	// 832BD434: 931F017C  stw r24, 0x17c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(380 as u32), ctx.r[24].u32 ) };
	// 832BD438: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832BD43C: 4B9EC00C  b 0x82ca9448
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9448);
	return;
}

pub fn sub_832BD440(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BD440 size=720
	// 832BD440: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BD444: 4B9EBFC9  bl 0x82ca940c
	ctx.lr = 0x832BD448;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA940C);
	// 832BD448: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BD44C: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BD450: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BD454: 419A02B4  beq cr6, 0x832bd708
	if ctx.cr[6].eq {
	pc = 0x832BD708; continue 'dispatch;
	}
	// 832BD458: 38800001  li r4, 1
	ctx.r[4].s64 = 1;
	// 832BD45C: 48000485  bl 0x832bd8e0
	ctx.lr = 0x832BD460;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BD8E0);
	// 832BD460: 3D608350  lis r11, -0x7cb0
	ctx.r[11].s64 = -2091909120;
	// 832BD464: 3BCBC4A8  addi r30, r11, -0x3b58
	ctx.r[30].s64 = ctx.r[11].s64 + -15192;
	// 832BD468: 817F0020  lwz r11, 0x20(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BD46C: 556A0108  rlwinm r10, r11, 0, 4, 4
	ctx.r[10].u64 = ctx.r[11].u32 as u64 & 0xFFFFFFFFu64;
	// 832BD470: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BD474: 409A0018  bne cr6, 0x832bd48c
	if !ctx.cr[6].eq {
	pc = 0x832BD48C; continue 'dispatch;
	}
	// 832BD478: 817F0148  lwz r11, 0x148(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(328 as u32) ) } as u64;
	// 832BD47C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD480: 409A000C  bne cr6, 0x832bd48c
	if !ctx.cr[6].eq {
	pc = 0x832BD48C; continue 'dispatch;
	}
	// 832BD484: 807EFFFC  lwz r3, -4(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(-4 as u32) ) } as u64;
	// 832BD488: 48000008  b 0x832bd490
	pc = 0x832BD490; continue 'dispatch;
	// 832BD48C: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BD490: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD494: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD498: 419A000C  beq cr6, 0x832bd4a4
	if ctx.cr[6].eq {
	pc = 0x832BD4A4; continue 'dispatch;
	}
	// 832BD49C: 80BE0000  lwz r5, 0(r30)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BD4A0: 48000008  b 0x832bd4a8
	pc = 0x832BD4A8; continue 'dispatch;
	// 832BD4A4: 38A00000  li r5, 0
	ctx.r[5].s64 = 0;
	// 832BD4A8: 38E00001  li r7, 1
	ctx.r[7].s64 = 1;
	// 832BD4AC: 38DF0354  addi r6, r31, 0x354
	ctx.r[6].s64 = ctx.r[31].s64 + 852;
	// 832BD4B0: 389F0254  addi r4, r31, 0x254
	ctx.r[4].s64 = ctx.r[31].s64 + 596;
	// 832BD4B4: 48001ACD  bl 0x832bef80
	ctx.lr = 0x832BD4B8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEF80);
	// 832BD4B8: 2F030001  cmpwi cr6, r3, 1
	ctx.cr[6].compare_i32(ctx.r[3].s32, 1, &mut ctx.xer);
	// 832BD4BC: 419A001C  beq cr6, 0x832bd4d8
	if ctx.cr[6].eq {
	pc = 0x832BD4D8; continue 'dispatch;
	}
	// 832BD4C0: 2F030002  cmpwi cr6, r3, 2
	ctx.cr[6].compare_i32(ctx.r[3].s32, 2, &mut ctx.xer);
	// 832BD4C4: 419A007C  beq cr6, 0x832bd540
	if ctx.cr[6].eq {
	pc = 0x832BD540; continue 'dispatch;
	}
	// 832BD4C8: 2F030003  cmpwi cr6, r3, 3
	ctx.cr[6].compare_i32(ctx.r[3].s32, 3, &mut ctx.xer);
	// 832BD4CC: 409A0014  bne cr6, 0x832bd4e0
	if !ctx.cr[6].eq {
	pc = 0x832BD4E0; continue 'dispatch;
	}
	// 832BD4D0: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832BD4D4: 915E0000  stw r10, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832BD4D8: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832BD4DC: 917EFFFC  stw r11, -4(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(-4 as u32), ctx.r[11].u32 ) };
	// 832BD4E0: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD4E4: 3BA00000  li r29, 0
	ctx.r[29].s64 = 0;
	// 832BD4E8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD4EC: 409900B4  ble cr6, 0x832bd5a0
	if !ctx.cr[6].gt {
	pc = 0x832BD5A0; continue 'dispatch;
	}
	// 832BD4F0: 3BC00000  li r30, 0
	ctx.r[30].s64 = 0;
	// 832BD4F4: 817F0118  lwz r11, 0x118(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BD4F8: 7C7E5A14  add r3, r30, r11
	ctx.r[3].u64 = ctx.r[30].u64 + ctx.r[11].u64;
	// 832BD4FC: 81630178  lwz r11, 0x178(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(376 as u32) ) } as u64;
	// 832BD500: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832BD504: 4E800421  bctrl
	ctx.lr = 0x832BD508;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD508: 817F0118  lwz r11, 0x118(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BD50C: 7D5E5A14  add r10, r30, r11
	ctx.r[10].u64 = ctx.r[30].u64 + ctx.r[11].u64;
	// 832BD510: 816A0014  lwz r11, 0x14(r10)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BD514: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD518: 419A0038  beq cr6, 0x832bd550
	if ctx.cr[6].eq {
	pc = 0x832BD550; continue 'dispatch;
	}
	// 832BD51C: 894BFFFE  lbz r10, -2(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832BD520: 2B0A0003  cmplwi cr6, r10, 3
	ctx.cr[6].compare_u32(ctx.r[10].u32, 3 as u32, &mut ctx.xer);
	// 832BD524: 894BFFFF  lbz r10, -1(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832BD528: 7C6A5850  subf r3, r10, r11
	ctx.r[3].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BD52C: 409A0020  bne cr6, 0x832bd54c
	if !ctx.cr[6].eq {
	pc = 0x832BD54C; continue 'dispatch;
	}
	// 832BD530: 812BFFF4  lwz r9, -0xc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832BD534: 7D2903A6  mtctr r9
	ctx.ctr.u64 = ctx.r[9].u64;
	// 832BD538: 4E800421  bctrl
	ctx.lr = 0x832BD53C;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD53C: 48000014  b 0x832bd550
	pc = 0x832BD550; continue 'dispatch;
	// 832BD540: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832BD544: 917E0000  stw r11, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BD548: 4BFFFF98  b 0x832bd4e0
	pc = 0x832BD4E0; continue 'dispatch;
	// 832BD54C: 4B9EEBF5  bl 0x82cac140
	ctx.lr = 0x832BD550;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CAC140);
	// 832BD550: 817F0118  lwz r11, 0x118(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BD554: 7D7E5A14  add r11, r30, r11
	ctx.r[11].u64 = ctx.r[30].u64 + ctx.r[11].u64;
	// 832BD558: 816B000C  lwz r11, 0xc(r11)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BD55C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD560: 419A002C  beq cr6, 0x832bd58c
	if ctx.cr[6].eq {
	pc = 0x832BD58C; continue 'dispatch;
	}
	// 832BD564: 894BFFFE  lbz r10, -2(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832BD568: 2B0A0003  cmplwi cr6, r10, 3
	ctx.cr[6].compare_u32(ctx.r[10].u32, 3 as u32, &mut ctx.xer);
	// 832BD56C: 894BFFFF  lbz r10, -1(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832BD570: 7C6A5850  subf r3, r10, r11
	ctx.r[3].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BD574: 409A0014  bne cr6, 0x832bd588
	if !ctx.cr[6].eq {
	pc = 0x832BD588; continue 'dispatch;
	}
	// 832BD578: 812BFFF4  lwz r9, -0xc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832BD57C: 7D2903A6  mtctr r9
	ctx.ctr.u64 = ctx.r[9].u64;
	// 832BD580: 4E800421  bctrl
	ctx.lr = 0x832BD584;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD584: 48000008  b 0x832bd58c
	pc = 0x832BD58C; continue 'dispatch;
	// 832BD588: 4B9EEBB9  bl 0x82cac140
	ctx.lr = 0x832BD58C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CAC140);
	// 832BD58C: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD590: 3BBD0001  addi r29, r29, 1
	ctx.r[29].s64 = ctx.r[29].s64 + 1;
	// 832BD594: 3BDE0184  addi r30, r30, 0x184
	ctx.r[30].s64 = ctx.r[30].s64 + 388;
	// 832BD598: 7F1D5840  cmplw cr6, r29, r11
	ctx.cr[6].compare_u32(ctx.r[29].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832BD59C: 4198FF58  blt cr6, 0x832bd4f4
	if ctx.cr[6].lt {
	pc = 0x832BD4F4; continue 'dispatch;
	}
	// 832BD5A0: 817F0148  lwz r11, 0x148(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(328 as u32) ) } as u64;
	// 832BD5A4: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD5A8: 419A0038  beq cr6, 0x832bd5e0
	if ctx.cr[6].eq {
	pc = 0x832BD5E0; continue 'dispatch;
	}
	// 832BD5AC: 815F0020  lwz r10, 0x20(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BD5B0: 5549014A  rlwinm r9, r10, 0, 5, 5
	ctx.r[9].u64 = ctx.r[10].u32 as u64 & 0xFFFFFFFFu64;
	// 832BD5B4: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832BD5B8: 409A006C  bne cr6, 0x832bd624
	if !ctx.cr[6].eq {
	pc = 0x832BD624; continue 'dispatch;
	}
	// 832BD5BC: 894BFFFE  lbz r10, -2(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832BD5C0: 2B0A0003  cmplwi cr6, r10, 3
	ctx.cr[6].compare_u32(ctx.r[10].u32, 3 as u32, &mut ctx.xer);
	// 832BD5C4: 894BFFFF  lbz r10, -1(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832BD5C8: 7C6A5850  subf r3, r10, r11
	ctx.r[3].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BD5CC: 409A0054  bne cr6, 0x832bd620
	if !ctx.cr[6].eq {
	pc = 0x832BD620; continue 'dispatch;
	}
	// 832BD5D0: 812BFFF4  lwz r9, -0xc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832BD5D4: 7D2903A6  mtctr r9
	ctx.ctr.u64 = ctx.r[9].u64;
	// 832BD5D8: 4E800421  bctrl
	ctx.lr = 0x832BD5DC;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD5DC: 48000048  b 0x832bd624
	pc = 0x832BD624; continue 'dispatch;
	// 832BD5E0: 817F0164  lwz r11, 0x164(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(356 as u32) ) } as u64;
	// 832BD5E4: 387F0150  addi r3, r31, 0x150
	ctx.r[3].s64 = ctx.r[31].s64 + 336;
	// 832BD5E8: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832BD5EC: 4E800421  bctrl
	ctx.lr = 0x832BD5F0;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD5F0: 817F0294  lwz r11, 0x294(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(660 as u32) ) } as u64;
	// 832BD5F4: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD5F8: 419A002C  beq cr6, 0x832bd624
	if ctx.cr[6].eq {
	pc = 0x832BD624; continue 'dispatch;
	}
	// 832BD5FC: 894BFFFE  lbz r10, -2(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832BD600: 2B0A0003  cmplwi cr6, r10, 3
	ctx.cr[6].compare_u32(ctx.r[10].u32, 3 as u32, &mut ctx.xer);
	// 832BD604: 894BFFFF  lbz r10, -1(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832BD608: 7C6A5850  subf r3, r10, r11
	ctx.r[3].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BD60C: 409A0014  bne cr6, 0x832bd620
	if !ctx.cr[6].eq {
	pc = 0x832BD620; continue 'dispatch;
	}
	// 832BD610: 812BFFF4  lwz r9, -0xc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832BD614: 7D2903A6  mtctr r9
	ctx.ctr.u64 = ctx.r[9].u64;
	// 832BD618: 4E800421  bctrl
	ctx.lr = 0x832BD61C;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD61C: 48000008  b 0x832bd624
	pc = 0x832BD624; continue 'dispatch;
	// 832BD620: 4B9EEB21  bl 0x82cac140
	ctx.lr = 0x832BD624;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CAC140);
	// 832BD624: 817F0398  lwz r11, 0x398(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(920 as u32) ) } as u64;
	// 832BD628: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD62C: 419A002C  beq cr6, 0x832bd658
	if ctx.cr[6].eq {
	pc = 0x832BD658; continue 'dispatch;
	}
	// 832BD630: 894BFFFE  lbz r10, -2(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832BD634: 2B0A0003  cmplwi cr6, r10, 3
	ctx.cr[6].compare_u32(ctx.r[10].u32, 3 as u32, &mut ctx.xer);
	// 832BD638: 894BFFFF  lbz r10, -1(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832BD63C: 7C6A5850  subf r3, r10, r11
	ctx.r[3].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BD640: 409A0014  bne cr6, 0x832bd654
	if !ctx.cr[6].eq {
	pc = 0x832BD654; continue 'dispatch;
	}
	// 832BD644: 812BFFF4  lwz r9, -0xc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832BD648: 7D2903A6  mtctr r9
	ctx.ctr.u64 = ctx.r[9].u64;
	// 832BD64C: 4E800421  bctrl
	ctx.lr = 0x832BD650;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD650: 48000008  b 0x832bd658
	pc = 0x832BD658; continue 'dispatch;
	// 832BD654: 4B9EEAED  bl 0x82cac140
	ctx.lr = 0x832BD658;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CAC140);
	// 832BD658: 817F00C0  lwz r11, 0xc0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(192 as u32) ) } as u64;
	// 832BD65C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD660: 419A002C  beq cr6, 0x832bd68c
	if ctx.cr[6].eq {
	pc = 0x832BD68C; continue 'dispatch;
	}
	// 832BD664: 894BFFFE  lbz r10, -2(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832BD668: 2B0A0003  cmplwi cr6, r10, 3
	ctx.cr[6].compare_u32(ctx.r[10].u32, 3 as u32, &mut ctx.xer);
	// 832BD66C: 894BFFFF  lbz r10, -1(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832BD670: 7C6A5850  subf r3, r10, r11
	ctx.r[3].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BD674: 409A0014  bne cr6, 0x832bd688
	if !ctx.cr[6].eq {
	pc = 0x832BD688; continue 'dispatch;
	}
	// 832BD678: 812BFFF4  lwz r9, -0xc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832BD67C: 7D2903A6  mtctr r9
	ctx.ctr.u64 = ctx.r[9].u64;
	// 832BD680: 4E800421  bctrl
	ctx.lr = 0x832BD684;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD684: 48000008  b 0x832bd68c
	pc = 0x832BD68C; continue 'dispatch;
	// 832BD688: 4B9EEAB9  bl 0x82cac140
	ctx.lr = 0x832BD68C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CAC140);
	// 832BD68C: 817F00CC  lwz r11, 0xcc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(204 as u32) ) } as u64;
	// 832BD690: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD694: 419A0038  beq cr6, 0x832bd6cc
	if ctx.cr[6].eq {
	pc = 0x832BD6CC; continue 'dispatch;
	}
	// 832BD698: 815F00C0  lwz r10, 0xc0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(192 as u32) ) } as u64;
	// 832BD69C: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BD6A0: 419A002C  beq cr6, 0x832bd6cc
	if ctx.cr[6].eq {
	pc = 0x832BD6CC; continue 'dispatch;
	}
	// 832BD6A4: 894BFFFE  lbz r10, -2(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832BD6A8: 2B0A0003  cmplwi cr6, r10, 3
	ctx.cr[6].compare_u32(ctx.r[10].u32, 3 as u32, &mut ctx.xer);
	// 832BD6AC: 894BFFFF  lbz r10, -1(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832BD6B0: 7C6A5850  subf r3, r10, r11
	ctx.r[3].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BD6B4: 409A0014  bne cr6, 0x832bd6c8
	if !ctx.cr[6].eq {
	pc = 0x832BD6C8; continue 'dispatch;
	}
	// 832BD6B8: 812BFFF4  lwz r9, -0xc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832BD6BC: 7D2903A6  mtctr r9
	ctx.ctr.u64 = ctx.r[9].u64;
	// 832BD6C0: 4E800421  bctrl
	ctx.lr = 0x832BD6C4;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD6C4: 48000008  b 0x832bd6cc
	pc = 0x832BD6CC; continue 'dispatch;
	// 832BD6C8: 4B9EEA79  bl 0x82cac140
	ctx.lr = 0x832BD6CC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CAC140);
	// 832BD6CC: 38A0039C  li r5, 0x39c
	ctx.r[5].s64 = 924;
	// 832BD6D0: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BD6D4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD6D8: 4B9EC2D9  bl 0x82ca99b0
	ctx.lr = 0x832BD6DC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BD6DC: 897FFFFE  lbz r11, -2(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[31].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832BD6E0: 2B0B0003  cmplwi cr6, r11, 3
	ctx.cr[6].compare_u32(ctx.r[11].u32, 3 as u32, &mut ctx.xer);
	// 832BD6E4: 897FFFFF  lbz r11, -1(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[31].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832BD6E8: 7C6BF850  subf r3, r11, r31
	ctx.r[3].s64 = ctx.r[31].s64 - ctx.r[11].s64;
	// 832BD6EC: 409A0018  bne cr6, 0x832bd704
	if !ctx.cr[6].eq {
	pc = 0x832BD704; continue 'dispatch;
	}
	// 832BD6F0: 815FFFF4  lwz r10, -0xc(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832BD6F4: 7D4903A6  mtctr r10
	ctx.ctr.u64 = ctx.r[10].u64;
	// 832BD6F8: 4E800421  bctrl
	ctx.lr = 0x832BD6FC;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD6FC: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BD700: 4B9EBD5C  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
	// 832BD704: 4B9EEA3D  bl 0x82cac140
	ctx.lr = 0x832BD708;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CAC140);
	// 832BD708: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BD70C: 4B9EBD50  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
}

pub fn sub_832BD710(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BD710 size=464
	// 832BD710: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BD714: 4B9EBCF9  bl 0x82ca940c
	ctx.lr = 0x832BD718;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA940C);
	// 832BD718: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BD71C: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BD720: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BD724: 409A0010  bne cr6, 0x832bd734
	if !ctx.cr[6].eq {
	pc = 0x832BD734; continue 'dispatch;
	}
	// 832BD728: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BD72C: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BD730: 4B9EBD2C  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
	// 832BD734: 817F02B4  lwz r11, 0x2b4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(692 as u32) ) } as u64;
	// 832BD738: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD73C: 409A0010  bne cr6, 0x832bd74c
	if !ctx.cr[6].eq {
	pc = 0x832BD74C; continue 'dispatch;
	}
	// 832BD740: 817F00EC  lwz r11, 0xec(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(236 as u32) ) } as u64;
	// 832BD744: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BD748: 419AFFE0  beq cr6, 0x832bd728
	if ctx.cr[6].eq {
	pc = 0x832BD728; continue 'dispatch;
	}
	// 832BD74C: 817F001C  lwz r11, 0x1c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(28 as u32) ) } as u64;
	// 832BD750: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD754: 409AFFD4  bne cr6, 0x832bd728
	if !ctx.cr[6].eq {
	pc = 0x832BD728; continue 'dispatch;
	}
	// 832BD758: 83BF02C0  lwz r29, 0x2c0(r31)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(704 as u32) ) } as u64;
	// 832BD75C: 3BC00000  li r30, 0
	ctx.r[30].s64 = 0;
	// 832BD760: 2F1D0000  cmpwi cr6, r29, 0
	ctx.cr[6].compare_i32(ctx.r[29].s32, 0, &mut ctx.xer);
	// 832BD764: 409A0028  bne cr6, 0x832bd78c
	if !ctx.cr[6].eq {
	pc = 0x832BD78C; continue 'dispatch;
	}
	// 832BD768: 2B1D0000  cmplwi cr6, r29, 0
	ctx.cr[6].compare_u32(ctx.r[29].u32, 0 as u32, &mut ctx.xer);
	// 832BD76C: 409A001C  bne cr6, 0x832bd788
	if !ctx.cr[6].eq {
	pc = 0x832BD788; continue 'dispatch;
	}
	// 832BD770: 48003349  bl 0x832c0ab8
	ctx.lr = 0x832BD774;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BD774: 817F02B4  lwz r11, 0x2b4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(692 as u32) ) } as u64;
	// 832BD778: 907F02C0  stw r3, 0x2c0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(704 as u32), ctx.r[3].u32 ) };
	// 832BD77C: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832BD780: 93DF0344  stw r30, 0x344(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(836 as u32), ctx.r[30].u32 ) };
	// 832BD784: 917F02C4  stw r11, 0x2c4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(708 as u32), ctx.r[11].u32 ) };
	// 832BD788: 83BF02C0  lwz r29, 0x2c0(r31)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(704 as u32) ) } as u64;
	// 832BD78C: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD790: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD794: 419A000C  beq cr6, 0x832bd7a0
	if ctx.cr[6].eq {
	pc = 0x832BD7A0; continue 'dispatch;
	}
	// 832BD798: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BD79C: 4BFFD565  bl 0x832bad00
	ctx.lr = 0x832BD7A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BAD00);
	// 832BD7A0: 48003319  bl 0x832c0ab8
	ctx.lr = 0x832BD7A4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BD7A4: 817F02BC  lwz r11, 0x2bc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(700 as u32) ) } as u64;
	// 832BD7A8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD7AC: 419A0020  beq cr6, 0x832bd7cc
	if ctx.cr[6].eq {
	pc = 0x832BD7CC; continue 'dispatch;
	}
	// 832BD7B0: 2F0BFFFF  cmpwi cr6, r11, -1
	ctx.cr[6].compare_i32(ctx.r[11].s32, -1, &mut ctx.xer);
	// 832BD7B4: 419A0018  beq cr6, 0x832bd7cc
	if ctx.cr[6].eq {
	pc = 0x832BD7CC; continue 'dispatch;
	}
	// 832BD7B8: 815F02E8  lwz r10, 0x2e8(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(744 as u32) ) } as u64;
	// 832BD7BC: 7D6B1850  subf r11, r11, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BD7C0: 93DF02BC  stw r30, 0x2bc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(700 as u32), ctx.r[30].u32 ) };
	// 832BD7C4: 7D6B5214  add r11, r11, r10
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832BD7C8: 917F02E8  stw r11, 0x2e8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(744 as u32), ctx.r[11].u32 ) };
	// 832BD7CC: 817F00EC  lwz r11, 0xec(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(236 as u32) ) } as u64;
	// 832BD7D0: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BD7D4: 409A0070  bne cr6, 0x832bd844
	if !ctx.cr[6].eq {
	pc = 0x832BD844; continue 'dispatch;
	}
	// 832BD7D8: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD7DC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD7E0: 419A0010  beq cr6, 0x832bd7f0
	if ctx.cr[6].eq {
	pc = 0x832BD7F0; continue 'dispatch;
	}
	// 832BD7E4: 817F00F8  lwz r11, 0xf8(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(248 as u32) ) } as u64;
	// 832BD7E8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD7EC: 419A0058  beq cr6, 0x832bd844
	if ctx.cr[6].eq {
	pc = 0x832BD844; continue 'dispatch;
	}
	// 832BD7F0: 817F0014  lwz r11, 0x14(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BD7F4: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD7F8: 419AFF30  beq cr6, 0x832bd728
	if ctx.cr[6].eq {
	pc = 0x832BD728; continue 'dispatch;
	}
	// 832BD7FC: 815F02B4  lwz r10, 0x2b4(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(692 as u32) ) } as u64;
	// 832BD800: 79690020  clrldi r9, r11, 0x20
	ctx.r[9].u64 = ctx.r[11].u64 & 0x00000000FFFFFFFFu64;
	// 832BD804: 811F02C4  lwz r8, 0x2c4(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(708 as u32) ) } as u64;
	// 832BD808: 80FF0018  lwz r7, 0x18(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BD80C: 7CC85050  subf r6, r8, r10
	ctx.r[6].s64 = ctx.r[10].s64 - ctx.r[8].s64;
	// 832BD810: 80BF0118  lwz r5, 0x118(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BD814: 78C40020  clrldi r4, r6, 0x20
	ctx.r[4].u64 = ctx.r[6].u64 & 0x00000000FFFFFFFFu64;
	// 832BD818: 7D6439D2  mulld r11, r4, r7
	ctx.r[11].s64 = ctx.r[4].s64 * ctx.r[7].s64;
	// 832BD81C: 81450040  lwz r10, 0x40(r5)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(64 as u32) ) } as u64;
	// 832BD820: 1D0B03E8  mulli r8, r11, 0x3e8
	ctx.r[8].s64 = ctx.r[11].s64 * 1000;
	// 832BD824: 7CE84B92  divdu r7, r8, r9
	ctx.r[7].u64 = ctx.r[8].u64 / ctx.r[9].u64;
	// 832BD828: 54E6003E  slwi r6, r7, 0
	// 832BD82C: 7CA651D2  mulld r5, r6, r10
	ctx.r[5].s64 = ctx.r[6].s64 * ctx.r[10].s64;
	// 832BD830: 78A48402  rldicl r4, r5, 0x30, 0x10
	ctx.r[4].u64 = ctx.r[5].u64 & 0x000000000000FFFFu64;
	// 832BD834: 548B003E  slwi r11, r4, 0
	// 832BD838: 7D4B1850  subf r10, r11, r3
	ctx.r[10].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BD83C: 7D3D5051  subf. r9, r29, r10
	ctx.r[9].s64 = ctx.r[10].s64 - ctx.r[29].s64;
	ctx.cr[0].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832BD840: 4080FEE8  bge 0x832bd728
	if !ctx.cr[0].lt {
	pc = 0x832BD728; continue 'dispatch;
	}
	// 832BD844: 817F0148  lwz r11, 0x148(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(328 as u32) ) } as u64;
	// 832BD848: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD84C: 409A0088  bne cr6, 0x832bd8d4
	if !ctx.cr[6].eq {
	pc = 0x832BD8D4; continue 'dispatch;
	}
	// 832BD850: 817F0020  lwz r11, 0x20(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BD854: 556A0108  rlwinm r10, r11, 0, 4, 4
	ctx.r[10].u64 = ctx.r[11].u32 as u64 & 0xFFFFFFFFu64;
	// 832BD858: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BD85C: 409A0028  bne cr6, 0x832bd884
	if !ctx.cr[6].eq {
	pc = 0x832BD884; continue 'dispatch;
	}
	// 832BD860: 817F00E4  lwz r11, 0xe4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(228 as u32) ) } as u64;
	// 832BD864: 815F0198  lwz r10, 0x198(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(408 as u32) ) } as u64;
	// 832BD868: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BD86C: 41980008  blt cr6, 0x832bd874
	if ctx.cr[6].lt {
	pc = 0x832BD874; continue 'dispatch;
	}
	// 832BD870: 817F0198  lwz r11, 0x198(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(408 as u32) ) } as u64;
	// 832BD874: 815F019C  lwz r10, 0x19c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(412 as u32) ) } as u64;
	// 832BD878: 5569F87E  srwi r9, r11, 1
	// 832BD87C: 7F0A4840  cmplw cr6, r10, r9
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BD880: 40980054  bge cr6, 0x832bd8d4
	if !ctx.cr[6].lt {
	pc = 0x832BD8D4; continue 'dispatch;
	}
	// 832BD884: 3BDF0254  addi r30, r31, 0x254
	ctx.r[30].s64 = ctx.r[31].s64 + 596;
	// 832BD888: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832BD88C: 419A0020  beq cr6, 0x832bd8ac
	if ctx.cr[6].eq {
	pc = 0x832BD8AC; continue 'dispatch;
	}
	// 832BD890: 807E0004  lwz r3, 4(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BD894: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BD898: 419A0014  beq cr6, 0x832bd8ac
	if ctx.cr[6].eq {
	pc = 0x832BD8AC; continue 'dispatch;
	}
	// 832BD89C: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BD8A0: 4AED9129  bl 0x821969c8
	ctx.lr = 0x832BD8A4;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BD8A4: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BD8A8: 409A002C  bne cr6, 0x832bd8d4
	if !ctx.cr[6].eq {
	pc = 0x832BD8D4; continue 'dispatch;
	}
	// 832BD8AC: 817F0160  lwz r11, 0x160(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(352 as u32) ) } as u64;
	// 832BD8B0: 387F0150  addi r3, r31, 0x150
	ctx.r[3].s64 = ctx.r[31].s64 + 336;
	// 832BD8B4: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832BD8B8: 4E800421  bctrl
	ctx.lr = 0x832BD8BC;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD8BC: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832BD8C0: 419A0014  beq cr6, 0x832bd8d4
	if ctx.cr[6].eq {
	pc = 0x832BD8D4; continue 'dispatch;
	}
	// 832BD8C4: 807E0004  lwz r3, 4(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BD8C8: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BD8CC: 419A0008  beq cr6, 0x832bd8d4
	if ctx.cr[6].eq {
	pc = 0x832BD8D4; continue 'dispatch;
	}
	// 832BD8D0: 4BD4E0F1  bl 0x8300b9c0
	ctx.lr = 0x832BD8D4;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BD8D4: 38600001  li r3, 1
	ctx.r[3].s64 = 1;
	// 832BD8D8: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BD8DC: 4B9EBB80  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
}

pub fn sub_832BD8E0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BD8E0 size=496
	// 832BD8E0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BD8E4: 4B9EBB1D  bl 0x82ca9400
	ctx.lr = 0x832BD8E8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9400);
	// 832BD8E8: 9421FF70  stwu r1, -0x90(r1)
	ea = ctx.r[1].u32.wrapping_add(-144 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BD8EC: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BD8F0: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BD8F4: 409A000C  bne cr6, 0x832bd900
	if !ctx.cr[6].eq {
	pc = 0x832BD900; continue 'dispatch;
	}
	// 832BD8F8: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832BD8FC: 4B9EBB54  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832BD900: 480031B9  bl 0x832c0ab8
	ctx.lr = 0x832BD904;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BD904: 817F02BC  lwz r11, 0x2bc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(700 as u32) ) } as u64;
	// 832BD908: 3B800000  li r28, 0
	ctx.r[28].s64 = 0;
	// 832BD90C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD910: 419A0020  beq cr6, 0x832bd930
	if ctx.cr[6].eq {
	pc = 0x832BD930; continue 'dispatch;
	}
	// 832BD914: 2F0BFFFF  cmpwi cr6, r11, -1
	ctx.cr[6].compare_i32(ctx.r[11].s32, -1, &mut ctx.xer);
	// 832BD918: 419A0018  beq cr6, 0x832bd930
	if ctx.cr[6].eq {
	pc = 0x832BD930; continue 'dispatch;
	}
	// 832BD91C: 815F02E8  lwz r10, 0x2e8(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(744 as u32) ) } as u64;
	// 832BD920: 939F02BC  stw r28, 0x2bc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(700 as u32), ctx.r[28].u32 ) };
	// 832BD924: 7D6B5050  subf r11, r11, r10
	ctx.r[11].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832BD928: 7D2B1A14  add r9, r11, r3
	ctx.r[9].u64 = ctx.r[11].u64 + ctx.r[3].u64;
	// 832BD92C: 913F02E8  stw r9, 0x2e8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(744 as u32), ctx.r[9].u32 ) };
	// 832BD930: 3B7F0354  addi r27, r31, 0x354
	ctx.r[27].s64 = ctx.r[31].s64 + 852;
	// 832BD934: 835F00EC  lwz r26, 0xec(r31)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(236 as u32) ) } as u64;
	// 832BD938: 39600001  li r11, 1
	ctx.r[11].s64 = 1;
	// 832BD93C: 939F0324  stw r28, 0x324(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(804 as u32), ctx.r[28].u32 ) };
	// 832BD940: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832BD944: 917F00EC  stw r11, 0xec(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(236 as u32), ctx.r[11].u32 ) };
	// 832BD948: 419A0018  beq cr6, 0x832bd960
	if ctx.cr[6].eq {
	pc = 0x832BD960; continue 'dispatch;
	}
	// 832BD94C: 807B0004  lwz r3, 4(r27)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BD950: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BD954: 419A000C  beq cr6, 0x832bd960
	if ctx.cr[6].eq {
	pc = 0x832BD960; continue 'dispatch;
	}
	// 832BD958: 3880FFFF  li r4, -1
	ctx.r[4].s64 = -1;
	// 832BD95C: 4AED906D  bl 0x821969c8
	ctx.lr = 0x832BD960;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BD960: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD964: 7F9DE378  mr r29, r28
	ctx.r[29].u64 = ctx.r[28].u64;
	// 832BD968: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD96C: 40990034  ble cr6, 0x832bd9a0
	if !ctx.cr[6].gt {
	pc = 0x832BD9A0; continue 'dispatch;
	}
	// 832BD970: 7F9EE378  mr r30, r28
	ctx.r[30].u64 = ctx.r[28].u64;
	// 832BD974: 817F0118  lwz r11, 0x118(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BD978: 38800001  li r4, 1
	ctx.r[4].s64 = 1;
	// 832BD97C: 7C6BF214  add r3, r11, r30
	ctx.r[3].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832BD980: 81630170  lwz r11, 0x170(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(368 as u32) ) } as u64;
	// 832BD984: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832BD988: 4E800421  bctrl
	ctx.lr = 0x832BD98C;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BD98C: 815F0114  lwz r10, 0x114(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BD990: 3BBD0001  addi r29, r29, 1
	ctx.r[29].s64 = ctx.r[29].s64 + 1;
	// 832BD994: 3BDE0184  addi r30, r30, 0x184
	ctx.r[30].s64 = ctx.r[30].s64 + 388;
	// 832BD998: 7F1D5040  cmplw cr6, r29, r10
	ctx.cr[6].compare_u32(ctx.r[29].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BD99C: 4198FFD8  blt cr6, 0x832bd974
	if ctx.cr[6].lt {
	pc = 0x832BD974; continue 'dispatch;
	}
	// 832BD9A0: 4AFA8711  bl 0x822660b0
	ctx.lr = 0x832BD9A4;
	crate::recompiler::externs::call(&mut ctx, base, 0x822660B0);
	// 832BD9A4: 3D608350  lis r11, -0x7cb0
	ctx.r[11].s64 = -2091909120;
	// 832BD9A8: 394BC200  addi r10, r11, -0x3e00
	ctx.r[10].s64 = ctx.r[11].s64 + -15872;
	// 832BD9AC: 816BC200  lwz r11, -0x3e00(r11)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-15872 as u32) ) } as u64;
	// 832BD9B0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD9B4: 409A000C  bne cr6, 0x832bd9c0
	if !ctx.cr[6].eq {
	pc = 0x832BD9C0; continue 'dispatch;
	}
	// 832BD9B8: 7C6B1B78  mr r11, r3
	ctx.r[11].u64 = ctx.r[3].u64;
	// 832BD9BC: 916A0000  stw r11, 0(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BD9C0: 812A0004  lwz r9, 4(r10)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BD9C4: 7D6B1850  subf r11, r11, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BD9C8: 3D00C000  lis r8, -0x4000
	ctx.r[8].s64 = -1073741824;
	// 832BD9CC: 7CE95850  subf r7, r9, r11
	ctx.r[7].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832BD9D0: 7F074040  cmplw cr6, r7, r8
	ctx.cr[6].compare_u32(ctx.r[7].u32, ctx.r[8].u32, &mut ctx.xer);
	// 832BD9D4: 4199000C  bgt cr6, 0x832bd9e0
	if ctx.cr[6].gt {
	pc = 0x832BD9E0; continue 'dispatch;
	}
	// 832BD9D8: 7D695B78  mr r9, r11
	ctx.r[9].u64 = ctx.r[11].u64;
	// 832BD9DC: 916A0004  stw r11, 4(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832BD9E0: 2B1A0000  cmplwi cr6, r26, 0
	ctx.cr[6].compare_u32(ctx.r[26].u32, 0 as u32, &mut ctx.xer);
	// 832BD9E4: 409A0020  bne cr6, 0x832bda04
	if !ctx.cr[6].eq {
	pc = 0x832BDA04; continue 'dispatch;
	}
	// 832BD9E8: 817F02C0  lwz r11, 0x2c0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(704 as u32) ) } as u64;
	// 832BD9EC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BD9F0: 419A0010  beq cr6, 0x832bda00
	if ctx.cr[6].eq {
	pc = 0x832BDA00; continue 'dispatch;
	}
	// 832BD9F4: 7D6B4850  subf r11, r11, r9
	ctx.r[11].s64 = ctx.r[9].s64 - ctx.r[11].s64;
	// 832BD9F8: 917F0344  stw r11, 0x344(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(836 as u32), ctx.r[11].u32 ) };
	// 832BD9FC: 48000008  b 0x832bda04
	pc = 0x832BDA04; continue 'dispatch;
	// 832BDA00: 939F0344  stw r28, 0x344(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(836 as u32), ctx.r[28].u32 ) };
	// 832BDA04: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832BDA08: 419A0014  beq cr6, 0x832bda1c
	if ctx.cr[6].eq {
	pc = 0x832BDA1C; continue 'dispatch;
	}
	// 832BDA0C: 807B0004  lwz r3, 4(r27)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BDA10: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BDA14: 419A0008  beq cr6, 0x832bda1c
	if ctx.cr[6].eq {
	pc = 0x832BDA1C; continue 'dispatch;
	}
	// 832BDA18: 4BD4DFA9  bl 0x8300b9c0
	ctx.lr = 0x832BDA1C;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BDA1C: 817F0114  lwz r11, 0x114(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BDA20: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BDA24: 419A000C  beq cr6, 0x832bda30
	if ctx.cr[6].eq {
	pc = 0x832BDA30; continue 'dispatch;
	}
	// 832BDA28: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BDA2C: 4BFFD2D5  bl 0x832bad00
	ctx.lr = 0x832BDA30;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BAD00);
	// 832BDA30: 807F00EC  lwz r3, 0xec(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(236 as u32) ) } as u64;
	// 832BDA34: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832BDA38: 4B9EBA18  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
}

pub fn sub_832BDAD0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BDAD0 size=464
	// 832BDAD0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BDAD4: 4B9EB919  bl 0x82ca93ec
	ctx.lr = 0x832BDAD8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93EC);
	// 832BDAD8: 9421FF50  stwu r1, -0xb0(r1)
	ea = ctx.r[1].u32.wrapping_add(-176 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BDADC: 7C7E1B78  mr r30, r3
	ctx.r[30].u64 = ctx.r[3].u64;
	// 832BDAE0: 3B400000  li r26, 0
	ctx.r[26].s64 = 0;
	// 832BDAE4: 7C982378  mr r24, r4
	ctx.r[24].u64 = ctx.r[4].u64;
	// 832BDAE8: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832BDAEC: 7F56D378  mr r22, r26
	ctx.r[22].u64 = ctx.r[26].u64;
	// 832BDAF0: 419A00D8  beq cr6, 0x832bdbc8
	if ctx.cr[6].eq {
	pc = 0x832BDBC8; continue 'dispatch;
	}
	// 832BDAF4: 817E0114  lwz r11, 0x114(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BDAF8: 7F59D378  mr r25, r26
	ctx.r[25].u64 = ctx.r[26].u64;
	// 832BDAFC: 7F5CD378  mr r28, r26
	ctx.r[28].u64 = ctx.r[26].u64;
	// 832BDB00: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BDB04: 409900C4  ble cr6, 0x832bdbc8
	if !ctx.cr[6].gt {
	pc = 0x832BDBC8; continue 'dispatch;
	}
	// 832BDB08: 7F5DD378  mr r29, r26
	ctx.r[29].u64 = ctx.r[26].u64;
	// 832BDB0C: 3B600001  li r27, 1
	ctx.r[27].s64 = 1;
	// 832BDB10: 3AE0FFFF  li r23, -1
	ctx.r[23].s64 = -1;
	// 832BDB14: 817E0118  lwz r11, 0x118(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BDB18: 7D6BEA14  add r11, r11, r29
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832BDB1C: 814B0174  lwz r10, 0x174(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(372 as u32) ) } as u64;
	// 832BDB20: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BDB24: 419A0070  beq cr6, 0x832bdb94
	if ctx.cr[6].eq {
	pc = 0x832BDB94; continue 'dispatch;
	}
	// 832BDB28: 2F190000  cmpwi cr6, r25, 0
	ctx.cr[6].compare_i32(ctx.r[25].s32, 0, &mut ctx.xer);
	// 832BDB2C: 409A0024  bne cr6, 0x832bdb50
	if !ctx.cr[6].eq {
	pc = 0x832BDB50; continue 'dispatch;
	}
	// 832BDB30: 357E0354  addic. r11, r30, 0x354
	ctx.xer.ca = (ctx.r[30].u32 > (!(852 as u32)));
	ctx.r[11].s64 = ctx.r[30].s64 + 852;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BDB34: 7F79DB78  mr r25, r27
	ctx.r[25].u64 = ctx.r[27].u64;
	// 832BDB38: 41820018  beq 0x832bdb50
	if ctx.cr[0].eq {
	pc = 0x832BDB50; continue 'dispatch;
	}
	// 832BDB3C: 806B0004  lwz r3, 4(r11)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BDB40: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BDB44: 419A000C  beq cr6, 0x832bdb50
	if ctx.cr[6].eq {
	pc = 0x832BDB50; continue 'dispatch;
	}
	// 832BDB48: 7EE4BB78  mr r4, r23
	ctx.r[4].u64 = ctx.r[23].u64;
	// 832BDB4C: 4AED8E7D  bl 0x821969c8
	ctx.lr = 0x832BDB50;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BDB50: 817E0118  lwz r11, 0x118(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BDB54: 7F0A0034  cntlzw r10, r24
	ctx.r[10].u64 = if ctx.r[24].u32 == 0 { 32 } else { ctx.r[24].u32.leading_zeros() as u64 };
	// 832BDB58: 7C6BEA14  add r3, r11, r29
	ctx.r[3].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832BDB5C: 5549DFFE  rlwinm r9, r10, 0x1b, 0x1f, 0x1f
	ctx.r[9].u64 = ctx.r[10].u32 as u64 & 0x0000001Fu64;
	// 832BDB60: 69240001  xori r4, r9, 1
	ctx.r[4].u64 = ctx.r[9].u64 ^ 1;
	// 832BDB64: 81030174  lwz r8, 0x174(r3)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(372 as u32) ) } as u64;
	// 832BDB68: 7D0903A6  mtctr r8
	ctx.ctr.u64 = ctx.r[8].u64;
	// 832BDB6C: 4E800421  bctrl
	ctx.lr = 0x832BDB70;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BDB70: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832BDB74: 419A0060  beq cr6, 0x832bdbd4
	if ctx.cr[6].eq {
	pc = 0x832BDBD4; continue 'dispatch;
	}
	// 832BDB78: 817E00F8  lwz r11, 0xf8(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(248 as u32) ) } as u64;
	// 832BDB7C: 7F76DB78  mr r22, r27
	ctx.r[22].u64 = ctx.r[27].u64;
	// 832BDB80: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BDB84: 409A0010  bne cr6, 0x832bdb94
	if !ctx.cr[6].eq {
	pc = 0x832BDB94; continue 'dispatch;
	}
	// 832BDB88: 937E00F8  stw r27, 0xf8(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(248 as u32), ctx.r[27].u32 ) };
	// 832BDB8C: 935E02C0  stw r26, 0x2c0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(704 as u32), ctx.r[26].u32 ) };
	// 832BDB90: 935E0344  stw r26, 0x344(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(836 as u32), ctx.r[26].u32 ) };
	// 832BDB94: 817E0114  lwz r11, 0x114(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(276 as u32) ) } as u64;
	// 832BDB98: 3B9C0001  addi r28, r28, 1
	ctx.r[28].s64 = ctx.r[28].s64 + 1;
	// 832BDB9C: 3BBD0184  addi r29, r29, 0x184
	ctx.r[29].s64 = ctx.r[29].s64 + 388;
	// 832BDBA0: 7F1C5840  cmplw cr6, r28, r11
	ctx.cr[6].compare_u32(ctx.r[28].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832BDBA4: 4198FF70  blt cr6, 0x832bdb14
	if ctx.cr[6].lt {
	pc = 0x832BDB14; continue 'dispatch;
	}
	// 832BDBA8: 2F190000  cmpwi cr6, r25, 0
	ctx.cr[6].compare_i32(ctx.r[25].s32, 0, &mut ctx.xer);
	// 832BDBAC: 419A001C  beq cr6, 0x832bdbc8
	if ctx.cr[6].eq {
	pc = 0x832BDBC8; continue 'dispatch;
	}
	// 832BDBB0: 357E0354  addic. r11, r30, 0x354
	ctx.xer.ca = (ctx.r[30].u32 > (!(852 as u32)));
	ctx.r[11].s64 = ctx.r[30].s64 + 852;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BDBB4: 41820014  beq 0x832bdbc8
	if ctx.cr[0].eq {
	pc = 0x832BDBC8; continue 'dispatch;
	}
	// 832BDBB8: 806B0004  lwz r3, 4(r11)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BDBBC: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BDBC0: 419A0008  beq cr6, 0x832bdbc8
	if ctx.cr[6].eq {
	pc = 0x832BDBC8; continue 'dispatch;
	}
	// 832BDBC4: 4BD4DDFD  bl 0x8300b9c0
	ctx.lr = 0x832BDBC8;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BDBC8: 7EC3B378  mr r3, r22
	ctx.r[3].u64 = ctx.r[22].u64;
	// 832BDBCC: 382100B0  addi r1, r1, 0xb0
	ctx.r[1].s64 = ctx.r[1].s64 + 176;
	// 832BDBD0: 4B9EB86C  b 0x82ca943c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA943C);
	return;
	// 832BDBD4: 817E00F8  lwz r11, 0xf8(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(248 as u32) ) } as u64;
	// 832BDBD8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BDBDC: 419AFFB8  beq cr6, 0x832bdb94
	if ctx.cr[6].eq {
	pc = 0x832BDB94; continue 'dispatch;
	}
	// 832BDBE0: 817E0118  lwz r11, 0x118(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BDBE4: 7D6BEA14  add r11, r11, r29
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832BDBE8: 814B000C  lwz r10, 0xc(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BDBEC: 914B0000  stw r10, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832BDBF0: 817E0118  lwz r11, 0x118(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BDBF4: 7D6BEA14  add r11, r11, r29
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832BDBF8: 812B000C  lwz r9, 0xc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BDBFC: 912B0018  stw r9, 0x18(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(24 as u32), ctx.r[9].u32 ) };
	// 832BDC00: 811E000C  lwz r8, 0xc(r30)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BDC04: 2B080001  cmplwi cr6, r8, 1
	ctx.cr[6].compare_u32(ctx.r[8].u32, 1 as u32, &mut ctx.xer);
	// 832BDC08: 409A0010  bne cr6, 0x832bdc18
	if !ctx.cr[6].eq {
	pc = 0x832BDC18; continue 'dispatch;
	}
	// 832BDC0C: 817E0320  lwz r11, 0x320(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(800 as u32) ) } as u64;
	// 832BDC10: 2B0B0001  cmplwi cr6, r11, 1
	ctx.cr[6].compare_u32(ctx.r[11].u32, 1 as u32, &mut ctx.xer);
	// 832BDC14: 409A0084  bne cr6, 0x832bdc98
	if !ctx.cr[6].eq {
	pc = 0x832BDC98; continue 'dispatch;
	}
	// 832BDC18: 817E0118  lwz r11, 0x118(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(280 as u32) ) } as u64;
	// 832BDC1C: 7FEBEA14  add r31, r11, r29
	ctx.r[31].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832BDC20: 7D6BE82E  lwzx r11, r11, r29
	ctx.r[11].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[29].u32)) } as u64;
	// 832BDC24: 813F0018  lwz r9, 0x18(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BDC28: 7D695851  subf. r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BDC2C: 4080000C  bge 0x832bdc38
	if !ctx.cr[0].lt {
	pc = 0x832BDC38; continue 'dispatch;
	}
	// 832BDC30: 815F0008  lwz r10, 8(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BDC34: 7D6A5A14  add r11, r10, r11
	ctx.r[11].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832BDC38: 815F004C  lwz r10, 0x4c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(76 as u32) ) } as u64;
	// 832BDC3C: 7F0B5000  cmpw cr6, r11, r10
	ctx.cr[6].compare_i32(ctx.r[11].s32, ctx.r[10].s32, &mut ctx.xer);
	// 832BDC40: 40980058  bge cr6, 0x832bdc98
	if !ctx.cr[6].lt {
	pc = 0x832BDC98; continue 'dispatch;
	}
	// 832BDC44: 7CAB5050  subf r5, r11, r10
	ctx.r[5].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832BDC48: 807F000C  lwz r3, 0xc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BDC4C: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BDC50: 7D654850  subf r11, r5, r9
	ctx.r[11].s64 = ctx.r[9].s64 - ctx.r[5].s64;
	// 832BDC54: 917F0018  stw r11, 0x18(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(24 as u32), ctx.r[11].u32 ) };
	// 832BDC58: 7F0B1840  cmplw cr6, r11, r3
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[3].u32, &mut ctx.xer);
	// 832BDC5C: 40980034  bge cr6, 0x832bdc90
	if !ctx.cr[6].lt {
	pc = 0x832BDC90; continue 'dispatch;
	}
	// 832BDC60: 815F0008  lwz r10, 8(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BDC64: 7EAB1850  subf r21, r11, r3
	ctx.r[21].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BDC68: 7D6A5A14  add r11, r10, r11
	ctx.r[11].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832BDC6C: 7CB52850  subf r5, r21, r5
	ctx.r[5].s64 = ctx.r[5].s64 - ctx.r[21].s64;
	// 832BDC70: 917F0018  stw r11, 0x18(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(24 as u32), ctx.r[11].u32 ) };
	// 832BDC74: 4B9EBD3D  bl 0x82ca99b0
	ctx.lr = 0x832BDC78;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BDC78: 7EA5AB78  mr r5, r21
	ctx.r[5].u64 = ctx.r[21].u64;
	// 832BDC7C: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BDC80: 807F0018  lwz r3, 0x18(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BDC84: 4B9EBD2D  bl 0x82ca99b0
	ctx.lr = 0x832BDC88;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BDC88: 935E00F8  stw r26, 0xf8(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(248 as u32), ctx.r[26].u32 ) };
	// 832BDC8C: 4BFFFF08  b 0x832bdb94
	pc = 0x832BDB94; continue 'dispatch;
	// 832BDC90: 7D635B78  mr r3, r11
	ctx.r[3].u64 = ctx.r[11].u64;
	// 832BDC94: 4B9EBD1D  bl 0x82ca99b0
	ctx.lr = 0x832BDC98;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BDC98: 935E00F8  stw r26, 0xf8(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(248 as u32), ctx.r[26].u32 ) };
	// 832BDC9C: 4BFFFEF8  b 0x832bdb94
	pc = 0x832BDB94; continue 'dispatch;
}

pub fn sub_832BDCA0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    let mut ea: u32 = 0;
    // ---- function 0x832BDCA0 size=408
	// 832BDCA0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BDCA4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BDCA8: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BDCAC: 9421FF20  stwu r1, -0xe0(r1)
	ea = ctx.r[1].u32.wrapping_add(-224 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BDCB0: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BDCB4: 817F005C  lwz r11, 0x5c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BDCB8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BDCBC: 419A0168  beq cr6, 0x832bde24
	if ctx.cr[6].eq {
	pc = 0x832BDE24; continue 'dispatch;
	}
	// 832BDCC0: 817F00C8  lwz r11, 0xc8(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(200 as u32) ) } as u64;
	// 832BDCC4: 2F0B7FFF  cmpwi cr6, r11, 0x7fff
	ctx.cr[6].compare_i32(ctx.r[11].s32, 32767, &mut ctx.xer);
	// 832BDCC8: 40990008  ble cr6, 0x832bdcd0
	if !ctx.cr[6].gt {
	pc = 0x832BDCD0; continue 'dispatch;
	}
	// 832BDCCC: 39607FFF  li r11, 0x7fff
	ctx.r[11].s64 = 32767;
	// 832BDCD0: 7D6B07B4  extsw r11, r11
	ctx.r[11].s64 = ctx.r[11].s32 as i64;
	// 832BDCD4: 3D208200  lis r9, -0x7e00
	ctx.r[9].s64 = -2113929216;
	// 832BDCD8: F9610060  std r11, 0x60(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(96 as u32), ctx.r[11].u64 ) };
	// 832BDCDC: C8010060  lfd f0, 0x60(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[0].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(96 as u32) ) };
	// 832BDCE0: FDA0069C  fcfid f13, f0
	ctx.f[13].f64 = (ctx.f[0].s64 as f64);
	// 832BDCE4: 3D408210  lis r10, -0x7df0
	ctx.r[10].s64 = -2112880640;
	// 832BDCE8: FD806818  frsp f12, f13
	ctx.f[12].f64 = (ctx.f[13].f64 as f32) as f64;
	// 832BDCEC: C0090C20  lfs f0, 0xc20(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(3104 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832BDCF0: C84A0F10  lfd f2, 0xf10(r10)
	ctx.f[2].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[10].u32.wrapping_add(3856 as u32) ) };
	// 832BDCF4: EC2C0032  fmuls f1, f12, f0
	ctx.f[1].f64 = (((ctx.f[12].f64 * ctx.f[0].f64) as f32) as f64);
	// 832BDCF8: 4AF407B1  bl 0x821fe4a8
	ctx.lr = 0x832BDCFC;
	crate::recompiler::externs::call(&mut ctx, base, 0x821FE4A8);
	// 832BDCFC: 807F005C  lwz r3, 0x5c(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BDD00: FC200818  frsp f1, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[1].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832BDD04: 4BA1BAAD  bl 0x82cd97b0
	ctx.lr = 0x832BDD08;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD97B0);
	// 832BDD08: 80DF00C4  lwz r6, 0xc4(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(196 as u32) ) } as u64;
	// 832BDD0C: 80BF0024  lwz r5, 0x24(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(36 as u32) ) } as u64;
	// 832BDD10: F8C10058  std r6, 0x58(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[6].u64 ) };
	// 832BDD14: C9610058  lfd f11, 0x58(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[11].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) };
	// 832BDD18: F8A10058  std r5, 0x58(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[5].u64 ) };
	// 832BDD1C: C9410058  lfd f10, 0x58(r1)
	ctx.f[10].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) };
	// 832BDD20: FD20569C  fcfid f9, f10
	ctx.f[9].f64 = (ctx.f[10].s64 as f64);
	// 832BDD24: FD005E9C  fcfid f8, f11
	ctx.f[8].f64 = (ctx.f[11].s64 as f64);
	// 832BDD28: FC284824  fdiv f1, f8, f9
	ctx.f[1].f64 = ctx.f[8].f64 / ctx.f[9].f64;
	// 832BDD2C: 4AF36085  bl 0x821f3db0
	ctx.lr = 0x832BDD30;
	crate::recompiler::externs::call(&mut ctx, base, 0x821F3DB0);
	// 832BDD30: 3C808210  lis r4, -0x7df0
	ctx.r[4].s64 = -2112880640;
	// 832BDD34: 807F005C  lwz r3, 0x5c(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BDD38: C8040E78  lfd f0, 0xe78(r4)
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[0].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[4].u32.wrapping_add(3704 as u32) ) };
	// 832BDD3C: FCE10032  fmul f7, f1, f0
	ctx.f[7].f64 = ctx.f[1].f64 * ctx.f[0].f64;
	// 832BDD40: FC203818  frsp f1, f7
	ctx.f[1].f64 = (ctx.f[7].f64 as f32) as f64;
	// 832BDD44: 4BA1BAE5  bl 0x82cd9828
	ctx.lr = 0x832BDD48;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9828);
	// 832BDD48: 807F00FC  lwz r3, 0xfc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(252 as u32) ) } as u64;
	// 832BDD4C: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832BDD50: 419A00D4  beq cr6, 0x832bde24
	if ctx.cr[6].eq {
	pc = 0x832BDE24; continue 'dispatch;
	}
	// 832BDD54: 80BF0100  lwz r5, 0x100(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(256 as u32) ) } as u64;
	// 832BDD58: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832BDD5C: 39600001  li r11, 1
	ctx.r[11].s64 = 1;
	// 832BDD60: 39210050  addi r9, r1, 0x50
	ctx.r[9].s64 = ctx.r[1].s64 + 80;
	// 832BDD64: 915F00FC  stw r10, 0xfc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(252 as u32), ctx.r[10].u32 ) };
	// 832BDD68: 39010058  addi r8, r1, 0x58
	ctx.r[8].s64 = ctx.r[1].s64 + 88;
	// 832BDD6C: 99410050  stb r10, 0x50(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[10].u8 ) };
	// 832BDD70: 38E10070  addi r7, r1, 0x70
	ctx.r[7].s64 = ctx.r[1].s64 + 112;
	// 832BDD74: 99610060  stb r11, 0x60(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(96 as u32), ctx.r[11].u8 ) };
	// 832BDD78: 91210064  stw r9, 0x64(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(100 as u32), ctx.r[9].u32 ) };
	// 832BDD7C: 2B050000  cmplwi cr6, r5, 0
	ctx.cr[6].compare_u32(ctx.r[5].u32, 0 as u32, &mut ctx.xer);
	// 832BDD80: 91010054  stw r8, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[8].u32 ) };
	// 832BDD84: 90E1005C  stw r7, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[7].u32 ) };
	// 832BDD88: 98A10058  stb r5, 0x58(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[5].u8 ) };
	// 832BDD8C: 419A0030  beq cr6, 0x832bddbc
	if ctx.cr[6].eq {
	pc = 0x832BDDBC; continue 'dispatch;
	}
	// 832BDD90: 3D00820A  lis r8, -0x7df6
	ctx.r[8].s64 = -2113273856;
	// 832BDD94: 54A9003E  slwi r9, r5, 0
	// 832BDD98: 39610074  addi r11, r1, 0x74
	ctx.r[11].s64 = ctx.r[1].s64 + 116;
	// 832BDD9C: C0089484  lfs f0, -0x6b7c(r8)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(-27516 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832BDDA0: 7D485378  mr r8, r10
	ctx.r[8].u64 = ctx.r[10].u64;
	// 832BDDA4: D00B0000  stfs f0, 0(r11)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832BDDA8: 394A0001  addi r10, r10, 1
	ctx.r[10].s64 = ctx.r[10].s64 + 1;
	// 832BDDAC: 990BFFFC  stb r8, -4(r11)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[11].u32.wrapping_add(-4 as u32), ctx.r[8].u8 ) };
	// 832BDDB0: 396B0008  addi r11, r11, 8
	ctx.r[11].s64 = ctx.r[11].s64 + 8;
	// 832BDDB4: 7F0A4840  cmplw cr6, r10, r9
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BDDB8: 4198FFE8  blt cr6, 0x832bdda0
	if ctx.cr[6].lt {
	pc = 0x832BDDA0; continue 'dispatch;
	}
	// 832BDDBC: 817F0104  lwz r11, 0x104(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(260 as u32) ) } as u64;
	// 832BDDC0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BDDC4: 40990054  ble cr6, 0x832bde18
	if !ctx.cr[6].gt {
	pc = 0x832BDE18; continue 'dispatch;
	}
	// 832BDDC8: 5566003E  slwi r6, r11, 0
	// 832BDDCC: 391F0168  addi r8, r31, 0x168
	ctx.r[8].s64 = ctx.r[31].s64 + 360;
	// 832BDDD0: 2B050000  cmplwi cr6, r5, 0
	ctx.cr[6].compare_u32(ctx.r[5].u32, 0 as u32, &mut ctx.xer);
	// 832BDDD4: 419A0038  beq cr6, 0x832bde0c
	if ctx.cr[6].eq {
	pc = 0x832BDE0C; continue 'dispatch;
	}
	// 832BDDD8: 80E8FFD0  lwz r7, -0x30(r8)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(-48 as u32) ) } as u64;
	// 832BDDDC: 39410074  addi r10, r1, 0x74
	ctx.r[10].s64 = ctx.r[1].s64 + 116;
	// 832BDDE0: 813F0100  lwz r9, 0x100(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(256 as u32) ) } as u64;
	// 832BDDE4: 397F0108  addi r11, r31, 0x108
	ctx.r[11].s64 = ctx.r[31].s64 + 264;
	// 832BDDE8: 808B0000  lwz r4, 0(r11)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BDDEC: 7F072040  cmplw cr6, r7, r4
	ctx.cr[6].compare_u32(ctx.r[7].u32, ctx.r[4].u32, &mut ctx.xer);
	// 832BDDF0: 409A000C  bne cr6, 0x832bddfc
	if !ctx.cr[6].eq {
	pc = 0x832BDDFC; continue 'dispatch;
	}
	// 832BDDF4: C0080000  lfs f0, 0(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832BDDF8: D00A0000  stfs f0, 0(r10)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832BDDFC: 3529FFFF  addic. r9, r9, -1
	ctx.xer.ca = (ctx.r[9].u32 > (!(-1 as u32)));
	ctx.r[9].s64 = ctx.r[9].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832BDE00: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832BDE04: 394A0008  addi r10, r10, 8
	ctx.r[10].s64 = ctx.r[10].s64 + 8;
	// 832BDE08: 4082FFE0  bne 0x832bdde8
	if !ctx.cr[0].eq {
	pc = 0x832BDDE8; continue 'dispatch;
	}
	// 832BDE0C: 34C6FFFF  addic. r6, r6, -1
	ctx.xer.ca = (ctx.r[6].u32 > (!(-1 as u32)));
	ctx.r[6].s64 = ctx.r[6].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[6].s32, 0, &mut ctx.xer);
	// 832BDE10: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832BDE14: 4082FFBC  bne 0x832bddd0
	if !ctx.cr[0].eq {
	pc = 0x832BDDD0; continue 'dispatch;
	}
	// 832BDE18: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832BDE1C: 807F005C  lwz r3, 0x5c(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BDE20: 4BA1B5C1  bl 0x82cd93e0
	ctx.lr = 0x832BDE24;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD93E0);
	// 832BDE24: 382100E0  addi r1, r1, 0xe0
	ctx.r[1].s64 = ctx.r[1].s64 + 224;
	// 832BDE28: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BDE2C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BDE30: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BDE34: 4E800020  blr
	return;
}

pub fn sub_832BDE38(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BDE38 size=104
	// 832BDE38: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BDE3C: 4B9EB5D1  bl 0x82ca940c
	ctx.lr = 0x832BDE40;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA940C);
	// 832BDE40: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BDE44: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BDE48: 7CBD2B78  mr r29, r5
	ctx.r[29].u64 = ctx.r[5].u64;
	// 832BDE4C: 7C8B2378  mr r11, r4
	ctx.r[11].u64 = ctx.r[4].u64;
	// 832BDE50: 7FBEEB78  mr r30, r29
	ctx.r[30].u64 = ctx.r[29].u64;
	// 832BDE54: 7D2BEA14  add r9, r11, r29
	ctx.r[9].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832BDE58: 815F0060  lwz r10, 0x60(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) } as u64;
	// 832BDE5C: 7F095040  cmplw cr6, r9, r10
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BDE60: 40990008  ble cr6, 0x832bde68
	if !ctx.cr[6].gt {
	pc = 0x832BDE68; continue 'dispatch;
	}
	// 832BDE64: 7FCB5050  subf r30, r11, r10
	ctx.r[30].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832BDE68: 815F005C  lwz r10, 0x5c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BDE6C: 7FC5F378  mr r5, r30
	ctx.r[5].u64 = ctx.r[30].u64;
	// 832BDE70: 809F0098  lwz r4, 0x98(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(152 as u32) ) } as u64;
	// 832BDE74: 7C6A5A14  add r3, r10, r11
	ctx.r[3].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832BDE78: 4B9EBB39  bl 0x82ca99b0
	ctx.lr = 0x832BDE7C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BDE7C: 7F1EE840  cmplw cr6, r30, r29
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[29].u32, &mut ctx.xer);
	// 832BDE80: 40980014  bge cr6, 0x832bde94
	if !ctx.cr[6].lt {
	pc = 0x832BDE94; continue 'dispatch;
	}
	// 832BDE84: 7CBEE850  subf r5, r30, r29
	ctx.r[5].s64 = ctx.r[29].s64 - ctx.r[30].s64;
	// 832BDE88: 809F0098  lwz r4, 0x98(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(152 as u32) ) } as u64;
	// 832BDE8C: 807F005C  lwz r3, 0x5c(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BDE90: 4B9EBB21  bl 0x82ca99b0
	ctx.lr = 0x832BDE94;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BDE94: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BDE98: 4B9EB5C4  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
}

pub fn sub_832BDEA0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BDEA0 size=12
	// 832BDEA0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BDEA4: 4B9EB55D  bl 0x82ca9400
	ctx.lr = 0x832BDEA8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9400);
	// 832BDEA8: 9421FF10  stwu r1, -0xf0(r1)
	ea = ctx.r[1].u32.wrapping_add(-240 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
}

pub fn sub_832BE098(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BE098 size=416
	// 832BE098: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BE09C: 4B9EB361  bl 0x82ca93fc
	ctx.lr = 0x832BE0A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93FC);
	// 832BE0A0: 9421FF40  stwu r1, -0xc0(r1)
	ea = ctx.r[1].u32.wrapping_add(-192 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BE0A4: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BE0A8: 39600001  li r11, 1
	ctx.r[11].s64 = 1;
	// 832BE0AC: 3B800000  li r28, 0
	ctx.r[28].s64 = 0;
	// 832BE0B0: 93810050  stw r28, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[28].u32 ) };
	// 832BE0B4: 917F0074  stw r11, 0x74(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(116 as u32), ctx.r[11].u32 ) };
	// 832BE0B8: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE0BC: 4BA1B435  bl 0x82cd94f0
	ctx.lr = 0x832BE0C0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD94F0);
	// 832BE0C0: 815F0060  lwz r10, 0x60(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) } as u64;
	// 832BE0C4: 813F005C  lwz r9, 0x5c(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BE0C8: 3BC0FFFF  li r30, -1
	ctx.r[30].s64 = -1;
	// 832BE0CC: 939F0010  stw r28, 0x10(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(16 as u32), ctx.r[28].u32 ) };
	// 832BE0D0: 389F0004  addi r4, r31, 4
	ctx.r[4].s64 = ctx.r[31].s64 + 4;
	// 832BE0D4: 93DF000C  stw r30, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[30].u32 ) };
	// 832BE0D8: 38A00001  li r5, 1
	ctx.r[5].s64 = 1;
	// 832BE0DC: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE0E0: 915F0008  stw r10, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[10].u32 ) };
	// 832BE0E4: 913F0004  stw r9, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[9].u32 ) };
	// 832BE0E8: 915F0014  stw r10, 0x14(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(20 as u32), ctx.r[10].u32 ) };
	// 832BE0EC: 4BA1B3A5  bl 0x82cd9490
	ctx.lr = 0x832BE0F0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9490);
	// 832BE0F0: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832BE0F4: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE0F8: 4BA1B609  bl 0x82cd9700
	ctx.lr = 0x832BE0FC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9700);
	// 832BE0FC: 38810050  addi r4, r1, 0x50
	ctx.r[4].s64 = ctx.r[1].s64 + 80;
	// 832BE100: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE104: 4BA1B5A5  bl 0x82cd96a8
	ctx.lr = 0x832BE108;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD96A8);
	// 832BE108: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BE10C: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE110: 4BA1B439  bl 0x82cd9548
	ctx.lr = 0x832BE114;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9548);
	// 832BE114: 939F0078  stw r28, 0x78(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(120 as u32), ctx.r[28].u32 ) };
	// 832BE118: 480029A1  bl 0x832c0ab8
	ctx.lr = 0x832BE11C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BE11C: 38810050  addi r4, r1, 0x50
	ctx.r[4].s64 = ctx.r[1].s64 + 80;
	// 832BE120: 7C791B78  mr r25, r3
	ctx.r[25].u64 = ctx.r[3].u64;
	// 832BE124: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE128: 4BA1B529  bl 0x82cd9650
	ctx.lr = 0x832BE12C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9650);
	// 832BE12C: 3D608350  lis r11, -0x7cb0
	ctx.r[11].s64 = -2091909120;
	// 832BE130: 7F9AE378  mr r26, r28
	ctx.r[26].u64 = ctx.r[28].u64;
	// 832BE134: 3F60C000  lis r27, -0x4000
	ctx.r[27].s64 = -1073741824;
	// 832BE138: 3BCBC200  addi r30, r11, -0x3e00
	ctx.r[30].s64 = ctx.r[11].s64 + -15872;
	// 832BE13C: 83A10050  lwz r29, 0x50(r1)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BE140: 93810050  stw r28, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[28].u32 ) };
	// 832BE144: 38810050  addi r4, r1, 0x50
	ctx.r[4].s64 = ctx.r[1].s64 + 80;
	// 832BE148: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE14C: 4BA1B505  bl 0x82cd9650
	ctx.lr = 0x832BE150;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9650);
	// 832BE150: 81610050  lwz r11, 0x50(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BE154: 7F0BE840  cmplw cr6, r11, r29
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[29].u32, &mut ctx.xer);
	// 832BE158: 917F0088  stw r11, 0x88(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(136 as u32), ctx.r[11].u32 ) };
	// 832BE15C: 419A000C  beq cr6, 0x832be168
	if ctx.cr[6].eq {
	pc = 0x832BE168; continue 'dispatch;
	}
	// 832BE160: 3B400001  li r26, 1
	ctx.r[26].s64 = 1;
	// 832BE164: 480000B0  b 0x832be214
	pc = 0x832BE214; continue 'dispatch;
	// 832BE168: 2B1A0000  cmplwi cr6, r26, 0
	ctx.cr[6].compare_u32(ctx.r[26].u32, 0 as u32, &mut ctx.xer);
	// 832BE16C: 409A00A8  bne cr6, 0x832be214
	if !ctx.cr[6].eq {
	pc = 0x832BE214; continue 'dispatch;
	}
	// 832BE170: 4AFA7F41  bl 0x822660b0
	ctx.lr = 0x832BE174;
	crate::recompiler::externs::call(&mut ctx, base, 0x822660B0);
	// 832BE174: 817E0000  lwz r11, 0(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE178: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BE17C: 409A000C  bne cr6, 0x832be188
	if !ctx.cr[6].eq {
	pc = 0x832BE188; continue 'dispatch;
	}
	// 832BE180: 7C6B1B78  mr r11, r3
	ctx.r[11].u64 = ctx.r[3].u64;
	// 832BE184: 917E0000  stw r11, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BE188: 815E0004  lwz r10, 4(r30)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BE18C: 7D6B1850  subf r11, r11, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BE190: 7D2A5850  subf r9, r10, r11
	ctx.r[9].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BE194: 7F09D840  cmplw cr6, r9, r27
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832BE198: 4199000C  bgt cr6, 0x832be1a4
	if ctx.cr[6].gt {
	pc = 0x832BE1A4; continue 'dispatch;
	}
	// 832BE19C: 7D6A5B78  mr r10, r11
	ctx.r[10].u64 = ctx.r[11].u64;
	// 832BE1A0: 917E0004  stw r11, 4(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832BE1A4: 7D795050  subf r11, r25, r10
	ctx.r[11].s64 = ctx.r[10].s64 - ctx.r[25].s64;
	// 832BE1A8: 2B0B0032  cmplwi cr6, r11, 0x32
	ctx.cr[6].compare_u32(ctx.r[11].u32, 50 as u32, &mut ctx.xer);
	// 832BE1AC: 40990068  ble cr6, 0x832be214
	if !ctx.cr[6].gt {
	pc = 0x832BE214; continue 'dispatch;
	}
	// 832BE1B0: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BE1B4: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE1B8: 4BA1B391  bl 0x82cd9548
	ctx.lr = 0x832BE1BC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9548);
	// 832BE1BC: 939F0078  stw r28, 0x78(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(120 as u32), ctx.r[28].u32 ) };
	// 832BE1C0: 4AFA7EF1  bl 0x822660b0
	ctx.lr = 0x832BE1C4;
	crate::recompiler::externs::call(&mut ctx, base, 0x822660B0);
	// 832BE1C4: 817E0000  lwz r11, 0(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE1C8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BE1CC: 409A000C  bne cr6, 0x832be1d8
	if !ctx.cr[6].eq {
	pc = 0x832BE1D8; continue 'dispatch;
	}
	// 832BE1D0: 7C6B1B78  mr r11, r3
	ctx.r[11].u64 = ctx.r[3].u64;
	// 832BE1D4: 917E0000  stw r11, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BE1D8: 815E0004  lwz r10, 4(r30)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BE1DC: 7D6B1850  subf r11, r11, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BE1E0: 7D2A5850  subf r9, r10, r11
	ctx.r[9].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BE1E4: 7F09D840  cmplw cr6, r9, r27
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832BE1E8: 4099000C  ble cr6, 0x832be1f4
	if !ctx.cr[6].gt {
	pc = 0x832BE1F4; continue 'dispatch;
	}
	// 832BE1EC: 7D595378  mr r25, r10
	ctx.r[25].u64 = ctx.r[10].u64;
	// 832BE1F0: 4800000C  b 0x832be1fc
	pc = 0x832BE1FC; continue 'dispatch;
	// 832BE1F4: 7D795B78  mr r25, r11
	ctx.r[25].u64 = ctx.r[11].u64;
	// 832BE1F8: 917E0004  stw r11, 4(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832BE1FC: 93810050  stw r28, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[28].u32 ) };
	// 832BE200: 38810050  addi r4, r1, 0x50
	ctx.r[4].s64 = ctx.r[1].s64 + 80;
	// 832BE204: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE208: 4BA1B449  bl 0x82cd9650
	ctx.lr = 0x832BE20C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9650);
	// 832BE20C: 7F9AE378  mr r26, r28
	ctx.r[26].u64 = ctx.r[28].u64;
	// 832BE210: 83A10050  lwz r29, 0x50(r1)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BE214: 817F0088  lwz r11, 0x88(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(136 as u32) ) } as u64;
	// 832BE218: 815F0064  lwz r10, 0x64(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(100 as u32) ) } as u64;
	// 832BE21C: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832BE220: 4199FF20  bgt cr6, 0x832be140
	if ctx.cr[6].gt {
	pc = 0x832BE140; continue 'dispatch;
	}
	// 832BE224: 48002895  bl 0x832c0ab8
	ctx.lr = 0x832BE228;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832BE228: 907F0090  stw r3, 0x90(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(144 as u32), ctx.r[3].u32 ) };
	// 832BE22C: 382100C0  addi r1, r1, 0xc0
	ctx.r[1].s64 = ctx.r[1].s64 + 192;
	// 832BE230: 4B9EB21C  b 0x82ca944c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA944C);
	return;
}

pub fn sub_832BE238(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BE238 size=176
	// 832BE238: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BE23C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BE240: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BE244: 7C6B1B78  mr r11, r3
	ctx.r[11].u64 = ctx.r[3].u64;
	// 832BE248: 814B00D8  lwz r10, 0xd8(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(216 as u32) ) } as u64;
	// 832BE24C: 812B00BC  lwz r9, 0xbc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(188 as u32) ) } as u64;
	// 832BE250: 7D445214  add r10, r4, r10
	ctx.r[10].u64 = ctx.r[4].u64 + ctx.r[10].u64;
	// 832BE254: 914B00D8  stw r10, 0xd8(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(216 as u32), ctx.r[10].u32 ) };
	// 832BE258: 7F0A4840  cmplw cr6, r10, r9
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BE25C: 4198000C  blt cr6, 0x832be268
	if ctx.cr[6].lt {
	pc = 0x832BE268; continue 'dispatch;
	}
	// 832BE260: 7D495050  subf r10, r9, r10
	ctx.r[10].s64 = ctx.r[10].s64 - ctx.r[9].s64;
	// 832BE264: 914B00D8  stw r10, 0xd8(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(216 as u32), ctx.r[10].u32 ) };
	// 832BE268: 814B00DC  lwz r10, 0xdc(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(220 as u32) ) } as u64;
	// 832BE26C: 812B00D0  lwz r9, 0xd0(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(208 as u32) ) } as u64;
	// 832BE270: 7D445050  subf r10, r4, r10
	ctx.r[10].s64 = ctx.r[10].s64 - ctx.r[4].s64;
	// 832BE274: 2F090000  cmpwi cr6, r9, 0
	ctx.cr[6].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832BE278: 914B00DC  stw r10, 0xdc(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(220 as u32), ctx.r[10].u32 ) };
	// 832BE27C: 409A0024  bne cr6, 0x832be2a0
	if !ctx.cr[6].eq {
	pc = 0x832BE2A0; continue 'dispatch;
	}
	// 832BE280: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BE284: 409A001C  bne cr6, 0x832be2a0
	if !ctx.cr[6].eq {
	pc = 0x832BE2A0; continue 'dispatch;
	}
	// 832BE288: 814B00D4  lwz r10, 0xd4(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(212 as u32) ) } as u64;
	// 832BE28C: 2F0A0000  cmpwi cr6, r10, 0
	ctx.cr[6].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832BE290: 409A0010  bne cr6, 0x832be2a0
	if !ctx.cr[6].eq {
	pc = 0x832BE2A0; continue 'dispatch;
	}
	// 832BE294: 386B005C  addi r3, r11, 0x5c
	ctx.r[3].s64 = ctx.r[11].s64 + 92;
	// 832BE298: 808B002C  lwz r4, 0x2c(r11)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(44 as u32) ) } as u64;
	// 832BE29C: 4BFFFDFD  bl 0x832be098
	ctx.lr = 0x832BE2A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BE098);
	// 832BE2A0: 38600001  li r3, 1
	ctx.r[3].s64 = 1;
	// 832BE2A4: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832BE2A8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BE2AC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BE2B0: 4E800020  blr
	return;
}

pub fn sub_832BE2E8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BE2E8 size=16
	// 832BE2E8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BE2EC: 4B9EB119  bl 0x82ca9404
	ctx.lr = 0x832BE2F0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9404);
	// 832BE2F0: DBE1FFC8  stfd f31, -0x38(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-56 as u32), ctx.f[31].u64 ) };
	// 832BE2F4: 9421FEF0  stwu r1, -0x110(r1)
	ea = ctx.r[1].u32.wrapping_add(-272 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
}

pub fn sub_832BE498(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BE498 size=192
	// 832BE498: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BE49C: 4B9EAF71  bl 0x82ca940c
	ctx.lr = 0x832BE4A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA940C);
	// 832BE4A0: 9421FF50  stwu r1, -0xb0(r1)
	ea = ctx.r[1].u32.wrapping_add(-176 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BE4A4: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BE4A8: 2F040000  cmpwi cr6, r4, 0
	ctx.cr[6].compare_i32(ctx.r[4].s32, 0, &mut ctx.xer);
	// 832BE4AC: 817F0038  lwz r11, 0x38(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(56 as u32) ) } as u64;
	// 832BE4B0: 419A0034  beq cr6, 0x832be4e4
	if ctx.cr[6].eq {
	pc = 0x832BE4E4; continue 'dispatch;
	}
	// 832BE4B4: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BE4B8: 409A0090  bne cr6, 0x832be548
	if !ctx.cr[6].eq {
	pc = 0x832BE548; continue 'dispatch;
	}
	// 832BE4BC: 817F00E0  lwz r11, 0xe0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(224 as u32) ) } as u64;
	// 832BE4C0: 39400001  li r10, 1
	ctx.r[10].s64 = 1;
	// 832BE4C4: 3BA00000  li r29, 0
	ctx.r[29].s64 = 0;
	// 832BE4C8: 915F0038  stw r10, 0x38(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(56 as u32), ctx.r[10].u32 ) };
	// 832BE4CC: 5543003E  slwi r3, r10, 0
	// 832BE4D0: 93BF00D0  stw r29, 0xd0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(208 as u32), ctx.r[29].u32 ) };
	// 832BE4D4: 93BF00D8  stw r29, 0xd8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(216 as u32), ctx.r[29].u32 ) };
	// 832BE4D8: 917F00DC  stw r11, 0xdc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(220 as u32), ctx.r[11].u32 ) };
	// 832BE4DC: 382100B0  addi r1, r1, 0xb0
	ctx.r[1].s64 = ctx.r[1].s64 + 176;
	// 832BE4E0: 4B9EAF7C  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
	// 832BE4E4: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BE4E8: 419A0060  beq cr6, 0x832be548
	if ctx.cr[6].eq {
	pc = 0x832BE548; continue 'dispatch;
	}
	// 832BE4EC: 3BA00000  li r29, 0
	ctx.r[29].s64 = 0;
	// 832BE4F0: 807F005C  lwz r3, 0x5c(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BE4F4: 38800001  li r4, 1
	ctx.r[4].s64 = 1;
	// 832BE4F8: 93A10060  stw r29, 0x60(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(96 as u32), ctx.r[29].u32 ) };
	// 832BE4FC: 3BDF005C  addi r30, r31, 0x5c
	ctx.r[30].s64 = ctx.r[31].s64 + 92;
	// 832BE500: 4BA1B091  bl 0x82cd9590
	ctx.lr = 0x832BE504;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9590);
	// 832BE504: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832BE508: 41980020  blt cr6, 0x832be528
	if ctx.cr[6].lt {
	pc = 0x832BE528; continue 'dispatch;
	}
	// 832BE50C: 9BA10050  stb r29, 0x50(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[29].u8 ) };
	// 832BE510: 38810050  addi r4, r1, 0x50
	ctx.r[4].s64 = ctx.r[1].s64 + 80;
	// 832BE514: 807E0000  lwz r3, 0(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE518: 4BA1AF21  bl 0x82cd9438
	ctx.lr = 0x832BE51C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9438);
	// 832BE51C: 89410050  lbz r10, 0x50(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BE520: 2B0A0001  cmplwi cr6, r10, 1
	ctx.cr[6].compare_u32(ctx.r[10].u32, 1 as u32, &mut ctx.xer);
	// 832BE524: 419AFFE8  beq cr6, 0x832be50c
	if ctx.cr[6].eq {
	pc = 0x832BE50C; continue 'dispatch;
	}
	// 832BE528: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832BE52C: 807E0000  lwz r3, 0(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BE530: 4BA1B179  bl 0x82cd96a8
	ctx.lr = 0x832BE534;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD96A8);
	// 832BE534: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BE538: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BE53C: 80BF00BC  lwz r5, 0xbc(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(188 as u32) ) } as u64;
	// 832BE540: 4BFFF8F9  bl 0x832bde38
	ctx.lr = 0x832BE544;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BDE38);
	// 832BE544: 93BF0038  stw r29, 0x38(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(56 as u32), ctx.r[29].u32 ) };
	// 832BE548: 807F0038  lwz r3, 0x38(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(56 as u32) ) } as u64;
	// 832BE54C: 382100B0  addi r1, r1, 0xb0
	ctx.r[1].s64 = ctx.r[1].s64 + 176;
	// 832BE550: 4B9EAF0C  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
}

pub fn sub_832BE558(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BE558 size=20
	// 832BE558: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BE55C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BE560: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832BE564: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BE568: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
}

pub fn sub_832BE5F8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BE5F8 size=160
	// 832BE5F8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BE5FC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BE600: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BE604: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BE608: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BE60C: 817F005C  lwz r11, 0x5c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BE610: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BE614: 419A006C  beq cr6, 0x832be680
	if ctx.cr[6].eq {
	pc = 0x832BE680; continue 'dispatch;
	}
	// 832BE618: 38800001  li r4, 1
	ctx.r[4].s64 = 1;
	// 832BE61C: 5563003E  slwi r3, r11, 0
	// 832BE620: 4BA1AF71  bl 0x82cd9590
	ctx.lr = 0x832BE624;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9590);
	// 832BE624: 39600001  li r11, 1
	ctx.r[11].s64 = 1;
	// 832BE628: 917F00D4  stw r11, 0xd4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(212 as u32), ctx.r[11].u32 ) };
	// 832BE62C: 807F005C  lwz r3, 0x5c(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BE630: 4BA1AC81  bl 0x82cd92b0
	ctx.lr = 0x832BE634;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD92B0);
	// 832BE634: 817F00B8  lwz r11, 0xb8(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(184 as u32) ) } as u64;
	// 832BE638: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832BE63C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BE640: 915F005C  stw r10, 0x5c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832BE644: 419A002C  beq cr6, 0x832be670
	if ctx.cr[6].eq {
	pc = 0x832BE670; continue 'dispatch;
	}
	// 832BE648: 894BFFFE  lbz r10, -2(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832BE64C: 2B0A0003  cmplwi cr6, r10, 3
	ctx.cr[6].compare_u32(ctx.r[10].u32, 3 as u32, &mut ctx.xer);
	// 832BE650: 894BFFFF  lbz r10, -1(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832BE654: 7C6A5850  subf r3, r10, r11
	ctx.r[3].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BE658: 409A0014  bne cr6, 0x832be66c
	if !ctx.cr[6].eq {
	pc = 0x832BE66C; continue 'dispatch;
	}
	// 832BE65C: 812BFFF4  lwz r9, -0xc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832BE660: 7D2903A6  mtctr r9
	ctx.ctr.u64 = ctx.r[9].u64;
	// 832BE664: 4E800421  bctrl
	ctx.lr = 0x832BE668;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BE668: 48000008  b 0x832be670
	pc = 0x832BE670; continue 'dispatch;
	// 832BE66C: 4B9EDAD5  bl 0x82cac140
	ctx.lr = 0x832BE670;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CAC140);
	// 832BE670: 3D408350  lis r10, -0x7cb0
	ctx.r[10].s64 = -2091909120;
	// 832BE674: 816AC494  lwz r11, -0x3b6c(r10)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(-15212 as u32) ) } as u64;
	// 832BE678: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832BE67C: 916AC494  stw r11, -0x3b6c(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(-15212 as u32), ctx.r[11].u32 ) };
	// 832BE680: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832BE684: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BE688: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BE68C: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BE690: 4E800020  blr
	return;
}

pub fn sub_832BE698(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BE698 size=744
	// 832BE698: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BE69C: 4B9EAD61  bl 0x82ca93fc
	ctx.lr = 0x832BE6A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93FC);
	// 832BE6A0: 9421FF10  stwu r1, -0xf0(r1)
	ea = ctx.r[1].u32.wrapping_add(-240 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BE6A4: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832BE6A8: 7CBB2B78  mr r27, r5
	ctx.r[27].u64 = ctx.r[5].u64;
	// 832BE6AC: 38A00184  li r5, 0x184
	ctx.r[5].s64 = 388;
	// 832BE6B0: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BE6B4: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BE6B8: 7CDA3378  mr r26, r6
	ctx.r[26].u64 = ctx.r[6].u64;
	// 832BE6BC: 7D1D4378  mr r29, r8
	ctx.r[29].u64 = ctx.r[8].u64;
	// 832BE6C0: 4B9EB2F1  bl 0x82ca99b0
	ctx.lr = 0x832BE6C4;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BE6C4: 3B800000  li r28, 0
	ctx.r[28].s64 = 0;
	// 832BE6C8: 93DF0024  stw r30, 0x24(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(36 as u32), ctx.r[30].u32 ) };
	// 832BE6CC: 38A00058  li r5, 0x58
	ctx.r[5].s64 = 88;
	// 832BE6D0: 937F0028  stw r27, 0x28(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(40 as u32), ctx.r[27].u32 ) };
	// 832BE6D4: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BE6D8: 939F0054  stw r28, 0x54(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(84 as u32), ctx.r[28].u32 ) };
	// 832BE6DC: 38610054  addi r3, r1, 0x54
	ctx.r[3].s64 = ctx.r[1].s64 + 84;
	// 832BE6E0: 935F002C  stw r26, 0x2c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(44 as u32), ctx.r[26].u32 ) };
	// 832BE6E4: 4B9EB2CD  bl 0x82ca99b0
	ctx.lr = 0x832BE6E8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BE6E8: 397BFFF0  addi r11, r27, -0x10
	ctx.r[11].s64 = ctx.r[27].s64 + -16;
	// 832BE6EC: 38E0000C  li r7, 0xc
	ctx.r[7].s64 = 12;
	// 832BE6F0: 9B410054  stb r26, 0x54(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[26].u8 ) };
	// 832BE6F4: 7D6A0034  cntlzw r10, r11
	ctx.r[10].u64 = if ctx.r[11].u32 == 0 { 32 } else { ctx.r[11].u32.leading_zeros() as u64 };
	// 832BE6F8: 93C10058  stw r30, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[30].u32 ) };
	// 832BE6FC: 3B200001  li r25, 1
	ctx.r[25].s64 = 1;
	// 832BE700: 98E1008A  stb r7, 0x8a(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(138 as u32), ctx.r[7].u8 ) };
	// 832BE704: 5548DFFE  rlwinm r8, r10, 0x1b, 0x1f, 0x1f
	ctx.r[8].u64 = ctx.r[10].u32 as u64 & 0x0000001Fu64;
	// 832BE708: 9B21008B  stb r25, 0x8b(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(139 as u32), ctx.r[25].u8 ) };
	// 832BE70C: 3B5F005C  addi r26, r31, 0x5c
	ctx.r[26].s64 = ctx.r[31].s64 + 92;
	// 832BE710: 690B0001  xori r11, r8, 1
	ctx.r[11].u64 = ctx.r[8].u64 ^ 1;
	// 832BE714: 38610050  addi r3, r1, 0x50
	ctx.r[3].s64 = ctx.r[1].s64 + 80;
	// 832BE718: 38CB0001  addi r6, r11, 1
	ctx.r[6].s64 = ctx.r[11].s64 + 1;
	// 832BE71C: 7F44D378  mr r4, r26
	ctx.r[4].u64 = ctx.r[26].u64;
	// 832BE720: 98C10050  stb r6, 0x50(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[6].u8 ) };
	// 832BE724: 4BA1B3DD  bl 0x82cd9b00
	ctx.lr = 0x832BE728;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CD9B00);
	// 832BE728: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832BE72C: 40980010  bge cr6, 0x832be73c
	if !ctx.cr[6].lt {
	pc = 0x832BE73C; continue 'dispatch;
	}
	// 832BE730: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BE734: 382100F0  addi r1, r1, 0xf0
	ctx.r[1].s64 = ctx.r[1].s64 + 240;
	// 832BE738: 4B9EAD14  b 0x82ca944c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA944C);
	return;
	// 832BE73C: 817F0028  lwz r11, 0x28(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(40 as u32) ) } as u64;
	// 832BE740: 3940021C  li r10, 0x21c
	ctx.r[10].s64 = 540;
	// 832BE744: 811F002C  lwz r8, 0x2c(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(44 as u32) ) } as u64;
	// 832BE748: 392003E8  li r9, 0x3e8
	ctx.r[9].s64 = 1000;
	// 832BE74C: 7D671E70  srawi r7, r11, 3
	ctx.xer.ca = (ctx.r[11].s32 < 0) && ((ctx.r[11].u32 & ((1u32 << 3) - 1)) != 0);
	ctx.r[7].s64 = (ctx.r[11].s32 >> 3) as i64;
	// 832BE750: 80DF0024  lwz r6, 0x24(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(36 as u32) ) } as u64;
	// 832BE754: 915F00F0  stw r10, 0xf0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(240 as u32), ctx.r[10].u32 ) };
	// 832BE758: 38A00064  li r5, 0x64
	ctx.r[5].s64 = 100;
	// 832BE75C: 7C8741D6  mullw r4, r7, r8
	ctx.r[4].s64 = (ctx.r[7].s32 as i64) * (ctx.r[8].s32 as i64);
	// 832BE760: 93DF00C4  stw r30, 0xc4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(196 as u32), ctx.r[30].u32 ) };
	// 832BE764: 939F003C  stw r28, 0x3c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(60 as u32), ctx.r[28].u32 ) };
	// 832BE768: 7D6431D6  mullw r11, r4, r6
	ctx.r[11].s64 = (ctx.r[4].s32 as i64) * (ctx.r[6].s32 as i64);
	// 832BE76C: 1C6B0258  mulli r3, r11, 0x258
	ctx.r[3].s64 = ctx.r[11].s64 * 600;
	// 832BE770: 7D434B96  divwu r10, r3, r9
	ctx.r[10].u32 = ctx.r[3].u32 / ctx.r[9].u32;
	// 832BE774: 1D0B0032  mulli r8, r11, 0x32
	ctx.r[8].s64 = ctx.r[11].s64 * 50;
	// 832BE778: 38EA03FF  addi r7, r10, 0x3ff
	ctx.r[7].s64 = ctx.r[10].s64 + 1023;
	// 832BE77C: 7D484B96  divwu r10, r8, r9
	ctx.r[10].u32 = ctx.r[8].u32 / ctx.r[9].u32;
	// 832BE780: 54E3002A  rlwinm r3, r7, 0, 0, 0x15
	ctx.r[3].u64 = ctx.r[7].u32 as u64 & 0xFFFFFFFFu64;
	// 832BE784: 38CA00FF  addi r6, r10, 0xff
	ctx.r[6].s64 = ctx.r[10].s64 + 255;
	// 832BE788: 1C83005A  mulli r4, r3, 0x5a
	ctx.r[4].s64 = ctx.r[3].s64 * 90;
	// 832BE78C: 907F00BC  stw r3, 0xbc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(188 as u32), ctx.r[3].u32 ) };
	// 832BE790: 54CA002E  rlwinm r10, r6, 0, 0, 0x17
	ctx.r[10].u64 = ctx.r[6].u32 as u64 & 0xFFFFFFFFu64;
	// 832BE794: 7D242B96  divwu r9, r4, r5
	ctx.r[9].u32 = ctx.r[4].u32 / ctx.r[5].u32;
	// 832BE798: 915F00C0  stw r10, 0xc0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(192 as u32), ctx.r[10].u32 ) };
	// 832BE79C: 913F00E8  stw r9, 0xe8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(232 as u32), ctx.r[9].u32 ) };
	// 832BE7A0: 815D0014  lwz r10, 0x14(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BE7A4: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BE7A8: 419A0034  beq cr6, 0x832be7dc
	if ctx.cr[6].eq {
	pc = 0x832BE7DC; continue 'dispatch;
	}
	// 832BE7AC: 813D0018  lwz r9, 0x18(r29)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BE7B0: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832BE7B4: 419A0028  beq cr6, 0x832be7dc
	if ctx.cr[6].eq {
	pc = 0x832BE7DC; continue 'dispatch;
	}
	// 832BE7B8: 811D0008  lwz r8, 8(r29)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BE7BC: 79270020  clrldi r7, r9, 0x20
	ctx.r[7].u64 = ctx.r[9].u64 & 0x00000000FFFFFFFFu64;
	// 832BE7C0: 79660020  clrldi r6, r11, 0x20
	ctx.r[6].u64 = ctx.r[11].u64 & 0x00000000FFFFFFFFu64;
	// 832BE7C4: 7CA839D2  mulld r5, r8, r7
	ctx.r[5].s64 = ctx.r[8].s64 * ctx.r[7].s64;
	// 832BE7C8: 7C8531D2  mulld r4, r5, r6
	ctx.r[4].s64 = ctx.r[5].s64 * ctx.r[6].s64;
	// 832BE7CC: 794B0020  clrldi r11, r10, 0x20
	ctx.r[11].u64 = ctx.r[10].u64 & 0x00000000FFFFFFFFu64;
	// 832BE7D0: 7D445B92  divdu r10, r4, r11
	ctx.r[10].u64 = ctx.r[4].u64 / ctx.r[11].u64;
	// 832BE7D4: 555E003E  slwi r30, r10, 0
	// 832BE7D8: 48000008  b 0x832be7e0
	pc = 0x832BE7E0; continue 'dispatch;
	// 832BE7DC: 7C7E1B78  mr r30, r3
	ctx.r[30].u64 = ctx.r[3].u64;
	// 832BE7E0: 4B92B5B1  bl 0x82be9d90
	ctx.lr = 0x832BE7E4;
	crate::recompiler::externs::call(&mut ctx, base, 0x82BE9D90);
	// 832BE7E4: 817F00C0  lwz r11, 0xc0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(192 as u32) ) } as u64;
	// 832BE7E8: 815F0028  lwz r10, 0x28(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(40 as u32) ) } as u64;
	// 832BE7EC: 907F00B8  stw r3, 0xb8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(184 as u32), ctx.r[3].u32 ) };
	// 832BE7F0: 2F0A0008  cmpwi cr6, r10, 8
	ctx.cr[6].compare_i32(ctx.r[10].s32, 8, &mut ctx.xer);
	// 832BE7F4: 917F0030  stw r11, 0x30(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(48 as u32), ctx.r[11].u32 ) };
	// 832BE7F8: 409A000C  bne cr6, 0x832be804
	if !ctx.cr[6].eq {
	pc = 0x832BE804; continue 'dispatch;
	}
	// 832BE7FC: 556B083C  slwi r11, r11, 1
	// 832BE800: 917F0030  stw r11, 0x30(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(48 as u32), ctx.r[11].u32 ) };
	// 832BE804: 39607FFF  li r11, 0x7fff
	ctx.r[11].s64 = 32767;
	// 832BE808: 939F0100  stw r28, 0x100(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(256 as u32), ctx.r[28].u32 ) };
	// 832BE80C: 2F1B0010  cmpwi cr6, r27, 0x10
	ctx.cr[6].compare_i32(ctx.r[27].s32, 16, &mut ctx.xer);
	// 832BE810: 939F0104  stw r28, 0x104(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(260 as u32), ctx.r[28].u32 ) };
	// 832BE814: 917F00C8  stw r11, 0xc8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(200 as u32), ctx.r[11].u32 ) };
	// 832BE818: 917F00CC  stw r11, 0xcc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(204 as u32), ctx.r[11].u32 ) };
	// 832BE81C: 409A000C  bne cr6, 0x832be828
	if !ctx.cr[6].eq {
	pc = 0x832BE828; continue 'dispatch;
	}
	// 832BE820: 7F8BE378  mr r11, r28
	ctx.r[11].u64 = ctx.r[28].u64;
	// 832BE824: 4800000C  b 0x832be830
	pc = 0x832BE830; continue 'dispatch;
	// 832BE828: 3D608080  lis r11, -0x7f80
	ctx.r[11].s64 = -2139095040;
	// 832BE82C: 616B8080  ori r11, r11, 0x8080
	ctx.r[11].u64 = ctx.r[11].u64 | 32896;
	// 832BE830: 917F00F4  stw r11, 0xf4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(244 as u32), ctx.r[11].u32 ) };
	// 832BE834: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BE838: 4BFFF469  bl 0x832bdca0
	ctx.lr = 0x832BE83C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BDCA0);
	// 832BE83C: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BE840: 7F43D378  mr r3, r26
	ctx.r[3].u64 = ctx.r[26].u64;
	// 832BE844: 80BF00BC  lwz r5, 0xbc(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(188 as u32) ) } as u64;
	// 832BE848: 4BFFF5F1  bl 0x832bde38
	ctx.lr = 0x832BE84C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BDE38);
	// 832BE84C: 817F00BC  lwz r11, 0xbc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(188 as u32) ) } as u64;
	// 832BE850: 939F00D8  stw r28, 0xd8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(216 as u32), ctx.r[28].u32 ) };
	// 832BE854: 939F00D0  stw r28, 0xd0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(208 as u32), ctx.r[28].u32 ) };
	// 832BE858: 7F1E5840  cmplw cr6, r30, r11
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832BE85C: 40980008  bge cr6, 0x832be864
	if !ctx.cr[6].lt {
	pc = 0x832BE864; continue 'dispatch;
	}
	// 832BE860: 7FCBF378  mr r11, r30
	ctx.r[11].u64 = ctx.r[30].u64;
	// 832BE864: 3D408350  lis r10, -0x7cb0
	ctx.r[10].s64 = -2091909120;
	// 832BE868: 917F00E0  stw r11, 0xe0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(224 as u32), ctx.r[11].u32 ) };
	// 832BE86C: 3D20832C  lis r9, -0x7cd4
	ctx.r[9].s64 = -2094268416;
	// 832BE870: 917F00DC  stw r11, 0xdc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(220 as u32), ctx.r[11].u32 ) };
	// 832BE874: 3D00832C  lis r8, -0x7cd4
	ctx.r[8].s64 = -2094268416;
	// 832BE878: 939F00D4  stw r28, 0xd4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(212 as u32), ctx.r[28].u32 ) };
	// 832BE87C: 3CE0832C  lis r7, -0x7cd4
	ctx.r[7].s64 = -2094268416;
	// 832BE880: 933F0038  stw r25, 0x38(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(56 as u32), ctx.r[25].u32 ) };
	// 832BE884: 3929DEA0  addi r9, r9, -0x2160
	ctx.r[9].s64 = ctx.r[9].s64 + -8544;
	// 832BE888: 3908E060  addi r8, r8, -0x1fa0
	ctx.r[8].s64 = ctx.r[8].s64 + -8096;
	// 832BE88C: 816AC494  lwz r11, -0x3b6c(r10)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(-15212 as u32) ) } as u64;
	// 832BE890: 38E7E238  addi r7, r7, -0x1dc8
	ctx.r[7].s64 = ctx.r[7].s64 + -7624;
	// 832BE894: 913F015C  stw r9, 0x15c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(348 as u32), ctx.r[9].u32 ) };
	// 832BE898: 3C60832C  lis r3, -0x7cd4
	ctx.r[3].s64 = -2094268416;
	// 832BE89C: 911F0160  stw r8, 0x160(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(352 as u32), ctx.r[8].u32 ) };
	// 832BE8A0: 3CC0832C  lis r6, -0x7cd4
	ctx.r[6].s64 = -2094268416;
	// 832BE8A4: 90FF0164  stw r7, 0x164(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(356 as u32), ctx.r[7].u32 ) };
	// 832BE8A8: 3CA0832C  lis r5, -0x7cd4
	ctx.r[5].s64 = -2094268416;
	// 832BE8AC: 3C80832C  lis r4, -0x7cd4
	ctx.r[4].s64 = -2094268416;
	// 832BE8B0: 3FC0832C  lis r30, -0x7cd4
	ctx.r[30].s64 = -2094268416;
	// 832BE8B4: 3FA0832C  lis r29, -0x7cd4
	ctx.r[29].s64 = -2094268416;
	// 832BE8B8: 3F80832C  lis r28, -0x7cd4
	ctx.r[28].s64 = -2094268416;
	// 832BE8BC: 3863E498  addi r3, r3, -0x1b68
	ctx.r[3].s64 = ctx.r[3].s64 + -7016;
	// 832BE8C0: 38C6E2B8  addi r6, r6, -0x1d48
	ctx.r[6].s64 = ctx.r[6].s64 + -7496;
	// 832BE8C4: 38A5E2D0  addi r5, r5, -0x1d30
	ctx.r[5].s64 = ctx.r[5].s64 + -7472;
	// 832BE8C8: 907F0174  stw r3, 0x174(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(372 as u32), ctx.r[3].u32 ) };
	// 832BE8CC: 3884E558  addi r4, r4, -0x1aa8
	ctx.r[4].s64 = ctx.r[4].s64 + -6824;
	// 832BE8D0: 90DF0168  stw r6, 0x168(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(360 as u32), ctx.r[6].u32 ) };
	// 832BE8D4: 393EE5F8  addi r9, r30, -0x1a08
	ctx.r[9].s64 = ctx.r[30].s64 + -6664;
	// 832BE8D8: 90BF016C  stw r5, 0x16c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(364 as u32), ctx.r[5].u32 ) };
	// 832BE8DC: 391DE468  addi r8, r29, -0x1b98
	ctx.r[8].s64 = ctx.r[29].s64 + -7064;
	// 832BE8E0: 909F0170  stw r4, 0x170(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(368 as u32), ctx.r[4].u32 ) };
	// 832BE8E4: 38FCE2E8  addi r7, r28, -0x1d18
	ctx.r[7].s64 = ctx.r[28].s64 + -7448;
	// 832BE8E8: 913F0178  stw r9, 0x178(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(376 as u32), ctx.r[9].u32 ) };
	// 832BE8EC: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832BE8F0: 911F017C  stw r8, 0x17c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(380 as u32), ctx.r[8].u32 ) };
	// 832BE8F4: 90FF0180  stw r7, 0x180(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(384 as u32), ctx.r[7].u32 ) };
	// 832BE8F8: 38600001  li r3, 1
	ctx.r[3].s64 = 1;
	// 832BE8FC: 916AC494  stw r11, -0x3b6c(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(-15212 as u32), ctx.r[11].u32 ) };
	// 832BE900: 382100F0  addi r1, r1, 0xf0
	ctx.r[1].s64 = ctx.r[1].s64 + 240;
	// 832BE904: 4B9EAB48  b 0x82ca944c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA944C);
	return;
	// 832BE908: 3D60832C  lis r11, -0x7cd4
	ctx.r[11].s64 = -2094268416;
	// 832BE90C: 386BE698  addi r3, r11, -0x1968
	ctx.r[3].s64 = ctx.r[11].s64 + -6504;
	// 832BE910: 4E800020  blr
	return;
}

pub fn sub_832BE980(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BE980 size=168
	// 832BE980: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BE984: 4B9EAA89  bl 0x82ca940c
	ctx.lr = 0x832BE988;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA940C);
	// 832BE988: 9421FF80  stwu r1, -0x80(r1)
	ea = ctx.r[1].u32.wrapping_add(-128 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BE98C: 7C7E1B78  mr r30, r3
	ctx.r[30].u64 = ctx.r[3].u64;
	// 832BE990: 7C9F2378  mr r31, r4
	ctx.r[31].u64 = ctx.r[4].u64;
	// 832BE994: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832BE998: 409A0020  bne cr6, 0x832be9b8
	if !ctx.cr[6].eq {
	pc = 0x832BE9B8; continue 'dispatch;
	}
	// 832BE99C: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BE9A0: 419A0080  beq cr6, 0x832bea20
	if ctx.cr[6].eq {
	pc = 0x832BEA20; continue 'dispatch;
	}
	// 832BE9A4: 3880FFFF  li r4, -1
	ctx.r[4].s64 = -1;
	// 832BE9A8: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BE9AC: 4AED801D  bl 0x821969c8
	ctx.lr = 0x832BE9B0;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BE9B0: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832BE9B4: 4B9EAAA8  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
	// 832BE9B8: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BE9BC: 409A0018  bne cr6, 0x832be9d4
	if !ctx.cr[6].eq {
	pc = 0x832BE9D4; continue 'dispatch;
	}
	// 832BE9C0: 3880FFFF  li r4, -1
	ctx.r[4].s64 = -1;
	// 832BE9C4: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BE9C8: 4AED8001  bl 0x821969c8
	ctx.lr = 0x832BE9CC;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BE9CC: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832BE9D0: 4B9EAA8C  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
	// 832BE9D4: 93C10050  stw r30, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[30].u32 ) };
	// 832BE9D8: 3BA0FFFF  li r29, -1
	ctx.r[29].s64 = -1;
	// 832BE9DC: 93E10054  stw r31, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[31].u32 ) };
	// 832BE9E0: 38A00000  li r5, 0
	ctx.r[5].s64 = 0;
	// 832BE9E4: 7FA6EB78  mr r6, r29
	ctx.r[6].u64 = ctx.r[29].u64;
	// 832BE9E8: 38810050  addi r4, r1, 0x50
	ctx.r[4].s64 = ctx.r[1].s64 + 80;
	// 832BE9EC: 38600002  li r3, 2
	ctx.r[3].s64 = 2;
	// 832BE9F0: 4AEBB921  bl 0x8217a310
	ctx.lr = 0x832BE9F4;
	crate::recompiler::externs::call(&mut ctx, base, 0x8217A310);
	// 832BE9F4: 2B030001  cmplwi cr6, r3, 1
	ctx.cr[6].compare_u32(ctx.r[3].u32, 1 as u32, &mut ctx.xer);
	// 832BE9F8: 4198001C  blt cr6, 0x832bea14
	if ctx.cr[6].lt {
	pc = 0x832BEA14; continue 'dispatch;
	}
	// 832BE9FC: 409A0024  bne cr6, 0x832bea20
	if !ctx.cr[6].eq {
	pc = 0x832BEA20; continue 'dispatch;
	}
	// 832BEA00: 7FA4EB78  mr r4, r29
	ctx.r[4].u64 = ctx.r[29].u64;
	// 832BEA04: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BEA08: 4AED7FC1  bl 0x821969c8
	ctx.lr = 0x832BEA0C;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BEA0C: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832BEA10: 4B9EAA4C  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
	// 832BEA14: 7FA4EB78  mr r4, r29
	ctx.r[4].u64 = ctx.r[29].u64;
	// 832BEA18: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BEA1C: 4AED7FAD  bl 0x821969c8
	ctx.lr = 0x832BEA20;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BEA20: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832BEA24: 4B9EAA38  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
}

pub fn sub_832BEA28(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BEA28 size=12
	// 832BEA28: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BEA2C: 4B9EA9DD  bl 0x82ca9408
	ctx.lr = 0x832BEA30;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9408);
	// 832BEA30: 9421FF00  stwu r1, -0x100(r1)
	ea = ctx.r[1].u32.wrapping_add(-256 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
}

pub fn sub_832BEBA8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BEBA8 size=168
	// 832BEBA8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BEBAC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BEBB0: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832BEBB4: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BEBB8: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BEBBC: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BEBC0: 809F0004  lwz r4, 4(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BEBC4: 807F0008  lwz r3, 8(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BEBC8: 4AED7E01  bl 0x821969c8
	ctx.lr = 0x832BEBCC;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BEBCC: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BEBD0: 419A0030  beq cr6, 0x832bec00
	if ctx.cr[6].eq {
	pc = 0x832BEC00; continue 'dispatch;
	}
	// 832BEBD4: 3BC0FFFF  li r30, -1
	ctx.r[30].s64 = -1;
	// 832BEBD8: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832BEBDC: 807F0014  lwz r3, 0x14(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BEBE0: 4AED7DE9  bl 0x821969c8
	ctx.lr = 0x832BEBE4;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BEBE4: 817F0000  lwz r11, 0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BEBE8: 2F0B0001  cmpwi cr6, r11, 1
	ctx.cr[6].compare_i32(ctx.r[11].s32, 1, &mut ctx.xer);
	// 832BEBEC: 419A0030  beq cr6, 0x832bec1c
	if ctx.cr[6].eq {
	pc = 0x832BEC1C; continue 'dispatch;
	}
	// 832BEBF0: 2F0B0003  cmpwi cr6, r11, 3
	ctx.cr[6].compare_i32(ctx.r[11].s32, 3, &mut ctx.xer);
	// 832BEBF4: 409A003C  bne cr6, 0x832bec30
	if !ctx.cr[6].eq {
	pc = 0x832BEC30; continue 'dispatch;
	}
	// 832BEBF8: 807F0014  lwz r3, 0x14(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BEBFC: 4BD4CDC5  bl 0x8300b9c0
	ctx.lr = 0x832BEC00;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BEC00: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BEC04: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BEC08: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BEC0C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BEC10: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832BEC14: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BEC18: 4E800020  blr
	return;
	// 832BEC1C: 817F0018  lwz r11, 0x18(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BEC20: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BEC24: 419A000C  beq cr6, 0x832bec30
	if ctx.cr[6].eq {
	pc = 0x832BEC30; continue 'dispatch;
	}
	// 832BEC28: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BEC2C: 4BFFFDFD  bl 0x832bea28
	ctx.lr = 0x832BEC30;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEA28);
	// 832BEC30: 807F0014  lwz r3, 0x14(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BEC34: 4BD4CD8D  bl 0x8300b9c0
	ctx.lr = 0x832BEC38;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BEC38: 809F0004  lwz r4, 4(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BEC3C: 807F0008  lwz r3, 8(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BEC40: 4AED7D89  bl 0x821969c8
	ctx.lr = 0x832BEC44;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BEC44: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832BEC48: 409AFF90  bne cr6, 0x832bebd8
	if !ctx.cr[6].eq {
	pc = 0x832BEBD8; continue 'dispatch;
	}
	// 832BEC4C: 4BFFFFB4  b 0x832bec00
	pc = 0x832BEC00; continue 'dispatch;
}

pub fn sub_832BEC50(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BEC50 size=312
	// 832BEC50: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BEC54: 4B9EA7B5  bl 0x82ca9408
	ctx.lr = 0x832BEC58;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9408);
	// 832BEC58: 9421FF80  stwu r1, -0x80(r1)
	ea = ctx.r[1].u32.wrapping_add(-128 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BEC5C: 3D408350  lis r10, -0x7cb0
	ctx.r[10].s64 = -2091909120;
	// 832BEC60: 3B800000  li r28, 0
	ctx.r[28].s64 = 0;
	// 832BEC64: 3BAAC474  addi r29, r10, -0x3b8c
	ctx.r[29].s64 = ctx.r[10].s64 + -15244;
	// 832BEC68: 7F8BE378  mr r11, r28
	ctx.r[11].u64 = ctx.r[28].u64;
	// 832BEC6C: 7FAAEB78  mr r10, r29
	ctx.r[10].u64 = ctx.r[29].u64;
	// 832BEC70: 812A0000  lwz r9, 0(r10)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BEC74: 2F090000  cmpwi cr6, r9, 0
	ctx.cr[6].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832BEC78: 419A0020  beq cr6, 0x832bec98
	if ctx.cr[6].eq {
	pc = 0x832BEC98; continue 'dispatch;
	}
	// 832BEC7C: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832BEC80: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832BEC84: 2B0B0008  cmplwi cr6, r11, 8
	ctx.cr[6].compare_u32(ctx.r[11].u32, 8 as u32, &mut ctx.xer);
	// 832BEC88: 4198FFE8  blt cr6, 0x832bec70
	if ctx.cr[6].lt {
	pc = 0x832BEC70; continue 'dispatch;
	}
	// 832BEC8C: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BEC90: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832BEC94: 4B9EA7C4  b 0x82ca9458
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9458);
	return;
	// 832BEC98: 55691838  slwi r9, r11, 3
	// 832BEC9C: 3D408349  lis r10, -0x7cb7
	ctx.r[10].s64 = -2092367872;
	// 832BECA0: 7D2B4A14  add r9, r11, r9
	ctx.r[9].u64 = ctx.r[11].u64 + ctx.r[9].u64;
	// 832BECA4: 5567103A  slwi r7, r11, 2
	// 832BECA8: 3BCA67B0  addi r30, r10, 0x67b0
	ctx.r[30].s64 = ctx.r[10].s64 + 26544;
	// 832BECAC: 552B103A  slwi r11, r9, 2
	// 832BECB0: 39000001  li r8, 1
	ctx.r[8].s64 = 1;
	// 832BECB4: 7FEBF214  add r31, r11, r30
	ctx.r[31].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832BECB8: 7D07E92E  stwx r8, r7, r29
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[7].u32.wrapping_add(ctx.r[29].u32), ctx.r[8].u32) };
	// 832BECBC: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BECC0: 419A0084  beq cr6, 0x832bed44
	if ctx.cr[6].eq {
	pc = 0x832BED44; continue 'dispatch;
	}
	// 832BECC4: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832BECC8: 40980008  bge cr6, 0x832becd0
	if !ctx.cr[6].lt {
	pc = 0x832BECD0; continue 'dispatch;
	}
	// 832BECCC: 7C6300D0  neg r3, r3
	ctx.r[3].s64 = -ctx.r[3].s64;
	// 832BECD0: 396003E8  li r11, 0x3e8
	ctx.r[11].s64 = 1000;
	// 832BECD4: 911F001C  stw r8, 0x1c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(28 as u32), ctx.r[8].u32 ) };
	// 832BECD8: 939F0000  stw r28, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[28].u32 ) };
	// 832BECDC: 3D40832C  lis r10, -0x7cd4
	ctx.r[10].s64 = -2094268416;
	// 832BECE0: 7D2B1BD6  divw r9, r11, r3
	ctx.r[9].s32 = ctx.r[11].s32 / ctx.r[3].s32;
	// 832BECE4: 939F0018  stw r28, 0x18(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(24 as u32), ctx.r[28].u32 ) };
	// 832BECE8: 939F0020  stw r28, 0x20(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(32 as u32), ctx.r[28].u32 ) };
	// 832BECEC: 3C800000  lis r4, 0
	ctx.r[4].s64 = 0;
	// 832BECF0: 913F0004  stw r9, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[9].u32 ) };
	// 832BECF4: 391F0010  addi r8, r31, 0x10
	ctx.r[8].s64 = ctx.r[31].s64 + 16;
	// 832BECF8: 38E00004  li r7, 4
	ctx.r[7].s64 = 4;
	// 832BECFC: 7FE6FB78  mr r6, r31
	ctx.r[6].u64 = ctx.r[31].u64;
	// 832BED00: 38AAEBA8  addi r5, r10, -0x1458
	ctx.r[5].s64 = ctx.r[10].s64 + -5208;
	// 832BED04: 60848000  ori r4, r4, 0x8000
	ctx.r[4].u64 = ctx.r[4].u64 | 32768;
	// 832BED08: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BED0C: 4BA04F0D  bl 0x82cc3c18
	ctx.lr = 0x832BED10;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC3C18);
	// 832BED10: 907F000C  stw r3, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[3].u32 ) };
	// 832BED14: 38800005  li r4, 5
	ctx.r[4].s64 = 5;
	// 832BED18: 4BA03959  bl 0x82cc2670
	ctx.lr = 0x832BED1C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC2670);
	// 832BED1C: 811F000C  lwz r8, 0xc(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BED20: 2B080000  cmplwi cr6, r8, 0
	ctx.cr[6].compare_u32(ctx.r[8].u32, 0 as u32, &mut ctx.xer);
	// 832BED24: 409A002C  bne cr6, 0x832bed50
	if !ctx.cr[6].eq {
	pc = 0x832BED50; continue 'dispatch;
	}
	// 832BED28: 39600024  li r11, 0x24
	ctx.r[11].s64 = 36;
	// 832BED2C: 7D5EF850  subf r10, r30, r31
	ctx.r[10].s64 = ctx.r[31].s64 - ctx.r[30].s64;
	// 832BED30: 7D6A5BD6  divw r11, r10, r11
	ctx.r[11].s32 = ctx.r[10].s32 / ctx.r[11].s32;
	// 832BED34: 2B0B0008  cmplwi cr6, r11, 8
	ctx.cr[6].compare_u32(ctx.r[11].u32, 8 as u32, &mut ctx.xer);
	// 832BED38: 4098000C  bge cr6, 0x832bed44
	if !ctx.cr[6].lt {
	pc = 0x832BED44; continue 'dispatch;
	}
	// 832BED3C: 556B103A  slwi r11, r11, 2
	// 832BED40: 7F8BE92E  stwx r28, r11, r29
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[11].u32.wrapping_add(ctx.r[29].u32), ctx.r[28].u32) };
	// 832BED44: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BED48: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832BED4C: 4B9EA70C  b 0x82ca9458
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9458);
	return;
	// 832BED50: 38A00000  li r5, 0
	ctx.r[5].s64 = 0;
	// 832BED54: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BED58: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BED5C: 4BD4CBDD  bl 0x8300b938
	ctx.lr = 0x832BED60;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B938);
	// 832BED60: 907F0014  stw r3, 0x14(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(20 as u32), ctx.r[3].u32 ) };
	// 832BED64: 38C00000  li r6, 0
	ctx.r[6].s64 = 0;
	// 832BED68: 38A00000  li r5, 0
	ctx.r[5].s64 = 0;
	// 832BED6C: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BED70: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BED74: 4BA03F6D  bl 0x82cc2ce0
	ctx.lr = 0x832BED78;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC2CE0);
	// 832BED78: 907F0008  stw r3, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[3].u32 ) };
	// 832BED7C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BED80: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832BED84: 4B9EA6D4  b 0x82ca9458
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9458);
	return;
}

pub fn sub_832BED88(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BED88 size=104
	// 832BED88: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BED8C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BED90: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832BED94: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BED98: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BED9C: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832BEDA0: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BEDA4: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BEDA8: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BEDAC: 90BE0010  stw r5, 0x10(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(16 as u32), ctx.r[5].u32 ) };
	// 832BEDB0: 38A00000  li r5, 0
	ctx.r[5].s64 = 0;
	// 832BEDB4: 90DE0014  stw r6, 0x14(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(20 as u32), ctx.r[6].u32 ) };
	// 832BEDB8: 4BD4CB81  bl 0x8300b938
	ctx.lr = 0x832BEDBC;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B938);
	// 832BEDBC: 907E0004  stw r3, 4(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(4 as u32), ctx.r[3].u32 ) };
	// 832BEDC0: 817F0018  lwz r11, 0x18(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BEDC4: 917E0000  stw r11, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BEDC8: 817F0020  lwz r11, 0x20(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BEDCC: 394B0001  addi r10, r11, 1
	ctx.r[10].s64 = ctx.r[11].s64 + 1;
	// 832BEDD0: 93DF0018  stw r30, 0x18(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(24 as u32), ctx.r[30].u32 ) };
	// 832BEDD4: 915F0020  stw r10, 0x20(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(32 as u32), ctx.r[10].u32 ) };
	// 832BEDD8: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BEDDC: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BEDE0: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BEDE4: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832BEDE8: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BEDEC: 4E800020  blr
	return;
}

pub fn sub_832BEDF0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BEDF0 size=168
	// 832BEDF0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BEDF4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BEDF8: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832BEDFC: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BEE00: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BEE04: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BEE08: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832BEE0C: 817F0020  lwz r11, 0x20(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BEE10: 815F0018  lwz r10, 0x18(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BEE14: 392BFFFF  addi r9, r11, -1
	ctx.r[9].s64 = ctx.r[11].s64 + -1;
	// 832BEE18: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BEE1C: 913F0020  stw r9, 0x20(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(32 as u32), ctx.r[9].u32 ) };
	// 832BEE20: 409A0024  bne cr6, 0x832bee44
	if !ctx.cr[6].eq {
	pc = 0x832BEE44; continue 'dispatch;
	}
	// 832BEE24: 48000075  bl 0x832bee98
	ctx.lr = 0x832BEE28;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEE98);
	// 832BEE28: 3880FFFF  li r4, -1
	ctx.r[4].s64 = -1;
	// 832BEE2C: 807F000C  lwz r3, 0xc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BEE30: 4AED7B99  bl 0x821969c8
	ctx.lr = 0x832BEE34;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BEE34: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BEE38: 480000D9  bl 0x832bef10
	ctx.lr = 0x832BEE3C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEF10);
	// 832BEE3C: 3BE00001  li r31, 1
	ctx.r[31].s64 = 1;
	// 832BEE40: 4800002C  b 0x832bee6c
	pc = 0x832BEE6C; continue 'dispatch;
	// 832BEE44: 39600004  li r11, 4
	ctx.r[11].s64 = 4;
	// 832BEE48: 807F0014  lwz r3, 0x14(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BEE4C: 3880FFFF  li r4, -1
	ctx.r[4].s64 = -1;
	// 832BEE50: 917F0000  stw r11, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BEE54: 4AED7B75  bl 0x821969c8
	ctx.lr = 0x832BEE58;
	crate::recompiler::externs::call(&mut ctx, base, 0x821969C8);
	// 832BEE58: 39400001  li r10, 1
	ctx.r[10].s64 = 1;
	// 832BEE5C: 807F0014  lwz r3, 0x14(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BEE60: 915F0000  stw r10, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832BEE64: 4BD4CB5D  bl 0x8300b9c0
	ctx.lr = 0x832BEE68;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BEE68: 3BE00000  li r31, 0
	ctx.r[31].s64 = 0;
	// 832BEE6C: 807E0004  lwz r3, 4(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BEE70: 4BA03941  bl 0x82cc27b0
	ctx.lr = 0x832BEE74;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC27B0);
	// 832BEE74: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832BEE78: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BEE7C: 917E0004  stw r11, 4(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832BEE80: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BEE84: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BEE88: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BEE8C: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832BEE90: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BEE94: 4E800020  blr
	return;
}

pub fn sub_832BEE98(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BEE98 size=120
	// 832BEE98: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BEE9C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BEEA0: FBC1FFE8  std r30, -0x18(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-24 as u32), ctx.r[30].u64 ) };
	// 832BEEA4: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BEEA8: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BEEAC: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BEEB0: 83DF0000  lwz r30, 0(r31)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BEEB4: 807F0008  lwz r3, 8(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BEEB8: 4B00A4D1  bl 0x822c9388
	ctx.lr = 0x832BEEBC;
	crate::recompiler::externs::call(&mut ctx, base, 0x822C9388);
	// 832BEEBC: 39600003  li r11, 3
	ctx.r[11].s64 = 3;
	// 832BEEC0: 807F000C  lwz r3, 0xc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BEEC4: 3880000F  li r4, 0xf
	ctx.r[4].s64 = 15;
	// 832BEEC8: 917F0000  stw r11, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BEECC: 4BA035F5  bl 0x82cc24c0
	ctx.lr = 0x832BEED0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC24C0);
	// 832BEED0: 2F1E0000  cmpwi cr6, r30, 0
	ctx.cr[6].compare_i32(ctx.r[30].s32, 0, &mut ctx.xer);
	// 832BEED4: 419A0018  beq cr6, 0x832beeec
	if ctx.cr[6].eq {
	pc = 0x832BEEEC; continue 'dispatch;
	}
	// 832BEED8: 2F1E0002  cmpwi cr6, r30, 2
	ctx.cr[6].compare_i32(ctx.r[30].s32, 2, &mut ctx.xer);
	// 832BEEDC: 409A0018  bne cr6, 0x832beef4
	if !ctx.cr[6].eq {
	pc = 0x832BEEF4; continue 'dispatch;
	}
	// 832BEEE0: 807F0014  lwz r3, 0x14(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BEEE4: 4BD4CADD  bl 0x8300b9c0
	ctx.lr = 0x832BEEE8;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BEEE8: 4800000C  b 0x832beef4
	pc = 0x832BEEF4; continue 'dispatch;
	// 832BEEEC: 807F000C  lwz r3, 0xc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BEEF0: 4BA08F79  bl 0x82cc7e68
	ctx.lr = 0x832BEEF4;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC7E68);
	// 832BEEF4: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832BEEF8: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BEEFC: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BEF00: EBC1FFE8  ld r30, -0x18(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-24 as u32) ) };
	// 832BEF04: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BEF08: 4E800020  blr
	return;
}

pub fn sub_832BEF10(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BEF10 size=112
	// 832BEF10: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BEF14: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832BEF18: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832BEF1C: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BEF20: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BEF24: 807F0014  lwz r3, 0x14(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BEF28: 4BA03889  bl 0x82cc27b0
	ctx.lr = 0x832BEF2C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC27B0);
	// 832BEF2C: 807F000C  lwz r3, 0xc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BEF30: 4BA03881  bl 0x82cc27b0
	ctx.lr = 0x832BEF34;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC27B0);
	// 832BEF34: 807F0008  lwz r3, 8(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BEF38: 4BA03879  bl 0x82cc27b0
	ctx.lr = 0x832BEF3C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC27B0);
	// 832BEF3C: 3D608349  lis r11, -0x7cb7
	ctx.r[11].s64 = -2092367872;
	// 832BEF40: 39400024  li r10, 0x24
	ctx.r[10].s64 = 36;
	// 832BEF44: 392B67B0  addi r9, r11, 0x67b0
	ctx.r[9].s64 = ctx.r[11].s64 + 26544;
	// 832BEF48: 7D09F850  subf r8, r9, r31
	ctx.r[8].s64 = ctx.r[31].s64 - ctx.r[9].s64;
	// 832BEF4C: 7D6853D6  divw r11, r8, r10
	ctx.r[11].s32 = ctx.r[8].s32 / ctx.r[10].s32;
	// 832BEF50: 2B0B0008  cmplwi cr6, r11, 8
	ctx.cr[6].compare_u32(ctx.r[11].u32, 8 as u32, &mut ctx.xer);
	// 832BEF54: 40980018  bge cr6, 0x832bef6c
	if !ctx.cr[6].lt {
	pc = 0x832BEF6C; continue 'dispatch;
	}
	// 832BEF58: 3D408350  lis r10, -0x7cb0
	ctx.r[10].s64 = -2091909120;
	// 832BEF5C: 5569103A  slwi r9, r11, 2
	// 832BEF60: 390AC474  addi r8, r10, -0x3b8c
	ctx.r[8].s64 = ctx.r[10].s64 + -15244;
	// 832BEF64: 38E00000  li r7, 0
	ctx.r[7].s64 = 0;
	// 832BEF68: 7CE9412E  stwx r7, r9, r8
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[8].u32), ctx.r[7].u32) };
	// 832BEF6C: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832BEF70: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832BEF74: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832BEF78: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832BEF7C: 4E800020  blr
	return;
}

pub fn sub_832BEF80(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BEF80 size=536
	// 832BEF80: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BEF84: 4B9EA47D  bl 0x82ca9400
	ctx.lr = 0x832BEF88;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9400);
	// 832BEF88: 9421FF70  stwu r1, -0x90(r1)
	ea = ctx.r[1].u32.wrapping_add(-144 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BEF8C: 3B400000  li r26, 0
	ctx.r[26].s64 = 0;
	// 832BEF90: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BEF94: 7C9C2378  mr r28, r4
	ctx.r[28].u64 = ctx.r[4].u64;
	// 832BEF98: 7CBE2B78  mr r30, r5
	ctx.r[30].u64 = ctx.r[5].u64;
	// 832BEF9C: 7CDB3378  mr r27, r6
	ctx.r[27].u64 = ctx.r[6].u64;
	// 832BEFA0: 7F48D378  mr r8, r26
	ctx.r[8].u64 = ctx.r[26].u64;
	// 832BEFA4: 4BFFF975  bl 0x832be918
	ctx.lr = 0x832BEFA8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BE918);
	// 832BEFA8: 3BA00001  li r29, 1
	ctx.r[29].s64 = 1;
	// 832BEFAC: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832BEFB0: 419A0008  beq cr6, 0x832befb8
	if ctx.cr[6].eq {
	pc = 0x832BEFB8; continue 'dispatch;
	}
	// 832BEFB4: 7FA8EB78  mr r8, r29
	ctx.r[8].u64 = ctx.r[29].u64;
	// 832BEFB8: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832BEFBC: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BEFC0: 4BFFF959  bl 0x832be918
	ctx.lr = 0x832BEFC4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BE918);
	// 832BEFC4: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832BEFC8: 419A0008  beq cr6, 0x832befd0
	if ctx.cr[6].eq {
	pc = 0x832BEFD0; continue 'dispatch;
	}
	// 832BEFCC: 61080002  ori r8, r8, 2
	ctx.r[8].u64 = ctx.r[8].u64 | 2;
	// 832BEFD0: 2F080001  cmpwi cr6, r8, 1
	ctx.cr[6].compare_i32(ctx.r[8].s32, 1, &mut ctx.xer);
	// 832BEFD4: 419A01A8  beq cr6, 0x832bf17c
	if ctx.cr[6].eq {
	pc = 0x832BF17C; continue 'dispatch;
	}
	// 832BEFD8: 2F080002  cmpwi cr6, r8, 2
	ctx.cr[6].compare_i32(ctx.r[8].s32, 2, &mut ctx.xer);
	// 832BEFDC: 419A0184  beq cr6, 0x832bf160
	if ctx.cr[6].eq {
	pc = 0x832BF160; continue 'dispatch;
	}
	// 832BEFE0: 2F080003  cmpwi cr6, r8, 3
	ctx.cr[6].compare_i32(ctx.r[8].s32, 3, &mut ctx.xer);
	// 832BEFE4: 419A0010  beq cr6, 0x832beff4
	if ctx.cr[6].eq {
	pc = 0x832BEFF4; continue 'dispatch;
	}
	// 832BEFE8: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BEFEC: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832BEFF0: 4B9EA460  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832BEFF4: 817F0020  lwz r11, 0x20(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BEFF8: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832BEFFC: 917F0020  stw r11, 0x20(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(32 as u32), ctx.r[11].u32 ) };
	// 832BF000: 817E0020  lwz r11, 0x20(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BF004: 394BFFFF  addi r10, r11, -1
	ctx.r[10].s64 = ctx.r[11].s64 + -1;
	// 832BF008: 915E0020  stw r10, 0x20(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(32 as u32), ctx.r[10].u32 ) };
	// 832BF00C: 813F0018  lwz r9, 0x18(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BF010: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832BF014: 817E0018  lwz r11, 0x18(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BF018: 409A0074  bne cr6, 0x832bf08c
	if !ctx.cr[6].eq {
	pc = 0x832BF08C; continue 'dispatch;
	}
	// 832BF01C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BF020: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BF024: 409A0034  bne cr6, 0x832bf058
	if !ctx.cr[6].eq {
	pc = 0x832BF058; continue 'dispatch;
	}
	// 832BF028: 4BFFFE71  bl 0x832bee98
	ctx.lr = 0x832BF02C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEE98);
	// 832BF02C: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BF030: 4BFFFE69  bl 0x832bee98
	ctx.lr = 0x832BF034;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEE98);
	// 832BF034: 809E000C  lwz r4, 0xc(r30)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BF038: 807F000C  lwz r3, 0xc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BF03C: 4BFFF945  bl 0x832be980
	ctx.lr = 0x832BF040;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BE980);
	// 832BF040: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BF044: 4BFFFECD  bl 0x832bef10
	ctx.lr = 0x832BF048;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEF10);
	// 832BF048: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BF04C: 4BFFFEC5  bl 0x832bef10
	ctx.lr = 0x832BF050;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEF10);
	// 832BF050: 3BE00003  li r31, 3
	ctx.r[31].s64 = 3;
	// 832BF054: 480000E8  b 0x832bf13c
	pc = 0x832BF13C; continue 'dispatch;
	// 832BF058: 4BFFFE41  bl 0x832bee98
	ctx.lr = 0x832BF05C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEE98);
	// 832BF05C: 39600004  li r11, 4
	ctx.r[11].s64 = 4;
	// 832BF060: 809E0014  lwz r4, 0x14(r30)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BF064: 917E0000  stw r11, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BF068: 807F000C  lwz r3, 0xc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BF06C: 4BFFF915  bl 0x832be980
	ctx.lr = 0x832BF070;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BE980);
	// 832BF070: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BF074: 4BFFFE9D  bl 0x832bef10
	ctx.lr = 0x832BF078;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEF10);
	// 832BF078: 93BE0000  stw r29, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[29].u32 ) };
	// 832BF07C: 807E0014  lwz r3, 0x14(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BF080: 4BD4C941  bl 0x8300b9c0
	ctx.lr = 0x832BF084;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BF084: 7FBFEB78  mr r31, r29
	ctx.r[31].u64 = ctx.r[29].u64;
	// 832BF088: 480000B4  b 0x832bf13c
	pc = 0x832BF13C; continue 'dispatch;
	// 832BF08C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BF090: 409A003C  bne cr6, 0x832bf0cc
	if !ctx.cr[6].eq {
	pc = 0x832BF0CC; continue 'dispatch;
	}
	// 832BF094: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BF098: 4BFFFE01  bl 0x832bee98
	ctx.lr = 0x832BF09C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEE98);
	// 832BF09C: 39600004  li r11, 4
	ctx.r[11].s64 = 4;
	// 832BF0A0: 809F0014  lwz r4, 0x14(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BF0A4: 917F0000  stw r11, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BF0A8: 807E000C  lwz r3, 0xc(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BF0AC: 4BFFF8D5  bl 0x832be980
	ctx.lr = 0x832BF0B0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BE980);
	// 832BF0B0: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BF0B4: 4BFFFE5D  bl 0x832bef10
	ctx.lr = 0x832BF0B8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEF10);
	// 832BF0B8: 93BF0000  stw r29, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[29].u32 ) };
	// 832BF0BC: 807F0014  lwz r3, 0x14(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BF0C0: 4BD4C901  bl 0x8300b9c0
	ctx.lr = 0x832BF0C4;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BF0C4: 3BE00002  li r31, 2
	ctx.r[31].s64 = 2;
	// 832BF0C8: 48000074  b 0x832bf13c
	pc = 0x832BF13C; continue 'dispatch;
	// 832BF0CC: 39600004  li r11, 4
	ctx.r[11].s64 = 4;
	// 832BF0D0: 917F0000  stw r11, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BF0D4: 917E0000  stw r11, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BF0D8: 809E0014  lwz r4, 0x14(r30)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BF0DC: 807F0014  lwz r3, 0x14(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BF0E0: 4BFFF8A1  bl 0x832be980
	ctx.lr = 0x832BF0E4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BE980);
	// 832BF0E4: 39600002  li r11, 2
	ctx.r[11].s64 = 2;
	// 832BF0E8: 917F0000  stw r11, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BF0EC: 917E0000  stw r11, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BF0F0: 817F0000  lwz r11, 0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF0F4: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BF0F8: 93BF0000  stw r29, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[29].u32 ) };
	// 832BF0FC: 409A0010  bne cr6, 0x832bf10c
	if !ctx.cr[6].eq {
	pc = 0x832BF10C; continue 'dispatch;
	}
	// 832BF100: 807F000C  lwz r3, 0xc(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BF104: 4BA08D65  bl 0x82cc7e68
	ctx.lr = 0x832BF108;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC7E68);
	// 832BF108: 4800000C  b 0x832bf114
	pc = 0x832BF114; continue 'dispatch;
	// 832BF10C: 807F0014  lwz r3, 0x14(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BF110: 4BD4C8B1  bl 0x8300b9c0
	ctx.lr = 0x832BF114;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BF114: 817E0000  lwz r11, 0(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF118: 93BE0000  stw r29, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[29].u32 ) };
	// 832BF11C: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BF120: 409A0010  bne cr6, 0x832bf130
	if !ctx.cr[6].eq {
	pc = 0x832BF130; continue 'dispatch;
	}
	// 832BF124: 807E000C  lwz r3, 0xc(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BF128: 4BA08D41  bl 0x82cc7e68
	ctx.lr = 0x832BF12C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC7E68);
	// 832BF12C: 4800000C  b 0x832bf138
	pc = 0x832BF138; continue 'dispatch;
	// 832BF130: 807E0014  lwz r3, 0x14(r30)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BF134: 4BD4C88D  bl 0x8300b9c0
	ctx.lr = 0x832BF138;
	crate::recompiler::externs::call(&mut ctx, base, 0x8300B9C0);
	// 832BF138: 7F5FD378  mr r31, r26
	ctx.r[31].u64 = ctx.r[26].u64;
	// 832BF13C: 807C0004  lwz r3, 4(r28)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BF140: 4BA03671  bl 0x82cc27b0
	ctx.lr = 0x832BF144;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC27B0);
	// 832BF144: 807B0004  lwz r3, 4(r27)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BF148: 4BA03669  bl 0x82cc27b0
	ctx.lr = 0x832BF14C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC27B0);
	// 832BF14C: 935C0004  stw r26, 4(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(4 as u32), ctx.r[26].u32 ) };
	// 832BF150: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BF154: 935B0004  stw r26, 4(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(4 as u32), ctx.r[26].u32 ) };
	// 832BF158: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832BF15C: 4B9EA2F4  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832BF160: 38A00001  li r5, 1
	ctx.r[5].s64 = 1;
	// 832BF164: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832BF168: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832BF16C: 4BFFFC85  bl 0x832bedf0
	ctx.lr = 0x832BF170;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEDF0);
	// 832BF170: 5463083C  slwi r3, r3, 1
	// 832BF174: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832BF178: 4B9EA2D8  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832BF17C: 38A00001  li r5, 1
	ctx.r[5].s64 = 1;
	// 832BF180: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832BF184: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BF188: 4BFFFC69  bl 0x832bedf0
	ctx.lr = 0x832BF18C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BEDF0);
	// 832BF18C: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832BF190: 4B9EA2C0  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
}

pub fn sub_832BF198(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BF198 size=288
	// 832BF198: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BF19C: 4B9EA25D  bl 0x82ca93f8
	ctx.lr = 0x832BF1A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93F8);
	// 832BF1A0: 9421FF60  stwu r1, -0xa0(r1)
	ea = ctx.r[1].u32.wrapping_add(-160 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BF1A4: 3D608350  lis r11, -0x7cb0
	ctx.r[11].s64 = -2091909120;
	// 832BF1A8: 7C7A1B78  mr r26, r3
	ctx.r[26].u64 = ctx.r[3].u64;
	// 832BF1AC: 3BEBC21C  addi r31, r11, -0x3de4
	ctx.r[31].s64 = ctx.r[11].s64 + -15844;
	// 832BF1B0: 7C992378  mr r25, r4
	ctx.r[25].u64 = ctx.r[4].u64;
	// 832BF1B4: 815F0254  lwz r10, 0x254(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(596 as u32) ) } as u64;
	// 832BF1B8: 817F0068  lwz r11, 0x68(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(104 as u32) ) } as u64;
	// 832BF1BC: 7F0A5840  cmplw cr6, r10, r11
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832BF1C0: 409A00A4  bne cr6, 0x832bf264
	if !ctx.cr[6].eq {
	pc = 0x832BF264; continue 'dispatch;
	}
	// 832BF1C4: 55432036  slwi r3, r10, 4
	// 832BF1C8: 4B92ABC9  bl 0x82be9d90
	ctx.lr = 0x832BF1CC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82BE9D90);
	// 832BF1CC: 83DF0254  lwz r30, 0x254(r31)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(596 as u32) ) } as u64;
	// 832BF1D0: 839F024C  lwz r28, 0x24c(r31)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(588 as u32) ) } as u64;
	// 832BF1D4: 7C7D1B78  mr r29, r3
	ctx.r[29].u64 = ctx.r[3].u64;
	// 832BF1D8: 57D8103A  slwi r24, r30, 2
	// 832BF1DC: 57CB1838  slwi r11, r30, 3
	// 832BF1E0: 7F05C378  mr r5, r24
	ctx.r[5].u64 = ctx.r[24].u64;
	// 832BF1E4: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832BF1E8: 7F6BEA14  add r27, r11, r29
	ctx.r[27].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832BF1EC: 4B9EA295  bl 0x82ca9480
	ctx.lr = 0x832BF1F0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832BF1F0: 7F05C378  mr r5, r24
	ctx.r[5].u64 = ctx.r[24].u64;
	// 832BF1F4: 7F63DB78  mr r3, r27
	ctx.r[3].u64 = ctx.r[27].u64;
	// 832BF1F8: 809F0250  lwz r4, 0x250(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(592 as u32) ) } as u64;
	// 832BF1FC: 4B9EA285  bl 0x82ca9480
	ctx.lr = 0x832BF200;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832BF200: 57CB083C  slwi r11, r30, 1
	// 832BF204: 917F0254  stw r11, 0x254(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(596 as u32), ctx.r[11].u32 ) };
	// 832BF208: 2B0B0040  cmplwi cr6, r11, 0x40
	ctx.cr[6].compare_u32(ctx.r[11].u32, 64 as u32, &mut ctx.xer);
	// 832BF20C: 419A0044  beq cr6, 0x832bf250
	if ctx.cr[6].eq {
	pc = 0x832BF250; continue 'dispatch;
	}
	// 832BF210: 2B1C0000  cmplwi cr6, r28, 0
	ctx.cr[6].compare_u32(ctx.r[28].u32, 0 as u32, &mut ctx.xer);
	// 832BF214: 419A003C  beq cr6, 0x832bf250
	if ctx.cr[6].eq {
	pc = 0x832BF250; continue 'dispatch;
	}
	// 832BF218: 897CFFFE  lbz r11, -2(r28)
	ctx.r[11].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[28].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832BF21C: 2B0B0003  cmplwi cr6, r11, 3
	ctx.cr[6].compare_u32(ctx.r[11].u32, 3 as u32, &mut ctx.xer);
	// 832BF220: 897CFFFF  lbz r11, -1(r28)
	ctx.r[11].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[28].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832BF224: 7C6BE050  subf r3, r11, r28
	ctx.r[3].s64 = ctx.r[28].s64 - ctx.r[11].s64;
	// 832BF228: 409A0024  bne cr6, 0x832bf24c
	if !ctx.cr[6].eq {
	pc = 0x832BF24C; continue 'dispatch;
	}
	// 832BF22C: 815CFFF4  lwz r10, -0xc(r28)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832BF230: 7D4903A6  mtctr r10
	ctx.ctr.u64 = ctx.r[10].u64;
	// 832BF234: 4E800421  bctrl
	ctx.lr = 0x832BF238;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832BF238: 7F68DB78  mr r8, r27
	ctx.r[8].u64 = ctx.r[27].u64;
	// 832BF23C: 817F0068  lwz r11, 0x68(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(104 as u32) ) } as u64;
	// 832BF240: 93BF024C  stw r29, 0x24c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(588 as u32), ctx.r[29].u32 ) };
	// 832BF244: 911F0250  stw r8, 0x250(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(592 as u32), ctx.r[8].u32 ) };
	// 832BF248: 48000024  b 0x832bf26c
	pc = 0x832BF26C; continue 'dispatch;
	// 832BF24C: 4B9ECEF5  bl 0x82cac140
	ctx.lr = 0x832BF250;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CAC140);
	// 832BF250: 7F68DB78  mr r8, r27
	ctx.r[8].u64 = ctx.r[27].u64;
	// 832BF254: 817F0068  lwz r11, 0x68(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(104 as u32) ) } as u64;
	// 832BF258: 93BF024C  stw r29, 0x24c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(588 as u32), ctx.r[29].u32 ) };
	// 832BF25C: 911F0250  stw r8, 0x250(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(592 as u32), ctx.r[8].u32 ) };
	// 832BF260: 4800000C  b 0x832bf26c
	pc = 0x832BF26C; continue 'dispatch;
	// 832BF264: 83BF024C  lwz r29, 0x24c(r31)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(588 as u32) ) } as u64;
	// 832BF268: 811F0250  lwz r8, 0x250(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(592 as u32) ) } as u64;
	// 832BF26C: 3939001F  addi r9, r25, 0x1f
	ctx.r[9].s64 = ctx.r[25].s64 + 31;
	// 832BF270: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF274: 5566103A  slwi r6, r11, 2
	// 832BF278: 55290034  rlwinm r9, r9, 0, 0, 0x1a
	ctx.r[9].u64 = ctx.r[9].u32 as u64 & 0xFFFFFFFFu64;
	// 832BF27C: 5545D97E  srwi r5, r10, 5
	// 832BF280: 5524D97E  srwi r4, r9, 5
	// 832BF284: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832BF288: 7CE42850  subf r7, r4, r5
	ctx.r[7].s64 = ctx.r[5].s64 - ctx.r[4].s64;
	// 832BF28C: 917F0068  stw r11, 0x68(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(104 as u32), ctx.r[11].u32 ) };
	// 832BF290: 38670001  addi r3, r7, 1
	ctx.r[3].s64 = ctx.r[7].s64 + 1;
	// 832BF294: 546B2DB4  rlwinm r11, r3, 5, 0x16, 0x1a
	ctx.r[11].u64 = ctx.r[3].u32 as u64 & 0x07FFFFFFu64;
	// 832BF298: 7D6B4A14  add r11, r11, r9
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[9].u64;
	// 832BF29C: 7D4A5A14  add r10, r10, r11
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832BF2A0: 7D66412E  stwx r11, r6, r8
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[6].u32.wrapping_add(ctx.r[8].u32), ctx.r[11].u32) };
	// 832BF2A4: 7F46E92E  stwx r26, r6, r29
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[6].u32.wrapping_add(ctx.r[29].u32), ctx.r[26].u32) };
	// 832BF2A8: 915F0000  stw r10, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832BF2AC: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832BF2B0: 4B9EA198  b 0x82ca9448
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9448);
	return;
}

pub fn sub_832BF2B8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    let mut ea: u32 = 0;
    // ---- function 0x832BF2B8 size=776
	// 832BF2B8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BF2BC: 4B9EA125  bl 0x82ca93e0
	ctx.lr = 0x832BF2C0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93E0);
	// 832BF2C0: DBA1FF70  stfd f29, -0x90(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-144 as u32), ctx.f[29].u64 ) };
	// 832BF2C4: DBC1FF78  stfd f30, -0x88(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-136 as u32), ctx.f[30].u64 ) };
	// 832BF2C8: DBE1FF80  stfd f31, -0x80(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-128 as u32), ctx.f[31].u64 ) };
	// 832BF2CC: 9421FF10  stwu r1, -0xf0(r1)
	ea = ctx.r[1].u32.wrapping_add(-240 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BF2D0: 7CF83B78  mr r24, r7
	ctx.r[24].u64 = ctx.r[7].u64;
	// 832BF2D4: 3D60820A  lis r11, -0x7df6
	ctx.r[11].s64 = -2113273856;
	// 832BF2D8: 7CB52B78  mr r21, r5
	ctx.r[21].u64 = ctx.r[5].u64;
	// 832BF2DC: 7C922378  mr r18, r4
	ctx.r[18].u64 = ctx.r[4].u64;
	// 832BF2E0: 7CD63378  mr r22, r6
	ctx.r[22].u64 = ctx.r[6].u64;
	// 832BF2E4: 81580000  lwz r10, 0(r24)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF2E8: 7D144378  mr r20, r8
	ctx.r[20].u64 = ctx.r[8].u64;
	// 832BF2EC: C3AB9484  lfs f29, -0x6b7c(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-27516 as u32) ) };
	ctx.f[29].f64 = (tmp.f32 as f64);
	// 832BF2F0: 3B400000  li r26, 0
	ctx.r[26].s64 = 0;
	// 832BF2F4: 5549083C  slwi r9, r10, 1
	// 832BF2F8: 83D50000  lwz r30, 0(r21)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[21].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF2FC: 83950004  lwz r28, 4(r21)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[21].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BF300: FFE0E890  fmr f31, f29
	ctx.f[31].f64 = ctx.f[29].f64;
	// 832BF304: 83F50008  lwz r31, 8(r21)
	ctx.r[31].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[21].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BF308: 2B090002  cmplwi cr6, r9, 2
	ctx.cr[6].compare_u32(ctx.r[9].u32, 2 as u32, &mut ctx.xer);
	// 832BF30C: FFC0E890  fmr f30, f29
	ctx.f[30].f64 = ctx.f[29].f64;
	// 832BF310: 40980028  bge cr6, 0x832bf338
	if !ctx.cr[6].lt {
	pc = 0x832BF338; continue 'dispatch;
	}
	// 832BF314: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832BF318: 3B5A0001  addi r26, r26, 1
	ctx.r[26].s64 = ctx.r[26].s64 + 1;
	// 832BF31C: 7FEBB42E  lfsx f31, r11, r22
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[22].u32)) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832BF320: FFC0F850  fneg f30, f31
	ctx.f[30].u64 = ctx.f[31].u64 ^ 0x8000_0000_0000_0000u64;
	// 832BF324: 574B103A  slwi r11, r26, 2
	// 832BF328: 7D4BC02E  lwzx r10, r11, r24
	ctx.r[10].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[24].u32)) } as u64;
	// 832BF32C: 5549083C  slwi r9, r10, 1
	// 832BF330: 2B090002  cmplwi cr6, r9, 2
	ctx.cr[6].compare_u32(ctx.r[9].u32, 2 as u32, &mut ctx.xer);
	// 832BF334: 4198FFE4  blt cr6, 0x832bf318
	if ctx.cr[6].lt {
	pc = 0x832BF318; continue 'dispatch;
	}
	// 832BF338: 3AE30008  addi r23, r3, 8
	ctx.r[23].s64 = ctx.r[3].s64 + 8;
	// 832BF33C: 38C00002  li r6, 2
	ctx.r[6].s64 = 2;
	// 832BF340: 2B120002  cmplwi cr6, r18, 2
	ctx.cr[6].compare_u32(ctx.r[18].u32, 2 as u32, &mut ctx.xer);
	// 832BF344: 40990258  ble cr6, 0x832bf59c
	if !ctx.cr[6].gt {
	pc = 0x832BF59C; continue 'dispatch;
	}
	// 832BF348: 3D608350  lis r11, -0x7cb0
	ctx.r[11].s64 = -2091909120;
	// 832BF34C: 3B20FFFF  li r25, -1
	ctx.r[25].s64 = -1;
	// 832BF350: 3A6BC20C  addi r19, r11, -0x3df4
	ctx.r[19].s64 = ctx.r[11].s64 + -15860;
	// 832BF354: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BF358: 409A0018  bne cr6, 0x832bf370
	if !ctx.cr[6].eq {
	pc = 0x832BF370; continue 'dispatch;
	}
	// 832BF35C: 7F1CA040  cmplw cr6, r28, r20
	ctx.cr[6].compare_u32(ctx.r[28].u32, ctx.r[20].u32, &mut ctx.xer);
	// 832BF360: 4098023C  bge cr6, 0x832bf59c
	if !ctx.cr[6].lt {
	pc = 0x832BF59C; continue 'dispatch;
	}
	// 832BF364: 83DC0000  lwz r30, 0(r28)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF368: 3B9C0004  addi r28, r28, 4
	ctx.r[28].s64 = ctx.r[28].s64 + 4;
	// 832BF36C: 3BE00020  li r31, 0x20
	ctx.r[31].s64 = 32;
	// 832BF370: 57CB07FE  clrlwi r11, r30, 0x1f
	ctx.r[11].u64 = ctx.r[30].u32 as u64 & 0x00000001u64;
	// 832BF374: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832BF378: 57DEF87E  srwi r30, r30, 1
	// 832BF37C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BF380: 419A0060  beq cr6, 0x832bf3e0
	if ctx.cr[6].eq {
	pc = 0x832BF3E0; continue 'dispatch;
	}
	// 832BF384: 2B1F0004  cmplwi cr6, r31, 4
	ctx.cr[6].compare_u32(ctx.r[31].u32, 4 as u32, &mut ctx.xer);
	// 832BF388: 4098003C  bge cr6, 0x832bf3c4
	if !ctx.cr[6].lt {
	pc = 0x832BF3C4; continue 'dispatch;
	}
	// 832BF38C: 7F1CA040  cmplw cr6, r28, r20
	ctx.cr[6].compare_u32(ctx.r[28].u32, ctx.r[20].u32, &mut ctx.xer);
	// 832BF390: 4098020C  bge cr6, 0x832bf59c
	if !ctx.cr[6].lt {
	pc = 0x832BF59C; continue 'dispatch;
	}
	// 832BF394: 817C0000  lwz r11, 0(r28)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF398: 215F0004  subfic r10, r31, 4
	ctx.xer.ca = ctx.r[31].u32 <= 4 as u32;
	ctx.r[10].s64 = (4 as i64) - ctx.r[31].s64;
	// 832BF39C: 3B9C0004  addi r28, r28, 4
	ctx.r[28].s64 = ctx.r[28].s64 + 4;
	// 832BF3A0: 7D69F830  slw r9, r11, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[11].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF3A4: 7D28F378  or r8, r9, r30
	ctx.r[8].u64 = ctx.r[9].u64 | ctx.r[30].u64;
	// 832BF3A8: 7D7E5430  srw r30, r11, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[11].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF3AC: 550B073E  clrlwi r11, r8, 0x1c
	ctx.r[11].u64 = ctx.r[8].u32 as u64 & 0x0000000Fu64;
	// 832BF3B0: 3BFF001C  addi r31, r31, 0x1c
	ctx.r[31].s64 = ctx.r[31].s64 + 28;
	// 832BF3B4: 7D6B98AE  lbzx r11, r11, r19
	ctx.r[11].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[19].u32)) } as u64;
	// 832BF3B8: 556B183E  rotlwi r11, r11, 3
	ctx.r[11].u64 = ((ctx.r[11].u32).rotate_left(3)) as u64;
	// 832BF3BC: 7F6B3214  add r27, r11, r6
	ctx.r[27].u64 = ctx.r[11].u64 + ctx.r[6].u64;
	// 832BF3C0: 48000024  b 0x832bf3e4
	pc = 0x832BF3E4; continue 'dispatch;
	// 832BF3C4: 57CB073E  clrlwi r11, r30, 0x1c
	ctx.r[11].u64 = ctx.r[30].u32 as u64 & 0x0000000Fu64;
	// 832BF3C8: 57DEE13E  srwi r30, r30, 4
	// 832BF3CC: 3BFFFFFC  addi r31, r31, -4
	ctx.r[31].s64 = ctx.r[31].s64 + -4;
	// 832BF3D0: 7D6B98AE  lbzx r11, r11, r19
	ctx.r[11].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[19].u32)) } as u64;
	// 832BF3D4: 556B183E  rotlwi r11, r11, 3
	ctx.r[11].u64 = ((ctx.r[11].u32).rotate_left(3)) as u64;
	// 832BF3D8: 7F6B3214  add r27, r11, r6
	ctx.r[27].u64 = ctx.r[11].u64 + ctx.r[6].u64;
	// 832BF3DC: 48000008  b 0x832bf3e4
	pc = 0x832BF3E4; continue 'dispatch;
	// 832BF3E0: 3B660008  addi r27, r6, 8
	ctx.r[27].s64 = ctx.r[6].s64 + 8;
	// 832BF3E4: 7F1B9040  cmplw cr6, r27, r18
	ctx.cr[6].compare_u32(ctx.r[27].u32, ctx.r[18].u32, &mut ctx.xer);
	// 832BF3E8: 40990008  ble cr6, 0x832bf3f0
	if !ctx.cr[6].gt {
	pc = 0x832BF3F0; continue 'dispatch;
	}
	// 832BF3EC: 7E5B9378  mr r27, r18
	ctx.r[27].u64 = ctx.r[18].u64;
	// 832BF3F0: 2B1F0004  cmplwi cr6, r31, 4
	ctx.cr[6].compare_u32(ctx.r[31].u32, 4 as u32, &mut ctx.xer);
	// 832BF3F4: 40980030  bge cr6, 0x832bf424
	if !ctx.cr[6].lt {
	pc = 0x832BF424; continue 'dispatch;
	}
	// 832BF3F8: 7F1CA040  cmplw cr6, r28, r20
	ctx.cr[6].compare_u32(ctx.r[28].u32, ctx.r[20].u32, &mut ctx.xer);
	// 832BF3FC: 409801A0  bge cr6, 0x832bf59c
	if !ctx.cr[6].lt {
	pc = 0x832BF59C; continue 'dispatch;
	}
	// 832BF400: 817C0000  lwz r11, 0(r28)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF404: 215F0004  subfic r10, r31, 4
	ctx.xer.ca = ctx.r[31].u32 <= 4 as u32;
	ctx.r[10].s64 = (4 as i64) - ctx.r[31].s64;
	// 832BF408: 3B9C0004  addi r28, r28, 4
	ctx.r[28].s64 = ctx.r[28].s64 + 4;
	// 832BF40C: 7D69F830  slw r9, r11, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[11].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF410: 7D28F378  or r8, r9, r30
	ctx.r[8].u64 = ctx.r[9].u64 | ctx.r[30].u64;
	// 832BF414: 7D7E5430  srw r30, r11, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[11].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF418: 550B073E  clrlwi r11, r8, 0x1c
	ctx.r[11].u64 = ctx.r[8].u32 as u64 & 0x0000000Fu64;
	// 832BF41C: 3BFF001C  addi r31, r31, 0x1c
	ctx.r[31].s64 = ctx.r[31].s64 + 28;
	// 832BF420: 48000010  b 0x832bf430
	pc = 0x832BF430; continue 'dispatch;
	// 832BF424: 57CB073E  clrlwi r11, r30, 0x1c
	ctx.r[11].u64 = ctx.r[30].u32 as u64 & 0x0000000Fu64;
	// 832BF428: 57DEE13E  srwi r30, r30, 4
	// 832BF42C: 3BFFFFFC  addi r31, r31, -4
	ctx.r[31].s64 = ctx.r[31].s64 + -4;
	// 832BF430: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832BF434: 409A005C  bne cr6, 0x832bf490
	if !ctx.cr[6].eq {
	pc = 0x832BF490; continue 'dispatch;
	}
	// 832BF438: 7D66D850  subf r11, r6, r27
	ctx.r[11].s64 = ctx.r[27].s64 - ctx.r[6].s64;
	// 832BF43C: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BF440: 557D103A  slwi r29, r11, 2
	// 832BF444: 7EE3BB78  mr r3, r23
	ctx.r[3].u64 = ctx.r[23].u64;
	// 832BF448: 7FA5EB78  mr r5, r29
	ctx.r[5].u64 = ctx.r[29].u64;
	// 832BF44C: 4B9EA565  bl 0x82ca99b0
	ctx.lr = 0x832BF450;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BF450: 574B103A  slwi r11, r26, 2
	// 832BF454: 7EFDBA14  add r23, r29, r23
	ctx.r[23].u64 = ctx.r[29].u64 + ctx.r[23].u64;
	// 832BF458: 7F66DB78  mr r6, r27
	ctx.r[6].u64 = ctx.r[27].u64;
	// 832BF45C: 7D4BC02E  lwzx r10, r11, r24
	ctx.r[10].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[24].u32)) } as u64;
	// 832BF460: 5549083C  slwi r9, r10, 1
	// 832BF464: 7F1B4840  cmplw cr6, r27, r9
	ctx.cr[6].compare_u32(ctx.r[27].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BF468: 4099012C  ble cr6, 0x832bf594
	if !ctx.cr[6].gt {
	pc = 0x832BF594; continue 'dispatch;
	}
	// 832BF46C: 3B5A0001  addi r26, r26, 1
	ctx.r[26].s64 = ctx.r[26].s64 + 1;
	// 832BF470: 7FEBB42E  lfsx f31, r11, r22
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[22].u32)) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832BF474: FFC0F850  fneg f30, f31
	ctx.f[30].u64 = ctx.f[31].u64 ^ 0x8000_0000_0000_0000u64;
	// 832BF478: 574B103A  slwi r11, r26, 2
	// 832BF47C: 7D4BC02E  lwzx r10, r11, r24
	ctx.r[10].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[24].u32)) } as u64;
	// 832BF480: 5549083C  slwi r9, r10, 1
	// 832BF484: 7F1B4840  cmplw cr6, r27, r9
	ctx.cr[6].compare_u32(ctx.r[27].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BF488: 4199FFE4  bgt cr6, 0x832bf46c
	if ctx.cr[6].gt {
	pc = 0x832BF46C; continue 'dispatch;
	}
	// 832BF48C: 48000108  b 0x832bf594
	pc = 0x832BF594; continue 'dispatch;
	// 832BF490: 7F06D840  cmplw cr6, r6, r27
	ctx.cr[6].compare_u32(ctx.r[6].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832BF494: 40980100  bge cr6, 0x832bf594
	if !ctx.cr[6].lt {
	pc = 0x832BF594; continue 'dispatch;
	}
	// 832BF498: 574A103A  slwi r10, r26, 2
	// 832BF49C: 7CF6C050  subf r7, r22, r24
	ctx.r[7].s64 = ctx.r[24].s64 - ctx.r[22].s64;
	// 832BF4A0: 7D0AB214  add r8, r10, r22
	ctx.r[8].u64 = ctx.r[10].u64 + ctx.r[22].u64;
	// 832BF4A4: 7D47402E  lwzx r10, r7, r8
	ctx.r[10].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[8].u32)) } as u64;
	// 832BF4A8: 5549083C  slwi r9, r10, 1
	// 832BF4AC: 7F064840  cmplw cr6, r6, r9
	ctx.cr[6].compare_u32(ctx.r[6].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BF4B0: 409A0014  bne cr6, 0x832bf4c4
	if !ctx.cr[6].eq {
	pc = 0x832BF4C4; continue 'dispatch;
	}
	// 832BF4B4: C3E80000  lfs f31, 0(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832BF4B8: 3B5A0001  addi r26, r26, 1
	ctx.r[26].s64 = ctx.r[26].s64 + 1;
	// 832BF4BC: FFC0F850  fneg f30, f31
	ctx.f[30].u64 = ctx.f[31].u64 ^ 0x8000_0000_0000_0000u64;
	// 832BF4C0: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832BF4C4: 7F1F5840  cmplw cr6, r31, r11
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832BF4C8: 4098003C  bge cr6, 0x832bf504
	if !ctx.cr[6].lt {
	pc = 0x832BF504; continue 'dispatch;
	}
	// 832BF4CC: 7F1CA040  cmplw cr6, r28, r20
	ctx.cr[6].compare_u32(ctx.r[28].u32, ctx.r[20].u32, &mut ctx.xer);
	// 832BF4D0: 409800C4  bge cr6, 0x832bf594
	if !ctx.cr[6].lt {
	pc = 0x832BF594; continue 'dispatch;
	}
	// 832BF4D4: 80BC0000  lwz r5, 0(r28)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF4D8: 208B0020  subfic r4, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[4].s64 = (32 as i64) - ctx.r[11].s64;
	// 832BF4DC: 7C7F5850  subf r3, r31, r11
	ctx.r[3].s64 = ctx.r[11].s64 - ctx.r[31].s64;
	// 832BF4E0: 7CAAF830  slw r10, r5, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[5].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF4E4: 7D49F378  or r9, r10, r30
	ctx.r[9].u64 = ctx.r[10].u64 | ctx.r[30].u64;
	// 832BF4E8: 7F242430  srw r4, r25, r4
	if (ctx.r[4].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[25].u32) >> ((ctx.r[4].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF4EC: 7D4BF850  subf r10, r11, r31
	ctx.r[10].s64 = ctx.r[31].s64 - ctx.r[11].s64;
	// 832BF4F0: 7C894838  and r9, r4, r9
	ctx.r[9].u64 = ctx.r[4].u64 & ctx.r[9].u64;
	// 832BF4F4: 7CBE1C30  srw r30, r5, r3
	if (ctx.r[3].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[5].u32) >> ((ctx.r[3].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF4F8: 3BEA0020  addi r31, r10, 0x20
	ctx.r[31].s64 = ctx.r[10].s64 + 32;
	// 832BF4FC: 3B9C0004  addi r28, r28, 4
	ctx.r[28].s64 = ctx.r[28].s64 + 4;
	// 832BF500: 48000018  b 0x832bf518
	pc = 0x832BF518; continue 'dispatch;
	// 832BF504: 214B0020  subfic r10, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[10].s64 = (32 as i64) - ctx.r[11].s64;
	// 832BF508: 7FEBF850  subf r31, r11, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[11].s64;
	// 832BF50C: 7F295430  srw r9, r25, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[25].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF510: 7D29F038  and r9, r9, r30
	ctx.r[9].u64 = ctx.r[9].u64 & ctx.r[30].u64;
	// 832BF514: 7FDE5C30  srw r30, r30, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF518: 2F090000  cmpwi cr6, r9, 0
	ctx.cr[6].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832BF51C: 419A0064  beq cr6, 0x832bf580
	if ctx.cr[6].eq {
	pc = 0x832BF580; continue 'dispatch;
	}
	// 832BF520: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BF524: 409A0018  bne cr6, 0x832bf53c
	if !ctx.cr[6].eq {
	pc = 0x832BF53C; continue 'dispatch;
	}
	// 832BF528: 815C0000  lwz r10, 0(r28)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF52C: 3B9C0004  addi r28, r28, 4
	ctx.r[28].s64 = ctx.r[28].s64 + 4;
	// 832BF530: 3BE0001F  li r31, 0x1f
	ctx.r[31].s64 = 31;
	// 832BF534: 555EF87E  srwi r30, r10, 1
	// 832BF538: 48000010  b 0x832bf548
	pc = 0x832BF548; continue 'dispatch;
	// 832BF53C: 7FCAF378  mr r10, r30
	ctx.r[10].u64 = ctx.r[30].u64;
	// 832BF540: 57DEF87E  srwi r30, r30, 1
	// 832BF544: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832BF548: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832BF54C: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BF550: 419A000C  beq cr6, 0x832bf55c
	if ctx.cr[6].eq {
	pc = 0x832BF55C; continue 'dispatch;
	}
	// 832BF554: FC00F090  fmr f0, f30
	ctx.f[0].f64 = ctx.f[30].f64;
	// 832BF558: 48000008  b 0x832bf560
	pc = 0x832BF560; continue 'dispatch;
	// 832BF55C: FC00F890  fmr f0, f31
	ctx.f[0].f64 = ctx.f[31].f64;
	// 832BF560: 7D2A07B4  extsw r10, r9
	ctx.r[10].s64 = ctx.r[9].s32 as i64;
	// 832BF564: F9410050  std r10, 0x50(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[10].u64 ) };
	// 832BF568: C9A10050  lfd f13, 0x50(r1)
	ctx.f[13].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832BF56C: FD806E9C  fcfid f12, f13
	ctx.f[12].f64 = (ctx.f[13].s64 as f64);
	// 832BF570: FD606018  frsp f11, f12
	ctx.f[11].f64 = (ctx.f[12].f64 as f32) as f64;
	// 832BF574: ED4B0032  fmuls f10, f11, f0
	ctx.f[10].f64 = (((ctx.f[11].f64 * ctx.f[0].f64) as f32) as f64);
	// 832BF578: D1570000  stfs f10, 0(r23)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[23].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832BF57C: 48000008  b 0x832bf584
	pc = 0x832BF584; continue 'dispatch;
	// 832BF580: D3B70000  stfs f29, 0(r23)
	tmp.f32 = (ctx.f[29].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[23].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832BF584: 38C60001  addi r6, r6, 1
	ctx.r[6].s64 = ctx.r[6].s64 + 1;
	// 832BF588: 3AF70004  addi r23, r23, 4
	ctx.r[23].s64 = ctx.r[23].s64 + 4;
	// 832BF58C: 7F06D840  cmplw cr6, r6, r27
	ctx.cr[6].compare_u32(ctx.r[6].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832BF590: 4198FF14  blt cr6, 0x832bf4a4
	if ctx.cr[6].lt {
	pc = 0x832BF4A4; continue 'dispatch;
	}
	// 832BF594: 7F069040  cmplw cr6, r6, r18
	ctx.cr[6].compare_u32(ctx.r[6].u32, ctx.r[18].u32, &mut ctx.xer);
	// 832BF598: 4198FDBC  blt cr6, 0x832bf354
	if ctx.cr[6].lt {
	pc = 0x832BF354; continue 'dispatch;
	}
	// 832BF59C: 93950004  stw r28, 4(r21)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[21].u32.wrapping_add(4 as u32), ctx.r[28].u32 ) };
	// 832BF5A0: 93D50000  stw r30, 0(r21)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[21].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832BF5A4: 93F50008  stw r31, 8(r21)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[21].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832BF5A8: 382100F0  addi r1, r1, 0xf0
	ctx.r[1].s64 = ctx.r[1].s64 + 240;
	// 832BF5AC: CBA1FF70  lfd f29, -0x90(r1)
	ctx.f[29].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-144 as u32) ) };
	// 832BF5B0: CBC1FF78  lfd f30, -0x88(r1)
	ctx.f[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-136 as u32) ) };
	// 832BF5B4: CBE1FF80  lfd f31, -0x80(r1)
	ctx.f[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-128 as u32) ) };
	// 832BF5B8: 4B9E9E78  b 0x82ca9430
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9430);
	return;
}

pub fn sub_832BF5C0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    let mut ea: u32 = 0;
    // ---- function 0x832BF5C0 size=1512
	// 832BF5C0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BF5C4: 4B9E9E25  bl 0x82ca93e8
	ctx.lr = 0x832BF5C8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93E8);
	// 832BF5C8: DBE1FF90  stfd f31, -0x70(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-112 as u32), ctx.f[31].u64 ) };
	// 832BF5CC: E981F000  ld r12, -0x1000(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-4096 as u32) ) };
	// 832BF5D0: E981E000  ld r12, -0x2000(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-8192 as u32) ) };
	// 832BF5D4: E981D000  ld r12, -0x3000(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-12288 as u32) ) };
	// 832BF5D8: E981C000  ld r12, -0x4000(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16384 as u32) ) };
	// 832BF5DC: 9421BEB0  stwu r1, -0x4150(r1)
	ea = ctx.r[1].u32.wrapping_add(-16720 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BF5E0: 54D907FE  clrlwi r25, r6, 0x1f
	ctx.r[25].u64 = ctx.r[6].u32 as u64 & 0x00000001u64;
	// 832BF5E4: 83C141A4  lwz r30, 0x41a4(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(16804 as u32) ) } as u64;
	// 832BF5E8: 7D344B78  mr r20, r9
	ctx.r[20].u64 = ctx.r[9].u64;
	// 832BF5EC: FFE00890  fmr f31, f1
	ctx.f[31].f64 = ctx.f[1].f64;
	// 832BF5F0: 7C7B1B78  mr r27, r3
	ctx.r[27].u64 = ctx.r[3].u64;
	// 832BF5F4: 9141005C  stw r10, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832BF5F8: 7CB82B78  mr r24, r5
	ctx.r[24].u64 = ctx.r[5].u64;
	// 832BF5FC: 91410054  stw r10, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[10].u32 ) };
	// 832BF600: 7CF63B78  mr r22, r7
	ctx.r[22].u64 = ctx.r[7].u64;
	// 832BF604: 7D154378  mr r21, r8
	ctx.r[21].u64 = ctx.r[8].u64;
	// 832BF608: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832BF60C: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832BF610: 2B190000  cmplwi cr6, r25, 0
	ctx.cr[6].compare_u32(ctx.r[25].u32, 0 as u32, &mut ctx.xer);
	// 832BF614: 419A0030  beq cr6, 0x832bf644
	if ctx.cr[6].eq {
	pc = 0x832BF644; continue 'dispatch;
	}
	// 832BF618: 7F0AF040  cmplw cr6, r10, r30
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832BF61C: 41980014  blt cr6, 0x832bf630
	if ctx.cr[6].lt {
	pc = 0x832BF630; continue 'dispatch;
	}
	// 832BF620: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832BF624: 38214150  addi r1, r1, 0x4150
	ctx.r[1].s64 = ctx.r[1].s64 + 16720;
	// 832BF628: CBE1FF90  lfd f31, -0x70(r1)
	ctx.f[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-112 as u32) ) };
	// 832BF62C: 4B9E9E0C  b 0x82ca9438
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9438);
	return;
	// 832BF630: 812A0000  lwz r9, 0(r10)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF634: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832BF638: 3960001E  li r11, 0x1e
	ctx.r[11].s64 = 30;
	// 832BF63C: 91410054  stw r10, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[10].u32 ) };
	// 832BF640: 5529F0BE  srwi r9, r9, 2
	// 832BF644: 3B8100E0  addi r28, r1, 0xe0
	ctx.r[28].s64 = ctx.r[1].s64 + 224;
	// 832BF648: 3B400000  li r26, 0
	ctx.r[26].s64 = 0;
	// 832BF64C: 2B180000  cmplwi cr6, r24, 0
	ctx.cr[6].compare_u32(ctx.r[24].u32, 0 as u32, &mut ctx.xer);
	// 832BF650: 419A03BC  beq cr6, 0x832bfa0c
	if ctx.cr[6].eq {
	pc = 0x832BFA0C; continue 'dispatch;
	}
	// 832BF654: 3D008350  lis r8, -0x7cb0
	ctx.r[8].s64 = -2091909120;
	// 832BF658: 82E141B4  lwz r23, 0x41b4(r1)
	ctx.r[23].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(16820 as u32) ) } as u64;
	// 832BF65C: 83A141AC  lwz r29, 0x41ac(r1)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(16812 as u32) ) } as u64;
	// 832BF660: 3BE8C408  addi r31, r8, -0x3bf8
	ctx.r[31].s64 = ctx.r[8].s64 + -15352;
	// 832BF664: 4800000C  b 0x832bf670
	pc = 0x832BF670; continue 'dispatch;
	// 832BF668: 81610058  lwz r11, 0x58(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(88 as u32) ) } as u64;
	// 832BF66C: 81210050  lwz r9, 0x50(r1)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832BF670: 2B0B001D  cmplwi cr6, r11, 0x1d
	ctx.cr[6].compare_u32(ctx.r[11].u32, 29 as u32, &mut ctx.xer);
	// 832BF674: 40980034  bge cr6, 0x832bf6a8
	if !ctx.cr[6].lt {
	pc = 0x832BF6A8; continue 'dispatch;
	}
	// 832BF678: 7F0AF040  cmplw cr6, r10, r30
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832BF67C: 40980390  bge cr6, 0x832bfa0c
	if !ctx.cr[6].lt {
	pc = 0x832BFA0C; continue 'dispatch;
	}
	// 832BF680: 810A0000  lwz r8, 0(r10)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF684: 20EB001D  subfic r7, r11, 0x1d
	ctx.xer.ca = ctx.r[11].u32 <= 29 as u32;
	ctx.r[7].s64 = (29 as i64) - ctx.r[11].s64;
	// 832BF688: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832BF68C: 7D065830  slw r6, r8, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[8].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF690: 7CC54B78  or r5, r6, r9
	ctx.r[5].u64 = ctx.r[6].u64 | ctx.r[9].u64;
	// 832BF694: 91410054  stw r10, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[10].u32 ) };
	// 832BF698: 7D093C30  srw r9, r8, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[8].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF69C: 54A800FE  clrlwi r8, r5, 3
	ctx.r[8].u64 = ctx.r[5].u32 as u64 & 0x1FFFFFFFu64;
	// 832BF6A0: 396B0003  addi r11, r11, 3
	ctx.r[11].s64 = ctx.r[11].s64 + 3;
	// 832BF6A4: 48000010  b 0x832bf6b4
	pc = 0x832BF6B4; continue 'dispatch;
	// 832BF6A8: 552800FE  clrlwi r8, r9, 3
	ctx.r[8].u64 = ctx.r[9].u32 as u64 & 0x1FFFFFFFu64;
	// 832BF6AC: 55291F7E  srwi r9, r9, 0x1d
	// 832BF6B0: 396BFFE3  addi r11, r11, -0x1d
	ctx.r[11].s64 = ctx.r[11].s64 + -29;
	// 832BF6B4: 5505D97E  srwi r5, r8, 5
	// 832BF6B8: 5506167A  rlwinm r6, r8, 2, 0x19, 0x1d
	ctx.r[6].u64 = ctx.r[8].u32 as u64 & 0x3FFFFFFFu64;
	// 832BF6BC: 54A5024E  rlwinm r5, r5, 0, 9, 7
	ctx.r[5].u64 = ctx.r[5].u32 as u64 & 0xFFFFFFFFu64;
	// 832BF6C0: 550400C6  rlwinm r4, r8, 0, 3, 3
	ctx.r[4].u64 = ctx.r[8].u32 as u64 & 0xFFFFFFFFu64;
	// 832BF6C4: 78A50020  clrldi r5, r5, 0x20
	ctx.r[5].u64 = ctx.r[5].u64 & 0x00000000FFFFFFFFu64;
	// 832BF6C8: 2B040000  cmplwi cr6, r4, 0
	ctx.cr[6].compare_u32(ctx.r[4].u32, 0 as u32, &mut ctx.xer);
	// 832BF6CC: F8A10060  std r5, 0x60(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(96 as u32), ctx.r[5].u64 ) };
	// 832BF6D0: C8010060  lfd f0, 0x60(r1)
	ctx.f[0].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(96 as u32) ) };
	// 832BF6D4: FDA0069C  fcfid f13, f0
	ctx.f[13].f64 = (ctx.f[0].s64 as f64);
	// 832BF6D8: 7D66FC2E  lfsx f11, r6, r31
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[6].u32.wrapping_add(ctx.r[31].u32)) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832BF6DC: FD806818  frsp f12, f13
	ctx.f[12].f64 = (ctx.f[13].f64 as f32) as f64;
	// 832BF6E0: EC0C02F2  fmuls f0, f12, f11
	ctx.f[0].f64 = (((ctx.f[12].f64 * ctx.f[11].f64) as f32) as f64);
	// 832BF6E4: 419A0008  beq cr6, 0x832bf6ec
	if ctx.cr[6].eq {
	pc = 0x832BF6EC; continue 'dispatch;
	}
	// 832BF6E8: FC000050  fneg f0, f0
	ctx.f[0].u64 = ctx.f[0].u64 ^ 0x8000_0000_0000_0000u64;
	// 832BF6EC: D01C0000  stfs f0, 0(r28)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832BF6F0: 2B0B001D  cmplwi cr6, r11, 0x1d
	ctx.cr[6].compare_u32(ctx.r[11].u32, 29 as u32, &mut ctx.xer);
	// 832BF6F4: 40980034  bge cr6, 0x832bf728
	if !ctx.cr[6].lt {
	pc = 0x832BF728; continue 'dispatch;
	}
	// 832BF6F8: 7F0AF040  cmplw cr6, r10, r30
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832BF6FC: 40980310  bge cr6, 0x832bfa0c
	if !ctx.cr[6].lt {
	pc = 0x832BFA0C; continue 'dispatch;
	}
	// 832BF700: 810A0000  lwz r8, 0(r10)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF704: 20EB001D  subfic r7, r11, 0x1d
	ctx.xer.ca = ctx.r[11].u32 <= 29 as u32;
	ctx.r[7].s64 = (29 as i64) - ctx.r[11].s64;
	// 832BF708: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832BF70C: 7D065830  slw r6, r8, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[8].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF710: 7CC54B78  or r5, r6, r9
	ctx.r[5].u64 = ctx.r[6].u64 | ctx.r[9].u64;
	// 832BF714: 91410054  stw r10, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[10].u32 ) };
	// 832BF718: 7D093C30  srw r9, r8, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[8].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF71C: 396B0003  addi r11, r11, 3
	ctx.r[11].s64 = ctx.r[11].s64 + 3;
	// 832BF720: 54A800FE  clrlwi r8, r5, 3
	ctx.r[8].u64 = ctx.r[5].u32 as u64 & 0x1FFFFFFFu64;
	// 832BF724: 48000010  b 0x832bf734
	pc = 0x832BF734; continue 'dispatch;
	// 832BF728: 552800FE  clrlwi r8, r9, 3
	ctx.r[8].u64 = ctx.r[9].u32 as u64 & 0x1FFFFFFFu64;
	// 832BF72C: 55291F7E  srwi r9, r9, 0x1d
	// 832BF730: 396BFFE3  addi r11, r11, -0x1d
	ctx.r[11].s64 = ctx.r[11].s64 + -29;
	// 832BF734: 5505D97E  srwi r5, r8, 5
	// 832BF738: 91610058  stw r11, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[11].u32 ) };
	// 832BF73C: 5506167A  rlwinm r6, r8, 2, 0x19, 0x1d
	ctx.r[6].u64 = ctx.r[8].u32 as u64 & 0x3FFFFFFFu64;
	// 832BF740: 91210050  stw r9, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[9].u32 ) };
	// 832BF744: 54A5024E  rlwinm r5, r5, 0, 9, 7
	ctx.r[5].u64 = ctx.r[5].u32 as u64 & 0xFFFFFFFFu64;
	// 832BF748: 550400C6  rlwinm r4, r8, 0, 3, 3
	ctx.r[4].u64 = ctx.r[8].u32 as u64 & 0xFFFFFFFFu64;
	// 832BF74C: 78A50020  clrldi r5, r5, 0x20
	ctx.r[5].u64 = ctx.r[5].u64 & 0x00000000FFFFFFFFu64;
	// 832BF750: 2B040000  cmplwi cr6, r4, 0
	ctx.cr[6].compare_u32(ctx.r[4].u32, 0 as u32, &mut ctx.xer);
	// 832BF754: F8A10068  std r5, 0x68(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(104 as u32), ctx.r[5].u64 ) };
	// 832BF758: 7D66FC2E  lfsx f11, r6, r31
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[6].u32.wrapping_add(ctx.r[31].u32)) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832BF75C: C8010068  lfd f0, 0x68(r1)
	ctx.f[0].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(104 as u32) ) };
	// 832BF760: FDA0069C  fcfid f13, f0
	ctx.f[13].f64 = (ctx.f[0].s64 as f64);
	// 832BF764: FD806818  frsp f12, f13
	ctx.f[12].f64 = (ctx.f[13].f64 as f32) as f64;
	// 832BF768: EC0C02F2  fmuls f0, f12, f11
	ctx.f[0].f64 = (((ctx.f[12].f64 * ctx.f[11].f64) as f32) as f64);
	// 832BF76C: 419A0008  beq cr6, 0x832bf774
	if ctx.cr[6].eq {
	pc = 0x832BF774; continue 'dispatch;
	}
	// 832BF770: FC000050  fneg f0, f0
	ctx.f[0].u64 = ctx.f[0].u64 ^ 0x8000_0000_0000_0000u64;
	// 832BF774: D01C0004  stfs f0, 4(r28)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832BF778: 38A00000  li r5, 0
	ctx.r[5].s64 = 0;
	// 832BF77C: 2F1D0004  cmpwi cr6, r29, 4
	ctx.cr[6].compare_i32(ctx.r[29].s32, 4, &mut ctx.xer);
	// 832BF780: 419801B0  blt cr6, 0x832bf930
	if ctx.cr[6].lt {
	pc = 0x832BF930; continue 'dispatch;
	}
	// 832BF784: 38C10074  addi r6, r1, 0x74
	ctx.r[6].s64 = ctx.r[1].s64 + 116;
	// 832BF788: 2B0B0008  cmplwi cr6, r11, 8
	ctx.cr[6].compare_u32(ctx.r[11].u32, 8 as u32, &mut ctx.xer);
	// 832BF78C: 40980030  bge cr6, 0x832bf7bc
	if !ctx.cr[6].lt {
	pc = 0x832BF7BC; continue 'dispatch;
	}
	// 832BF790: 7F0AF040  cmplw cr6, r10, r30
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832BF794: 40980224  bge cr6, 0x832bf9b8
	if !ctx.cr[6].lt {
	pc = 0x832BF9B8; continue 'dispatch;
	}
	// 832BF798: 810A0000  lwz r8, 0(r10)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF79C: 20EB0008  subfic r7, r11, 8
	ctx.xer.ca = ctx.r[11].u32 <= 8 as u32;
	ctx.r[7].s64 = (8 as i64) - ctx.r[11].s64;
	// 832BF7A0: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832BF7A4: 7D045830  slw r4, r8, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[8].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF7A8: 7C834B78  or r3, r4, r9
	ctx.r[3].u64 = ctx.r[4].u64 | ctx.r[9].u64;
	// 832BF7AC: 7D073C30  srw r7, r8, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[8].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF7B0: 396B0018  addi r11, r11, 0x18
	ctx.r[11].s64 = ctx.r[11].s64 + 24;
	// 832BF7B4: 5468063E  clrlwi r8, r3, 0x18
	ctx.r[8].u64 = ctx.r[3].u32 as u64 & 0x000000FFu64;
	// 832BF7B8: 48000010  b 0x832bf7c8
	pc = 0x832BF7C8; continue 'dispatch;
	// 832BF7BC: 5527C23E  srwi r7, r9, 8
	// 832BF7C0: 396BFFF8  addi r11, r11, -8
	ctx.r[11].s64 = ctx.r[11].s64 + -8;
	// 832BF7C4: 5528063E  clrlwi r8, r9, 0x18
	ctx.r[8].u64 = ctx.r[9].u32 as u64 & 0x000000FFu64;
	// 832BF7C8: 91610058  stw r11, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[11].u32 ) };
	// 832BF7CC: 2B08005F  cmplwi cr6, r8, 0x5f
	ctx.cr[6].compare_u32(ctx.r[8].u32, 95 as u32, &mut ctx.xer);
	// 832BF7D0: 90E10050  stw r7, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[7].u32 ) };
	// 832BF7D4: 40990008  ble cr6, 0x832bf7dc
	if !ctx.cr[6].gt {
	pc = 0x832BF7DC; continue 'dispatch;
	}
	// 832BF7D8: 3900005F  li r8, 0x5f
	ctx.r[8].s64 = 95;
	// 832BF7DC: 393FFE80  addi r9, r31, -0x180
	ctx.r[9].s64 = ctx.r[31].s64 + -384;
	// 832BF7E0: 5508103A  slwi r8, r8, 2
	// 832BF7E4: 2B0B0008  cmplwi cr6, r11, 8
	ctx.cr[6].compare_u32(ctx.r[11].u32, 8 as u32, &mut ctx.xer);
	// 832BF7E8: 7C084C2E  lfsx f0, r8, r9
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[9].u32)) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832BF7EC: D006FFFC  stfs f0, -4(r6)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[6].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832BF7F0: 40980030  bge cr6, 0x832bf820
	if !ctx.cr[6].lt {
	pc = 0x832BF820; continue 'dispatch;
	}
	// 832BF7F4: 7F0AF040  cmplw cr6, r10, r30
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832BF7F8: 409801C0  bge cr6, 0x832bf9b8
	if !ctx.cr[6].lt {
	pc = 0x832BF9B8; continue 'dispatch;
	}
	// 832BF7FC: 812A0000  lwz r9, 0(r10)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF800: 210B0008  subfic r8, r11, 8
	ctx.xer.ca = ctx.r[11].u32 <= 8 as u32;
	ctx.r[8].s64 = (8 as i64) - ctx.r[11].s64;
	// 832BF804: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832BF808: 7D245830  slw r4, r9, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[9].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF80C: 7C833B78  or r3, r4, r7
	ctx.r[3].u64 = ctx.r[4].u64 | ctx.r[7].u64;
	// 832BF810: 7D284430  srw r8, r9, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[8].u64 = 0;
	} else {
		ctx.r[8].u64 = ((ctx.r[9].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF814: 396B0018  addi r11, r11, 0x18
	ctx.r[11].s64 = ctx.r[11].s64 + 24;
	// 832BF818: 5469063E  clrlwi r9, r3, 0x18
	ctx.r[9].u64 = ctx.r[3].u32 as u64 & 0x000000FFu64;
	// 832BF81C: 48000010  b 0x832bf82c
	pc = 0x832BF82C; continue 'dispatch;
	// 832BF820: 54E8C23E  srwi r8, r7, 8
	// 832BF824: 396BFFF8  addi r11, r11, -8
	ctx.r[11].s64 = ctx.r[11].s64 + -8;
	// 832BF828: 54E9063E  clrlwi r9, r7, 0x18
	ctx.r[9].u64 = ctx.r[7].u32 as u64 & 0x000000FFu64;
	// 832BF82C: 91610058  stw r11, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[11].u32 ) };
	// 832BF830: 2B09005F  cmplwi cr6, r9, 0x5f
	ctx.cr[6].compare_u32(ctx.r[9].u32, 95 as u32, &mut ctx.xer);
	// 832BF834: 91010050  stw r8, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[8].u32 ) };
	// 832BF838: 40990008  ble cr6, 0x832bf840
	if !ctx.cr[6].gt {
	pc = 0x832BF840; continue 'dispatch;
	}
	// 832BF83C: 3920005F  li r9, 0x5f
	ctx.r[9].s64 = 95;
	// 832BF840: 38FFFE80  addi r7, r31, -0x180
	ctx.r[7].s64 = ctx.r[31].s64 + -384;
	// 832BF844: 5524103A  slwi r4, r9, 2
	// 832BF848: 2B0B0008  cmplwi cr6, r11, 8
	ctx.cr[6].compare_u32(ctx.r[11].u32, 8 as u32, &mut ctx.xer);
	// 832BF84C: 7C043C2E  lfsx f0, r4, r7
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[4].u32.wrapping_add(ctx.r[7].u32)) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832BF850: D0060000  stfs f0, 0(r6)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[6].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832BF854: 40980030  bge cr6, 0x832bf884
	if !ctx.cr[6].lt {
	pc = 0x832BF884; continue 'dispatch;
	}
	// 832BF858: 7F0AF040  cmplw cr6, r10, r30
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832BF85C: 4098015C  bge cr6, 0x832bf9b8
	if !ctx.cr[6].lt {
	pc = 0x832BF9B8; continue 'dispatch;
	}
	// 832BF860: 812A0000  lwz r9, 0(r10)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF864: 20EB0008  subfic r7, r11, 8
	ctx.xer.ca = ctx.r[11].u32 <= 8 as u32;
	ctx.r[7].s64 = (8 as i64) - ctx.r[11].s64;
	// 832BF868: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832BF86C: 7D245830  slw r4, r9, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[9].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF870: 7C834378  or r3, r4, r8
	ctx.r[3].u64 = ctx.r[4].u64 | ctx.r[8].u64;
	// 832BF874: 7D273C30  srw r7, r9, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[9].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF878: 396B0018  addi r11, r11, 0x18
	ctx.r[11].s64 = ctx.r[11].s64 + 24;
	// 832BF87C: 5469063E  clrlwi r9, r3, 0x18
	ctx.r[9].u64 = ctx.r[3].u32 as u64 & 0x000000FFu64;
	// 832BF880: 48000010  b 0x832bf890
	pc = 0x832BF890; continue 'dispatch;
	// 832BF884: 5507C23E  srwi r7, r8, 8
	// 832BF888: 396BFFF8  addi r11, r11, -8
	ctx.r[11].s64 = ctx.r[11].s64 + -8;
	// 832BF88C: 5509063E  clrlwi r9, r8, 0x18
	ctx.r[9].u64 = ctx.r[8].u32 as u64 & 0x000000FFu64;
	// 832BF890: 91610058  stw r11, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[11].u32 ) };
	// 832BF894: 2B09005F  cmplwi cr6, r9, 0x5f
	ctx.cr[6].compare_u32(ctx.r[9].u32, 95 as u32, &mut ctx.xer);
	// 832BF898: 90E10050  stw r7, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[7].u32 ) };
	// 832BF89C: 40990008  ble cr6, 0x832bf8a4
	if !ctx.cr[6].gt {
	pc = 0x832BF8A4; continue 'dispatch;
	}
	// 832BF8A0: 3920005F  li r9, 0x5f
	ctx.r[9].s64 = 95;
	// 832BF8A4: 391FFE80  addi r8, r31, -0x180
	ctx.r[8].s64 = ctx.r[31].s64 + -384;
	// 832BF8A8: 5524103A  slwi r4, r9, 2
	// 832BF8AC: 2B0B0008  cmplwi cr6, r11, 8
	ctx.cr[6].compare_u32(ctx.r[11].u32, 8 as u32, &mut ctx.xer);
	// 832BF8B0: 7C04442E  lfsx f0, r4, r8
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[4].u32.wrapping_add(ctx.r[8].u32)) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832BF8B4: D0060004  stfs f0, 4(r6)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[6].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832BF8B8: 40980030  bge cr6, 0x832bf8e8
	if !ctx.cr[6].lt {
	pc = 0x832BF8E8; continue 'dispatch;
	}
	// 832BF8BC: 7F0AF040  cmplw cr6, r10, r30
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832BF8C0: 409800F8  bge cr6, 0x832bf9b8
	if !ctx.cr[6].lt {
	pc = 0x832BF9B8; continue 'dispatch;
	}
	// 832BF8C4: 812A0000  lwz r9, 0(r10)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF8C8: 210B0008  subfic r8, r11, 8
	ctx.xer.ca = ctx.r[11].u32 <= 8 as u32;
	ctx.r[8].s64 = (8 as i64) - ctx.r[11].s64;
	// 832BF8CC: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832BF8D0: 7D245830  slw r4, r9, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[9].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF8D4: 7C833B78  or r3, r4, r7
	ctx.r[3].u64 = ctx.r[4].u64 | ctx.r[7].u64;
	// 832BF8D8: 7D294430  srw r9, r9, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[9].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF8DC: 396B0018  addi r11, r11, 0x18
	ctx.r[11].s64 = ctx.r[11].s64 + 24;
	// 832BF8E0: 5468063E  clrlwi r8, r3, 0x18
	ctx.r[8].u64 = ctx.r[3].u32 as u64 & 0x000000FFu64;
	// 832BF8E4: 48000010  b 0x832bf8f4
	pc = 0x832BF8F4; continue 'dispatch;
	// 832BF8E8: 54E9C23E  srwi r9, r7, 8
	// 832BF8EC: 396BFFF8  addi r11, r11, -8
	ctx.r[11].s64 = ctx.r[11].s64 + -8;
	// 832BF8F0: 54E8063E  clrlwi r8, r7, 0x18
	ctx.r[8].u64 = ctx.r[7].u32 as u64 & 0x000000FFu64;
	// 832BF8F4: 91610058  stw r11, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[11].u32 ) };
	// 832BF8F8: 2B08005F  cmplwi cr6, r8, 0x5f
	ctx.cr[6].compare_u32(ctx.r[8].u32, 95 as u32, &mut ctx.xer);
	// 832BF8FC: 91210050  stw r9, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[9].u32 ) };
	// 832BF900: 40990008  ble cr6, 0x832bf908
	if !ctx.cr[6].gt {
	pc = 0x832BF908; continue 'dispatch;
	}
	// 832BF904: 3900005F  li r8, 0x5f
	ctx.r[8].s64 = 95;
	// 832BF908: 38FFFE80  addi r7, r31, -0x180
	ctx.r[7].s64 = ctx.r[31].s64 + -384;
	// 832BF90C: 5504103A  slwi r4, r8, 2
	// 832BF910: 38A50004  addi r5, r5, 4
	ctx.r[5].s64 = ctx.r[5].s64 + 4;
	// 832BF914: 387DFFFD  addi r3, r29, -3
	ctx.r[3].s64 = ctx.r[29].s64 + -3;
	// 832BF918: 7F051840  cmplw cr6, r5, r3
	ctx.cr[6].compare_u32(ctx.r[5].u32, ctx.r[3].u32, &mut ctx.xer);
	// 832BF91C: 7C043C2E  lfsx f0, r4, r7
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[4].u32.wrapping_add(ctx.r[7].u32)) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832BF920: D0060008  stfs f0, 8(r6)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[6].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832BF924: 38C60010  addi r6, r6, 0x10
	ctx.r[6].s64 = ctx.r[6].s64 + 16;
	// 832BF928: 4198FE60  blt cr6, 0x832bf788
	if ctx.cr[6].lt {
	pc = 0x832BF788; continue 'dispatch;
	}
	// 832BF92C: 91410054  stw r10, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[10].u32 ) };
	// 832BF930: 7F05E840  cmplw cr6, r5, r29
	ctx.cr[6].compare_u32(ctx.r[5].u32, ctx.r[29].u32, &mut ctx.xer);
	// 832BF934: 40980088  bge cr6, 0x832bf9bc
	if !ctx.cr[6].lt {
	pc = 0x832BF9BC; continue 'dispatch;
	}
	// 832BF938: 54A7103A  slwi r7, r5, 2
	// 832BF93C: 39010070  addi r8, r1, 0x70
	ctx.r[8].s64 = ctx.r[1].s64 + 112;
	// 832BF940: 7CE74214  add r7, r7, r8
	ctx.r[7].u64 = ctx.r[7].u64 + ctx.r[8].u64;
	// 832BF944: 2B0B0008  cmplwi cr6, r11, 8
	ctx.cr[6].compare_u32(ctx.r[11].u32, 8 as u32, &mut ctx.xer);
	// 832BF948: 40980030  bge cr6, 0x832bf978
	if !ctx.cr[6].lt {
	pc = 0x832BF978; continue 'dispatch;
	}
	// 832BF94C: 7F0AF040  cmplw cr6, r10, r30
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832BF950: 40980060  bge cr6, 0x832bf9b0
	if !ctx.cr[6].lt {
	pc = 0x832BF9B0; continue 'dispatch;
	}
	// 832BF954: 810A0000  lwz r8, 0(r10)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BF958: 20CB0008  subfic r6, r11, 8
	ctx.xer.ca = ctx.r[11].u32 <= 8 as u32;
	ctx.r[6].s64 = (8 as i64) - ctx.r[11].s64;
	// 832BF95C: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832BF960: 7D045830  slw r4, r8, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[8].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF964: 7C834B78  or r3, r4, r9
	ctx.r[3].u64 = ctx.r[4].u64 | ctx.r[9].u64;
	// 832BF968: 7D093430  srw r9, r8, r6
	if (ctx.r[6].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[8].u32) >> ((ctx.r[6].u8 & 0x1F) as u32)) as u64;
	}
	// 832BF96C: 396B0018  addi r11, r11, 0x18
	ctx.r[11].s64 = ctx.r[11].s64 + 24;
	// 832BF970: 5468063E  clrlwi r8, r3, 0x18
	ctx.r[8].u64 = ctx.r[3].u32 as u64 & 0x000000FFu64;
	// 832BF974: 48000010  b 0x832bf984
	pc = 0x832BF984; continue 'dispatch;
	// 832BF978: 5528063E  clrlwi r8, r9, 0x18
	ctx.r[8].u64 = ctx.r[9].u32 as u64 & 0x000000FFu64;
	// 832BF97C: 5529C23E  srwi r9, r9, 8
	// 832BF980: 396BFFF8  addi r11, r11, -8
	ctx.r[11].s64 = ctx.r[11].s64 + -8;
	// 832BF984: 2B08005F  cmplwi cr6, r8, 0x5f
	ctx.cr[6].compare_u32(ctx.r[8].u32, 95 as u32, &mut ctx.xer);
	// 832BF988: 40990008  ble cr6, 0x832bf990
	if !ctx.cr[6].gt {
	pc = 0x832BF990; continue 'dispatch;
	}
	// 832BF98C: 3900005F  li r8, 0x5f
	ctx.r[8].s64 = 95;
	// 832BF990: 38DFFE80  addi r6, r31, -0x180
	ctx.r[6].s64 = ctx.r[31].s64 + -384;
	// 832BF994: 5504103A  slwi r4, r8, 2
	// 832BF998: 38A50001  addi r5, r5, 1
	ctx.r[5].s64 = ctx.r[5].s64 + 1;
	// 832BF99C: 7F05E840  cmplw cr6, r5, r29
	ctx.cr[6].compare_u32(ctx.r[5].u32, ctx.r[29].u32, &mut ctx.xer);
	// 832BF9A0: 7C04342E  lfsx f0, r4, r6
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[4].u32.wrapping_add(ctx.r[6].u32)) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832BF9A4: D0070000  stfs f0, 0(r7)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832BF9A8: 38E70004  addi r7, r7, 4
	ctx.r[7].s64 = ctx.r[7].s64 + 4;
	// 832BF9AC: 4198FF98  blt cr6, 0x832bf944
	if ctx.cr[6].lt {
	pc = 0x832BF944; continue 'dispatch;
	}
	// 832BF9B0: 91210050  stw r9, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[9].u32 ) };
	// 832BF9B4: 91610058  stw r11, 0x58(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(88 as u32), ctx.r[11].u32 ) };
	// 832BF9B8: 91410054  stw r10, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[10].u32 ) };
	// 832BF9BC: 7FC8F378  mr r8, r30
	ctx.r[8].u64 = ctx.r[30].u64;
	// 832BF9C0: 7EE7BB78  mr r7, r23
	ctx.r[7].u64 = ctx.r[23].u64;
	// 832BF9C4: 38C10070  addi r6, r1, 0x70
	ctx.r[6].s64 = ctx.r[1].s64 + 112;
	// 832BF9C8: 38A10050  addi r5, r1, 0x50
	ctx.r[5].s64 = ctx.r[1].s64 + 80;
	// 832BF9CC: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832BF9D0: 7F83E378  mr r3, r28
	ctx.r[3].u64 = ctx.r[28].u64;
	// 832BF9D4: 4BFFF8E5  bl 0x832bf2b8
	ctx.lr = 0x832BF9D8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BF2B8);
	// 832BF9D8: 2B190000  cmplwi cr6, r25, 0
	ctx.cr[6].compare_u32(ctx.r[25].u32, 0 as u32, &mut ctx.xer);
	// 832BF9DC: 419A0018  beq cr6, 0x832bf9f4
	if ctx.cr[6].eq {
	pc = 0x832BF9F4; continue 'dispatch;
	}
	// 832BF9E0: 7EA6AB78  mr r6, r21
	ctx.r[6].u64 = ctx.r[21].u64;
	// 832BF9E4: 7EC5B378  mr r5, r22
	ctx.r[5].u64 = ctx.r[22].u64;
	// 832BF9E8: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832BF9EC: 7F63DB78  mr r3, r27
	ctx.r[3].u64 = ctx.r[27].u64;
	// 832BF9F0: 480053C1  bl 0x832c4db0
	ctx.lr = 0x832BF9F4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C4DB0);
	// 832BF9F4: 576B103A  slwi r11, r27, 2
	// 832BF9F8: 81410054  lwz r10, 0x54(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BF9FC: 3B5A0001  addi r26, r26, 1
	ctx.r[26].s64 = ctx.r[26].s64 + 1;
	// 832BFA00: 7F8BE214  add r28, r11, r28
	ctx.r[28].u64 = ctx.r[11].u64 + ctx.r[28].u64;
	// 832BFA04: 7F1AC040  cmplw cr6, r26, r24
	ctx.cr[6].compare_u32(ctx.r[26].u32, ctx.r[24].u32, &mut ctx.xer);
	// 832BFA08: 4198FC60  blt cr6, 0x832bf668
	if ctx.cr[6].lt {
	pc = 0x832BF668; continue 'dispatch;
	}
	// 832BFA0C: 2B180001  cmplwi cr6, r24, 1
	ctx.cr[6].compare_u32(ctx.r[24].u32, 1 as u32, &mut ctx.xer);
	// 832BFA10: 396100E0  addi r11, r1, 0xe0
	ctx.r[11].s64 = ctx.r[1].s64 + 224;
	// 832BFA14: 409A00BC  bne cr6, 0x832bfad0
	if !ctx.cr[6].eq {
	pc = 0x832BFAD0; continue 'dispatch;
	}
	// 832BFA18: 7F69DB78  mr r9, r27
	ctx.r[9].u64 = ctx.r[27].u64;
	// 832BFA1C: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832BFA20: 419A0080  beq cr6, 0x832bfaa0
	if ctx.cr[6].eq {
	pc = 0x832BFAA0; continue 'dispatch;
	}
	// 832BFA24: 3CE0820A  lis r7, -0x7df6
	ctx.r[7].s64 = -2113273856;
	// 832BFA28: 3CC08210  lis r6, -0x7df0
	ctx.r[6].s64 = -2112880640;
	// 832BFA2C: 3CA08210  lis r5, -0x7df0
	ctx.r[5].s64 = -2112880640;
	// 832BFA30: 3D008210  lis r8, -0x7df0
	ctx.r[8].s64 = -2112880640;
	// 832BFA34: C1A79484  lfs f13, -0x6b7c(r7)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(-27516 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832BFA38: 39080E68  addi r8, r8, 0xe68
	ctx.r[8].s64 = ctx.r[8].s64 + 3688;
	// 832BFA3C: C1860E74  lfs f12, 0xe74(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(3700 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832BFA40: C0050E70  lfs f0, 0xe70(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(3696 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832BFA44: C16B0000  lfs f11, 0(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832BFA48: 3529FFFF  addic. r9, r9, -1
	ctx.xer.ca = (ctx.r[9].u32 > (!(-1 as u32)));
	ctx.r[9].s64 = ctx.r[9].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832BFA4C: ED4B07F2  fmuls f10, f11, f31
	ctx.f[10].f64 = (((ctx.f[11].f64 * ctx.f[31].f64) as f32) as f64);
	// 832BFA50: ED2A002A  fadds f9, f10, f0
	ctx.f[9].f64 = ((ctx.f[10].f64 + ctx.f[0].f64) as f32) as f64;
	// 832BFA54: FF096800  fcmpu cr6, f9, f13
	ctx.cr[6].compare_f64(ctx.f[9].f64, ctx.f[13].f64);
	// 832BFA58: 7CE00026  mfcr r7
	// MFCR packs CR[0..7] (lt,gt,eq,so per field) into GPR
	ctx.r[7].u64 = if ctx.cr[0].lt { 0x80000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[0].gt { 0x40000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[0].eq { 0x20000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[0].so { 0x10000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[1].lt { 0x8000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[1].gt { 0x4000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[1].eq { 0x2000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[1].so { 0x1000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[2].lt { 0x800000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[2].gt { 0x400000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[2].eq { 0x200000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[2].so { 0x100000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[3].lt { 0x80000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[3].gt { 0x40000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[3].eq { 0x20000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[3].so { 0x10000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[4].lt { 0x8000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[4].gt { 0x4000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[4].eq { 0x2000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[4].so { 0x1000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[5].lt { 0x800 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[5].gt { 0x400 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[5].eq { 0x200 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[5].so { 0x100 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[6].lt { 0x80 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[6].gt { 0x40 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[6].eq { 0x20 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[6].so { 0x10 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[7].lt { 0x8 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[7].gt { 0x4 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[7].eq { 0x2 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[7].so { 0x1 } else { 0 };
	// 832BFA5C: 54E6DF7A  rlwinm r6, r7, 0x1b, 0x1d, 0x1d
	ctx.r[6].u64 = ctx.r[7].u32 as u64 & 0x0000001Fu64;
	// 832BFA60: 54E5F77A  rlwinm r5, r7, 0x1e, 0x1d, 0x1d
	ctx.r[5].u64 = ctx.r[7].u32 as u64 & 0x00000003u64;
	// 832BFA64: 7CC42B78  or r4, r6, r5
	ctx.r[4].u64 = ctx.r[6].u64 | ctx.r[5].u64;
	// 832BFA68: 7D08242E  lfsx f8, r8, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832BFA6C: FCE862AE  fsel f7, f8, f10, f12
	ctx.f[7].f64 = if ctx.f[8].f64 >= 0.0 { ctx.f[10].f64 } else { ctx.f[12].f64 };
	// 832BFA70: ECC70028  fsubs f6, f7, f0
	ctx.f[6].f64 = (((ctx.f[7].f64 - ctx.f[0].f64) as f32) as f64);
	// 832BFA74: FF066800  fcmpu cr6, f6, f13
	ctx.cr[6].compare_f64(ctx.f[6].f64, ctx.f[13].f64);
	// 832BFA78: 7C600026  mfcr r3
	// MFCR packs CR[0..7] (lt,gt,eq,so per field) into GPR
	ctx.r[3].u64 = if ctx.cr[0].lt { 0x80000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[0].gt { 0x40000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[0].eq { 0x20000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[0].so { 0x10000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[1].lt { 0x8000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[1].gt { 0x4000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[1].eq { 0x2000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[1].so { 0x1000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[2].lt { 0x800000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[2].gt { 0x400000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[2].eq { 0x200000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[2].so { 0x100000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[3].lt { 0x80000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[3].gt { 0x40000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[3].eq { 0x20000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[3].so { 0x10000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[4].lt { 0x8000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[4].gt { 0x4000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[4].eq { 0x2000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[4].so { 0x1000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[5].lt { 0x800 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[5].gt { 0x400 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[5].eq { 0x200 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[5].so { 0x100 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[6].lt { 0x80 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[6].gt { 0x40 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[6].eq { 0x20 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[6].so { 0x10 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[7].lt { 0x8 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[7].gt { 0x4 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[7].eq { 0x2 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[7].so { 0x1 } else { 0 };
	// 832BFA7C: 5467DF7A  rlwinm r7, r3, 0x1b, 0x1d, 0x1d
	ctx.r[7].u64 = ctx.r[3].u32 as u64 & 0x0000001Fu64;
	// 832BFA80: 5466F77A  rlwinm r6, r3, 0x1e, 0x1d, 0x1d
	ctx.r[6].u64 = ctx.r[3].u32 as u64 & 0x00000003u64;
	// 832BFA84: 7CE53378  or r5, r7, r6
	ctx.r[5].u64 = ctx.r[7].u64 | ctx.r[6].u64;
	// 832BFA88: 7CA82C2E  lfsx f5, r8, r5
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[5].u32)) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832BFA8C: FC85382E  fsel f4, f5, f0, f7
	ctx.f[4].f64 = if ctx.f[5].f64 >= 0.0 { ctx.f[0].f64 } else { ctx.f[7].f64 };
	// 832BFA90: FC60201E  fctiwz f3, f4
	ctx.f[3].s64 = if ctx.f[4].f64 > (i32::MAX as f64) { i32::MAX as i64 } else { ctx.f[4].f64.trunc() as i32 as i64 };
	// 832BFA94: 7C605FAE  stfiwx f3, 0, r11
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[11].u32, tmp.u32) };
	// 832BFA98: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832BFA9C: 4082FFA8  bne 0x832bfa44
	if !ctx.cr[0].eq {
	pc = 0x832BFA44; continue 'dispatch;
	}
	// 832BFAA0: 392100E0  addi r9, r1, 0xe0
	ctx.r[9].s64 = ctx.r[1].s64 + 224;
	// 832BFAA4: 7E88A378  mr r8, r20
	ctx.r[8].u64 = ctx.r[20].u64;
	// 832BFAA8: 7F6BDB78  mr r11, r27
	ctx.r[11].u64 = ctx.r[27].u64;
	// 832BFAAC: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832BFAB0: 419A00E0  beq cr6, 0x832bfb90
	if ctx.cr[6].eq {
	pc = 0x832BFB90; continue 'dispatch;
	}
	// 832BFAB4: 80E90000  lwz r7, 0(r9)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BFAB8: 356BFFFF  addic. r11, r11, -1
	ctx.xer.ca = (ctx.r[11].u32 > (!(-1 as u32)));
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BFABC: 39290004  addi r9, r9, 4
	ctx.r[9].s64 = ctx.r[9].s64 + 4;
	// 832BFAC0: B0E80000  sth r7, 0(r8)
	unsafe { crate::rt::store_u16( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[7].u16 ) };
	// 832BFAC4: 39080002  addi r8, r8, 2
	ctx.r[8].s64 = ctx.r[8].s64 + 2;
	// 832BFAC8: 4082FFEC  bne 0x832bfab4
	if !ctx.cr[0].eq {
	pc = 0x832BFAB4; continue 'dispatch;
	}
	// 832BFACC: 480000C4  b 0x832bfb90
	pc = 0x832BFB90; continue 'dispatch;
	// 832BFAD0: 5769083C  slwi r9, r27, 1
	// 832BFAD4: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832BFAD8: 419A0080  beq cr6, 0x832bfb58
	if ctx.cr[6].eq {
	pc = 0x832BFB58; continue 'dispatch;
	}
	// 832BFADC: 3CE0820A  lis r7, -0x7df6
	ctx.r[7].s64 = -2113273856;
	// 832BFAE0: 3CC08210  lis r6, -0x7df0
	ctx.r[6].s64 = -2112880640;
	// 832BFAE4: 3CA08210  lis r5, -0x7df0
	ctx.r[5].s64 = -2112880640;
	// 832BFAE8: 3D008210  lis r8, -0x7df0
	ctx.r[8].s64 = -2112880640;
	// 832BFAEC: C1A79484  lfs f13, -0x6b7c(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(-27516 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832BFAF0: 39080E68  addi r8, r8, 0xe68
	ctx.r[8].s64 = ctx.r[8].s64 + 3688;
	// 832BFAF4: C1860E74  lfs f12, 0xe74(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(3700 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832BFAF8: C0050E70  lfs f0, 0xe70(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(3696 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832BFAFC: C16B0000  lfs f11, 0(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832BFB00: 3529FFFF  addic. r9, r9, -1
	ctx.xer.ca = (ctx.r[9].u32 > (!(-1 as u32)));
	ctx.r[9].s64 = ctx.r[9].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832BFB04: ED5F02F2  fmuls f10, f31, f11
	ctx.f[10].f64 = (((ctx.f[31].f64 * ctx.f[11].f64) as f32) as f64);
	// 832BFB08: ED2A002A  fadds f9, f10, f0
	ctx.f[9].f64 = ((ctx.f[10].f64 + ctx.f[0].f64) as f32) as f64;
	// 832BFB0C: FF096800  fcmpu cr6, f9, f13
	ctx.cr[6].compare_f64(ctx.f[9].f64, ctx.f[13].f64);
	// 832BFB10: 7CE00026  mfcr r7
	// MFCR packs CR[0..7] (lt,gt,eq,so per field) into GPR
	ctx.r[7].u64 = if ctx.cr[0].lt { 0x80000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[0].gt { 0x40000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[0].eq { 0x20000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[0].so { 0x10000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[1].lt { 0x8000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[1].gt { 0x4000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[1].eq { 0x2000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[1].so { 0x1000000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[2].lt { 0x800000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[2].gt { 0x400000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[2].eq { 0x200000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[2].so { 0x100000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[3].lt { 0x80000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[3].gt { 0x40000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[3].eq { 0x20000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[3].so { 0x10000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[4].lt { 0x8000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[4].gt { 0x4000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[4].eq { 0x2000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[4].so { 0x1000 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[5].lt { 0x800 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[5].gt { 0x400 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[5].eq { 0x200 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[5].so { 0x100 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[6].lt { 0x80 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[6].gt { 0x40 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[6].eq { 0x20 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[6].so { 0x10 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[7].lt { 0x8 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[7].gt { 0x4 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[7].eq { 0x2 } else { 0 };
	ctx.r[7].u64 |= if ctx.cr[7].so { 0x1 } else { 0 };
	// 832BFB14: 54E6DF7A  rlwinm r6, r7, 0x1b, 0x1d, 0x1d
	ctx.r[6].u64 = ctx.r[7].u32 as u64 & 0x0000001Fu64;
	// 832BFB18: 54E5F77A  rlwinm r5, r7, 0x1e, 0x1d, 0x1d
	ctx.r[5].u64 = ctx.r[7].u32 as u64 & 0x00000003u64;
	// 832BFB1C: 7CC42B78  or r4, r6, r5
	ctx.r[4].u64 = ctx.r[6].u64 | ctx.r[5].u64;
	// 832BFB20: 7D08242E  lfsx f8, r8, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832BFB24: FCE862AE  fsel f7, f8, f10, f12
	ctx.f[7].f64 = if ctx.f[8].f64 >= 0.0 { ctx.f[10].f64 } else { ctx.f[12].f64 };
	// 832BFB28: ECC70028  fsubs f6, f7, f0
	ctx.f[6].f64 = (((ctx.f[7].f64 - ctx.f[0].f64) as f32) as f64);
	// 832BFB2C: FF066800  fcmpu cr6, f6, f13
	ctx.cr[6].compare_f64(ctx.f[6].f64, ctx.f[13].f64);
	// 832BFB30: 7C600026  mfcr r3
	// MFCR packs CR[0..7] (lt,gt,eq,so per field) into GPR
	ctx.r[3].u64 = if ctx.cr[0].lt { 0x80000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[0].gt { 0x40000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[0].eq { 0x20000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[0].so { 0x10000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[1].lt { 0x8000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[1].gt { 0x4000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[1].eq { 0x2000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[1].so { 0x1000000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[2].lt { 0x800000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[2].gt { 0x400000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[2].eq { 0x200000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[2].so { 0x100000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[3].lt { 0x80000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[3].gt { 0x40000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[3].eq { 0x20000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[3].so { 0x10000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[4].lt { 0x8000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[4].gt { 0x4000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[4].eq { 0x2000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[4].so { 0x1000 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[5].lt { 0x800 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[5].gt { 0x400 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[5].eq { 0x200 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[5].so { 0x100 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[6].lt { 0x80 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[6].gt { 0x40 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[6].eq { 0x20 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[6].so { 0x10 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[7].lt { 0x8 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[7].gt { 0x4 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[7].eq { 0x2 } else { 0 };
	ctx.r[3].u64 |= if ctx.cr[7].so { 0x1 } else { 0 };
	// 832BFB34: 5467DF7A  rlwinm r7, r3, 0x1b, 0x1d, 0x1d
	ctx.r[7].u64 = ctx.r[3].u32 as u64 & 0x0000001Fu64;
	// 832BFB38: 5466F77A  rlwinm r6, r3, 0x1e, 0x1d, 0x1d
	ctx.r[6].u64 = ctx.r[3].u32 as u64 & 0x00000003u64;
	// 832BFB3C: 7CE53378  or r5, r7, r6
	ctx.r[5].u64 = ctx.r[7].u64 | ctx.r[6].u64;
	// 832BFB40: 7CA82C2E  lfsx f5, r8, r5
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[5].u32)) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832BFB44: FC85382E  fsel f4, f5, f0, f7
	ctx.f[4].f64 = if ctx.f[5].f64 >= 0.0 { ctx.f[0].f64 } else { ctx.f[7].f64 };
	// 832BFB48: FC60201E  fctiwz f3, f4
	ctx.f[3].s64 = if ctx.f[4].f64 > (i32::MAX as f64) { i32::MAX as i64 } else { ctx.f[4].f64.trunc() as i32 as i64 };
	// 832BFB4C: 7C605FAE  stfiwx f3, 0, r11
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[11].u32, tmp.u32) };
	// 832BFB50: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832BFB54: 4082FFA8  bne 0x832bfafc
	if !ctx.cr[0].eq {
	pc = 0x832BFAFC; continue 'dispatch;
	}
	// 832BFB58: 396100E0  addi r11, r1, 0xe0
	ctx.r[11].s64 = ctx.r[1].s64 + 224;
	// 832BFB5C: 7F69DB78  mr r9, r27
	ctx.r[9].u64 = ctx.r[27].u64;
	// 832BFB60: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832BFB64: 419A002C  beq cr6, 0x832bfb90
	if ctx.cr[6].eq {
	pc = 0x832BFB90; continue 'dispatch;
	}
	// 832BFB68: 38E100E0  addi r7, r1, 0xe0
	ctx.r[7].s64 = ctx.r[1].s64 + 224;
	// 832BFB6C: 5768103A  slwi r8, r27, 2
	// 832BFB70: 7CE7A050  subf r7, r7, r20
	ctx.r[7].s64 = ctx.r[20].s64 - ctx.r[7].s64;
	// 832BFB74: 7CC8582E  lwzx r6, r8, r11
	ctx.r[6].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[11].u32)) } as u64;
	// 832BFB78: 3529FFFF  addic. r9, r9, -1
	ctx.xer.ca = (ctx.r[9].u32 > (!(-1 as u32)));
	ctx.r[9].s64 = ctx.r[9].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832BFB7C: 80AB0000  lwz r5, 0(r11)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BFB80: 50A6801E  rlwimi r6, r5, 0x10, 0, 0xf
	ctx.r[6].u64 = (((ctx.r[5].u32).rotate_left(16) as u64) & 0x00000000FFFF0000) | (ctx.r[6].u64 & 0xFFFFFFFF0000FFFF);
	// 832BFB84: 7CC7592E  stwx r6, r7, r11
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[7].u32.wrapping_add(ctx.r[11].u32), ctx.r[6].u32) };
	// 832BFB88: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832BFB8C: 4082FFE8  bne 0x832bfb74
	if !ctx.cr[0].eq {
	pc = 0x832BFB74; continue 'dispatch;
	}
	// 832BFB90: 8161005C  lwz r11, 0x5c(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(92 as u32) ) } as u64;
	// 832BFB94: 7D4B5050  subf r10, r11, r10
	ctx.r[10].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832BFB98: 554300FE  clrlwi r3, r10, 3
	ctx.r[3].u64 = ctx.r[10].u32 as u64 & 0x1FFFFFFFu64;
	// 832BFB9C: 38214150  addi r1, r1, 0x4150
	ctx.r[1].s64 = ctx.r[1].s64 + 16720;
	// 832BFBA0: CBE1FF90  lfd f31, -0x70(r1)
	ctx.f[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-112 as u32) ) };
	// 832BFBA4: 4B9E9894  b 0x82ca9438
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9438);
	return;
}

pub fn sub_832BFBA8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    let mut ea: u32 = 0;
    // ---- function 0x832BFBA8 size=800
	// 832BFBA8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BFBAC: 4B9E9835  bl 0x82ca93e0
	ctx.lr = 0x832BFBB0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93E0);
	// 832BFBB0: DBA1FF70  stfd f29, -0x90(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-144 as u32), ctx.f[29].u64 ) };
	// 832BFBB4: DBC1FF78  stfd f30, -0x88(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-136 as u32), ctx.f[30].u64 ) };
	// 832BFBB8: DBE1FF80  stfd f31, -0x80(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-128 as u32), ctx.f[31].u64 ) };
	// 832BFBBC: 9421FF10  stwu r1, -0xf0(r1)
	ea = ctx.r[1].u32.wrapping_add(-240 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BFBC0: 7C952378  mr r21, r4
	ctx.r[21].u64 = ctx.r[4].u64;
	// 832BFBC4: 7CB32B78  mr r19, r5
	ctx.r[19].u64 = ctx.r[5].u64;
	// 832BFBC8: 2B03AC44  cmplwi cr6, r3, 0xac44
	ctx.cr[6].compare_u32(ctx.r[3].u32, 44100 as u32, &mut ctx.xer);
	// 832BFBCC: 4198000C  blt cr6, 0x832bfbd8
	if ctx.cr[6].lt {
	pc = 0x832BFBD8; continue 'dispatch;
	}
	// 832BFBD0: 3B400800  li r26, 0x800
	ctx.r[26].s64 = 2048;
	// 832BFBD4: 48000018  b 0x832bfbec
	pc = 0x832BFBEC; continue 'dispatch;
	// 832BFBD8: 39605622  li r11, 0x5622
	ctx.r[11].s64 = 22050;
	// 832BFBDC: 7D4B1810  subfc r10, r11, r3
	ctx.xer.ca = ctx.r[3].u32 >= ctx.r[11].u32;
	ctx.r[10].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832BFBE0: 7D2A5110  subfe r9, r10, r10
	let x = (!ctx.r[10].u32);
	let y = ctx.r[10].u32;
	let s = x.wrapping_add(y);
	let res = s.wrapping_add(ctx.xer.ca as u32);
	tmp.u8 = (s < x) as u8 | (res < s) as u8;
	ctx.r[9].u32 = res;
	ctx.r[9].u64 = ctx.r[9].u32 as u64;
	ctx.xer.ca = (tmp.u8 != 0);
	// 832BFBE4: 552B002C  rlwinm r11, r9, 0, 0, 0x16
	ctx.r[11].u64 = ctx.r[9].u32 as u64 & 0xFFFFFFFFu64;
	// 832BFBE8: 3B4B0400  addi r26, r11, 0x400
	ctx.r[26].s64 = ctx.r[11].s64 + 1024;
	// 832BFBEC: 7D7AA9D6  mullw r11, r26, r21
	ctx.r[11].s64 = (ctx.r[26].s32 as i64) * (ctx.r[21].s32 as i64);
	// 832BFBF0: 566A07FE  clrlwi r10, r19, 0x1f
	ctx.r[10].u64 = ctx.r[19].u32 as u64 & 0x00000001u64;
	// 832BFBF4: 3A400001  li r18, 1
	ctx.r[18].s64 = 1;
	// 832BFBF8: 5578083C  slwi r24, r11, 1
	// 832BFBFC: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BFC00: 409A0010  bne cr6, 0x832bfc10
	if !ctx.cr[6].eq {
	pc = 0x832BFC10; continue 'dispatch;
	}
	// 832BFC04: 7C63A9D6  mullw r3, r3, r21
	ctx.r[3].s64 = (ctx.r[3].s32 as i64) * (ctx.r[21].s32 as i64);
	// 832BFC08: 7D7A5B78  mr r26, r11
	ctx.r[26].u64 = ctx.r[11].u64;
	// 832BFC0C: 7E559378  mr r21, r18
	ctx.r[21].u64 = ctx.r[18].u64;
	// 832BFC10: 3D608350  lis r11, -0x7cb0
	ctx.r[11].s64 = -2091909120;
	// 832BFC14: 39230001  addi r9, r3, 1
	ctx.r[9].s64 = ctx.r[3].s64 + 1;
	// 832BFC18: 3A8BC220  addi r20, r11, -0x3de0
	ctx.r[20].s64 = ctx.r[11].s64 + -15840;
	// 832BFC1C: 5756F87E  srwi r22, r26, 1
	// 832BFC20: 5537F87E  srwi r23, r9, 1
	// 832BFC24: 3B600000  li r27, 0
	ctx.r[27].s64 = 0;
	// 832BFC28: 39740004  addi r11, r20, 4
	ctx.r[11].s64 = ctx.r[20].s64 + 4;
	// 832BFC2C: 812BFFFC  lwz r9, -4(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-4 as u32) ) } as u64;
	// 832BFC30: 7F09B840  cmplw cr6, r9, r23
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BFC34: 40980064  bge cr6, 0x832bfc98
	if !ctx.cr[6].lt {
	pc = 0x832BFC98; continue 'dispatch;
	}
	// 832BFC38: 812B0000  lwz r9, 0(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BFC3C: 7F09B840  cmplw cr6, r9, r23
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BFC40: 4098003C  bge cr6, 0x832bfc7c
	if !ctx.cr[6].lt {
	pc = 0x832BFC7C; continue 'dispatch;
	}
	// 832BFC44: 812B0004  lwz r9, 4(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832BFC48: 7F09B840  cmplw cr6, r9, r23
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BFC4C: 40980038  bge cr6, 0x832bfc84
	if !ctx.cr[6].lt {
	pc = 0x832BFC84; continue 'dispatch;
	}
	// 832BFC50: 812B0008  lwz r9, 8(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BFC54: 7F09B840  cmplw cr6, r9, r23
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BFC58: 40980034  bge cr6, 0x832bfc8c
	if !ctx.cr[6].lt {
	pc = 0x832BFC8C; continue 'dispatch;
	}
	// 832BFC5C: 812B000C  lwz r9, 0xc(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BFC60: 7F09B840  cmplw cr6, r9, r23
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832BFC64: 40980030  bge cr6, 0x832bfc94
	if !ctx.cr[6].lt {
	pc = 0x832BFC94; continue 'dispatch;
	}
	// 832BFC68: 3B7B0005  addi r27, r27, 5
	ctx.r[27].s64 = ctx.r[27].s64 + 5;
	// 832BFC6C: 396B0014  addi r11, r11, 0x14
	ctx.r[11].s64 = ctx.r[11].s64 + 20;
	// 832BFC70: 2B1B0019  cmplwi cr6, r27, 0x19
	ctx.cr[6].compare_u32(ctx.r[27].u32, 25 as u32, &mut ctx.xer);
	// 832BFC74: 4198FFB8  blt cr6, 0x832bfc2c
	if ctx.cr[6].lt {
	pc = 0x832BFC2C; continue 'dispatch;
	}
	// 832BFC78: 48000020  b 0x832bfc98
	pc = 0x832BFC98; continue 'dispatch;
	// 832BFC7C: 3B7B0001  addi r27, r27, 1
	ctx.r[27].s64 = ctx.r[27].s64 + 1;
	// 832BFC80: 48000018  b 0x832bfc98
	pc = 0x832BFC98; continue 'dispatch;
	// 832BFC84: 3B7B0002  addi r27, r27, 2
	ctx.r[27].s64 = ctx.r[27].s64 + 2;
	// 832BFC88: 48000010  b 0x832bfc98
	pc = 0x832BFC98; continue 'dispatch;
	// 832BFC8C: 3B7B0003  addi r27, r27, 3
	ctx.r[27].s64 = ctx.r[27].s64 + 3;
	// 832BFC90: 48000008  b 0x832bfc98
	pc = 0x832BFC98; continue 'dispatch;
	// 832BFC94: 3B7B0004  addi r27, r27, 4
	ctx.r[27].s64 = ctx.r[27].s64 + 4;
	// 832BFC98: 7AC90020  clrldi r9, r22, 0x20
	ctx.r[9].u64 = ctx.r[22].u64 & 0x00000000FFFFFFFFu64;
	// 832BFC9C: 3D60820A  lis r11, -0x7df6
	ctx.r[11].s64 = -2113273856;
	// 832BFCA0: F9210050  std r9, 0x50(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[9].u64 ) };
	// 832BFCA4: C8010050  lfd f0, 0x50(r1)
	ctx.f[0].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832BFCA8: FDA0069C  fcfid f13, f0
	ctx.f[13].f64 = (ctx.f[0].s64 as f64);
	// 832BFCAC: 3B2BB480  addi r25, r11, -0x4b80
	ctx.r[25].s64 = ctx.r[11].s64 + -19328;
	// 832BFCB0: FC006818  frsp f0, f13
	ctx.f[0].f64 = (ctx.f[13].f64 as f32) as f64;
	// 832BFCB4: 570BF87E  srwi r11, r24, 1
	// 832BFCB8: 3D008200  lis r8, -0x7e00
	ctx.r[8].s64 = -2113929216;
	// 832BFCBC: C3B9E004  lfs f29, -0x1ffc(r25)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[25].u32.wrapping_add(-8188 as u32) ) };
	ctx.f[29].f64 = (tmp.f32 as f64);
	// 832BFCC0: 3CE08200  lis r7, -0x7e00
	ctx.r[7].s64 = -2113929216;
	// 832BFCC4: 38CB00AF  addi r6, r11, 0xaf
	ctx.r[6].s64 = ctx.r[11].s64 + 175;
	// 832BFCC8: 54DC0036  rlwinm r28, r6, 0, 0, 0x1b
	ctx.r[28].u64 = ctx.r[6].u32 as u64 & 0xFFFFFFFFu64;
	// 832BFCCC: CBC80D30  lfd f30, 0xd30(r8)
	ctx.f[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[8].u32.wrapping_add(3376 as u32) ) };
	// 832BFCD0: CBE70DF0  lfd f31, 0xdf0(r7)
	ctx.f[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[7].u32.wrapping_add(3568 as u32) ) };
	// 832BFCD4: FF00E800  fcmpu cr6, f0, f29
	ctx.cr[6].compare_f64(ctx.f[0].f64, ctx.f[29].f64);
	// 832BFCD8: 40990040  ble cr6, 0x832bfd18
	if !ctx.cr[6].gt {
	pc = 0x832BFD18; continue 'dispatch;
	}
	// 832BFCDC: FDA00034  frsqrte f13, f0
	// 832BFCE0: FD800372  fmul f12, f0, f13
	ctx.f[12].f64 = ctx.f[0].f64 * ctx.f[13].f64;
	// 832BFCE4: FD6CFB7C  fnmsub f11, f12, f13, f31
	ctx.f[11].f64 = -(ctx.f[12].f64 * ctx.f[13].f64 - ctx.f[31].f64);
	// 832BFCE8: FD4B0372  fmul f10, f11, f13
	ctx.f[10].f64 = ctx.f[11].f64 * ctx.f[13].f64;
	// 832BFCEC: FD2A07B2  fmul f9, f10, f30
	ctx.f[9].f64 = ctx.f[10].f64 * ctx.f[30].f64;
	// 832BFCF0: FD000272  fmul f8, f0, f9
	ctx.f[8].f64 = ctx.f[0].f64 * ctx.f[9].f64;
	// 832BFCF4: FCE8FA7C  fnmsub f7, f8, f9, f31
	ctx.f[7].f64 = -(ctx.f[8].f64 * ctx.f[9].f64 - ctx.f[31].f64);
	// 832BFCF8: FCC70272  fmul f6, f7, f9
	ctx.f[6].f64 = ctx.f[7].f64 * ctx.f[9].f64;
	// 832BFCFC: FCA607B2  fmul f5, f6, f30
	ctx.f[5].f64 = ctx.f[6].f64 * ctx.f[30].f64;
	// 832BFD00: FC800172  fmul f4, f0, f5
	ctx.f[4].f64 = ctx.f[0].f64 * ctx.f[5].f64;
	// 832BFD04: FC64F97C  fnmsub f3, f4, f5, f31
	ctx.f[3].f64 = -(ctx.f[4].f64 * ctx.f[5].f64 - ctx.f[31].f64);
	// 832BFD08: FC430172  fmul f2, f3, f5
	ctx.f[2].f64 = ctx.f[3].f64 * ctx.f[5].f64;
	// 832BFD0C: FC2207B2  fmul f1, f2, f30
	ctx.f[1].f64 = ctx.f[2].f64 * ctx.f[30].f64;
	// 832BFD10: FC010032  fmul f0, f1, f0
	ctx.f[0].f64 = ctx.f[1].f64 * ctx.f[0].f64;
	// 832BFD14: FC000018  frsp f0, f0
	ctx.f[0].f64 = (ctx.f[0].f64 as f32) as f64;
	// 832BFD18: FC00001E  fctiwz f0, f0
	ctx.f[0].s64 = if ctx.f[0].f64 > (i32::MAX as f64) { i32::MAX as i64 } else { ctx.f[0].f64.trunc() as i32 as i64 };
	// 832BFD1C: D8010050  stfd f0, 0x50(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.f[0].u64 ) };
	// 832BFD20: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832BFD24: 81610054  lwz r11, 0x54(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(84 as u32) ) } as u64;
	// 832BFD28: 556B103A  slwi r11, r11, 2
	// 832BFD2C: 7D6BE214  add r11, r11, r28
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[28].u64;
	// 832BFD30: 394B0017  addi r10, r11, 0x17
	ctx.r[10].s64 = ctx.r[11].s64 + 23;
	// 832BFD34: 555D0036  rlwinm r29, r10, 0, 0, 0x1b
	ctx.r[29].u64 = ctx.r[10].u32 as u64 & 0xFFFFFFFFu64;
	// 832BFD38: 419A0020  beq cr6, 0x832bfd58
	if ctx.cr[6].eq {
	pc = 0x832BFD58; continue 'dispatch;
	}
	// 832BFD3C: 574B103A  slwi r11, r26, 2
	// 832BFD40: 7D7A5A14  add r11, r26, r11
	ctx.r[11].u64 = ctx.r[26].u64 + ctx.r[11].u64;
	// 832BFD44: 556B00BE  clrlwi r11, r11, 2
	ctx.r[11].u64 = ctx.r[11].u32 as u64 & 0x3FFFFFFFu64;
	// 832BFD48: 7D6BEA14  add r11, r11, r29
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832BFD4C: 394B000F  addi r10, r11, 0xf
	ctx.r[10].s64 = ctx.r[11].s64 + 15;
	// 832BFD50: 555E0036  rlwinm r30, r10, 0, 0, 0x1b
	ctx.r[30].u64 = ctx.r[10].u32 as u64 & 0xFFFFFFFFu64;
	// 832BFD54: 48000014  b 0x832bfd68
	pc = 0x832BFD68; continue 'dispatch;
	// 832BFD58: 56CB103A  slwi r11, r22, 2
	// 832BFD5C: 7D6BEA14  add r11, r11, r29
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832BFD60: 396B000F  addi r11, r11, 0xf
	ctx.r[11].s64 = ctx.r[11].s64 + 15;
	// 832BFD64: 557E0036  rlwinm r30, r11, 0, 0, 0x1b
	ctx.r[30].u64 = ctx.r[11].u32 as u64 & 0xFFFFFFFFu64;
	// 832BFD68: 7D7EC214  add r11, r30, r24
	ctx.r[11].u64 = ctx.r[30].u64 + ctx.r[24].u64;
	// 832BFD6C: 396B000F  addi r11, r11, 0xf
	ctx.r[11].s64 = ctx.r[11].s64 + 15;
	// 832BFD70: 55630036  rlwinm r3, r11, 0, 0, 0x1b
	ctx.r[3].u64 = ctx.r[11].u32 as u64 & 0xFFFFFFFFu64;
	// 832BFD74: 4B92A01D  bl 0x82be9d90
	ctx.lr = 0x832BFD78;
	crate::recompiler::externs::call(&mut ctx, base, 0x82BE9D90);
	// 832BFD78: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BFD7C: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832BFD80: 409A0018  bne cr6, 0x832bfd98
	if !ctx.cr[6].eq {
	pc = 0x832BFD98; continue 'dispatch;
	}
	// 832BFD84: 382100F0  addi r1, r1, 0xf0
	ctx.r[1].s64 = ctx.r[1].s64 + 240;
	// 832BFD88: CBA1FF70  lfd f29, -0x90(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[29].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-144 as u32) ) };
	// 832BFD8C: CBC1FF78  lfd f30, -0x88(r1)
	ctx.f[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-136 as u32) ) };
	// 832BFD90: CBE1FF80  lfd f31, -0x80(r1)
	ctx.f[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-128 as u32) ) };
	// 832BFD94: 4B9E969C  b 0x82ca9430
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9430);
	return;
	// 832BFD98: 38A00098  li r5, 0x98
	ctx.r[5].s64 = 152;
	// 832BFD9C: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832BFDA0: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BFDA4: 4B9E9C0D  bl 0x82ca99b0
	ctx.lr = 0x832BFDA8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832BFDA8: 7B4B0020  clrldi r11, r26, 0x20
	ctx.r[11].u64 = ctx.r[26].u64 & 0x00000000FFFFFFFFu64;
	// 832BFDAC: 395F00A0  addi r10, r31, 0xa0
	ctx.r[10].s64 = ctx.r[31].s64 + 160;
	// 832BFDB0: 93DF0030  stw r30, 0x30(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(48 as u32), ctx.r[30].u32 ) };
	// 832BFDB4: F9610050  std r11, 0x50(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[11].u64 ) };
	// 832BFDB8: C8010050  lfd f0, 0x50(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[0].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832BFDBC: FDA0069C  fcfid f13, f0
	ctx.f[13].f64 = (ctx.f[0].s64 as f64);
	// 832BFDC0: 7D3FE214  add r9, r31, r28
	ctx.r[9].u64 = ctx.r[31].u64 + ctx.r[28].u64;
	// 832BFDC4: FC006818  frsp f0, f13
	ctx.f[0].f64 = (ctx.f[13].f64 as f32) as f64;
	// 832BFDC8: 7D1FEA14  add r8, r31, r29
	ctx.r[8].u64 = ctx.r[31].u64 + ctx.r[29].u64;
	// 832BFDCC: 7CFFF214  add r7, r31, r30
	ctx.r[7].u64 = ctx.r[31].u64 + ctx.r[30].u64;
	// 832BFDD0: 915F0020  stw r10, 0x20(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(32 as u32), ctx.r[10].u32 ) };
	// 832BFDD4: 5706E13E  srwi r6, r24, 4
	// 832BFDD8: 913F0024  stw r9, 0x24(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(36 as u32), ctx.r[9].u32 ) };
	// 832BFDDC: 911F0028  stw r8, 0x28(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(40 as u32), ctx.r[8].u32 ) };
	// 832BFDE0: 90FF001C  stw r7, 0x1c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(28 as u32), ctx.r[7].u32 ) };
	// 832BFDE4: 927F002C  stw r19, 0x2c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(44 as u32), ctx.r[19].u32 ) };
	// 832BFDE8: 92BF0010  stw r21, 0x10(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(16 as u32), ctx.r[21].u32 ) };
	// 832BFDEC: 937F0018  stw r27, 0x18(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(24 as u32), ctx.r[27].u32 ) };
	// 832BFDF0: 935F0000  stw r26, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[26].u32 ) };
	// 832BFDF4: 931F0008  stw r24, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[24].u32 ) };
	// 832BFDF8: 90DF000C  stw r6, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[6].u32 ) };
	// 832BFDFC: FF00E800  fcmpu cr6, f0, f29
	ctx.cr[6].compare_f64(ctx.f[0].f64, ctx.f[29].f64);
	// 832BFE00: 40990040  ble cr6, 0x832bfe40
	if !ctx.cr[6].gt {
	pc = 0x832BFE40; continue 'dispatch;
	}
	// 832BFE04: FDA00034  frsqrte f13, f0
	// 832BFE08: FD800372  fmul f12, f0, f13
	ctx.f[12].f64 = ctx.f[0].f64 * ctx.f[13].f64;
	// 832BFE0C: FD6CFB7C  fnmsub f11, f12, f13, f31
	ctx.f[11].f64 = -(ctx.f[12].f64 * ctx.f[13].f64 - ctx.f[31].f64);
	// 832BFE10: FD4B0372  fmul f10, f11, f13
	ctx.f[10].f64 = ctx.f[11].f64 * ctx.f[13].f64;
	// 832BFE14: FD2A07B2  fmul f9, f10, f30
	ctx.f[9].f64 = ctx.f[10].f64 * ctx.f[30].f64;
	// 832BFE18: FD000272  fmul f8, f0, f9
	ctx.f[8].f64 = ctx.f[0].f64 * ctx.f[9].f64;
	// 832BFE1C: FCE8FA7C  fnmsub f7, f8, f9, f31
	ctx.f[7].f64 = -(ctx.f[8].f64 * ctx.f[9].f64 - ctx.f[31].f64);
	// 832BFE20: FCC70272  fmul f6, f7, f9
	ctx.f[6].f64 = ctx.f[7].f64 * ctx.f[9].f64;
	// 832BFE24: FCA607B2  fmul f5, f6, f30
	ctx.f[5].f64 = ctx.f[6].f64 * ctx.f[30].f64;
	// 832BFE28: FC800172  fmul f4, f0, f5
	ctx.f[4].f64 = ctx.f[0].f64 * ctx.f[5].f64;
	// 832BFE2C: FC64F97C  fnmsub f3, f4, f5, f31
	ctx.f[3].f64 = -(ctx.f[4].f64 * ctx.f[5].f64 - ctx.f[31].f64);
	// 832BFE30: FC430172  fmul f2, f3, f5
	ctx.f[2].f64 = ctx.f[3].f64 * ctx.f[5].f64;
	// 832BFE34: FC2207B2  fmul f1, f2, f30
	ctx.f[1].f64 = ctx.f[2].f64 * ctx.f[30].f64;
	// 832BFE38: FC010032  fmul f0, f1, f0
	ctx.f[0].f64 = ctx.f[1].f64 * ctx.f[0].f64;
	// 832BFE3C: FC000018  frsp f0, f0
	ctx.f[0].f64 = (ctx.f[0].f64 as f32) as f64;
	// 832BFE40: C1B90000  lfs f13, 0(r25)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[25].u32.wrapping_add(0 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832BFE44: 39000000  li r8, 0
	ctx.r[8].s64 = 0;
	// 832BFE48: EC0D0024  fdivs f0, f13, f0
	ctx.f[0].f64 = ((ctx.f[13].f64 / ctx.f[0].f64) as f32) as f64;
	// 832BFE4C: D01F0004  stfs f0, 4(r31)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832BFE50: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832BFE54: 419A003C  beq cr6, 0x832bfe90
	if ctx.cr[6].eq {
	pc = 0x832BFE90; continue 'dispatch;
	}
	// 832BFE58: 7E8AA378  mr r10, r20
	ctx.r[10].u64 = ctx.r[20].u64;
	// 832BFE5C: 397F0034  addi r11, r31, 0x34
	ctx.r[11].s64 = ctx.r[31].s64 + 52;
	// 832BFE60: 7F69DB78  mr r9, r27
	ctx.r[9].u64 = ctx.r[27].u64;
	// 832BFE64: 7F68DB78  mr r8, r27
	ctx.r[8].u64 = ctx.r[27].u64;
	// 832BFE68: 80EA0000  lwz r7, 0(r10)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BFE6C: 7CD639D6  mullw r6, r22, r7
	ctx.r[6].s64 = (ctx.r[22].s32 as i64) * (ctx.r[7].s32 as i64);
	// 832BFE70: 7CA6BB97  divwu. r5, r6, r23
	ctx.r[5].u32 = ctx.r[6].u32 / ctx.r[23].u32;
	ctx.cr[0].compare_i32(ctx.r[5].s32, 0, &mut ctx.xer);
	// 832BFE74: 90AB0000  stw r5, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[5].u32 ) };
	// 832BFE78: 40820008  bne 0x832bfe80
	if !ctx.cr[0].eq {
	pc = 0x832BFE80; continue 'dispatch;
	}
	// 832BFE7C: 924B0000  stw r18, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[18].u32 ) };
	// 832BFE80: 3529FFFF  addic. r9, r9, -1
	ctx.xer.ca = (ctx.r[9].u32 > (!(-1 as u32)));
	ctx.r[9].s64 = ctx.r[9].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832BFE84: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832BFE88: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832BFE8C: 4082FFDC  bne 0x832bfe68
	if !ctx.cr[0].eq {
	pc = 0x832BFE68; continue 'dispatch;
	}
	// 832BFE90: 3968000D  addi r11, r8, 0xd
	ctx.r[11].s64 = ctx.r[8].s64 + 13;
	// 832BFE94: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832BFE98: 5569103A  slwi r9, r11, 2
	// 832BFE9C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832BFEA0: 7EC9F92E  stwx r22, r9, r31
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[31].u32), ctx.r[22].u32) };
	// 832BFEA4: 811F0024  lwz r8, 0x24(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(36 as u32) ) } as u64;
	// 832BFEA8: 91480000  stw r10, 0(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832BFEAC: 925F0014  stw r18, 0x14(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(20 as u32), ctx.r[18].u32 ) };
	// 832BFEB0: 382100F0  addi r1, r1, 0xf0
	ctx.r[1].s64 = ctx.r[1].s64 + 240;
	// 832BFEB4: CBA1FF70  lfd f29, -0x90(r1)
	ctx.f[29].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-144 as u32) ) };
	// 832BFEB8: CBC1FF78  lfd f30, -0x88(r1)
	ctx.f[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-136 as u32) ) };
	// 832BFEBC: CBE1FF80  lfd f31, -0x80(r1)
	ctx.f[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-128 as u32) ) };
	// 832BFEC0: 4B9E9570  b 0x82ca9430
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9430);
	return;
}

pub fn sub_832BFEC8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    let mut ea: u32 = 0;
    // ---- function 0x832BFEC8 size=304
	// 832BFEC8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BFECC: 4B9E9535  bl 0x82ca9400
	ctx.lr = 0x832BFED0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9400);
	// 832BFED0: 9421FF60  stwu r1, -0xa0(r1)
	ea = ctx.r[1].u32.wrapping_add(-160 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832BFED4: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832BFED8: 7C9C2378  mr r28, r4
	ctx.r[28].u64 = ctx.r[4].u64;
	// 832BFEDC: 7CDD3378  mr r29, r6
	ctx.r[29].u64 = ctx.r[6].u64;
	// 832BFEE0: 7D0B4378  mr r11, r8
	ctx.r[11].u64 = ctx.r[8].u64;
	// 832BFEE4: 3B5F0034  addi r26, r31, 0x34
	ctx.r[26].s64 = ctx.r[31].s64 + 52;
	// 832BFEE8: 809F0018  lwz r4, 0x18(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) } as u64;
	// 832BFEEC: 7FAAEB78  mr r10, r29
	ctx.r[10].u64 = ctx.r[29].u64;
	// 832BFEF0: 7CBE2B78  mr r30, r5
	ctx.r[30].u64 = ctx.r[5].u64;
	// 832BFEF4: 811F0028  lwz r8, 0x28(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(40 as u32) ) } as u64;
	// 832BFEF8: 7CFB3B78  mr r27, r7
	ctx.r[27].u64 = ctx.r[7].u64;
	// 832BFEFC: 80FF0024  lwz r7, 0x24(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(36 as u32) ) } as u64;
	// 832BFF00: C03F0004  lfs f1, 4(r31)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832BFF04: 80DF002C  lwz r6, 0x2c(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(44 as u32) ) } as u64;
	// 832BFF08: 80BF0010  lwz r5, 0x10(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(16 as u32) ) } as u64;
	// 832BFF0C: 813F001C  lwz r9, 0x1c(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(28 as u32) ) } as u64;
	// 832BFF10: 807F0000  lwz r3, 0(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BFF14: 93410064  stw r26, 0x64(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(100 as u32), ctx.r[26].u32 ) };
	// 832BFF18: 9081005C  stw r4, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[4].u32 ) };
	// 832BFF1C: 91610054  stw r11, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[11].u32 ) };
	// 832BFF20: 4BFFF6A1  bl 0x832bf5c0
	ctx.lr = 0x832BFF24;
	crate::recompiler::externs::call(&mut ctx, base, 0x832BF5C0);
	// 832BFF24: 817F0014  lwz r11, 0x14(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832BFF28: 7C7A1B78  mr r26, r3
	ctx.r[26].u64 = ctx.r[3].u64;
	// 832BFF2C: 2F0B0000  cmpwi cr6, r11, 0
	ctx.cr[6].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832BFF30: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832BFF34: 419A000C  beq cr6, 0x832bff40
	if ctx.cr[6].eq {
	pc = 0x832BFF40; continue 'dispatch;
	}
	// 832BFF38: 917F0014  stw r11, 0x14(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(20 as u32), ctx.r[11].u32 ) };
	// 832BFF3C: 4800005C  b 0x832bff98
	pc = 0x832BFF98; continue 'dispatch;
	// 832BFF40: 815F000C  lwz r10, 0xc(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BFF44: 5549F87E  srwi r9, r10, 1
	// 832BFF48: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832BFF4C: 419A004C  beq cr6, 0x832bff98
	if ctx.cr[6].eq {
	pc = 0x832BFF98; continue 'dispatch;
	}
	// 832BFF50: 39400000  li r10, 0
	ctx.r[10].s64 = 0;
	// 832BFF54: 811F001C  lwz r8, 0x1c(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(28 as u32) ) } as u64;
	// 832BFF58: 7CEB4850  subf r7, r11, r9
	ctx.r[7].s64 = ctx.r[9].s64 - ctx.r[11].s64;
	// 832BFF5C: 80DF0020  lwz r6, 0x20(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BFF60: 7D0A4214  add r8, r10, r8
	ctx.r[8].u64 = ctx.r[10].u64 + ctx.r[8].u64;
	// 832BFF64: 7CAA322E  lhzx r5, r10, r6
	ctx.r[5].u64 = unsafe { crate::rt::load_u16(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[6].u32)) } as u64;
	// 832BFF68: 394A0002  addi r10, r10, 2
	ctx.r[10].s64 = ctx.r[10].s64 + 2;
	// 832BFF6C: A0880000  lhz r4, 0(r8)
	ctx.r[4].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832BFF70: 7CA30734  extsh r3, r5
	ctx.r[3].s64 = ctx.r[5].s16 as i64;
	// 832BFF74: 7C860734  extsh r6, r4
	ctx.r[6].s64 = ctx.r[4].s16 as i64;
	// 832BFF78: 7CE339D6  mullw r7, r3, r7
	ctx.r[7].s64 = (ctx.r[3].s32 as i64) * (ctx.r[7].s32 as i64);
	// 832BFF7C: 7CC659D6  mullw r6, r6, r11
	ctx.r[6].s64 = (ctx.r[6].s32 as i64) * (ctx.r[11].s32 as i64);
	// 832BFF80: 7CA73214  add r5, r7, r6
	ctx.r[5].u64 = ctx.r[7].u64 + ctx.r[6].u64;
	// 832BFF84: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832BFF88: 7C854B96  divwu r4, r5, r9
	ctx.r[4].u32 = ctx.r[5].u32 / ctx.r[9].u32;
	// 832BFF8C: 7F0B4840  cmplw cr6, r11, r9
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832BFF90: B0880000  sth r4, 0(r8)
	unsafe { crate::rt::store_u16( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[4].u16 ) };
	// 832BFF94: 4198FFC0  blt cr6, 0x832bff54
	if ctx.cr[6].lt {
	pc = 0x832BFF54; continue 'dispatch;
	}
	// 832BFF98: 817F000C  lwz r11, 0xc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BFF9C: 813F0008  lwz r9, 8(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BFFA0: 7D655B78  mr r5, r11
	ctx.r[5].u64 = ctx.r[11].u64;
	// 832BFFA4: 815F001C  lwz r10, 0x1c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(28 as u32) ) } as u64;
	// 832BFFA8: 7D6B4850  subf r11, r11, r9
	ctx.r[11].s64 = ctx.r[9].s64 - ctx.r[11].s64;
	// 832BFFAC: 807F0020  lwz r3, 0x20(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832BFFB0: 7C8B5214  add r4, r11, r10
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832BFFB4: 4B9E94CD  bl 0x82ca9480
	ctx.lr = 0x832BFFB8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832BFFB8: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832BFFBC: 419A0014  beq cr6, 0x832bffd0
	if ctx.cr[6].eq {
	pc = 0x832BFFD0; continue 'dispatch;
	}
	// 832BFFC0: 817F0008  lwz r11, 8(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832BFFC4: 815F000C  lwz r10, 0xc(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832BFFC8: 7D2A5850  subf r9, r10, r11
	ctx.r[9].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832BFFCC: 913E0000  stw r9, 0(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), ctx.r[9].u32 ) };
	// 832BFFD0: 2B1C0000  cmplwi cr6, r28, 0
	ctx.cr[6].compare_u32(ctx.r[28].u32, 0 as u32, &mut ctx.xer);
	// 832BFFD4: 419A000C  beq cr6, 0x832bffe0
	if ctx.cr[6].eq {
	pc = 0x832BFFE0; continue 'dispatch;
	}
	// 832BFFD8: 817F001C  lwz r11, 0x1c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(28 as u32) ) } as u64;
	// 832BFFDC: 917C0000  stw r11, 0(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BFFE0: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832BFFE4: 419A000C  beq cr6, 0x832bfff0
	if ctx.cr[6].eq {
	pc = 0x832BFFF0; continue 'dispatch;
	}
	// 832BFFE8: 7D7AEA14  add r11, r26, r29
	ctx.r[11].u64 = ctx.r[26].u64 + ctx.r[29].u64;
	// 832BFFEC: 917B0000  stw r11, 0(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832BFFF0: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832BFFF4: 4B9E945C  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
}

pub fn sub_832BFFF8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832BFFF8 size=272
	// 832BFFF8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832BFFFC: 4B9E940D  bl 0x82ca9408
	ctx.lr = 0x832C0000;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9408);
	// 832C0000: 9421FF80  stwu r1, -0x80(r1)
	ea = ctx.r[1].u32.wrapping_add(-128 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C0004: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832C0008: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832C000C: 7CBC2B78  mr r28, r5
	ctx.r[28].u64 = ctx.r[5].u64;
	// 832C0010: 7CDD3378  mr r29, r6
	ctx.r[29].u64 = ctx.r[6].u64;
	// 832C0014: 817F00F4  lwz r11, 0xf4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(244 as u32) ) } as u64;
	// 832C0018: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C001C: 419A000C  beq cr6, 0x832c0028
	if ctx.cr[6].eq {
	pc = 0x832C0028; continue 'dispatch;
	}
	// 832C0020: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C0024: 4E800421  bctrl
	ctx.lr = 0x832C0028;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C0028: 2F1EFFFF  cmpwi cr6, r30, -1
	ctx.cr[6].compare_i32(ctx.r[30].s32, -1, &mut ctx.xer);
	// 832C002C: 419A002C  beq cr6, 0x832c0058
	if ctx.cr[6].eq {
	pc = 0x832C0058; continue 'dispatch;
	}
	// 832C0030: 817F0058  lwz r11, 0x58(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(88 as u32) ) } as u64;
	// 832C0034: 7F0BF040  cmplw cr6, r11, r30
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832C0038: 419A0020  beq cr6, 0x832c0058
	if ctx.cr[6].eq {
	pc = 0x832C0058; continue 'dispatch;
	}
	// 832C003C: 817F007C  lwz r11, 0x7c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(124 as u32) ) } as u64;
	// 832C0040: 38C00000  li r6, 0
	ctx.r[6].s64 = 0;
	// 832C0044: 38A00000  li r5, 0
	ctx.r[5].s64 = 0;
	// 832C0048: 807F0054  lwz r3, 0x54(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(84 as u32) ) } as u64;
	// 832C004C: 7C8BF214  add r4, r11, r30
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C0050: 4BA078E9  bl 0x82cc7938
	ctx.lr = 0x832C0054;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC7938);
	// 832C0054: 93DF0058  stw r30, 0x58(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(88 as u32), ctx.r[30].u32 ) };
	// 832C0058: 38E00000  li r7, 0
	ctx.r[7].s64 = 0;
	// 832C005C: 807F0054  lwz r3, 0x54(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(84 as u32) ) } as u64;
	// 832C0060: 38C10050  addi r6, r1, 0x50
	ctx.r[6].s64 = ctx.r[1].s64 + 80;
	// 832C0064: 7FA5EB78  mr r5, r29
	ctx.r[5].u64 = ctx.r[29].u64;
	// 832C0068: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832C006C: 4BA0361D  bl 0x82cc3688
	ctx.lr = 0x832C0070;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC3688);
	// 832C0070: 80610050  lwz r3, 0x50(r1)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C0074: 7F03E840  cmplw cr6, r3, r29
	ctx.cr[6].compare_u32(ctx.r[3].u32, ctx.r[29].u32, &mut ctx.xer);
	// 832C0078: 419A000C  beq cr6, 0x832c0084
	if ctx.cr[6].eq {
	pc = 0x832C0084; continue 'dispatch;
	}
	// 832C007C: 39600001  li r11, 1
	ctx.r[11].s64 = 1;
	// 832C0080: 917F0020  stw r11, 0x20(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(32 as u32), ctx.r[11].u32 ) };
	// 832C0084: 817F0058  lwz r11, 0x58(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(88 as u32) ) } as u64;
	// 832C0088: 7D6B1A14  add r11, r11, r3
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[3].u64;
	// 832C008C: 917F0058  stw r11, 0x58(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(88 as u32), ctx.r[11].u32 ) };
	// 832C0090: 815F0058  lwz r10, 0x58(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(88 as u32) ) } as u64;
	// 832C0094: 915F005C  stw r10, 0x5c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832C0098: 813F0080  lwz r9, 0x80(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(128 as u32) ) } as u64;
	// 832C009C: 811F005C  lwz r8, 0x5c(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832C00A0: 80FF0040  lwz r7, 0x40(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C00A4: 7D684850  subf r11, r8, r9
	ctx.r[11].s64 = ctx.r[9].s64 - ctx.r[8].s64;
	// 832C00A8: 7F0B3840  cmplw cr6, r11, r7
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[7].u32, &mut ctx.xer);
	// 832C00AC: 41980008  blt cr6, 0x832c00b4
	if ctx.cr[6].lt {
	pc = 0x832C00B4; continue 'dispatch;
	}
	// 832C00B0: 817F0040  lwz r11, 0x40(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C00B4: 815F00FC  lwz r10, 0xfc(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(252 as u32) ) } as u64;
	// 832C00B8: 917F0048  stw r11, 0x48(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(72 as u32), ctx.r[11].u32 ) };
	// 832C00BC: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C00C0: 419A0014  beq cr6, 0x832c00d4
	if ctx.cr[6].eq {
	pc = 0x832C00D4; continue 'dispatch;
	}
	// 832C00C4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C00C8: 7D4903A6  mtctr r10
	ctx.ctr.u64 = ctx.r[10].u64;
	// 832C00CC: 4E800421  bctrl
	ctx.lr = 0x832C00D0;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C00D0: 80610050  lwz r3, 0x50(r1)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C00D4: 39630003  addi r11, r3, 3
	ctx.r[11].s64 = ctx.r[3].s64 + 3;
	// 832C00D8: 7F8AE378  mr r10, r28
	ctx.r[10].u64 = ctx.r[28].u64;
	// 832C00DC: 556BF0BE  srwi r11, r11, 2
	// 832C00E0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C00E4: 419A0018  beq cr6, 0x832c00fc
	if ctx.cr[6].eq {
	pc = 0x832C00FC; continue 'dispatch;
	}
	// 832C00E8: 7D20542C  lwbrx r9, 0, r10
	ctx.r[9].u64 = (unsafe { crate::rt::load_u32(base as *const u8, ctx.r[10].u32) }).swap_bytes() as u64;
	// 832C00EC: 356BFFFF  addic. r11, r11, -1
	ctx.xer.ca = (ctx.r[11].u32 > (!(-1 as u32)));
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832C00F0: 912A0000  stw r9, 0(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), ctx.r[9].u32 ) };
	// 832C00F4: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832C00F8: 4082FFF0  bne 0x832c00e8
	if !ctx.cr[0].eq {
	pc = 0x832C00E8; continue 'dispatch;
	}
	// 832C00FC: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832C0100: 4B9E9358  b 0x82ca9458
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9458);
	return;
}

pub fn sub_832C0108(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C0108 size=200
	// 832C0108: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C010C: 4B9E92FD  bl 0x82ca9408
	ctx.lr = 0x832C0110;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9408);
	// 832C0110: 9421FF80  stwu r1, -0x80(r1)
	ea = ctx.r[1].u32.wrapping_add(-128 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C0114: 7C7E1B78  mr r30, r3
	ctx.r[30].u64 = ctx.r[3].u64;
	// 832C0118: 788B0020  clrldi r11, r4, 0x20
	ctx.r[11].u64 = ctx.r[4].u64 & 0x00000000FFFFFFFFu64;
	// 832C011C: 7CBF2B78  mr r31, r5
	ctx.r[31].u64 = ctx.r[5].u64;
	// 832C0120: 1D2B03E8  mulli r9, r11, 0x3e8
	ctx.r[9].s64 = ctx.r[11].s64 * 1000;
	// 832C0124: 815E0084  lwz r10, 0x84(r30)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(132 as u32) ) } as u64;
	// 832C0128: 7D095392  divdu r8, r9, r10
	ctx.r[8].u64 = ctx.r[9].u64 / ctx.r[10].u64;
	// 832C012C: 551D003E  slwi r29, r8, 0
	// 832C0130: 48000989  bl 0x832c0ab8
	ctx.lr = 0x832C0134;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832C0134: 7C7C1B78  mr r28, r3
	ctx.r[28].u64 = ctx.r[3].u64;
	// 832C0138: 817E0088  lwz r11, 0x88(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(136 as u32) ) } as u64;
	// 832C013C: 7D5CE850  subf r10, r28, r29
	ctx.r[10].s64 = ctx.r[29].s64 - ctx.r[28].s64;
	// 832C0140: 7D6A5A14  add r11, r10, r11
	ctx.r[11].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C0144: 7CEBFA14  add r7, r11, r31
	ctx.r[7].u64 = ctx.r[11].u64 + ctx.r[31].u64;
	// 832C0148: 90FE0088  stw r7, 0x88(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(136 as u32), ctx.r[7].u32 ) };
	// 832C014C: 80DE0088  lwz r6, 0x88(r30)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(136 as u32) ) } as u64;
	// 832C0150: 2F060000  cmpwi cr6, r6, 0
	ctx.cr[6].compare_i32(ctx.r[6].s32, 0, &mut ctx.xer);
	// 832C0154: 40990074  ble cr6, 0x832c01c8
	if !ctx.cr[6].gt {
	pc = 0x832C01C8; continue 'dispatch;
	}
	// 832C0158: 3D608350  lis r11, -0x7cb0
	ctx.r[11].s64 = -2091909120;
	// 832C015C: 3FA0C000  lis r29, -0x4000
	ctx.r[29].s64 = -1073741824;
	// 832C0160: 3BEBC200  addi r31, r11, -0x3e00
	ctx.r[31].s64 = ctx.r[11].s64 + -15872;
	// 832C0164: 4AFA5F4D  bl 0x822660b0
	ctx.lr = 0x832C0168;
	crate::recompiler::externs::call(&mut ctx, base, 0x822660B0);
	// 832C0168: 817F0000  lwz r11, 0(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C016C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C0170: 409A000C  bne cr6, 0x832c017c
	if !ctx.cr[6].eq {
	pc = 0x832C017C; continue 'dispatch;
	}
	// 832C0174: 7C6B1B78  mr r11, r3
	ctx.r[11].u64 = ctx.r[3].u64;
	// 832C0178: 917F0000  stw r11, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832C017C: 815F0004  lwz r10, 4(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C0180: 7D6B1850  subf r11, r11, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[11].s64;
	// 832C0184: 7D2A5850  subf r9, r10, r11
	ctx.r[9].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C0188: 7F09E840  cmplw cr6, r9, r29
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[29].u32, &mut ctx.xer);
	// 832C018C: 4199000C  bgt cr6, 0x832c0198
	if ctx.cr[6].gt {
	pc = 0x832C0198; continue 'dispatch;
	}
	// 832C0190: 7D6A5B78  mr r10, r11
	ctx.r[10].u64 = ctx.r[11].u64;
	// 832C0194: 917F0004  stw r11, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832C0198: 817E0088  lwz r11, 0x88(r30)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(136 as u32) ) } as u64;
	// 832C019C: 7D3C5050  subf r9, r28, r10
	ctx.r[9].s64 = ctx.r[10].s64 - ctx.r[28].s64;
	// 832C01A0: 7F095800  cmpw cr6, r9, r11
	ctx.cr[6].compare_i32(ctx.r[9].s32, ctx.r[11].s32, &mut ctx.xer);
	// 832C01A4: 4198FFC0  blt cr6, 0x832c0164
	if ctx.cr[6].lt {
	pc = 0x832C0164; continue 'dispatch;
	}
	// 832C01A8: 813E0088  lwz r9, 0x88(r30)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(136 as u32) ) } as u64;
	// 832C01AC: 7D6AE050  subf r11, r10, r28
	ctx.r[11].s64 = ctx.r[28].s64 - ctx.r[10].s64;
	// 832C01B0: 7D5C5378  mr r28, r10
	ctx.r[28].u64 = ctx.r[10].u64;
	// 832C01B4: 7D6B4A14  add r11, r11, r9
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[9].u64;
	// 832C01B8: 917E0088  stw r11, 0x88(r30)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(136 as u32), ctx.r[11].u32 ) };
	// 832C01BC: 815E0088  lwz r10, 0x88(r30)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(136 as u32) ) } as u64;
	// 832C01C0: 2F0A0000  cmpwi cr6, r10, 0
	ctx.cr[6].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832C01C4: 4199FFA0  bgt cr6, 0x832c0164
	if ctx.cr[6].gt {
	pc = 0x832C0164; continue 'dispatch;
	}
	// 832C01C8: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832C01CC: 4B9E928C  b 0x82ca9458
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9458);
	return;
}

pub fn sub_832C01D0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C01D0 size=976
	// 832C01D0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C01D4: 4B9E921D  bl 0x82ca93f0
	ctx.lr = 0x832C01D8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93F0);
	// 832C01D8: 9421FF50  stwu r1, -0xb0(r1)
	ea = ctx.r[1].u32.wrapping_add(-176 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C01DC: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832C01E0: 7CDC3378  mr r28, r6
	ctx.r[28].u64 = ctx.r[6].u64;
	// 832C01E4: 3B200000  li r25, 0
	ctx.r[25].s64 = 0;
	// 832C01E8: 7CBE2B78  mr r30, r5
	ctx.r[30].u64 = ctx.r[5].u64;
	// 832C01EC: 7CFB3B78  mr r27, r7
	ctx.r[27].u64 = ctx.r[7].u64;
	// 832C01F0: 817F0020  lwz r11, 0x20(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C01F4: 7F96E378  mr r22, r28
	ctx.r[22].u64 = ctx.r[28].u64;
	// 832C01F8: 7F37CB78  mr r23, r25
	ctx.r[23].u64 = ctx.r[25].u64;
	// 832C01FC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C0200: 419A0010  beq cr6, 0x832c0210
	if ctx.cr[6].eq {
	pc = 0x832C0210; continue 'dispatch;
	}
	// 832C0204: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832C0208: 382100B0  addi r1, r1, 0xb0
	ctx.r[1].s64 = ctx.r[1].s64 + 176;
	// 832C020C: 4B9E9234  b 0x82ca9440
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9440);
	return;
	// 832C0210: 480008A9  bl 0x832c0ab8
	ctx.lr = 0x832C0214;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832C0214: 7C781B78  mr r24, r3
	ctx.r[24].u64 = ctx.r[3].u64;
	// 832C0218: 2F1EFFFF  cmpwi cr6, r30, -1
	ctx.cr[6].compare_i32(ctx.r[30].s32, -1, &mut ctx.xer);
	// 832C021C: 419A00D8  beq cr6, 0x832c02f4
	if ctx.cr[6].eq {
	pc = 0x832C02F4; continue 'dispatch;
	}
	// 832C0220: 817F005C  lwz r11, 0x5c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832C0224: 7F0BF040  cmplw cr6, r11, r30
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832C0228: 419A00CC  beq cr6, 0x832c02f4
	if ctx.cr[6].eq {
	pc = 0x832C02F4; continue 'dispatch;
	}
	// 832C022C: 817F00F4  lwz r11, 0xf4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(244 as u32) ) } as u64;
	// 832C0230: 3AE00001  li r23, 1
	ctx.r[23].s64 = 1;
	// 832C0234: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C0238: 419A0010  beq cr6, 0x832c0248
	if ctx.cr[6].eq {
	pc = 0x832C0248; continue 'dispatch;
	}
	// 832C023C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C0240: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C0244: 4E800421  bctrl
	ctx.lr = 0x832C0248;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C0248: 817F005C  lwz r11, 0x5c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832C024C: 7F1E5840  cmplw cr6, r30, r11
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C0250: 40990068  ble cr6, 0x832c02b8
	if !ctx.cr[6].gt {
	pc = 0x832C02B8; continue 'dispatch;
	}
	// 832C0254: 817F0058  lwz r11, 0x58(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(88 as u32) ) } as u64;
	// 832C0258: 7F1E5840  cmplw cr6, r30, r11
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C025C: 4199005C  bgt cr6, 0x832c02b8
	if ctx.cr[6].gt {
	pc = 0x832C02B8; continue 'dispatch;
	}
	// 832C0260: 817F005C  lwz r11, 0x5c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832C0264: 93DF005C  stw r30, 0x5c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(92 as u32), ctx.r[30].u32 ) };
	// 832C0268: 815F0064  lwz r10, 0x64(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(100 as u32) ) } as u64;
	// 832C026C: 7D6BF050  subf r11, r11, r30
	ctx.r[11].s64 = ctx.r[30].s64 - ctx.r[11].s64;
	// 832C0270: 7D4A5A14  add r10, r10, r11
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C0274: 91610050  stw r11, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[11].u32 ) };
	// 832C0278: 915F0064  stw r10, 0x64(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(100 as u32), ctx.r[10].u32 ) };
	// 832C027C: 813F004C  lwz r9, 0x4c(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(76 as u32) ) } as u64;
	// 832C0280: 7D0B4850  subf r8, r11, r9
	ctx.r[8].s64 = ctx.r[9].s64 - ctx.r[11].s64;
	// 832C0284: 911F004C  stw r8, 0x4c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(76 as u32), ctx.r[8].u32 ) };
	// 832C0288: 815F0060  lwz r10, 0x60(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C028C: 7CEA5A14  add r7, r10, r11
	ctx.r[7].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C0290: 90FF0060  stw r7, 0x60(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(96 as u32), ctx.r[7].u32 ) };
	// 832C0294: 80DF0070  lwz r6, 0x70(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(112 as u32) ) } as u64;
	// 832C0298: 80BF0060  lwz r5, 0x60(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C029C: 7F053040  cmplw cr6, r5, r6
	ctx.cr[6].compare_u32(ctx.r[5].u32, ctx.r[6].u32, &mut ctx.xer);
	// 832C02A0: 40990054  ble cr6, 0x832c02f4
	if !ctx.cr[6].gt {
	pc = 0x832C02F4; continue 'dispatch;
	}
	// 832C02A4: 817F0040  lwz r11, 0x40(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C02A8: 815F0060  lwz r10, 0x60(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C02AC: 7D2B5050  subf r9, r11, r10
	ctx.r[9].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C02B0: 913F0060  stw r9, 0x60(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(96 as u32), ctx.r[9].u32 ) };
	// 832C02B4: 48000040  b 0x832c02f4
	pc = 0x832C02F4; continue 'dispatch;
	// 832C02B8: 817F007C  lwz r11, 0x7c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(124 as u32) ) } as u64;
	// 832C02BC: 38C00000  li r6, 0
	ctx.r[6].s64 = 0;
	// 832C02C0: 38A00000  li r5, 0
	ctx.r[5].s64 = 0;
	// 832C02C4: 807F0054  lwz r3, 0x54(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(84 as u32) ) } as u64;
	// 832C02C8: 7C8BF214  add r4, r11, r30
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C02CC: 4BA0766D  bl 0x82cc7938
	ctx.lr = 0x832C02D0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC7938);
	// 832C02D0: 93DF0058  stw r30, 0x58(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(88 as u32), ctx.r[30].u32 ) };
	// 832C02D4: 93DF005C  stw r30, 0x5c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(92 as u32), ctx.r[30].u32 ) };
	// 832C02D8: 817F0040  lwz r11, 0x40(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C02DC: 917F0064  stw r11, 0x64(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(100 as u32), ctx.r[11].u32 ) };
	// 832C02E0: 933F004C  stw r25, 0x4c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(76 as u32), ctx.r[25].u32 ) };
	// 832C02E4: 815F006C  lwz r10, 0x6c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(108 as u32) ) } as u64;
	// 832C02E8: 915F0060  stw r10, 0x60(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(96 as u32), ctx.r[10].u32 ) };
	// 832C02EC: 813F006C  lwz r9, 0x6c(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(108 as u32) ) } as u64;
	// 832C02F0: 913F0074  stw r9, 0x74(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(116 as u32), ctx.r[9].u32 ) };
	// 832C02F4: 3B5F004C  addi r26, r31, 0x4c
	ctx.r[26].s64 = ctx.r[31].s64 + 76;
	// 832C02F8: 83DA0000  lwz r30, 0(r26)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[26].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C02FC: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832C0300: 419A0100  beq cr6, 0x832c0400
	if ctx.cr[6].eq {
	pc = 0x832C0400; continue 'dispatch;
	}
	// 832C0304: 7F1ED840  cmplw cr6, r30, r27
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832C0308: 40990008  ble cr6, 0x832c0310
	if !ctx.cr[6].gt {
	pc = 0x832C0310; continue 'dispatch;
	}
	// 832C030C: 7F7EDB78  mr r30, r27
	ctx.r[30].u64 = ctx.r[27].u64;
	// 832C0310: 817F005C  lwz r11, 0x5c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832C0314: 7F7ED850  subf r27, r30, r27
	ctx.r[27].s64 = ctx.r[27].s64 - ctx.r[30].s64;
	// 832C0318: 7F3ECA14  add r25, r30, r25
	ctx.r[25].u64 = ctx.r[30].u64 + ctx.r[25].u64;
	// 832C031C: 7D6BF214  add r11, r11, r30
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C0320: 917F005C  stw r11, 0x5c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(92 as u32), ctx.r[11].u32 ) };
	// 832C0324: 815F0070  lwz r10, 0x70(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(112 as u32) ) } as u64;
	// 832C0328: 813F0060  lwz r9, 0x60(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C032C: 7FA95050  subf r29, r9, r10
	ctx.r[29].s64 = ctx.r[10].s64 - ctx.r[9].s64;
	// 832C0330: 7F1DF040  cmplw cr6, r29, r30
	ctx.cr[6].compare_u32(ctx.r[29].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832C0334: 4199006C  bgt cr6, 0x832c03a0
	if ctx.cr[6].gt {
	pc = 0x832C03A0; continue 'dispatch;
	}
	// 832C0338: 7FA5EB78  mr r5, r29
	ctx.r[5].u64 = ctx.r[29].u64;
	// 832C033C: 809F0060  lwz r4, 0x60(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C0340: 7F83E378  mr r3, r28
	ctx.r[3].u64 = ctx.r[28].u64;
	// 832C0344: 4B9E913D  bl 0x82ca9480
	ctx.lr = 0x832C0348;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832C0348: 80FF006C  lwz r7, 0x6c(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(108 as u32) ) } as u64;
	// 832C034C: 7F9DE214  add r28, r29, r28
	ctx.r[28].u64 = ctx.r[29].u64 + ctx.r[28].u64;
	// 832C0350: 7FDDF050  subf r30, r29, r30
	ctx.r[30].s64 = ctx.r[30].s64 - ctx.r[29].s64;
	// 832C0354: 7D1D00D0  neg r8, r29
	ctx.r[8].s64 = -ctx.r[29].s64;
	// 832C0358: 90FF0060  stw r7, 0x60(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(96 as u32), ctx.r[7].u32 ) };
	// 832C035C: 7D2000A6  mfmsr r9
	ctx.r[9].u64 = ctx.msr;
	// 832C0360: 7DA10164  mtmsrd r13, 1
	ctx.msr = (ctx.r[13].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C0364: 7D60D028  lwarx r11, 0, r26
	// lwarx
	let ea = ctx.r[26].u32;
	ctx.reserved.u32 = unsafe { crate::rt::load_u32(base as *const u8, ea) };
	ctx.r[11].u64 = ctx.reserved.u32 as u64;
	// 832C0368: 7D485A14  add r10, r8, r11
	ctx.r[10].u64 = ctx.r[8].u64 + ctx.r[11].u64;
	// 832C036C: 7D40D12D  stwcx. r10, 0, r26
	// stwcx.
	let addr = ctx.r[26].u32;
	ctx.cr[0].lt = false;
	ctx.cr[0].gt = false;
	let ok = unsafe { crate::rt::stwcx32(base as *mut u8, addr, ctx.reserved.u32, ctx.r[10].u32) };
	ctx.cr[0].eq = ok;
	ctx.cr[0].so = ctx.xer.so;
	// 832C0370: 7D210164  mtmsrd r9, 1
	ctx.msr = (ctx.r[9].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C0374: 4082FFE8  bne 0x832c035c
	if !ctx.cr[0].eq {
	pc = 0x832C035C; continue 'dispatch;
	}
	// 832C0378: 38DF0064  addi r6, r31, 0x64
	ctx.r[6].s64 = ctx.r[31].s64 + 100;
	// 832C037C: 7C6000A6  mfmsr r3
	ctx.r[3].u64 = ctx.msr;
	// 832C0380: 7DA10164  mtmsrd r13, 1
	ctx.msr = (ctx.r[13].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C0384: 7CA03028  lwarx r5, 0, r6
	// lwarx
	let ea = ctx.r[6].u32;
	ctx.reserved.u32 = unsafe { crate::rt::load_u32(base as *const u8, ea) };
	ctx.r[5].u64 = ctx.reserved.u32 as u64;
	// 832C0388: 7C9D2A14  add r4, r29, r5
	ctx.r[4].u64 = ctx.r[29].u64 + ctx.r[5].u64;
	// 832C038C: 7C80312D  stwcx. r4, 0, r6
	// stwcx.
	let addr = ctx.r[6].u32;
	ctx.cr[0].lt = false;
	ctx.cr[0].gt = false;
	let ok = unsafe { crate::rt::stwcx32(base as *mut u8, addr, ctx.reserved.u32, ctx.r[4].u32) };
	ctx.cr[0].eq = ok;
	ctx.cr[0].so = ctx.xer.so;
	// 832C0390: 7C610164  mtmsrd r3, 1
	ctx.msr = (ctx.r[3].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C0394: 4082FFE8  bne 0x832c037c
	if !ctx.cr[0].eq {
	pc = 0x832C037C; continue 'dispatch;
	}
	// 832C0398: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832C039C: 419A0064  beq cr6, 0x832c0400
	if ctx.cr[6].eq {
	pc = 0x832C0400; continue 'dispatch;
	}
	// 832C03A0: 7FC5F378  mr r5, r30
	ctx.r[5].u64 = ctx.r[30].u64;
	// 832C03A4: 809F0060  lwz r4, 0x60(r31)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C03A8: 7F83E378  mr r3, r28
	ctx.r[3].u64 = ctx.r[28].u64;
	// 832C03AC: 4B9E90D5  bl 0x82ca9480
	ctx.lr = 0x832C03B0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9480);
	// 832C03B0: 817F0060  lwz r11, 0x60(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C03B4: 7F9EE214  add r28, r30, r28
	ctx.r[28].u64 = ctx.r[30].u64 + ctx.r[28].u64;
	// 832C03B8: 7CEBF214  add r7, r11, r30
	ctx.r[7].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C03BC: 7CDE00D0  neg r6, r30
	ctx.r[6].s64 = -ctx.r[30].s64;
	// 832C03C0: 90FF0060  stw r7, 0x60(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(96 as u32), ctx.r[7].u32 ) };
	// 832C03C4: 7D0000A6  mfmsr r8
	ctx.r[8].u64 = ctx.msr;
	// 832C03C8: 7DA10164  mtmsrd r13, 1
	ctx.msr = (ctx.r[13].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C03CC: 7D40D028  lwarx r10, 0, r26
	// lwarx
	let ea = ctx.r[26].u32;
	ctx.reserved.u32 = unsafe { crate::rt::load_u32(base as *const u8, ea) };
	ctx.r[10].u64 = ctx.reserved.u32 as u64;
	// 832C03D0: 7D265214  add r9, r6, r10
	ctx.r[9].u64 = ctx.r[6].u64 + ctx.r[10].u64;
	// 832C03D4: 7D20D12D  stwcx. r9, 0, r26
	// stwcx.
	let addr = ctx.r[26].u32;
	ctx.cr[0].lt = false;
	ctx.cr[0].gt = false;
	let ok = unsafe { crate::rt::stwcx32(base as *mut u8, addr, ctx.reserved.u32, ctx.r[9].u32) };
	ctx.cr[0].eq = ok;
	ctx.cr[0].so = ctx.xer.so;
	// 832C03D8: 7D010164  mtmsrd r8, 1
	ctx.msr = (ctx.r[8].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C03DC: 4082FFE8  bne 0x832c03c4
	if !ctx.cr[0].eq {
	pc = 0x832C03C4; continue 'dispatch;
	}
	// 832C03E0: 38BF0064  addi r5, r31, 0x64
	ctx.r[5].s64 = ctx.r[31].s64 + 100;
	// 832C03E4: 7D6000A6  mfmsr r11
	ctx.r[11].u64 = ctx.msr;
	// 832C03E8: 7DA10164  mtmsrd r13, 1
	ctx.msr = (ctx.r[13].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C03EC: 7C802828  lwarx r4, 0, r5
	// lwarx
	let ea = ctx.r[5].u32;
	ctx.reserved.u32 = unsafe { crate::rt::load_u32(base as *const u8, ea) };
	ctx.r[4].u64 = ctx.reserved.u32 as u64;
	// 832C03F0: 7C7E2214  add r3, r30, r4
	ctx.r[3].u64 = ctx.r[30].u64 + ctx.r[4].u64;
	// 832C03F4: 7C60292D  stwcx. r3, 0, r5
	// stwcx.
	let addr = ctx.r[5].u32;
	ctx.cr[0].lt = false;
	ctx.cr[0].gt = false;
	let ok = unsafe { crate::rt::stwcx32(base as *mut u8, addr, ctx.reserved.u32, ctx.r[3].u32) };
	ctx.cr[0].eq = ok;
	ctx.cr[0].so = ctx.xer.so;
	// 832C03F8: 7D610164  mtmsrd r11, 1
	ctx.msr = (ctx.r[11].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C03FC: 4082FFE8  bne 0x832c03e4
	if !ctx.cr[0].eq {
	pc = 0x832C03E4; continue 'dispatch;
	}
	// 832C0400: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832C0404: 419A00C8  beq cr6, 0x832c04cc
	if ctx.cr[6].eq {
	pc = 0x832C04CC; continue 'dispatch;
	}
	// 832C0408: 2F170000  cmpwi cr6, r23, 0
	ctx.cr[6].compare_i32(ctx.r[23].s32, 0, &mut ctx.xer);
	// 832C040C: 409A0024  bne cr6, 0x832c0430
	if !ctx.cr[6].eq {
	pc = 0x832C0430; continue 'dispatch;
	}
	// 832C0410: 817F00F4  lwz r11, 0xf4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(244 as u32) ) } as u64;
	// 832C0414: 3AE00001  li r23, 1
	ctx.r[23].s64 = 1;
	// 832C0418: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C041C: 419AFEDC  beq cr6, 0x832c02f8
	if ctx.cr[6].eq {
	pc = 0x832C02F8; continue 'dispatch;
	}
	// 832C0420: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C0424: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C0428: 4E800421  bctrl
	ctx.lr = 0x832C042C;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C042C: 4BFFFECC  b 0x832c02f8
	pc = 0x832C02F8; continue 'dispatch;
	// 832C0430: 48000689  bl 0x832c0ab8
	ctx.lr = 0x832C0434;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832C0434: 38E00000  li r7, 0
	ctx.r[7].s64 = 0;
	// 832C0438: 38C10050  addi r6, r1, 0x50
	ctx.r[6].s64 = ctx.r[1].s64 + 80;
	// 832C043C: 7F65DB78  mr r5, r27
	ctx.r[5].u64 = ctx.r[27].u64;
	// 832C0440: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832C0444: 7C7E1B78  mr r30, r3
	ctx.r[30].u64 = ctx.r[3].u64;
	// 832C0448: 807F0054  lwz r3, 0x54(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(84 as u32) ) } as u64;
	// 832C044C: 4BA0323D  bl 0x82cc3688
	ctx.lr = 0x832C0450;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC3688);
	// 832C0450: 80810050  lwz r4, 0x50(r1)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C0454: 7F04D840  cmplw cr6, r4, r27
	ctx.cr[6].compare_u32(ctx.r[4].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832C0458: 4098000C  bge cr6, 0x832c0464
	if !ctx.cr[6].lt {
	pc = 0x832C0464; continue 'dispatch;
	}
	// 832C045C: 39600001  li r11, 1
	ctx.r[11].s64 = 1;
	// 832C0460: 917F0020  stw r11, 0x20(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(32 as u32), ctx.r[11].u32 ) };
	// 832C0464: 817F0058  lwz r11, 0x58(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(88 as u32) ) } as u64;
	// 832C0468: 7F24CA14  add r25, r4, r25
	ctx.r[25].u64 = ctx.r[4].u64 + ctx.r[25].u64;
	// 832C046C: 7D6B2214  add r11, r11, r4
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[4].u64;
	// 832C0470: 917F0058  stw r11, 0x58(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(88 as u32), ctx.r[11].u32 ) };
	// 832C0474: 817F005C  lwz r11, 0x5c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832C0478: 7D4B2214  add r10, r11, r4
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[4].u64;
	// 832C047C: 915F005C  stw r10, 0x5c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(92 as u32), ctx.r[10].u32 ) };
	// 832C0480: 817F0028  lwz r11, 0x28(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C0484: 7D2B2214  add r9, r11, r4
	ctx.r[9].u64 = ctx.r[11].u64 + ctx.r[4].u64;
	// 832C0488: 913F0028  stw r9, 0x28(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(40 as u32), ctx.r[9].u32 ) };
	// 832C048C: 811F0084  lwz r8, 0x84(r31)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(132 as u32) ) } as u64;
	// 832C0490: 2B080000  cmplwi cr6, r8, 0
	ctx.cr[6].compare_u32(ctx.r[8].u32, 0 as u32, &mut ctx.xer);
	// 832C0494: 419A0010  beq cr6, 0x832c04a4
	if ctx.cr[6].eq {
	pc = 0x832C04A4; continue 'dispatch;
	}
	// 832C0498: 7FC5F378  mr r5, r30
	ctx.r[5].u64 = ctx.r[30].u64;
	// 832C049C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C04A0: 4BFFFC69  bl 0x832c0108
	ctx.lr = 0x832C04A4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0108);
	// 832C04A4: 48000615  bl 0x832c0ab8
	ctx.lr = 0x832C04A8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832C04A8: 813F0030  lwz r9, 0x30(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C04AC: 7D5E1850  subf r10, r30, r3
	ctx.r[10].s64 = ctx.r[3].s64 - ctx.r[30].s64;
	// 832C04B0: 7D781850  subf r11, r24, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[24].s64;
	// 832C04B4: 7D4A4A14  add r10, r10, r9
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[9].u64;
	// 832C04B8: 915F0030  stw r10, 0x30(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(48 as u32), ctx.r[10].u32 ) };
	// 832C04BC: 815F0034  lwz r10, 0x34(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(52 as u32) ) } as u64;
	// 832C04C0: 7D2B5214  add r9, r11, r10
	ctx.r[9].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832C04C4: 913F0034  stw r9, 0x34(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(52 as u32), ctx.r[9].u32 ) };
	// 832C04C8: 48000018  b 0x832c04e0
	pc = 0x832C04E0; continue 'dispatch;
	// 832C04CC: 480005ED  bl 0x832c0ab8
	ctx.lr = 0x832C04D0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832C04D0: 815F0034  lwz r10, 0x34(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(52 as u32) ) } as u64;
	// 832C04D4: 7D781850  subf r11, r24, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[24].s64;
	// 832C04D8: 7D6B5214  add r11, r11, r10
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832C04DC: 917F0034  stw r11, 0x34(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(52 as u32), ctx.r[11].u32 ) };
	// 832C04E0: 817F0080  lwz r11, 0x80(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(128 as u32) ) } as u64;
	// 832C04E4: 815F005C  lwz r10, 0x5c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) } as u64;
	// 832C04E8: 813F0040  lwz r9, 0x40(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C04EC: 7D6A5850  subf r11, r10, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C04F0: 91610050  stw r11, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[11].u32 ) };
	// 832C04F4: 7F0B4840  cmplw cr6, r11, r9
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832C04F8: 41980008  blt cr6, 0x832c0500
	if ctx.cr[6].lt {
	pc = 0x832C0500; continue 'dispatch;
	}
	// 832C04FC: 817F0040  lwz r11, 0x40(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C0500: 917F0048  stw r11, 0x48(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(72 as u32), ctx.r[11].u32 ) };
	// 832C0504: 817A0000  lwz r11, 0(r26)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[26].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0508: 815F0048  lwz r10, 0x48(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(72 as u32) ) } as u64;
	// 832C050C: 3D2B0002  addis r9, r11, 2
	ctx.r[9].s64 = ctx.r[11].s64 + 131072;
	// 832C0510: 7F095040  cmplw cr6, r9, r10
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C0514: 4099000C  ble cr6, 0x832c0520
	if !ctx.cr[6].gt {
	pc = 0x832C0520; continue 'dispatch;
	}
	// 832C0518: 817A0000  lwz r11, 0(r26)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[26].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C051C: 917F0048  stw r11, 0x48(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(72 as u32), ctx.r[11].u32 ) };
	// 832C0520: 2F170000  cmpwi cr6, r23, 0
	ctx.cr[6].compare_i32(ctx.r[23].s32, 0, &mut ctx.xer);
	// 832C0524: 419A001C  beq cr6, 0x832c0540
	if ctx.cr[6].eq {
	pc = 0x832C0540; continue 'dispatch;
	}
	// 832C0528: 817F00FC  lwz r11, 0xfc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(252 as u32) ) } as u64;
	// 832C052C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C0530: 419A0010  beq cr6, 0x832c0540
	if ctx.cr[6].eq {
	pc = 0x832C0540; continue 'dispatch;
	}
	// 832C0534: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C0538: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C053C: 4E800421  bctrl
	ctx.lr = 0x832C0540;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C0540: 39790003  addi r11, r25, 3
	ctx.r[11].s64 = ctx.r[25].s64 + 3;
	// 832C0544: 7ECAB378  mr r10, r22
	ctx.r[10].u64 = ctx.r[22].u64;
	// 832C0548: 556BF0BE  srwi r11, r11, 2
	// 832C054C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C0550: 419A0018  beq cr6, 0x832c0568
	if ctx.cr[6].eq {
	pc = 0x832C0568; continue 'dispatch;
	}
	// 832C0554: 7D20542C  lwbrx r9, 0, r10
	ctx.r[9].u64 = (unsafe { crate::rt::load_u32(base as *const u8, ctx.r[10].u32) }).swap_bytes() as u64;
	// 832C0558: 356BFFFF  addic. r11, r11, -1
	ctx.xer.ca = (ctx.r[11].u32 > (!(-1 as u32)));
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[11].s32, 0, &mut ctx.xer);
	// 832C055C: 912A0000  stw r9, 0(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), ctx.r[9].u32 ) };
	// 832C0560: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832C0564: 4082FFF0  bne 0x832c0554
	if !ctx.cr[0].eq {
	pc = 0x832C0554; continue 'dispatch;
	}
	// 832C0568: 7F23CB78  mr r3, r25
	ctx.r[3].u64 = ctx.r[25].u64;
	// 832C056C: 382100B0  addi r1, r1, 0xb0
	ctx.r[1].s64 = ctx.r[1].s64 + 176;
	// 832C0570: 4B9E8ED0  b 0x82ca9440
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9440);
	return;
}

pub fn sub_832C05A0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C05A0 size=144
	// 832C05A0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C05A4: 4B9E8E61  bl 0x82ca9404
	ctx.lr = 0x832C05A8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9404);
	// 832C05A8: 9421FF80  stwu r1, -0x80(r1)
	ea = ctx.r[1].u32.wrapping_add(-128 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C05AC: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832C05B0: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832C05B4: 7CBD2B78  mr r29, r5
	ctx.r[29].u64 = ctx.r[5].u64;
	// 832C05B8: 7CDC3378  mr r28, r6
	ctx.r[28].u64 = ctx.r[6].u64;
	// 832C05BC: 7CFB3B78  mr r27, r7
	ctx.r[27].u64 = ctx.r[7].u64;
	// 832C05C0: 817F00F4  lwz r11, 0xf4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(244 as u32) ) } as u64;
	// 832C05C4: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C05C8: 419A000C  beq cr6, 0x832c05d4
	if ctx.cr[6].eq {
	pc = 0x832C05D4; continue 'dispatch;
	}
	// 832C05CC: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C05D0: 4E800421  bctrl
	ctx.lr = 0x832C05D4;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C05D4: 397E0FFF  addi r11, r30, 0xfff
	ctx.r[11].s64 = ctx.r[30].s64 + 4095;
	// 832C05D8: 813F00FC  lwz r9, 0xfc(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(252 as u32) ) } as u64;
	// 832C05DC: 57AA001C  rlwinm r10, r29, 0, 0, 0xe
	ctx.r[10].u64 = ctx.r[29].u32 as u64 & 0xFFFFFFFFu64;
	// 832C05E0: 556B0026  rlwinm r11, r11, 0, 0, 0x13
	ctx.r[11].u64 = ctx.r[11].u32 as u64 & 0xFFFFFFFFu64;
	// 832C05E4: 39000000  li r8, 0
	ctx.r[8].s64 = 0;
	// 832C05E8: 917F006C  stw r11, 0x6c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(108 as u32), ctx.r[11].u32 ) };
	// 832C05EC: 7CEB5214  add r7, r11, r10
	ctx.r[7].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832C05F0: 917F0060  stw r11, 0x60(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(96 as u32), ctx.r[11].u32 ) };
	// 832C05F4: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832C05F8: 917F0074  stw r11, 0x74(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(116 as u32), ctx.r[11].u32 ) };
	// 832C05FC: 90FF0070  stw r7, 0x70(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(112 as u32), ctx.r[7].u32 ) };
	// 832C0600: 915F0040  stw r10, 0x40(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(64 as u32), ctx.r[10].u32 ) };
	// 832C0604: 915F0064  stw r10, 0x64(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(100 as u32), ctx.r[10].u32 ) };
	// 832C0608: 911F004C  stw r8, 0x4c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(76 as u32), ctx.r[8].u32 ) };
	// 832C060C: 939F0080  stw r28, 0x80(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(128 as u32), ctx.r[28].u32 ) };
	// 832C0610: 937F0084  stw r27, 0x84(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(132 as u32), ctx.r[27].u32 ) };
	// 832C0614: 419A0010  beq cr6, 0x832c0624
	if ctx.cr[6].eq {
	pc = 0x832C0624; continue 'dispatch;
	}
	// 832C0618: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C061C: 7D2903A6  mtctr r9
	ctx.ctr.u64 = ctx.r[9].u64;
	// 832C0620: 4E800421  bctrl
	ctx.lr = 0x832C0624;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C0624: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832C0628: 4B9E8E2C  b 0x82ca9454
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9454);
	return;
}

pub fn sub_832C0630(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C0630 size=104
	// 832C0630: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C0634: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832C0638: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832C063C: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C0640: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832C0644: 817F00F4  lwz r11, 0xf4(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(244 as u32) ) } as u64;
	// 832C0648: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C064C: 419A000C  beq cr6, 0x832c0658
	if ctx.cr[6].eq {
	pc = 0x832C0658; continue 'dispatch;
	}
	// 832C0650: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C0654: 4E800421  bctrl
	ctx.lr = 0x832C0658;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C0658: 817F0078  lwz r11, 0x78(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(120 as u32) ) } as u64;
	// 832C065C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C0660: 409A000C  bne cr6, 0x832c066c
	if !ctx.cr[6].eq {
	pc = 0x832C066C; continue 'dispatch;
	}
	// 832C0664: 807F0054  lwz r3, 0x54(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(84 as u32) ) } as u64;
	// 832C0668: 4BA02149  bl 0x82cc27b0
	ctx.lr = 0x832C066C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC27B0);
	// 832C066C: 817F00FC  lwz r11, 0xfc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(252 as u32) ) } as u64;
	// 832C0670: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C0674: 419A0010  beq cr6, 0x832c0684
	if ctx.cr[6].eq {
	pc = 0x832C0684; continue 'dispatch;
	}
	// 832C0678: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C067C: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C0680: 4E800421  bctrl
	ctx.lr = 0x832C0684;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C0684: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832C0688: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832C068C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832C0690: EBE1FFF0  ld r31, -0x10(r1)
	ctx.r[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-16 as u32) ) };
	// 832C0694: 4E800020  blr
	return;
}

pub fn sub_832C0698(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C0698 size=616
	// 832C0698: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C069C: 4B9E8D5D  bl 0x82ca93f8
	ctx.lr = 0x832C06A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93F8);
	// 832C06A0: 9421FF60  stwu r1, -0xa0(r1)
	ea = ctx.r[1].u32.wrapping_add(-160 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C06A4: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832C06A8: 3B600000  li r27, 0
	ctx.r[27].s64 = 0;
	// 832C06AC: 93610050  stw r27, 0x50(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[27].u32 ) };
	// 832C06B0: 831F002C  lwz r24, 0x2c(r31)
	ctx.r[24].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(44 as u32) ) } as u64;
	// 832C06B4: 817F0020  lwz r11, 0x20(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C06B8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C06BC: 419A0010  beq cr6, 0x832c06cc
	if ctx.cr[6].eq {
	pc = 0x832C06CC; continue 'dispatch;
	}
	// 832C06C0: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832C06C4: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C06C8: 4B9E8D80  b 0x82ca9448
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9448);
	return;
	// 832C06CC: 817F0050  lwz r11, 0x50(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C06D0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C06D4: 409AFFEC  bne cr6, 0x832c06c0
	if !ctx.cr[6].eq {
	pc = 0x832C06C0; continue 'dispatch;
	}
	// 832C06D8: 817F00F8  lwz r11, 0xf8(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(248 as u32) ) } as u64;
	// 832C06DC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C06E0: 419A01FC  beq cr6, 0x832c08dc
	if ctx.cr[6].eq {
	pc = 0x832C08DC; continue 'dispatch;
	}
	// 832C06E4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C06E8: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C06EC: 4E800421  bctrl
	ctx.lr = 0x832C06F0;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C06F0: 2F030000  cmpwi cr6, r3, 0
	ctx.cr[6].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832C06F4: 419A01E8  beq cr6, 0x832c08dc
	if ctx.cr[6].eq {
	pc = 0x832C08DC; continue 'dispatch;
	}
	// 832C06F8: 817F0080  lwz r11, 0x80(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(128 as u32) ) } as u64;
	// 832C06FC: 3F800002  lis r28, 2
	ctx.r[28].s64 = 131072;
	// 832C0700: 815F0058  lwz r10, 0x58(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(88 as u32) ) } as u64;
	// 832C0704: 3B3F0064  addi r25, r31, 0x64
	ctx.r[25].s64 = ctx.r[31].s64 + 100;
	// 832C0708: 813F0064  lwz r9, 0x64(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(100 as u32) ) } as u64;
	// 832C070C: 7FAA5850  subf r29, r10, r11
	ctx.r[29].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C0710: 7F09E040  cmplw cr6, r9, r28
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[28].u32, &mut ctx.xer);
	// 832C0714: 41980198  blt cr6, 0x832c08ac
	if ctx.cr[6].lt {
	pc = 0x832C08AC; continue 'dispatch;
	}
	// 832C0718: 2B1D0000  cmplwi cr6, r29, 0
	ctx.cr[6].compare_u32(ctx.r[29].u32, 0 as u32, &mut ctx.xer);
	// 832C071C: 419A0190  beq cr6, 0x832c08ac
	if ctx.cr[6].eq {
	pc = 0x832C08AC; continue 'dispatch;
	}
	// 832C0720: 7F9EE378  mr r30, r28
	ctx.r[30].u64 = ctx.r[28].u64;
	// 832C0724: 7F1DE040  cmplw cr6, r29, r28
	ctx.cr[6].compare_u32(ctx.r[29].u32, ctx.r[28].u32, &mut ctx.xer);
	// 832C0728: 40980008  bge cr6, 0x832c0730
	if !ctx.cr[6].lt {
	pc = 0x832C0730; continue 'dispatch;
	}
	// 832C072C: 7FBEEB78  mr r30, r29
	ctx.r[30].u64 = ctx.r[29].u64;
	// 832C0730: 48000389  bl 0x832c0ab8
	ctx.lr = 0x832C0734;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832C0734: 817F0058  lwz r11, 0x58(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(88 as u32) ) } as u64;
	// 832C0738: 7C7A1B78  mr r26, r3
	ctx.r[26].u64 = ctx.r[3].u64;
	// 832C073C: 556B03FE  clrlwi r11, r11, 0xf
	ctx.r[11].u64 = ctx.r[11].u32 as u64 & 0x0001FFFFu64;
	// 832C0740: 7F0BF040  cmplw cr6, r11, r30
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832C0744: 4199000C  bgt cr6, 0x832c0750
	if ctx.cr[6].gt {
	pc = 0x832C0750; continue 'dispatch;
	}
	// 832C0748: 7F1EE840  cmplw cr6, r30, r29
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[29].u32, &mut ctx.xer);
	// 832C074C: 409A000C  bne cr6, 0x832c0758
	if !ctx.cr[6].eq {
	pc = 0x832C0758; continue 'dispatch;
	}
	// 832C0750: 7F6BDB78  mr r11, r27
	ctx.r[11].u64 = ctx.r[27].u64;
	// 832C0754: 48000018  b 0x832c076c
	pc = 0x832C076C; continue 'dispatch;
	// 832C0758: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C075C: 419A0010  beq cr6, 0x832c076c
	if ctx.cr[6].eq {
	pc = 0x832C076C; continue 'dispatch;
	}
	// 832C0760: 815F0060  lwz r10, 0x60(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C0764: 7D4A5A14  add r10, r10, r11
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C0768: 915F0060  stw r10, 0x60(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(96 as u32), ctx.r[10].u32 ) };
	// 832C076C: 3BA00001  li r29, 1
	ctx.r[29].s64 = 1;
	// 832C0770: 7FCBF050  subf r30, r11, r30
	ctx.r[30].s64 = ctx.r[30].s64 - ctx.r[11].s64;
	// 832C0774: 93BF0024  stw r29, 0x24(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(36 as u32), ctx.r[29].u32 ) };
	// 832C0778: 38E00000  li r7, 0
	ctx.r[7].s64 = 0;
	// 832C077C: 815F0074  lwz r10, 0x74(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(116 as u32) ) } as u64;
	// 832C0780: 38C10050  addi r6, r1, 0x50
	ctx.r[6].s64 = ctx.r[1].s64 + 80;
	// 832C0784: 7FC5F378  mr r5, r30
	ctx.r[5].u64 = ctx.r[30].u64;
	// 832C0788: 807F0054  lwz r3, 0x54(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(84 as u32) ) } as u64;
	// 832C078C: 7C8A5A14  add r4, r10, r11
	ctx.r[4].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C0790: 4BA02EF9  bl 0x82cc3688
	ctx.lr = 0x832C0794;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC3688);
	// 832C0794: 80610050  lwz r3, 0x50(r1)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C0798: 937F0024  stw r27, 0x24(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(36 as u32), ctx.r[27].u32 ) };
	// 832C079C: 7F03F040  cmplw cr6, r3, r30
	ctx.cr[6].compare_u32(ctx.r[3].u32, ctx.r[30].u32, &mut ctx.xer);
	// 832C07A0: 419A0008  beq cr6, 0x832c07a8
	if ctx.cr[6].eq {
	pc = 0x832C07A8; continue 'dispatch;
	}
	// 832C07A4: 93BF0020  stw r29, 0x20(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(32 as u32), ctx.r[29].u32 ) };
	// 832C07A8: 2B030000  cmplwi cr6, r3, 0
	ctx.cr[6].compare_u32(ctx.r[3].u32, 0 as u32, &mut ctx.xer);
	// 832C07AC: 419A010C  beq cr6, 0x832c08b8
	if ctx.cr[6].eq {
	pc = 0x832C08B8; continue 'dispatch;
	}
	// 832C07B0: 817F0028  lwz r11, 0x28(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C07B4: 7D6B1A14  add r11, r11, r3
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[3].u64;
	// 832C07B8: 917F0028  stw r11, 0x28(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(40 as u32), ctx.r[11].u32 ) };
	// 832C07BC: 817F0058  lwz r11, 0x58(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(88 as u32) ) } as u64;
	// 832C07C0: 7D4B1A14  add r10, r11, r3
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[3].u64;
	// 832C07C4: 915F0058  stw r10, 0x58(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(88 as u32), ctx.r[10].u32 ) };
	// 832C07C8: 813F0074  lwz r9, 0x74(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(116 as u32) ) } as u64;
	// 832C07CC: 7D09E214  add r8, r9, r28
	ctx.r[8].u64 = ctx.r[9].u64 + ctx.r[28].u64;
	// 832C07D0: 911F0074  stw r8, 0x74(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(116 as u32), ctx.r[8].u32 ) };
	// 832C07D4: 80FF0070  lwz r7, 0x70(r31)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(112 as u32) ) } as u64;
	// 832C07D8: 80DF0074  lwz r6, 0x74(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(116 as u32) ) } as u64;
	// 832C07DC: 7F063840  cmplw cr6, r6, r7
	ctx.cr[6].compare_u32(ctx.r[6].u32, ctx.r[7].u32, &mut ctx.xer);
	// 832C07E0: 4198000C  blt cr6, 0x832c07ec
	if ctx.cr[6].lt {
	pc = 0x832C07EC; continue 'dispatch;
	}
	// 832C07E4: 817F006C  lwz r11, 0x6c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(108 as u32) ) } as u64;
	// 832C07E8: 917F0074  stw r11, 0x74(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(116 as u32), ctx.r[11].u32 ) };
	// 832C07EC: 7D6300D0  neg r11, r3
	ctx.r[11].s64 = -ctx.r[3].s64;
	// 832C07F0: 7D0000A6  mfmsr r8
	ctx.r[8].u64 = ctx.msr;
	// 832C07F4: 7DA10164  mtmsrd r13, 1
	ctx.msr = (ctx.r[13].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C07F8: 7D40C828  lwarx r10, 0, r25
	// lwarx
	let ea = ctx.r[25].u32;
	ctx.reserved.u32 = unsafe { crate::rt::load_u32(base as *const u8, ea) };
	ctx.r[10].u64 = ctx.reserved.u32 as u64;
	// 832C07FC: 7D2B5214  add r9, r11, r10
	ctx.r[9].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832C0800: 7D20C92D  stwcx. r9, 0, r25
	// stwcx.
	let addr = ctx.r[25].u32;
	ctx.cr[0].lt = false;
	ctx.cr[0].gt = false;
	let ok = unsafe { crate::rt::stwcx32(base as *mut u8, addr, ctx.reserved.u32, ctx.r[9].u32) };
	ctx.cr[0].eq = ok;
	ctx.cr[0].so = ctx.xer.so;
	// 832C0804: 7D010164  mtmsrd r8, 1
	ctx.msr = (ctx.r[8].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C0808: 4082FFE8  bne 0x832c07f0
	if !ctx.cr[0].eq {
	pc = 0x832C07F0; continue 'dispatch;
	}
	// 832C080C: 80E10050  lwz r7, 0x50(r1)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C0810: 397F004C  addi r11, r31, 0x4c
	ctx.r[11].s64 = ctx.r[31].s64 + 76;
	// 832C0814: 7C8000A6  mfmsr r4
	ctx.r[4].u64 = ctx.msr;
	// 832C0818: 7DA10164  mtmsrd r13, 1
	ctx.msr = (ctx.r[13].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C081C: 7CC05828  lwarx r6, 0, r11
	// lwarx
	let ea = ctx.r[11].u32;
	ctx.reserved.u32 = unsafe { crate::rt::load_u32(base as *const u8, ea) };
	ctx.r[6].u64 = ctx.reserved.u32 as u64;
	// 832C0820: 7CA73214  add r5, r7, r6
	ctx.r[5].u64 = ctx.r[7].u64 + ctx.r[6].u64;
	// 832C0824: 7CA0592D  stwcx. r5, 0, r11
	// stwcx.
	let addr = ctx.r[11].u32;
	ctx.cr[0].lt = false;
	ctx.cr[0].gt = false;
	let ok = unsafe { crate::rt::stwcx32(base as *mut u8, addr, ctx.reserved.u32, ctx.r[5].u32) };
	ctx.cr[0].eq = ok;
	ctx.cr[0].so = ctx.xer.so;
	// 832C0828: 7C810164  mtmsrd r4, 1
	ctx.msr = (ctx.r[4].u32 & 0x8020) | (ctx.msr & !0x8020);
	// 832C082C: 4082FFE8  bne 0x832c0814
	if !ctx.cr[0].eq {
	pc = 0x832C0814; continue 'dispatch;
	}
	// 832C0830: 807F004C  lwz r3, 0x4c(r31)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(76 as u32) ) } as u64;
	// 832C0834: 815F0044  lwz r10, 0x44(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(68 as u32) ) } as u64;
	// 832C0838: 7F035040  cmplw cr6, r3, r10
	ctx.cr[6].compare_u32(ctx.r[3].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C083C: 4099000C  ble cr6, 0x832c0848
	if !ctx.cr[6].gt {
	pc = 0x832C0848; continue 'dispatch;
	}
	// 832C0840: 816B0000  lwz r11, 0(r11)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0844: 917F0044  stw r11, 0x44(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(68 as u32), ctx.r[11].u32 ) };
	// 832C0848: 817F0084  lwz r11, 0x84(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(132 as u32) ) } as u64;
	// 832C084C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C0850: 419A0014  beq cr6, 0x832c0864
	if ctx.cr[6].eq {
	pc = 0x832C0864; continue 'dispatch;
	}
	// 832C0854: 7F45D378  mr r5, r26
	ctx.r[5].u64 = ctx.r[26].u64;
	// 832C0858: 80810050  lwz r4, 0x50(r1)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C085C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C0860: 4BFFF8A9  bl 0x832c0108
	ctx.lr = 0x832C0864;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0108);
	// 832C0864: 48000255  bl 0x832c0ab8
	ctx.lr = 0x832C0868;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0AB8);
	// 832C0868: 815F0030  lwz r10, 0x30(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C086C: 7D7A1850  subf r11, r26, r3
	ctx.r[11].s64 = ctx.r[3].s64 - ctx.r[26].s64;
	// 832C0870: 2F180000  cmpwi cr6, r24, 0
	ctx.cr[6].compare_i32(ctx.r[24].s32, 0, &mut ctx.xer);
	// 832C0874: 7D4A5A14  add r10, r10, r11
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C0878: 915F0030  stw r10, 0x30(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(48 as u32), ctx.r[10].u32 ) };
	// 832C087C: 409A0020  bne cr6, 0x832c089c
	if !ctx.cr[6].eq {
	pc = 0x832C089C; continue 'dispatch;
	}
	// 832C0880: 815F002C  lwz r10, 0x2c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(44 as u32) ) } as u64;
	// 832C0884: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C0888: 409A0014  bne cr6, 0x832c089c
	if !ctx.cr[6].eq {
	pc = 0x832C089C; continue 'dispatch;
	}
	// 832C088C: 815F0038  lwz r10, 0x38(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(56 as u32) ) } as u64;
	// 832C0890: 7D6A5A14  add r11, r10, r11
	ctx.r[11].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C0894: 917F0038  stw r11, 0x38(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(56 as u32), ctx.r[11].u32 ) };
	// 832C0898: 4800001C  b 0x832c08b4
	pc = 0x832C08B4; continue 'dispatch;
	// 832C089C: 815F003C  lwz r10, 0x3c(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(60 as u32) ) } as u64;
	// 832C08A0: 7D6A5A14  add r11, r10, r11
	ctx.r[11].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C08A4: 917F003C  stw r11, 0x3c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(60 as u32), ctx.r[11].u32 ) };
	// 832C08A8: 4800000C  b 0x832c08b4
	pc = 0x832C08B4; continue 'dispatch;
	// 832C08AC: 817F004C  lwz r11, 0x4c(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(76 as u32) ) } as u64;
	// 832C08B0: 917F0048  stw r11, 0x48(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(72 as u32), ctx.r[11].u32 ) };
	// 832C08B4: 80610050  lwz r3, 0x50(r1)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C08B8: 817F00FC  lwz r11, 0xfc(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(252 as u32) ) } as u64;
	// 832C08BC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C08C0: 419A0038  beq cr6, 0x832c08f8
	if ctx.cr[6].eq {
	pc = 0x832C08F8; continue 'dispatch;
	}
	// 832C08C4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C08C8: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C08CC: 4E800421  bctrl
	ctx.lr = 0x832C08D0;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C08D0: 80610050  lwz r3, 0x50(r1)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C08D4: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C08D8: 4B9E8B70  b 0x82ca9448
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9448);
	return;
	// 832C08DC: 817F0100  lwz r11, 0x100(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(256 as u32) ) } as u64;
	// 832C08E0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C08E4: 419A0010  beq cr6, 0x832c08f4
	if ctx.cr[6].eq {
	pc = 0x832C08F4; continue 'dispatch;
	}
	// 832C08E8: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C08EC: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C08F0: 4E800421  bctrl
	ctx.lr = 0x832C08F4;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C08F4: 3860FFFF  li r3, -1
	ctx.r[3].s64 = -1;
	// 832C08F8: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C08FC: 4B9E8B4C  b 0x82ca9448
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9448);
	return;
}

pub fn sub_832C0900(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C0900 size=16
	// 832C0900: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C0904: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832C0908: FBE1FFF0  std r31, -0x10(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-16 as u32), ctx.r[31].u64 ) };
	// 832C090C: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
}

pub fn sub_832C09C0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C09C0 size=248
	// 832C09C0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C09C4: 4B9E8A49  bl 0x82ca940c
	ctx.lr = 0x832C09C8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA940C);
	// 832C09C8: 9421FF90  stwu r1, -0x70(r1)
	ea = ctx.r[1].u32.wrapping_add(-112 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C09CC: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832C09D0: 7CBD2B78  mr r29, r5
	ctx.r[29].u64 = ctx.r[5].u64;
	// 832C09D4: 38A00144  li r5, 0x144
	ctx.r[5].s64 = 324;
	// 832C09D8: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832C09DC: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832C09E0: 4B9E8FD1  bl 0x82ca99b0
	ctx.lr = 0x832C09E4;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832C09E4: 57AB0210  rlwinm r11, r29, 0, 8, 8
	ctx.r[11].u64 = ctx.r[29].u32 as u64 & 0xFFFFFFFFu64;
	// 832C09E8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C09EC: 419A0088  beq cr6, 0x832c0a74
	if ctx.cr[6].eq {
	pc = 0x832C0A74; continue 'dispatch;
	}
	// 832C09F0: 39600001  li r11, 1
	ctx.r[11].s64 = 1;
	// 832C09F4: 93DF0054  stw r30, 0x54(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(84 as u32), ctx.r[30].u32 ) };
	// 832C09F8: 38C00001  li r6, 1
	ctx.r[6].s64 = 1;
	// 832C09FC: 917F0078  stw r11, 0x78(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(120 as u32), ctx.r[11].u32 ) };
	// 832C0A00: 38A00000  li r5, 0
	ctx.r[5].s64 = 0;
	// 832C0A04: 38800000  li r4, 0
	ctx.r[4].s64 = 0;
	// 832C0A08: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832C0A0C: 4BA06F2D  bl 0x82cc7938
	ctx.lr = 0x832C0A10;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC7938);
	// 832C0A10: 907F007C  stw r3, 0x7c(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(124 as u32), ctx.r[3].u32 ) };
	// 832C0A14: 3D60832C  lis r11, -0x7cd4
	ctx.r[11].s64 = -2094268416;
	// 832C0A18: 3D40832C  lis r10, -0x7cd4
	ctx.r[10].s64 = -2094268416;
	// 832C0A1C: 3D20832C  lis r9, -0x7cd4
	ctx.r[9].s64 = -2094268416;
	// 832C0A20: 3D00832C  lis r8, -0x7cd4
	ctx.r[8].s64 = -2094268416;
	// 832C0A24: 3CE0832C  lis r7, -0x7cd4
	ctx.r[7].s64 = -2094268416;
	// 832C0A28: 388BFFF8  addi r4, r11, -8
	ctx.r[4].s64 = ctx.r[11].s64 + -8;
	// 832C0A2C: 3CC0832C  lis r6, -0x7cd4
	ctx.r[6].s64 = -2094268416;
	// 832C0A30: 3CA0832C  lis r5, -0x7cd4
	ctx.r[5].s64 = -2094268416;
	// 832C0A34: 909F0000  stw r4, 0(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), ctx.r[4].u32 ) };
	// 832C0A38: 386A01D0  addi r3, r10, 0x1d0
	ctx.r[3].s64 = ctx.r[10].s64 + 464;
	// 832C0A3C: 39690578  addi r11, r9, 0x578
	ctx.r[11].s64 = ctx.r[9].s64 + 1400;
	// 832C0A40: 394805A0  addi r10, r8, 0x5a0
	ctx.r[10].s64 = ctx.r[8].s64 + 1440;
	// 832C0A44: 907F0004  stw r3, 4(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), ctx.r[3].u32 ) };
	// 832C0A48: 39270698  addi r9, r7, 0x698
	ctx.r[9].s64 = ctx.r[7].s64 + 1688;
	// 832C0A4C: 917F0008  stw r11, 8(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), ctx.r[11].u32 ) };
	// 832C0A50: 39060630  addi r8, r6, 0x630
	ctx.r[8].s64 = ctx.r[6].s64 + 1584;
	// 832C0A54: 915F000C  stw r10, 0xc(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), ctx.r[10].u32 ) };
	// 832C0A58: 38E50900  addi r7, r5, 0x900
	ctx.r[7].s64 = ctx.r[5].s64 + 2304;
	// 832C0A5C: 913F0010  stw r9, 0x10(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(16 as u32), ctx.r[9].u32 ) };
	// 832C0A60: 911F0014  stw r8, 0x14(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(20 as u32), ctx.r[8].u32 ) };
	// 832C0A64: 38600001  li r3, 1
	ctx.r[3].s64 = 1;
	// 832C0A68: 90FF0018  stw r7, 0x18(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(24 as u32), ctx.r[7].u32 ) };
	// 832C0A6C: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832C0A70: 4B9E89EC  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
	// 832C0A74: 3D000800  lis r8, 0x800
	ctx.r[8].s64 = 134217728;
	// 832C0A78: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832C0A7C: 61080080  ori r8, r8, 0x80
	ctx.r[8].u64 = ctx.r[8].u64 | 128;
	// 832C0A80: 38E00003  li r7, 3
	ctx.r[7].s64 = 3;
	// 832C0A84: 38C00000  li r6, 0
	ctx.r[6].s64 = 0;
	// 832C0A88: 38A00001  li r5, 1
	ctx.r[5].s64 = 1;
	// 832C0A8C: 3C808000  lis r4, -0x8000
	ctx.r[4].s64 = -2147483648;
	// 832C0A90: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832C0A94: 4BA029FD  bl 0x82cc3490
	ctx.lr = 0x832C0A98;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CC3490);
	// 832C0A98: 907F0054  stw r3, 0x54(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(84 as u32), ctx.r[3].u32 ) };
	// 832C0A9C: 817F0054  lwz r11, 0x54(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(84 as u32) ) } as u64;
	// 832C0AA0: 2F0BFFFF  cmpwi cr6, r11, -1
	ctx.cr[6].compare_i32(ctx.r[11].s32, -1, &mut ctx.xer);
	// 832C0AA4: 409AFF70  bne cr6, 0x832c0a14
	if !ctx.cr[6].eq {
	pc = 0x832C0A14; continue 'dispatch;
	}
	// 832C0AA8: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832C0AAC: 38210070  addi r1, r1, 0x70
	ctx.r[1].s64 = ctx.r[1].s64 + 112;
	// 832C0AB0: 4B9E89AC  b 0x82ca945c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA945C);
	return;
}

pub fn sub_832C0AB8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C0AB8 size=12
	// 832C0AB8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C0ABC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832C0AC0: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
}

pub fn sub_832C0BB0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C0BB0 size=1036
	// 832C0BB0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C0BB4: 4B9E8851  bl 0x82ca9404
	ctx.lr = 0x832C0BB8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9404);
	// 832C0BB8: 9421FF60  stwu r1, -0xa0(r1)
	ea = ctx.r[1].u32.wrapping_add(-160 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C0BBC: 7C7B1B78  mr r27, r3
	ctx.r[27].u64 = ctx.r[3].u64;
	// 832C0BC0: 7CDC3378  mr r28, r6
	ctx.r[28].u64 = ctx.r[6].u64;
	// 832C0BC4: 817B0008  lwz r11, 8(r27)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C0BC8: 813B0000  lwz r9, 0(r27)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0BCC: 811B0004  lwz r8, 4(r27)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C0BD0: 2B0B0004  cmplwi cr6, r11, 4
	ctx.cr[6].compare_u32(ctx.r[11].u32, 4 as u32, &mut ctx.xer);
	// 832C0BD4: 40980028  bge cr6, 0x832c0bfc
	if !ctx.cr[6].lt {
	pc = 0x832C0BFC; continue 'dispatch;
	}
	// 832C0BD8: 81480000  lwz r10, 0(r8)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0BDC: 20EB0004  subfic r7, r11, 4
	ctx.xer.ca = ctx.r[11].u32 <= 4 as u32;
	ctx.r[7].s64 = (4 as i64) - ctx.r[11].s64;
	// 832C0BE0: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C0BE4: 7D465830  slw r6, r10, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[10].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0BE8: 7CC34B78  or r3, r6, r9
	ctx.r[3].u64 = ctx.r[6].u64 | ctx.r[9].u64;
	// 832C0BEC: 7D5E3C30  srw r30, r10, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0BF0: 546A073E  clrlwi r10, r3, 0x1c
	ctx.r[10].u64 = ctx.r[3].u32 as u64 & 0x0000000Fu64;
	// 832C0BF4: 3BEB001C  addi r31, r11, 0x1c
	ctx.r[31].s64 = ctx.r[11].s64 + 28;
	// 832C0BF8: 48000010  b 0x832c0c08
	pc = 0x832C0C08; continue 'dispatch;
	// 832C0BFC: 552A073E  clrlwi r10, r9, 0x1c
	ctx.r[10].u64 = ctx.r[9].u32 as u64 & 0x0000000Fu64;
	// 832C0C00: 553EE13E  srwi r30, r9, 4
	// 832C0C04: 3BEBFFFC  addi r31, r11, -4
	ctx.r[31].s64 = ctx.r[11].s64 + -4;
	// 832C0C08: 3D608217  lis r11, -0x7de9
	ctx.r[11].s64 = -2112421888;
	// 832C0C0C: 5547103A  slwi r7, r10, 2
	// 832C0C10: 396B8410  addi r11, r11, -0x7bf0
	ctx.r[11].s64 = ctx.r[11].s64 + -31728;
	// 832C0C14: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C0C18: 38CB0010  addi r6, r11, 0x10
	ctx.r[6].s64 = ctx.r[11].s64 + 16;
	// 832C0C1C: 392BFC30  addi r9, r11, -0x3d0
	ctx.r[9].s64 = ctx.r[11].s64 + -976;
	// 832C0C20: 7CE7302E  lwzx r7, r7, r6
	ctx.r[7].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[6].u32)) } as u64;
	// 832C0C24: 7C674A14  add r3, r7, r9
	ctx.r[3].u64 = ctx.r[7].u64 + ctx.r[9].u64;
	// 832C0C28: 90640000  stw r3, 0(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(0 as u32), ctx.r[3].u32 ) };
	// 832C0C2C: 7D6A58AE  lbzx r11, r10, r11
	ctx.r[11].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[11].u32)) } as u64;
	// 832C0C30: 91650000  stw r11, 0(r5)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[5].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832C0C34: 409A002C  bne cr6, 0x832c0c60
	if !ctx.cr[6].eq {
	pc = 0x832C0C60; continue 'dispatch;
	}
	// 832C0C38: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832C0C3C: 7D6BE1AE  stbx r11, r11, r28
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[11].u32.wrapping_add(ctx.r[28].u32), ctx.r[11].u8) };
	// 832C0C40: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832C0C44: 2B0B0010  cmplwi cr6, r11, 0x10
	ctx.cr[6].compare_u32(ctx.r[11].u32, 16 as u32, &mut ctx.xer);
	// 832C0C48: 4198FFF4  blt cr6, 0x832c0c3c
	if ctx.cr[6].lt {
	pc = 0x832C0C3C; continue 'dispatch;
	}
	// 832C0C4C: 911B0004  stw r8, 4(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(4 as u32), ctx.r[8].u32 ) };
	// 832C0C50: 93DB0000  stw r30, 0(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C0C54: 93FB0008  stw r31, 8(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C0C58: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C0C5C: 4B9E87F8  b 0x82ca9454
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9454);
	return;
	// 832C0C60: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832C0C64: 409A0018  bne cr6, 0x832c0c7c
	if !ctx.cr[6].eq {
	pc = 0x832C0C7C; continue 'dispatch;
	}
	// 832C0C68: 81280000  lwz r9, 0(r8)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0C6C: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C0C70: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C0C74: 552AF87E  srwi r10, r9, 1
	// 832C0C78: 48000010  b 0x832c0c88
	pc = 0x832C0C88; continue 'dispatch;
	// 832C0C7C: 7FC9F378  mr r9, r30
	ctx.r[9].u64 = ctx.r[30].u64;
	// 832C0C80: 57CAF87E  srwi r10, r30, 1
	// 832C0C84: 397FFFFF  addi r11, r31, -1
	ctx.r[11].s64 = ctx.r[31].s64 + -1;
	// 832C0C88: 552907FE  clrlwi r9, r9, 0x1f
	ctx.r[9].u64 = ctx.r[9].u32 as u64 & 0x00000001u64;
	// 832C0C8C: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832C0C90: 409A0648  bne cr6, 0x832c12d8
	if !ctx.cr[6].eq {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C12D8);
		return;
	}
	// 832C0C94: 2B0B0002  cmplwi cr6, r11, 2
	ctx.cr[6].compare_u32(ctx.r[11].u32, 2 as u32, &mut ctx.xer);
	// 832C0C98: 40980028  bge cr6, 0x832c0cc0
	if !ctx.cr[6].lt {
	pc = 0x832C0CC0; continue 'dispatch;
	}
	// 832C0C9C: 81280000  lwz r9, 0(r8)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0CA0: 20EB0002  subfic r7, r11, 2
	ctx.xer.ca = ctx.r[11].u32 <= 2 as u32;
	ctx.r[7].s64 = (2 as i64) - ctx.r[11].s64;
	// 832C0CA4: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C0CA8: 7D265830  slw r6, r9, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[9].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0CAC: 7CC55378  or r5, r6, r10
	ctx.r[5].u64 = ctx.r[6].u64 | ctx.r[10].u64;
	// 832C0CB0: 7D3E3C30  srw r30, r9, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[9].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0CB4: 54BD07BE  clrlwi r29, r5, 0x1e
	ctx.r[29].u64 = ctx.r[5].u32 as u64 & 0x00000003u64;
	// 832C0CB8: 3BEB001E  addi r31, r11, 0x1e
	ctx.r[31].s64 = ctx.r[11].s64 + 30;
	// 832C0CBC: 48000010  b 0x832c0ccc
	pc = 0x832C0CCC; continue 'dispatch;
	// 832C0CC0: 555D07BE  clrlwi r29, r10, 0x1e
	ctx.r[29].u64 = ctx.r[10].u32 as u64 & 0x00000003u64;
	// 832C0CC4: 555EF0BE  srwi r30, r10, 2
	// 832C0CC8: 3BEBFFFE  addi r31, r11, -2
	ctx.r[31].s64 = ctx.r[11].s64 + -2;
	// 832C0CCC: 2B1D0000  cmplwi cr6, r29, 0
	ctx.cr[6].compare_u32(ctx.r[29].u32, 0 as u32, &mut ctx.xer);
	// 832C0CD0: 39600001  li r11, 1
	ctx.r[11].s64 = 1;
	// 832C0CD4: 409A0078  bne cr6, 0x832c0d4c
	if !ctx.cr[6].eq {
	pc = 0x832C0D4C; continue 'dispatch;
	}
	// 832C0CD8: 393CFFFF  addi r9, r28, -1
	ctx.r[9].s64 = ctx.r[28].s64 + -1;
	// 832C0CDC: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832C0CE0: 409A0018  bne cr6, 0x832c0cf8
	if !ctx.cr[6].eq {
	pc = 0x832C0CF8; continue 'dispatch;
	}
	// 832C0CE4: 81480000  lwz r10, 0(r8)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0CE8: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C0CEC: 3BE0001F  li r31, 0x1f
	ctx.r[31].s64 = 31;
	// 832C0CF0: 555EF87E  srwi r30, r10, 1
	// 832C0CF4: 48000010  b 0x832c0d04
	pc = 0x832C0D04; continue 'dispatch;
	// 832C0CF8: 7FCAF378  mr r10, r30
	ctx.r[10].u64 = ctx.r[30].u64;
	// 832C0CFC: 57DEF87E  srwi r30, r30, 1
	// 832C0D00: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832C0D04: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C0D08: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C0D0C: 394B00FF  addi r10, r11, 0xff
	ctx.r[10].s64 = ctx.r[11].s64 + 255;
	// 832C0D10: 419A0010  beq cr6, 0x832c0d20
	if ctx.cr[6].eq {
	pc = 0x832C0D20; continue 'dispatch;
	}
	// 832C0D14: 7D6959AE  stbx r11, r9, r11
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[11].u32), ctx.r[11].u8) };
	// 832C0D18: 7D4BE1AE  stbx r10, r11, r28
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[11].u32.wrapping_add(ctx.r[28].u32), ctx.r[10].u8) };
	// 832C0D1C: 4800000C  b 0x832c0d28
	pc = 0x832C0D28; continue 'dispatch;
	// 832C0D20: 7D6BE1AE  stbx r11, r11, r28
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[11].u32.wrapping_add(ctx.r[28].u32), ctx.r[11].u8) };
	// 832C0D24: 7D4959AE  stbx r10, r9, r11
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[11].u32), ctx.r[10].u8) };
	// 832C0D28: 396B0002  addi r11, r11, 2
	ctx.r[11].s64 = ctx.r[11].s64 + 2;
	// 832C0D2C: 394BFFFF  addi r10, r11, -1
	ctx.r[10].s64 = ctx.r[11].s64 + -1;
	// 832C0D30: 2B0A0010  cmplwi cr6, r10, 0x10
	ctx.cr[6].compare_u32(ctx.r[10].u32, 16 as u32, &mut ctx.xer);
	// 832C0D34: 4198FFA8  blt cr6, 0x832c0cdc
	if ctx.cr[6].lt {
	pc = 0x832C0CDC; continue 'dispatch;
	}
	// 832C0D38: 911B0004  stw r8, 4(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(4 as u32), ctx.r[8].u32 ) };
	// 832C0D3C: 93DB0000  stw r30, 0(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C0D40: 93FB0008  stw r31, 8(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C0D44: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C0D48: 4B9E870C  b 0x82ca9454
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9454);
	return;
	// 832C0D4C: 39410050  addi r10, r1, 0x50
	ctx.r[10].s64 = ctx.r[1].s64 + 80;
	// 832C0D50: 39200000  li r9, 0
	ctx.r[9].s64 = 0;
	// 832C0D54: 38EAFFFF  addi r7, r10, -1
	ctx.r[7].s64 = ctx.r[10].s64 + -1;
	// 832C0D58: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832C0D5C: 409A0018  bne cr6, 0x832c0d74
	if !ctx.cr[6].eq {
	pc = 0x832C0D74; continue 'dispatch;
	}
	// 832C0D60: 81480000  lwz r10, 0(r8)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0D64: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C0D68: 3BE0001F  li r31, 0x1f
	ctx.r[31].s64 = 31;
	// 832C0D6C: 555EF87E  srwi r30, r10, 1
	// 832C0D70: 48000010  b 0x832c0d80
	pc = 0x832C0D80; continue 'dispatch;
	// 832C0D74: 7FCAF378  mr r10, r30
	ctx.r[10].u64 = ctx.r[30].u64;
	// 832C0D78: 57DEF87E  srwi r30, r30, 1
	// 832C0D7C: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832C0D80: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C0D84: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C0D88: 39410050  addi r10, r1, 0x50
	ctx.r[10].s64 = ctx.r[1].s64 + 80;
	// 832C0D8C: 419A0010  beq cr6, 0x832c0d9c
	if ctx.cr[6].eq {
	pc = 0x832C0D9C; continue 'dispatch;
	}
	// 832C0D90: 7D6759AE  stbx r11, r7, r11
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[7].u32.wrapping_add(ctx.r[11].u32), ctx.r[11].u8) };
	// 832C0D94: 7D2B51AE  stbx r9, r11, r10
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[11].u32.wrapping_add(ctx.r[10].u32), ctx.r[9].u8) };
	// 832C0D98: 4800000C  b 0x832c0da4
	pc = 0x832C0DA4; continue 'dispatch;
	// 832C0D9C: 7D2759AE  stbx r9, r7, r11
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[7].u32.wrapping_add(ctx.r[11].u32), ctx.r[9].u8) };
	// 832C0DA0: 7D6B51AE  stbx r11, r11, r10
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[11].u32.wrapping_add(ctx.r[10].u32), ctx.r[11].u8) };
	// 832C0DA4: 396B0002  addi r11, r11, 2
	ctx.r[11].s64 = ctx.r[11].s64 + 2;
	// 832C0DA8: 39290002  addi r9, r9, 2
	ctx.r[9].s64 = ctx.r[9].s64 + 2;
	// 832C0DAC: 2B0B0011  cmplwi cr6, r11, 0x11
	ctx.cr[6].compare_u32(ctx.r[11].u32, 17 as u32, &mut ctx.xer);
	// 832C0DB0: 4198FFA8  blt cr6, 0x832c0d58
	if ctx.cr[6].lt {
	pc = 0x832C0D58; continue 'dispatch;
	}
	// 832C0DB4: 2B1D0001  cmplwi cr6, r29, 1
	ctx.cr[6].compare_u32(ctx.r[29].u32, 1 as u32, &mut ctx.xer);
	// 832C0DB8: 409A01A8  bne cr6, 0x832c0f60
	if !ctx.cr[6].eq {
	pc = 0x832C0F60; continue 'dispatch;
	}
	// 832C0DBC: 2B1F0003  cmplwi cr6, r31, 3
	ctx.cr[6].compare_u32(ctx.r[31].u32, 3 as u32, &mut ctx.xer);
	// 832C0DC0: 40980018  bge cr6, 0x832c0dd8
	if !ctx.cr[6].lt {
	pc = 0x832C0DD8; continue 'dispatch;
	}
	// 832C0DC4: 81680000  lwz r11, 0(r8)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0DC8: 7D6AF830  slw r10, r11, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[11].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0DCC: 7D49F378  or r9, r10, r30
	ctx.r[9].u64 = ctx.r[10].u64 | ctx.r[30].u64;
	// 832C0DD0: 5523077E  clrlwi r3, r9, 0x1d
	ctx.r[3].u64 = ctx.r[9].u32 as u64 & 0x00000007u64;
	// 832C0DD4: 48000008  b 0x832c0ddc
	pc = 0x832C0DDC; continue 'dispatch;
	// 832C0DD8: 57C3077E  clrlwi r3, r30, 0x1d
	ctx.r[3].u64 = ctx.r[30].u32 as u64 & 0x00000007u64;
	// 832C0DDC: 38E00002  li r7, 2
	ctx.r[7].s64 = 2;
	// 832C0DE0: 38C10052  addi r6, r1, 0x52
	ctx.r[6].s64 = ctx.r[1].s64 + 82;
	// 832C0DE4: 38A10050  addi r5, r1, 0x50
	ctx.r[5].s64 = ctx.r[1].s64 + 80;
	// 832C0DE8: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832C0DEC: 4BFFFD2D  bl 0x832c0b18
	ctx.lr = 0x832C0DF0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0B18);
	// 832C0DF0: 7F1F1840  cmplw cr6, r31, r3
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[3].u32, &mut ctx.xer);
	// 832C0DF4: 40980020  bge cr6, 0x832c0e14
	if !ctx.cr[6].lt {
	pc = 0x832C0E14; continue 'dispatch;
	}
	// 832C0DF8: 81480000  lwz r10, 0(r8)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0DFC: 7D3F1850  subf r9, r31, r3
	ctx.r[9].s64 = ctx.r[3].s64 - ctx.r[31].s64;
	// 832C0E00: 7D63F850  subf r11, r3, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[3].s64;
	// 832C0E04: 7D5E4C30  srw r30, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0E08: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C0E0C: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C0E10: 4800000C  b 0x832c0e1c
	pc = 0x832C0E1C; continue 'dispatch;
	// 832C0E14: 7FDE1C30  srw r30, r30, r3
	if (ctx.r[3].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[3].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0E18: 7FE3F850  subf r31, r3, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[3].s64;
	// 832C0E1C: 2B1F0003  cmplwi cr6, r31, 3
	ctx.cr[6].compare_u32(ctx.r[31].u32, 3 as u32, &mut ctx.xer);
	// 832C0E20: 40980018  bge cr6, 0x832c0e38
	if !ctx.cr[6].lt {
	pc = 0x832C0E38; continue 'dispatch;
	}
	// 832C0E24: 81680000  lwz r11, 0(r8)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0E28: 7D6AF830  slw r10, r11, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[11].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0E2C: 7D49F378  or r9, r10, r30
	ctx.r[9].u64 = ctx.r[10].u64 | ctx.r[30].u64;
	// 832C0E30: 5523077E  clrlwi r3, r9, 0x1d
	ctx.r[3].u64 = ctx.r[9].u32 as u64 & 0x00000007u64;
	// 832C0E34: 48000008  b 0x832c0e3c
	pc = 0x832C0E3C; continue 'dispatch;
	// 832C0E38: 57C3077E  clrlwi r3, r30, 0x1d
	ctx.r[3].u64 = ctx.r[30].u32 as u64 & 0x00000007u64;
	// 832C0E3C: 38E00002  li r7, 2
	ctx.r[7].s64 = 2;
	// 832C0E40: 38C10056  addi r6, r1, 0x56
	ctx.r[6].s64 = ctx.r[1].s64 + 86;
	// 832C0E44: 38A10054  addi r5, r1, 0x54
	ctx.r[5].s64 = ctx.r[1].s64 + 84;
	// 832C0E48: 389C0004  addi r4, r28, 4
	ctx.r[4].s64 = ctx.r[28].s64 + 4;
	// 832C0E4C: 4BFFFCCD  bl 0x832c0b18
	ctx.lr = 0x832C0E50;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0B18);
	// 832C0E50: 7F1F1840  cmplw cr6, r31, r3
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[3].u32, &mut ctx.xer);
	// 832C0E54: 40980020  bge cr6, 0x832c0e74
	if !ctx.cr[6].lt {
	pc = 0x832C0E74; continue 'dispatch;
	}
	// 832C0E58: 81480000  lwz r10, 0(r8)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0E5C: 7D3F1850  subf r9, r31, r3
	ctx.r[9].s64 = ctx.r[3].s64 - ctx.r[31].s64;
	// 832C0E60: 7D63F850  subf r11, r3, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[3].s64;
	// 832C0E64: 7D5E4C30  srw r30, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0E68: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C0E6C: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C0E70: 4800000C  b 0x832c0e7c
	pc = 0x832C0E7C; continue 'dispatch;
	// 832C0E74: 7FDE1C30  srw r30, r30, r3
	if (ctx.r[3].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[3].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0E78: 7FE3F850  subf r31, r3, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[3].s64;
	// 832C0E7C: 2B1F0003  cmplwi cr6, r31, 3
	ctx.cr[6].compare_u32(ctx.r[31].u32, 3 as u32, &mut ctx.xer);
	// 832C0E80: 40980018  bge cr6, 0x832c0e98
	if !ctx.cr[6].lt {
	pc = 0x832C0E98; continue 'dispatch;
	}
	// 832C0E84: 81680000  lwz r11, 0(r8)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0E88: 7D6AF830  slw r10, r11, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[11].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0E8C: 7D49F378  or r9, r10, r30
	ctx.r[9].u64 = ctx.r[10].u64 | ctx.r[30].u64;
	// 832C0E90: 5523077E  clrlwi r3, r9, 0x1d
	ctx.r[3].u64 = ctx.r[9].u32 as u64 & 0x00000007u64;
	// 832C0E94: 48000008  b 0x832c0e9c
	pc = 0x832C0E9C; continue 'dispatch;
	// 832C0E98: 57C3077E  clrlwi r3, r30, 0x1d
	ctx.r[3].u64 = ctx.r[30].u32 as u64 & 0x00000007u64;
	// 832C0E9C: 38E00002  li r7, 2
	ctx.r[7].s64 = 2;
	// 832C0EA0: 38C1005A  addi r6, r1, 0x5a
	ctx.r[6].s64 = ctx.r[1].s64 + 90;
	// 832C0EA4: 38A10058  addi r5, r1, 0x58
	ctx.r[5].s64 = ctx.r[1].s64 + 88;
	// 832C0EA8: 389C0008  addi r4, r28, 8
	ctx.r[4].s64 = ctx.r[28].s64 + 8;
	// 832C0EAC: 4BFFFC6D  bl 0x832c0b18
	ctx.lr = 0x832C0EB0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0B18);
	// 832C0EB0: 7F1F1840  cmplw cr6, r31, r3
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[3].u32, &mut ctx.xer);
	// 832C0EB4: 40980020  bge cr6, 0x832c0ed4
	if !ctx.cr[6].lt {
	pc = 0x832C0ED4; continue 'dispatch;
	}
	// 832C0EB8: 81480000  lwz r10, 0(r8)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0EBC: 7D3F1850  subf r9, r31, r3
	ctx.r[9].s64 = ctx.r[3].s64 - ctx.r[31].s64;
	// 832C0EC0: 7D63F850  subf r11, r3, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[3].s64;
	// 832C0EC4: 7D5E4C30  srw r30, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0EC8: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C0ECC: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C0ED0: 4800000C  b 0x832c0edc
	pc = 0x832C0EDC; continue 'dispatch;
	// 832C0ED4: 7FDE1C30  srw r30, r30, r3
	if (ctx.r[3].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[3].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0ED8: 7FE3F850  subf r31, r3, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[3].s64;
	// 832C0EDC: 2B1F0003  cmplwi cr6, r31, 3
	ctx.cr[6].compare_u32(ctx.r[31].u32, 3 as u32, &mut ctx.xer);
	// 832C0EE0: 40980018  bge cr6, 0x832c0ef8
	if !ctx.cr[6].lt {
	pc = 0x832C0EF8; continue 'dispatch;
	}
	// 832C0EE4: 81680000  lwz r11, 0(r8)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0EE8: 7D6AF830  slw r10, r11, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[11].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0EEC: 7D49F378  or r9, r10, r30
	ctx.r[9].u64 = ctx.r[10].u64 | ctx.r[30].u64;
	// 832C0EF0: 5523077E  clrlwi r3, r9, 0x1d
	ctx.r[3].u64 = ctx.r[9].u32 as u64 & 0x00000007u64;
	// 832C0EF4: 48000008  b 0x832c0efc
	pc = 0x832C0EFC; continue 'dispatch;
	// 832C0EF8: 57C3077E  clrlwi r3, r30, 0x1d
	ctx.r[3].u64 = ctx.r[30].u32 as u64 & 0x00000007u64;
	// 832C0EFC: 38E00002  li r7, 2
	ctx.r[7].s64 = 2;
	// 832C0F00: 38C1005E  addi r6, r1, 0x5e
	ctx.r[6].s64 = ctx.r[1].s64 + 94;
	// 832C0F04: 38A1005C  addi r5, r1, 0x5c
	ctx.r[5].s64 = ctx.r[1].s64 + 92;
	// 832C0F08: 389C000C  addi r4, r28, 0xc
	ctx.r[4].s64 = ctx.r[28].s64 + 12;
	// 832C0F0C: 4BFFFC0D  bl 0x832c0b18
	ctx.lr = 0x832C0F10;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0B18);
	// 832C0F10: 7F1F1840  cmplw cr6, r31, r3
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[3].u32, &mut ctx.xer);
	// 832C0F14: 40980030  bge cr6, 0x832c0f44
	if !ctx.cr[6].lt {
	pc = 0x832C0F44; continue 'dispatch;
	}
	// 832C0F18: 81480000  lwz r10, 0(r8)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0F1C: 7D3F1850  subf r9, r31, r3
	ctx.r[9].s64 = ctx.r[3].s64 - ctx.r[31].s64;
	// 832C0F20: 7D63F850  subf r11, r3, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[3].s64;
	// 832C0F24: 7D5E4C30  srw r30, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0F28: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C0F2C: 93DB0000  stw r30, 0(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C0F30: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C0F34: 93FB0008  stw r31, 8(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C0F38: 911B0004  stw r8, 4(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(4 as u32), ctx.r[8].u32 ) };
	// 832C0F3C: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C0F40: 4B9E8514  b 0x82ca9454
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9454);
	return;
	// 832C0F44: 7FDE1C30  srw r30, r30, r3
	if (ctx.r[3].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[3].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0F48: 911B0004  stw r8, 4(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(4 as u32), ctx.r[8].u32 ) };
	// 832C0F4C: 7FE3F850  subf r31, r3, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[3].s64;
	// 832C0F50: 93DB0000  stw r30, 0(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C0F54: 93FB0008  stw r31, 8(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C0F58: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C0F5C: 4B9E84F8  b 0x82ca9454
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9454);
	return;
	// 832C0F60: 2B1F0003  cmplwi cr6, r31, 3
	ctx.cr[6].compare_u32(ctx.r[31].u32, 3 as u32, &mut ctx.xer);
	// 832C0F64: 40980018  bge cr6, 0x832c0f7c
	if !ctx.cr[6].lt {
	pc = 0x832C0F7C; continue 'dispatch;
	}
	// 832C0F68: 81680000  lwz r11, 0(r8)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0F6C: 7D6AF830  slw r10, r11, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[11].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0F70: 7D49F378  or r9, r10, r30
	ctx.r[9].u64 = ctx.r[10].u64 | ctx.r[30].u64;
	// 832C0F74: 5523077E  clrlwi r3, r9, 0x1d
	ctx.r[3].u64 = ctx.r[9].u32 as u64 & 0x00000007u64;
	// 832C0F78: 48000008  b 0x832c0f80
	pc = 0x832C0F80; continue 'dispatch;
	// 832C0F7C: 57C3077E  clrlwi r3, r30, 0x1d
	ctx.r[3].u64 = ctx.r[30].u32 as u64 & 0x00000007u64;
	// 832C0F80: 38E00002  li r7, 2
	ctx.r[7].s64 = 2;
	// 832C0F84: 38C10052  addi r6, r1, 0x52
	ctx.r[6].s64 = ctx.r[1].s64 + 82;
	// 832C0F88: 38A10050  addi r5, r1, 0x50
	ctx.r[5].s64 = ctx.r[1].s64 + 80;
	// 832C0F8C: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C0F90: 4BFFFB89  bl 0x832c0b18
	ctx.lr = 0x832C0F94;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0B18);
	// 832C0F94: 7F1F1840  cmplw cr6, r31, r3
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[3].u32, &mut ctx.xer);
	// 832C0F98: 40980020  bge cr6, 0x832c0fb8
	if !ctx.cr[6].lt {
	pc = 0x832C0FB8; continue 'dispatch;
	}
	// 832C0F9C: 81480000  lwz r10, 0(r8)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C0FA0: 7D3F1850  subf r9, r31, r3
	ctx.r[9].s64 = ctx.r[3].s64 - ctx.r[31].s64;
	// 832C0FA4: 7D63F850  subf r11, r3, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[3].s64;
	// 832C0FA8: 7D5E4C30  srw r30, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C0FAC: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C0FB0: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C0FB4: 4800000C  b 0x832c0fc0
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0FC0);
	return;
	// 832C0FB8: 7FDE1C30  srw r30, r30, r3
	if (ctx.r[3].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[3].u8 & 0x1F) as u32)) as u64;
	}
}

pub fn sub_832C13B8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C13B8 size=760
	// 832C13B8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C13BC: 4B9E8039  bl 0x82ca93f4
	ctx.lr = 0x832C13C0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93F4);
	// 832C13C0: 9421FF60  stwu r1, -0xa0(r1)
	ea = ctx.r[1].u32.wrapping_add(-160 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C13C4: 81630000  lwz r11, 0(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C13C8: 7C982378  mr r24, r4
	ctx.r[24].u64 = ctx.r[4].u64;
	// 832C13CC: 81430004  lwz r10, 4(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C13D0: 7CB72B78  mr r23, r5
	ctx.r[23].u64 = ctx.r[5].u64;
	// 832C13D4: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C13D8: 409A02D0  bne cr6, 0x832c16a8
	if !ctx.cr[6].eq {
	pc = 0x832C16A8; continue 'dispatch;
	}
	// 832C13DC: 81630028  lwz r11, 0x28(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C13E0: 38C0FFFF  li r6, -1
	ctx.r[6].s64 = -1;
	// 832C13E4: 81580008  lwz r10, 8(r24)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C13E8: 81380000  lwz r9, 0(r24)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C13EC: 83B80004  lwz r29, 4(r24)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C13F0: 7F0A5840  cmplw cr6, r10, r11
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C13F4: 40980030  bge cr6, 0x832c1424
	if !ctx.cr[6].lt {
	pc = 0x832C1424; continue 'dispatch;
	}
	// 832C13F8: 811D0000  lwz r8, 0(r29)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C13FC: 20EB0020  subfic r7, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[7].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C1400: 7C8A5850  subf r4, r10, r11
	ctx.r[4].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C1404: 7D055030  slw r5, r8, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[5].u64 = 0;
	} else {
		ctx.r[5].u64 = ((ctx.r[8].u32) << ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1408: 7D6B5050  subf r11, r11, r10
	ctx.r[11].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C140C: 7CA94B78  or r9, r5, r9
	ctx.r[9].u64 = ctx.r[5].u64 | ctx.r[9].u64;
	// 832C1410: 7CC73C30  srw r7, r6, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[6].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1414: 7D1E2430  srw r30, r8, r4
	if (ctx.r[4].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[8].u32) >> ((ctx.r[4].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1418: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C141C: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C1420: 48000014  b 0x832c1434
	pc = 0x832C1434; continue 'dispatch;
	// 832C1424: 210B0020  subfic r8, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[8].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C1428: 7FEB5050  subf r31, r11, r10
	ctx.r[31].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C142C: 7CC74430  srw r7, r6, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[6].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1430: 7D3E5C30  srw r30, r9, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[9].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1434: 7CE54838  and r5, r7, r9
	ctx.r[5].u64 = ctx.r[7].u64 & ctx.r[9].u64;
	// 832C1438: 2B050000  cmplwi cr6, r5, 0
	ctx.cr[6].compare_u32(ctx.r[5].u32, 0 as u32, &mut ctx.xer);
	// 832C143C: 419A0250  beq cr6, 0x832c168c
	if ctx.cr[6].eq {
	pc = 0x832C168C; continue 'dispatch;
	}
	// 832C1440: 8163002C  lwz r11, 0x2c(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) } as u64;
	// 832C1444: 7F055840  cmplw cr6, r5, r11
	ctx.cr[6].compare_u32(ctx.r[5].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C1448: 41990244  bgt cr6, 0x832c168c
	if ctx.cr[6].gt {
	pc = 0x832C168C; continue 'dispatch;
	}
	// 832C144C: 81630030  lwz r11, 0x30(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C1450: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832C1454: 7D4B2A14  add r10, r11, r5
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[5].u64;
	// 832C1458: 91430004  stw r10, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[10].u32 ) };
	// 832C145C: 91630000  stw r11, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832C1460: 409A0018  bne cr6, 0x832c1478
	if !ctx.cr[6].eq {
	pc = 0x832C1478; continue 'dispatch;
	}
	// 832C1464: 815D0000  lwz r10, 0(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1468: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C146C: 3BE0001F  li r31, 0x1f
	ctx.r[31].s64 = 31;
	// 832C1470: 555EF87E  srwi r30, r10, 1
	// 832C1474: 48000010  b 0x832c1484
	pc = 0x832C1484; continue 'dispatch;
	// 832C1478: 7FCAF378  mr r10, r30
	ctx.r[10].u64 = ctx.r[30].u64;
	// 832C147C: 57DEF87E  srwi r30, r30, 1
	// 832C1480: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832C1484: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C1488: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C148C: 409A0194  bne cr6, 0x832c1620
	if !ctx.cr[6].eq {
	pc = 0x832C1620; continue 'dispatch;
	}
	// 832C1490: 3D408217  lis r10, -0x7de9
	ctx.r[10].s64 = -2112421888;
	// 832C1494: 80830020  lwz r4, 0x20(r3)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C1498: 83830024  lwz r28, 0x24(r3)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(36 as u32) ) } as u64;
	// 832C149C: 3B630010  addi r27, r3, 0x10
	ctx.r[27].s64 = ctx.r[3].s64 + 16;
	// 832C14A0: 3B400000  li r26, 0
	ctx.r[26].s64 = 0;
	// 832C14A4: 3B2A8460  addi r25, r10, -0x7ba0
	ctx.r[25].s64 = ctx.r[10].s64 + -31648;
	// 832C14A8: 7F1F2040  cmplw cr6, r31, r4
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[4].u32, &mut ctx.xer);
	// 832C14AC: 4198002C  blt cr6, 0x832c14d8
	if ctx.cr[6].lt {
	pc = 0x832C14D8; continue 'dispatch;
	}
	// 832C14B0: 21440020  subfic r10, r4, 0x20
	ctx.xer.ca = ctx.r[4].u32 <= 32 as u32;
	ctx.r[10].s64 = (32 as i64) - ctx.r[4].s64;
	// 832C14B4: 7CC95430  srw r9, r6, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[6].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C14B8: 7D28F038  and r8, r9, r30
	ctx.r[8].u64 = ctx.r[9].u64 & ctx.r[30].u64;
	// 832C14BC: 7CE8E0AE  lbzx r7, r8, r28
	ctx.r[7].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[28].u32)) } as u64;
	// 832C14C0: 54EA073E  clrlwi r10, r7, 0x1c
	ctx.r[10].u64 = ctx.r[7].u32 as u64 & 0x0000000Fu64;
	// 832C14C4: 54E9E13E  srwi r9, r7, 4
	// 832C14C8: 7FDE4C30  srw r30, r30, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C14CC: 7D0AD8AE  lbzx r8, r10, r27
	ctx.r[8].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C14D0: 7FE9F850  subf r31, r9, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[9].s64;
	// 832C14D4: 4800005C  b 0x832c1530
	pc = 0x832C1530; continue 'dispatch;
	// 832C14D8: 7F1DB840  cmplw cr6, r29, r23
	ctx.cr[6].compare_u32(ctx.r[29].u32, ctx.r[23].u32, &mut ctx.xer);
	// 832C14DC: 409801B0  bge cr6, 0x832c168c
	if !ctx.cr[6].lt {
	pc = 0x832C168C; continue 'dispatch;
	}
	// 832C14E0: 813D0000  lwz r9, 0(r29)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C14E4: 21440020  subfic r10, r4, 0x20
	ctx.xer.ca = ctx.r[4].u32 <= 32 as u32;
	ctx.r[10].s64 = (32 as i64) - ctx.r[4].s64;
	// 832C14E8: 7D28F830  slw r8, r9, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[8].u64 = 0;
	} else {
		ctx.r[8].u64 = ((ctx.r[9].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C14EC: 7CC75430  srw r7, r6, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[6].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C14F0: 7D0AF378  or r10, r8, r30
	ctx.r[10].u64 = ctx.r[8].u64 | ctx.r[30].u64;
	// 832C14F4: 7CE85038  and r8, r7, r10
	ctx.r[8].u64 = ctx.r[7].u64 & ctx.r[10].u64;
	// 832C14F8: 7CE8E0AE  lbzx r7, r8, r28
	ctx.r[7].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[28].u32)) } as u64;
	// 832C14FC: 54E8073E  clrlwi r8, r7, 0x1c
	ctx.r[8].u64 = ctx.r[7].u32 as u64 & 0x0000000Fu64;
	// 832C1500: 54EAE13E  srwi r10, r7, 4
	// 832C1504: 7F1F5040  cmplw cr6, r31, r10
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C1508: 7D08D8AE  lbzx r8, r8, r27
	ctx.r[8].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C150C: 41980010  blt cr6, 0x832c151c
	if ctx.cr[6].lt {
	pc = 0x832C151C; continue 'dispatch;
	}
	// 832C1510: 7FDE5430  srw r30, r30, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1514: 7FEAF850  subf r31, r10, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[10].s64;
	// 832C1518: 48000018  b 0x832c1530
	pc = 0x832C1530; continue 'dispatch;
	// 832C151C: 7CFF5050  subf r7, r31, r10
	ctx.r[7].s64 = ctx.r[10].s64 - ctx.r[31].s64;
	// 832C1520: 7D4AF850  subf r10, r10, r31
	ctx.r[10].s64 = ctx.r[31].s64 - ctx.r[10].s64;
	// 832C1524: 7D3E3C30  srw r30, r9, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[9].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1528: 3BEA0020  addi r31, r10, 0x20
	ctx.r[31].s64 = ctx.r[10].s64 + 32;
	// 832C152C: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C1530: 550A063E  clrlwi r10, r8, 0x18
	ctx.r[10].u64 = ctx.r[8].u32 as u64 & 0x000000FFu64;
	// 832C1534: 2B0A000C  cmplwi cr6, r10, 0xc
	ctx.cr[6].compare_u32(ctx.r[10].u32, 12 as u32, &mut ctx.xer);
	// 832C1538: 419800BC  blt cr6, 0x832c15f4
	if ctx.cr[6].lt {
	pc = 0x832C15F4; continue 'dispatch;
	}
	// 832C153C: 5749402E  slwi r9, r26, 8
	// 832C1540: 7D4ACA14  add r10, r10, r25
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[25].u64;
	// 832C1544: 7D28D378  or r8, r9, r26
	ctx.r[8].u64 = ctx.r[9].u64 | ctx.r[26].u64;
	// 832C1548: 5509801E  slwi r9, r8, 0x10
	// 832C154C: 894AFFF4  lbz r10, -0xc(r10)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832C1550: 7D284378  or r8, r9, r8
	ctx.r[8].u64 = ctx.r[9].u64 | ctx.r[8].u64;
	// 832C1554: 7F0A2840  cmplw cr6, r10, r5
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[5].u32, &mut ctx.xer);
	// 832C1558: 41990144  bgt cr6, 0x832c169c
	if ctx.cr[6].gt {
	pc = 0x832C169C; continue 'dispatch;
	}
	// 832C155C: 556907BE  clrlwi r9, r11, 0x1e
	ctx.r[9].u64 = ctx.r[11].u32 as u64 & 0x00000003u64;
	// 832C1560: 7CAA2850  subf r5, r10, r5
	ctx.r[5].s64 = ctx.r[5].s64 - ctx.r[10].s64;
	// 832C1564: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832C1568: 419A0070  beq cr6, 0x832c15d8
	if ctx.cr[6].eq {
	pc = 0x832C15D8; continue 'dispatch;
	}
	// 832C156C: 5507063E  clrlwi r7, r8, 0x18
	ctx.r[7].u64 = ctx.r[8].u32 as u64 & 0x000000FFu64;
	// 832C1570: 394A00FF  addi r10, r10, 0xff
	ctx.r[10].s64 = ctx.r[10].s64 + 255;
	// 832C1574: 98EB0000  stb r7, 0(r11)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[7].u8 ) };
	// 832C1578: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832C157C: 554A063E  clrlwi r10, r10, 0x18
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x000000FFu64;
	// 832C1580: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C1584: 409AFFEC  bne cr6, 0x832c1570
	if !ctx.cr[6].eq {
	pc = 0x832C1570; continue 'dispatch;
	}
	// 832C1588: 5549063E  clrlwi r9, r10, 0x18
	ctx.r[9].u64 = ctx.r[10].u32 as u64 & 0x000000FFu64;
	// 832C158C: 2B090004  cmplwi cr6, r9, 4
	ctx.cr[6].compare_u32(ctx.r[9].u32, 4 as u32, &mut ctx.xer);
	// 832C1590: 41980020  blt cr6, 0x832c15b0
	if ctx.cr[6].lt {
	pc = 0x832C15B0; continue 'dispatch;
	}
	// 832C1594: 394900FC  addi r10, r9, 0xfc
	ctx.r[10].s64 = ctx.r[9].s64 + 252;
	// 832C1598: 910B0000  stw r8, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[8].u32 ) };
	// 832C159C: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832C15A0: 554A063E  clrlwi r10, r10, 0x18
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x000000FFu64;
	// 832C15A4: 7D495378  mr r9, r10
	ctx.r[9].u64 = ctx.r[10].u64;
	// 832C15A8: 2B0A0004  cmplwi cr6, r10, 4
	ctx.cr[6].compare_u32(ctx.r[10].u32, 4 as u32, &mut ctx.xer);
	// 832C15AC: 4098FFE8  bge cr6, 0x832c1594
	if !ctx.cr[6].lt {
	pc = 0x832C1594; continue 'dispatch;
	}
	// 832C15B0: 554A063E  clrlwi r10, r10, 0x18
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x000000FFu64;
	// 832C15B4: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C15B8: 419A004C  beq cr6, 0x832c1604
	if ctx.cr[6].eq {
	pc = 0x832C1604; continue 'dispatch;
	}
	// 832C15BC: 394A00FF  addi r10, r10, 0xff
	ctx.r[10].s64 = ctx.r[10].s64 + 255;
	// 832C15C0: 98EB0000  stb r7, 0(r11)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[7].u8 ) };
	// 832C15C4: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832C15C8: 554A063E  clrlwi r10, r10, 0x18
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x000000FFu64;
	// 832C15CC: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C15D0: 409AFFEC  bne cr6, 0x832c15bc
	if !ctx.cr[6].eq {
	pc = 0x832C15BC; continue 'dispatch;
	}
	// 832C15D4: 48000030  b 0x832c1604
	pc = 0x832C1604; continue 'dispatch;
	// 832C15D8: 394A00FC  addi r10, r10, 0xfc
	ctx.r[10].s64 = ctx.r[10].s64 + 252;
	// 832C15DC: 910B0000  stw r8, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[8].u32 ) };
	// 832C15E0: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832C15E4: 554A063E  clrlwi r10, r10, 0x18
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x000000FFu64;
	// 832C15E8: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C15EC: 409AFFEC  bne cr6, 0x832c15d8
	if !ctx.cr[6].eq {
	pc = 0x832C15D8; continue 'dispatch;
	}
	// 832C15F0: 48000014  b 0x832c1604
	pc = 0x832C1604; continue 'dispatch;
	// 832C15F4: 990B0000  stb r8, 0(r11)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[8].u8 ) };
	// 832C15F8: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832C15FC: 7D5A5378  mr r26, r10
	ctx.r[26].u64 = ctx.r[10].u64;
	// 832C1600: 38A5FFFF  addi r5, r5, -1
	ctx.r[5].s64 = ctx.r[5].s64 + -1;
	// 832C1604: 2B050000  cmplwi cr6, r5, 0
	ctx.cr[6].compare_u32(ctx.r[5].u32, 0 as u32, &mut ctx.xer);
	// 832C1608: 409AFEA0  bne cr6, 0x832c14a8
	if !ctx.cr[6].eq {
	pc = 0x832C14A8; continue 'dispatch;
	}
	// 832C160C: 93B80004  stw r29, 4(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C1610: 93D80000  stw r30, 0(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1614: 93F80008  stw r31, 8(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C1618: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C161C: 4B9E7E28  b 0x82ca9444
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9444);
	return;
	// 832C1620: 2B1F0004  cmplwi cr6, r31, 4
	ctx.cr[6].compare_u32(ctx.r[31].u32, 4 as u32, &mut ctx.xer);
	// 832C1624: 40980040  bge cr6, 0x832c1664
	if !ctx.cr[6].lt {
	pc = 0x832C1664; continue 'dispatch;
	}
	// 832C1628: 815D0000  lwz r10, 0(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C162C: 213F0004  subfic r9, r31, 4
	ctx.xer.ca = ctx.r[31].u32 <= 4 as u32;
	ctx.r[9].s64 = (4 as i64) - ctx.r[31].s64;
	// 832C1630: 7D635B78  mr r3, r11
	ctx.r[3].u64 = ctx.r[11].u64;
	// 832C1634: 7D48F830  slw r8, r10, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[8].u64 = 0;
	} else {
		ctx.r[8].u64 = ((ctx.r[10].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1638: 7D07F378  or r7, r8, r30
	ctx.r[7].u64 = ctx.r[8].u64 | ctx.r[30].u64;
	// 832C163C: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C1640: 54E4073E  clrlwi r4, r7, 0x1c
	ctx.r[4].u64 = ctx.r[7].u32 as u64 & 0x0000000Fu64;
	// 832C1644: 7D5E4C30  srw r30, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1648: 3BFF001C  addi r31, r31, 0x1c
	ctx.r[31].s64 = ctx.r[31].s64 + 28;
	// 832C164C: 4B9E8365  bl 0x82ca99b0
	ctx.lr = 0x832C1650;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832C1650: 93B80004  stw r29, 4(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C1654: 93D80000  stw r30, 0(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1658: 93F80008  stw r31, 8(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C165C: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C1660: 4B9E7DE4  b 0x82ca9444
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9444);
	return;
	// 832C1664: 57C4073E  clrlwi r4, r30, 0x1c
	ctx.r[4].u64 = ctx.r[30].u32 as u64 & 0x0000000Fu64;
	// 832C1668: 7D635B78  mr r3, r11
	ctx.r[3].u64 = ctx.r[11].u64;
	// 832C166C: 57DEE13E  srwi r30, r30, 4
	// 832C1670: 3BFFFFFC  addi r31, r31, -4
	ctx.r[31].s64 = ctx.r[31].s64 + -4;
	// 832C1674: 4B9E833D  bl 0x82ca99b0
	ctx.lr = 0x832C1678;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832C1678: 93B80004  stw r29, 4(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C167C: 93D80000  stw r30, 0(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1680: 93F80008  stw r31, 8(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C1684: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C1688: 4B9E7DBC  b 0x82ca9444
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9444);
	return;
	// 832C168C: 81630030  lwz r11, 0x30(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C1690: 394B0004  addi r10, r11, 4
	ctx.r[10].s64 = ctx.r[11].s64 + 4;
	// 832C1694: 91430000  stw r10, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832C1698: 91630004  stw r11, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832C169C: 93B80004  stw r29, 4(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C16A0: 93D80000  stw r30, 0(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C16A4: 93F80008  stw r31, 8(r24)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[24].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C16A8: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C16AC: 4B9E7D98  b 0x82ca9444
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9444);
	return;
}

pub fn sub_832C16B0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C16B0 size=696
	// 832C16B0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C16B4: 4B9E7D39  bl 0x82ca93ec
	ctx.lr = 0x832C16B8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93EC);
	// 832C16B8: 9421FF50  stwu r1, -0xb0(r1)
	ea = ctx.r[1].u32.wrapping_add(-176 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C16BC: 81630000  lwz r11, 0(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C16C0: 7C952378  mr r21, r4
	ctx.r[21].u64 = ctx.r[4].u64;
	// 832C16C4: 81430004  lwz r10, 4(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C16C8: 7CBB2B78  mr r27, r5
	ctx.r[27].u64 = ctx.r[5].u64;
	// 832C16CC: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C16D0: 409A028C  bne cr6, 0x832c195c
	if !ctx.cr[6].eq {
	pc = 0x832C195C; continue 'dispatch;
	}
	// 832C16D4: 81630028  lwz r11, 0x28(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C16D8: 3900FFFF  li r8, -1
	ctx.r[8].s64 = -1;
	// 832C16DC: 81550008  lwz r10, 8(r21)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[21].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C16E0: 81350000  lwz r9, 0(r21)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[21].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C16E4: 83950004  lwz r28, 4(r21)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[21].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C16E8: 7F0A5840  cmplw cr6, r10, r11
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C16EC: 40980030  bge cr6, 0x832c171c
	if !ctx.cr[6].lt {
	pc = 0x832C171C; continue 'dispatch;
	}
	// 832C16F0: 80FC0000  lwz r7, 0(r28)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C16F4: 7FEA5850  subf r31, r10, r11
	ctx.r[31].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C16F8: 20AB0020  subfic r5, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[5].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C16FC: 7CE45030  slw r4, r7, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[7].u32) << ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1700: 7D6B5050  subf r11, r11, r10
	ctx.r[11].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C1704: 7CFEFC30  srw r30, r7, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[7].u32) >> ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1708: 7C894B78  or r9, r4, r9
	ctx.r[9].u64 = ctx.r[4].u64 | ctx.r[9].u64;
	// 832C170C: 7D052C30  srw r5, r8, r5
	if (ctx.r[5].u8 & 0x20) != 0 {
		ctx.r[5].u64 = 0;
	} else {
		ctx.r[5].u64 = ((ctx.r[8].u32) >> ((ctx.r[5].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1710: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C1714: 3B9C0004  addi r28, r28, 4
	ctx.r[28].s64 = ctx.r[28].s64 + 4;
	// 832C1718: 48000014  b 0x832c172c
	pc = 0x832C172C; continue 'dispatch;
	// 832C171C: 20EB0020  subfic r7, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[7].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C1720: 7FEB5050  subf r31, r11, r10
	ctx.r[31].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C1724: 7D053C30  srw r5, r8, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[5].u64 = 0;
	} else {
		ctx.r[5].u64 = ((ctx.r[8].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1728: 7D3E5C30  srw r30, r9, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[9].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C172C: 7CB84838  and r24, r5, r9
	ctx.r[24].u64 = ctx.r[5].u64 & ctx.r[9].u64;
	// 832C1730: 2F180000  cmpwi cr6, r24, 0
	ctx.cr[6].compare_i32(ctx.r[24].s32, 0, &mut ctx.xer);
	// 832C1734: 419A020C  beq cr6, 0x832c1940
	if ctx.cr[6].eq {
	pc = 0x832C1940; continue 'dispatch;
	}
	// 832C1738: 8163002C  lwz r11, 0x2c(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) } as u64;
	// 832C173C: 7F185800  cmpw cr6, r24, r11
	ctx.cr[6].compare_i32(ctx.r[24].s32, ctx.r[11].s32, &mut ctx.xer);
	// 832C1740: 41990200  bgt cr6, 0x832c1940
	if ctx.cr[6].gt {
	pc = 0x832C1940; continue 'dispatch;
	}
	// 832C1744: 82E30030  lwz r23, 0x30(r3)
	ctx.r[23].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C1748: 3B230010  addi r25, r3, 0x10
	ctx.r[25].s64 = ctx.r[3].s64 + 16;
	// 832C174C: 80830020  lwz r4, 0x20(r3)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C1750: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832C1754: 7D77C214  add r11, r23, r24
	ctx.r[11].u64 = ctx.r[23].u64 + ctx.r[24].u64;
	// 832C1758: 83430024  lwz r26, 0x24(r3)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(36 as u32) ) } as u64;
	// 832C175C: 7EF6BB78  mr r22, r23
	ctx.r[22].u64 = ctx.r[23].u64;
	// 832C1760: 91630004  stw r11, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832C1764: 92E30000  stw r23, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[23].u32 ) };
	// 832C1768: 83BB0180  lwz r29, 0x180(r27)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(384 as u32) ) } as u64;
	// 832C176C: 409A0018  bne cr6, 0x832c1784
	if !ctx.cr[6].eq {
	pc = 0x832C1784; continue 'dispatch;
	}
	// 832C1770: 817C0000  lwz r11, 0(r28)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1774: 3B9C0004  addi r28, r28, 4
	ctx.r[28].s64 = ctx.r[28].s64 + 4;
	// 832C1778: 3BE0001F  li r31, 0x1f
	ctx.r[31].s64 = 31;
	// 832C177C: 557EF87E  srwi r30, r11, 1
	// 832C1780: 48000010  b 0x832c1790
	pc = 0x832C1790; continue 'dispatch;
	// 832C1784: 7FCBF378  mr r11, r30
	ctx.r[11].u64 = ctx.r[30].u64;
	// 832C1788: 57DEF87E  srwi r30, r30, 1
	// 832C178C: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832C1790: 556B07FE  clrlwi r11, r11, 0x1f
	ctx.r[11].u64 = ctx.r[11].u32 as u64 & 0x00000001u64;
	// 832C1794: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C1798: 419A0008  beq cr6, 0x832c17a0
	if ctx.cr[6].eq {
	pc = 0x832C17A0; continue 'dispatch;
	}
	// 832C179C: 2318FFEC  subfic r24, r24, -0x14
	ctx.xer.ca = ctx.r[24].u32 <= -20 as u32;
	ctx.r[24].s64 = (-20 as i64) - ctx.r[24].s64;
	// 832C17A0: 397D0040  addi r11, r29, 0x40
	ctx.r[11].s64 = ctx.r[29].s64 + 64;
	// 832C17A4: 3B18FFFF  addi r24, r24, -1
	ctx.r[24].s64 = ctx.r[24].s64 + -1;
	// 832C17A8: 556A103A  slwi r10, r11, 2
	// 832C17AC: 7D6AD82E  lwzx r11, r10, r27
	ctx.r[11].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C17B0: 7F1F5840  cmplw cr6, r31, r11
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C17B4: 41980040  blt cr6, 0x832c17f4
	if ctx.cr[6].lt {
	pc = 0x832C17F4; continue 'dispatch;
	}
	// 832C17B8: 395D0050  addi r10, r29, 0x50
	ctx.r[10].s64 = ctx.r[29].s64 + 80;
	// 832C17BC: 212B0020  subfic r9, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[9].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C17C0: 5547103A  slwi r7, r10, 2
	// 832C17C4: 7D054C30  srw r5, r8, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[5].u64 = 0;
	} else {
		ctx.r[5].u64 = ((ctx.r[8].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C17C8: 7D47D82E  lwzx r10, r7, r27
	ctx.r[10].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C17CC: 7CA9F038  and r9, r5, r30
	ctx.r[9].u64 = ctx.r[5].u64 & ctx.r[30].u64;
	// 832C17D0: 57AB2036  slwi r11, r29, 4
	// 832C17D4: 7CE950AE  lbzx r7, r9, r10
	ctx.r[7].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[9].u32.wrapping_add(ctx.r[10].u32)) } as u64;
	// 832C17D8: 54EA073E  clrlwi r10, r7, 0x1c
	ctx.r[10].u64 = ctx.r[7].u32 as u64 & 0x0000000Fu64;
	// 832C17DC: 54E5E13E  srwi r5, r7, 4
	// 832C17E0: 7D6A5A14  add r11, r10, r11
	ctx.r[11].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C17E4: 7FCA2C30  srw r10, r30, r5
	if (ctx.r[5].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[30].u32) >> ((ctx.r[5].u8 & 0x1F) as u32)) as u64;
	}
	// 832C17E8: 7FABD8AE  lbzx r29, r11, r27
	ctx.r[29].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C17EC: 7D65F850  subf r11, r5, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[5].s64;
	// 832C17F0: 48000070  b 0x832c1860
	pc = 0x832C1860; continue 'dispatch;
	// 832C17F4: 7F1C3040  cmplw cr6, r28, r6
	ctx.cr[6].compare_u32(ctx.r[28].u32, ctx.r[6].u32, &mut ctx.xer);
	// 832C17F8: 40980148  bge cr6, 0x832c1940
	if !ctx.cr[6].lt {
	pc = 0x832C1940; continue 'dispatch;
	}
	// 832C17FC: 815C0000  lwz r10, 0(r28)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1800: 393D0050  addi r9, r29, 0x50
	ctx.r[9].s64 = ctx.r[29].s64 + 80;
	// 832C1804: 20EB0020  subfic r7, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[7].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C1808: 552B103A  slwi r11, r9, 2
	// 832C180C: 7D45F830  slw r5, r10, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[5].u64 = 0;
	} else {
		ctx.r[5].u64 = ((ctx.r[10].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1810: 7D073C30  srw r7, r8, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[8].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1814: 7D6BD82E  lwzx r11, r11, r27
	ctx.r[11].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C1818: 7CA5F378  or r5, r5, r30
	ctx.r[5].u64 = ctx.r[5].u64 | ctx.r[30].u64;
	// 832C181C: 57A92036  slwi r9, r29, 4
	// 832C1820: 7CE72838  and r7, r7, r5
	ctx.r[7].u64 = ctx.r[7].u64 & ctx.r[5].u64;
	// 832C1824: 7CA758AE  lbzx r5, r7, r11
	ctx.r[5].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[11].u32)) } as u64;
	// 832C1828: 54A7073E  clrlwi r7, r5, 0x1c
	ctx.r[7].u64 = ctx.r[5].u32 as u64 & 0x0000000Fu64;
	// 832C182C: 54ABE13E  srwi r11, r5, 4
	// 832C1830: 7D274A14  add r9, r7, r9
	ctx.r[9].u64 = ctx.r[7].u64 + ctx.r[9].u64;
	// 832C1834: 7F1F5840  cmplw cr6, r31, r11
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C1838: 7FA9D8AE  lbzx r29, r9, r27
	ctx.r[29].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[9].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C183C: 41980010  blt cr6, 0x832c184c
	if ctx.cr[6].lt {
	pc = 0x832C184C; continue 'dispatch;
	}
	// 832C1840: 7FCA5C30  srw r10, r30, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[30].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1844: 7D6BF850  subf r11, r11, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[11].s64;
	// 832C1848: 48000018  b 0x832c1860
	pc = 0x832C1860; continue 'dispatch;
	// 832C184C: 7D3F5850  subf r9, r31, r11
	ctx.r[9].s64 = ctx.r[11].s64 - ctx.r[31].s64;
	// 832C1850: 7D6BF850  subf r11, r11, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[11].s64;
	// 832C1854: 7D4A4C30  srw r10, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1858: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C185C: 3B9C0004  addi r28, r28, 4
	ctx.r[28].s64 = ctx.r[28].s64 + 4;
	// 832C1860: 7F0B2040  cmplw cr6, r11, r4
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[4].u32, &mut ctx.xer);
	// 832C1864: 21240020  subfic r9, r4, 0x20
	ctx.xer.ca = ctx.r[4].u32 <= 32 as u32;
	ctx.r[9].s64 = (32 as i64) - ctx.r[4].s64;
	// 832C1868: 41980028  blt cr6, 0x832c1890
	if ctx.cr[6].lt {
	pc = 0x832C1890; continue 'dispatch;
	}
	// 832C186C: 7D074C30  srw r7, r8, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[8].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1870: 7CE55038  and r5, r7, r10
	ctx.r[5].u64 = ctx.r[7].u64 & ctx.r[10].u64;
	// 832C1874: 7D25D0AE  lbzx r9, r5, r26
	ctx.r[9].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[5].u32.wrapping_add(ctx.r[26].u32)) } as u64;
	// 832C1878: 5527073E  clrlwi r7, r9, 0x1c
	ctx.r[7].u64 = ctx.r[9].u32 as u64 & 0x0000000Fu64;
	// 832C187C: 5529E13E  srwi r9, r9, 4
	// 832C1880: 7D5E4C30  srw r30, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1884: 7CA7C8AE  lbzx r5, r7, r25
	ctx.r[5].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[25].u32)) } as u64;
	// 832C1888: 7FE95850  subf r31, r9, r11
	ctx.r[31].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C188C: 48000050  b 0x832c18dc
	pc = 0x832C18DC; continue 'dispatch;
	// 832C1890: 80FC0000  lwz r7, 0(r28)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1894: 7D094C30  srw r9, r8, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[8].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1898: 7CE55830  slw r5, r7, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[5].u64 = 0;
	} else {
		ctx.r[5].u64 = ((ctx.r[7].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C189C: 7CA55378  or r5, r5, r10
	ctx.r[5].u64 = ctx.r[5].u64 | ctx.r[10].u64;
	// 832C18A0: 7D292838  and r9, r9, r5
	ctx.r[9].u64 = ctx.r[9].u64 & ctx.r[5].u64;
	// 832C18A4: 7CA9D0AE  lbzx r5, r9, r26
	ctx.r[5].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[9].u32.wrapping_add(ctx.r[26].u32)) } as u64;
	// 832C18A8: 54BF073E  clrlwi r31, r5, 0x1c
	ctx.r[31].u64 = ctx.r[5].u32 as u64 & 0x0000000Fu64;
	// 832C18AC: 54A9E13E  srwi r9, r5, 4
	// 832C18B0: 7F0B4840  cmplw cr6, r11, r9
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832C18B4: 7CBFC8AE  lbzx r5, r31, r25
	ctx.r[5].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[31].u32.wrapping_add(ctx.r[25].u32)) } as u64;
	// 832C18B8: 41980010  blt cr6, 0x832c18c8
	if ctx.cr[6].lt {
	pc = 0x832C18C8; continue 'dispatch;
	}
	// 832C18BC: 7D5E4C30  srw r30, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C18C0: 7FE95850  subf r31, r9, r11
	ctx.r[31].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C18C4: 48000018  b 0x832c18dc
	pc = 0x832C18DC; continue 'dispatch;
	// 832C18C8: 7D4B4850  subf r10, r11, r9
	ctx.r[10].s64 = ctx.r[9].s64 - ctx.r[11].s64;
	// 832C18CC: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C18D0: 7CFE5430  srw r30, r7, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[7].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C18D4: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C18D8: 3B9C0004  addi r28, r28, 4
	ctx.r[28].s64 = ctx.r[28].s64 + 4;
	// 832C18DC: 57AB2036  slwi r11, r29, 4
	// 832C18E0: 7D6B2B78  or r11, r11, r5
	ctx.r[11].u64 = ctx.r[11].u64 | ctx.r[5].u64;
	// 832C18E4: 556A0630  rlwinm r10, r11, 0, 0x18, 0x18
	ctx.r[10].u64 = ctx.r[11].u32 as u64 & 0xFFFFFFFFu64;
	// 832C18E8: 556B067E  clrlwi r11, r11, 0x19
	ctx.r[11].u64 = ctx.r[11].u32 as u64 & 0x0000007Fu64;
	// 832C18EC: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C18F0: 419A000C  beq cr6, 0x832c18fc
	if ctx.cr[6].eq {
	pc = 0x832C18FC; continue 'dispatch;
	}
	// 832C18F4: 7D4B00D0  neg r10, r11
	ctx.r[10].s64 = -ctx.r[11].s64;
	// 832C18F8: 554B063E  clrlwi r11, r10, 0x18
	ctx.r[11].u64 = ctx.r[10].u32 as u64 & 0x000000FFu64;
	// 832C18FC: 396B0080  addi r11, r11, 0x80
	ctx.r[11].s64 = ctx.r[11].s64 + 128;
	// 832C1900: 2F180000  cmpwi cr6, r24, 0
	ctx.cr[6].compare_i32(ctx.r[24].s32, 0, &mut ctx.xer);
	// 832C1904: 99760000  stb r11, 0(r22)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[22].u32.wrapping_add(0 as u32), ctx.r[11].u8 ) };
	// 832C1908: 3AD60001  addi r22, r22, 1
	ctx.r[22].s64 = ctx.r[22].s64 + 1;
	// 832C190C: 4199FE94  bgt cr6, 0x832c17a0
	if ctx.cr[6].gt {
	pc = 0x832C17A0; continue 'dispatch;
	}
	// 832C1910: 2F18FFEA  cmpwi cr6, r24, -0x16
	ctx.cr[6].compare_i32(ctx.r[24].s32, -22, &mut ctx.xer);
	// 832C1914: 40980014  bge cr6, 0x832c1928
	if !ctx.cr[6].lt {
	pc = 0x832C1928; continue 'dispatch;
	}
	// 832C1918: 20B8FFEB  subfic r5, r24, -0x15
	ctx.xer.ca = ctx.r[24].u32 <= -21 as u32;
	ctx.r[5].s64 = (-21 as i64) - ctx.r[24].s64;
	// 832C191C: 88970000  lbz r4, 0(r23)
	ctx.r[4].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[23].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1920: 7EE3BB78  mr r3, r23
	ctx.r[3].u64 = ctx.r[23].u64;
	// 832C1924: 4B9E808D  bl 0x82ca99b0
	ctx.lr = 0x832C1928;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832C1928: 93BB0180  stw r29, 0x180(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(384 as u32), ctx.r[29].u32 ) };
	// 832C192C: 93950004  stw r28, 4(r21)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[21].u32.wrapping_add(4 as u32), ctx.r[28].u32 ) };
	// 832C1930: 93D50000  stw r30, 0(r21)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[21].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1934: 93F50008  stw r31, 8(r21)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[21].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C1938: 382100B0  addi r1, r1, 0xb0
	ctx.r[1].s64 = ctx.r[1].s64 + 176;
	// 832C193C: 4B9E7B00  b 0x82ca943c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA943C);
	return;
	// 832C1940: 81630030  lwz r11, 0x30(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C1944: 394B0004  addi r10, r11, 4
	ctx.r[10].s64 = ctx.r[11].s64 + 4;
	// 832C1948: 91430000  stw r10, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832C194C: 91630004  stw r11, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832C1950: 93950004  stw r28, 4(r21)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[21].u32.wrapping_add(4 as u32), ctx.r[28].u32 ) };
	// 832C1954: 93D50000  stw r30, 0(r21)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[21].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1958: 93F50008  stw r31, 8(r21)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[21].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C195C: 382100B0  addi r1, r1, 0xb0
	ctx.r[1].s64 = ctx.r[1].s64 + 176;
	// 832C1960: 4B9E7ADC  b 0x82ca943c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA943C);
	return;
}

pub fn sub_832C1968(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C1968 size=672
	// 832C1968: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C196C: 4B9E7A7D  bl 0x82ca93e8
	ctx.lr = 0x832C1970;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93E8);
	// 832C1970: 9421FF40  stwu r1, -0xc0(r1)
	ea = ctx.r[1].u32.wrapping_add(-192 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C1974: 81630000  lwz r11, 0(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1978: 7C942378  mr r20, r4
	ctx.r[20].u64 = ctx.r[4].u64;
	// 832C197C: 81430004  lwz r10, 4(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C1980: 7CBB2B78  mr r27, r5
	ctx.r[27].u64 = ctx.r[5].u64;
	// 832C1984: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C1988: 409A0274  bne cr6, 0x832c1bfc
	if !ctx.cr[6].eq {
	pc = 0x832C1BFC; continue 'dispatch;
	}
	// 832C198C: 81630028  lwz r11, 0x28(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C1990: 3920FFFF  li r9, -1
	ctx.r[9].s64 = -1;
	// 832C1994: 81540008  lwz r10, 8(r20)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[20].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C1998: 81140000  lwz r8, 0(r20)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[20].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C199C: 83B40004  lwz r29, 4(r20)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[20].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C19A0: 7F0A5840  cmplw cr6, r10, r11
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C19A4: 40980030  bge cr6, 0x832c19d4
	if !ctx.cr[6].lt {
	pc = 0x832C19D4; continue 'dispatch;
	}
	// 832C19A8: 80FD0000  lwz r7, 0(r29)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C19AC: 7FEA5850  subf r31, r10, r11
	ctx.r[31].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C19B0: 20AB0020  subfic r5, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[5].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C19B4: 7CE45030  slw r4, r7, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[7].u32) << ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C19B8: 7D6B5050  subf r11, r11, r10
	ctx.r[11].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C19BC: 7CFEFC30  srw r30, r7, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[7].u32) >> ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C19C0: 7C884378  or r8, r4, r8
	ctx.r[8].u64 = ctx.r[4].u64 | ctx.r[8].u64;
	// 832C19C4: 7D252C30  srw r5, r9, r5
	if (ctx.r[5].u8 & 0x20) != 0 {
		ctx.r[5].u64 = 0;
	} else {
		ctx.r[5].u64 = ((ctx.r[9].u32) >> ((ctx.r[5].u8 & 0x1F) as u32)) as u64;
	}
	// 832C19C8: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C19CC: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C19D0: 48000014  b 0x832c19e4
	pc = 0x832C19E4; continue 'dispatch;
	// 832C19D4: 20EB0020  subfic r7, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[7].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C19D8: 7FEB5050  subf r31, r11, r10
	ctx.r[31].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C19DC: 7D253C30  srw r5, r9, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[5].u64 = 0;
	} else {
		ctx.r[5].u64 = ((ctx.r[9].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C19E0: 7D1E5C30  srw r30, r8, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[8].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C19E4: 7CB74038  and r23, r5, r8
	ctx.r[23].u64 = ctx.r[5].u64 & ctx.r[8].u64;
	// 832C19E8: 2F170000  cmpwi cr6, r23, 0
	ctx.cr[6].compare_i32(ctx.r[23].s32, 0, &mut ctx.xer);
	// 832C19EC: 419A01F4  beq cr6, 0x832c1be0
	if ctx.cr[6].eq {
	pc = 0x832C1BE0; continue 'dispatch;
	}
	// 832C19F0: 8163002C  lwz r11, 0x2c(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) } as u64;
	// 832C19F4: 7F175800  cmpw cr6, r23, r11
	ctx.cr[6].compare_i32(ctx.r[23].s32, ctx.r[11].s32, &mut ctx.xer);
	// 832C19F8: 419901E8  bgt cr6, 0x832c1be0
	if ctx.cr[6].gt {
	pc = 0x832C1BE0; continue 'dispatch;
	}
	// 832C19FC: 82C30030  lwz r22, 0x30(r3)
	ctx.r[22].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C1A00: 3B030010  addi r24, r3, 0x10
	ctx.r[24].s64 = ctx.r[3].s64 + 16;
	// 832C1A04: 83430020  lwz r26, 0x20(r3)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C1A08: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832C1A0C: 7D76BA14  add r11, r22, r23
	ctx.r[11].u64 = ctx.r[22].u64 + ctx.r[23].u64;
	// 832C1A10: 83230024  lwz r25, 0x24(r3)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(36 as u32) ) } as u64;
	// 832C1A14: 7ED5B378  mr r21, r22
	ctx.r[21].u64 = ctx.r[22].u64;
	// 832C1A18: 91630004  stw r11, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832C1A1C: 92C30000  stw r22, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[22].u32 ) };
	// 832C1A20: 80BB0180  lwz r5, 0x180(r27)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(384 as u32) ) } as u64;
	// 832C1A24: 409A0018  bne cr6, 0x832c1a3c
	if !ctx.cr[6].eq {
	pc = 0x832C1A3C; continue 'dispatch;
	}
	// 832C1A28: 817D0000  lwz r11, 0(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1A2C: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C1A30: 3BE0001F  li r31, 0x1f
	ctx.r[31].s64 = 31;
	// 832C1A34: 557EF87E  srwi r30, r11, 1
	// 832C1A38: 48000010  b 0x832c1a48
	pc = 0x832C1A48; continue 'dispatch;
	// 832C1A3C: 7FCBF378  mr r11, r30
	ctx.r[11].u64 = ctx.r[30].u64;
	// 832C1A40: 57DEF87E  srwi r30, r30, 1
	// 832C1A44: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832C1A48: 556B07FE  clrlwi r11, r11, 0x1f
	ctx.r[11].u64 = ctx.r[11].u32 as u64 & 0x00000001u64;
	// 832C1A4C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C1A50: 419A0008  beq cr6, 0x832c1a58
	if ctx.cr[6].eq {
	pc = 0x832C1A58; continue 'dispatch;
	}
	// 832C1A54: 22F7FFEC  subfic r23, r23, -0x14
	ctx.xer.ca = ctx.r[23].u32 <= -20 as u32;
	ctx.r[23].s64 = (-20 as i64) - ctx.r[23].s64;
	// 832C1A58: 39650040  addi r11, r5, 0x40
	ctx.r[11].s64 = ctx.r[5].s64 + 64;
	// 832C1A5C: 3AF7FFFF  addi r23, r23, -1
	ctx.r[23].s64 = ctx.r[23].s64 + -1;
	// 832C1A60: 556A103A  slwi r10, r11, 2
	// 832C1A64: 7D6AD82E  lwzx r11, r10, r27
	ctx.r[11].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C1A68: 7F1F5840  cmplw cr6, r31, r11
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C1A6C: 41980040  blt cr6, 0x832c1aac
	if ctx.cr[6].lt {
	pc = 0x832C1AAC; continue 'dispatch;
	}
	// 832C1A70: 39450050  addi r10, r5, 0x50
	ctx.r[10].s64 = ctx.r[5].s64 + 80;
	// 832C1A74: 210B0020  subfic r8, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[8].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C1A78: 5547103A  slwi r7, r10, 2
	// 832C1A7C: 7D244430  srw r4, r9, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[9].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1A80: 7D47D82E  lwzx r10, r7, r27
	ctx.r[10].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C1A84: 7C88F038  and r8, r4, r30
	ctx.r[8].u64 = ctx.r[4].u64 & ctx.r[30].u64;
	// 832C1A88: 54AB2036  slwi r11, r5, 4
	// 832C1A8C: 7CE850AE  lbzx r7, r8, r10
	ctx.r[7].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[10].u32)) } as u64;
	// 832C1A90: 54EA073E  clrlwi r10, r7, 0x1c
	ctx.r[10].u64 = ctx.r[7].u32 as u64 & 0x0000000Fu64;
	// 832C1A94: 54E5E13E  srwi r5, r7, 4
	// 832C1A98: 7C8A5A14  add r4, r10, r11
	ctx.r[4].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C1A9C: 7FCA2C30  srw r10, r30, r5
	if (ctx.r[5].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[30].u32) >> ((ctx.r[5].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1AA0: 7F84D8AE  lbzx r28, r4, r27
	ctx.r[28].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[4].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C1AA4: 7D65F850  subf r11, r5, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[5].s64;
	// 832C1AA8: 48000070  b 0x832c1b18
	pc = 0x832C1B18; continue 'dispatch;
	// 832C1AAC: 7F1D3040  cmplw cr6, r29, r6
	ctx.cr[6].compare_u32(ctx.r[29].u32, ctx.r[6].u32, &mut ctx.xer);
	// 832C1AB0: 40980130  bge cr6, 0x832c1be0
	if !ctx.cr[6].lt {
	pc = 0x832C1BE0; continue 'dispatch;
	}
	// 832C1AB4: 815D0000  lwz r10, 0(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1AB8: 39050050  addi r8, r5, 0x50
	ctx.r[8].s64 = ctx.r[5].s64 + 80;
	// 832C1ABC: 20EB0020  subfic r7, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[7].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C1AC0: 550B103A  slwi r11, r8, 2
	// 832C1AC4: 7D44F830  slw r4, r10, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[10].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1AC8: 7D273C30  srw r7, r9, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[9].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1ACC: 7D6BD82E  lwzx r11, r11, r27
	ctx.r[11].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C1AD0: 7C84F378  or r4, r4, r30
	ctx.r[4].u64 = ctx.r[4].u64 | ctx.r[30].u64;
	// 832C1AD4: 54A82036  slwi r8, r5, 4
	// 832C1AD8: 7CE72038  and r7, r7, r4
	ctx.r[7].u64 = ctx.r[7].u64 & ctx.r[4].u64;
	// 832C1ADC: 7CA758AE  lbzx r5, r7, r11
	ctx.r[5].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[11].u32)) } as u64;
	// 832C1AE0: 54A7073E  clrlwi r7, r5, 0x1c
	ctx.r[7].u64 = ctx.r[5].u32 as u64 & 0x0000000Fu64;
	// 832C1AE4: 54ABE13E  srwi r11, r5, 4
	// 832C1AE8: 7C874214  add r4, r7, r8
	ctx.r[4].u64 = ctx.r[7].u64 + ctx.r[8].u64;
	// 832C1AEC: 7F1F5840  cmplw cr6, r31, r11
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C1AF0: 7F84D8AE  lbzx r28, r4, r27
	ctx.r[28].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[4].u32.wrapping_add(ctx.r[27].u32)) } as u64;
	// 832C1AF4: 41980010  blt cr6, 0x832c1b04
	if ctx.cr[6].lt {
	pc = 0x832C1B04; continue 'dispatch;
	}
	// 832C1AF8: 7FCA5C30  srw r10, r30, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[30].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1AFC: 7D6BF850  subf r11, r11, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[11].s64;
	// 832C1B00: 48000018  b 0x832c1b18
	pc = 0x832C1B18; continue 'dispatch;
	// 832C1B04: 7D1F5850  subf r8, r31, r11
	ctx.r[8].s64 = ctx.r[11].s64 - ctx.r[31].s64;
	// 832C1B08: 7D6BF850  subf r11, r11, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[11].s64;
	// 832C1B0C: 7D4A4430  srw r10, r10, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[10].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1B10: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C1B14: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C1B18: 7F85E378  mr r5, r28
	ctx.r[5].u64 = ctx.r[28].u64;
	// 832C1B1C: 7F0BD040  cmplw cr6, r11, r26
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[26].u32, &mut ctx.xer);
	// 832C1B20: 211A0020  subfic r8, r26, 0x20
	ctx.xer.ca = ctx.r[26].u32 <= 32 as u32;
	ctx.r[8].s64 = (32 as i64) - ctx.r[26].s64;
	// 832C1B24: 41980028  blt cr6, 0x832c1b4c
	if ctx.cr[6].lt {
	pc = 0x832C1B4C; continue 'dispatch;
	}
	// 832C1B28: 7D274430  srw r7, r9, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[9].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1B2C: 7CE45038  and r4, r7, r10
	ctx.r[4].u64 = ctx.r[7].u64 & ctx.r[10].u64;
	// 832C1B30: 7D04C8AE  lbzx r8, r4, r25
	ctx.r[8].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[4].u32.wrapping_add(ctx.r[25].u32)) } as u64;
	// 832C1B34: 5507073E  clrlwi r7, r8, 0x1c
	ctx.r[7].u64 = ctx.r[8].u32 as u64 & 0x0000000Fu64;
	// 832C1B38: 5508E13E  srwi r8, r8, 4
	// 832C1B3C: 7D5E4430  srw r30, r10, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1B40: 7C87C0AE  lbzx r4, r7, r24
	ctx.r[4].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[24].u32)) } as u64;
	// 832C1B44: 7FE85850  subf r31, r8, r11
	ctx.r[31].s64 = ctx.r[11].s64 - ctx.r[8].s64;
	// 832C1B48: 48000050  b 0x832c1b98
	pc = 0x832C1B98; continue 'dispatch;
	// 832C1B4C: 80FD0000  lwz r7, 0(r29)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1B50: 7D284430  srw r8, r9, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[8].u64 = 0;
	} else {
		ctx.r[8].u64 = ((ctx.r[9].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1B54: 7CE45830  slw r4, r7, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[7].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1B58: 7C845378  or r4, r4, r10
	ctx.r[4].u64 = ctx.r[4].u64 | ctx.r[10].u64;
	// 832C1B5C: 7D082038  and r8, r8, r4
	ctx.r[8].u64 = ctx.r[8].u64 & ctx.r[4].u64;
	// 832C1B60: 7C88C8AE  lbzx r4, r8, r25
	ctx.r[4].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[25].u32)) } as u64;
	// 832C1B64: 549F073E  clrlwi r31, r4, 0x1c
	ctx.r[31].u64 = ctx.r[4].u32 as u64 & 0x0000000Fu64;
	// 832C1B68: 5488E13E  srwi r8, r4, 4
	// 832C1B6C: 7F0B4040  cmplw cr6, r11, r8
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[8].u32, &mut ctx.xer);
	// 832C1B70: 7C9FC0AE  lbzx r4, r31, r24
	ctx.r[4].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[31].u32.wrapping_add(ctx.r[24].u32)) } as u64;
	// 832C1B74: 41980010  blt cr6, 0x832c1b84
	if ctx.cr[6].lt {
	pc = 0x832C1B84; continue 'dispatch;
	}
	// 832C1B78: 7D5E4430  srw r30, r10, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1B7C: 7FE85850  subf r31, r8, r11
	ctx.r[31].s64 = ctx.r[11].s64 - ctx.r[8].s64;
	// 832C1B80: 48000018  b 0x832c1b98
	pc = 0x832C1B98; continue 'dispatch;
	// 832C1B84: 7D4B4050  subf r10, r11, r8
	ctx.r[10].s64 = ctx.r[8].s64 - ctx.r[11].s64;
	// 832C1B88: 7D685850  subf r11, r8, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[8].s64;
	// 832C1B8C: 7CFE5430  srw r30, r7, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[7].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1B90: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C1B94: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C1B98: 578B2036  slwi r11, r28, 4
	// 832C1B9C: 2F170000  cmpwi cr6, r23, 0
	ctx.cr[6].compare_i32(ctx.r[23].s32, 0, &mut ctx.xer);
	// 832C1BA0: 7D6A2378  or r10, r11, r4
	ctx.r[10].u64 = ctx.r[11].u64 | ctx.r[4].u64;
	// 832C1BA4: 99550000  stb r10, 0(r21)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[21].u32.wrapping_add(0 as u32), ctx.r[10].u8 ) };
	// 832C1BA8: 3AB50001  addi r21, r21, 1
	ctx.r[21].s64 = ctx.r[21].s64 + 1;
	// 832C1BAC: 4199FEAC  bgt cr6, 0x832c1a58
	if ctx.cr[6].gt {
	pc = 0x832C1A58; continue 'dispatch;
	}
	// 832C1BB0: 2F17FFEA  cmpwi cr6, r23, -0x16
	ctx.cr[6].compare_i32(ctx.r[23].s32, -22, &mut ctx.xer);
	// 832C1BB4: 40980014  bge cr6, 0x832c1bc8
	if !ctx.cr[6].lt {
	pc = 0x832C1BC8; continue 'dispatch;
	}
	// 832C1BB8: 20B7FFEB  subfic r5, r23, -0x15
	ctx.xer.ca = ctx.r[23].u32 <= -21 as u32;
	ctx.r[5].s64 = (-21 as i64) - ctx.r[23].s64;
	// 832C1BBC: 88960000  lbz r4, 0(r22)
	ctx.r[4].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[22].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1BC0: 7EC3B378  mr r3, r22
	ctx.r[3].u64 = ctx.r[22].u64;
	// 832C1BC4: 4B9E7DED  bl 0x82ca99b0
	ctx.lr = 0x832C1BC8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832C1BC8: 939B0180  stw r28, 0x180(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(384 as u32), ctx.r[28].u32 ) };
	// 832C1BCC: 93B40004  stw r29, 4(r20)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[20].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C1BD0: 93D40000  stw r30, 0(r20)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[20].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1BD4: 93F40008  stw r31, 8(r20)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[20].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C1BD8: 382100C0  addi r1, r1, 0xc0
	ctx.r[1].s64 = ctx.r[1].s64 + 192;
	// 832C1BDC: 4B9E785C  b 0x82ca9438
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9438);
	return;
	// 832C1BE0: 81630030  lwz r11, 0x30(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C1BE4: 394B0004  addi r10, r11, 4
	ctx.r[10].s64 = ctx.r[11].s64 + 4;
	// 832C1BE8: 91430000  stw r10, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832C1BEC: 91630004  stw r11, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832C1BF0: 93B40004  stw r29, 4(r20)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[20].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C1BF4: 93D40000  stw r30, 0(r20)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[20].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1BF8: 93F40008  stw r31, 8(r20)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[20].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C1BFC: 382100C0  addi r1, r1, 0xc0
	ctx.r[1].s64 = ctx.r[1].s64 + 192;
	// 832C1C00: 4B9E7838  b 0x82ca9438
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9438);
	return;
}

pub fn sub_832C1C08(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C1C08 size=560
	// 832C1C08: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C1C0C: 4B9E77F5  bl 0x82ca9400
	ctx.lr = 0x832C1C10;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9400);
	// 832C1C10: 9421FF70  stwu r1, -0x90(r1)
	ea = ctx.r[1].u32.wrapping_add(-144 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C1C14: 81630000  lwz r11, 0(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1C18: 7C9C2378  mr r28, r4
	ctx.r[28].u64 = ctx.r[4].u64;
	// 832C1C1C: 81430004  lwz r10, 4(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C1C20: 7CBB2B78  mr r27, r5
	ctx.r[27].u64 = ctx.r[5].u64;
	// 832C1C24: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C1C28: 409A0204  bne cr6, 0x832c1e2c
	if !ctx.cr[6].eq {
	pc = 0x832C1E2C; continue 'dispatch;
	}
	// 832C1C2C: 81630028  lwz r11, 0x28(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C1C30: 3900FFFF  li r8, -1
	ctx.r[8].s64 = -1;
	// 832C1C34: 815C0008  lwz r10, 8(r28)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C1C38: 813C0000  lwz r9, 0(r28)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1C3C: 83BC0004  lwz r29, 4(r28)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C1C40: 7F0A5840  cmplw cr6, r10, r11
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C1C44: 40980030  bge cr6, 0x832c1c74
	if !ctx.cr[6].lt {
	pc = 0x832C1C74; continue 'dispatch;
	}
	// 832C1C48: 80FD0000  lwz r7, 0(r29)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1C4C: 20CB0020  subfic r6, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[6].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C1C50: 7C8A5850  subf r4, r10, r11
	ctx.r[4].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C1C54: 7CE55030  slw r5, r7, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[5].u64 = 0;
	} else {
		ctx.r[5].u64 = ((ctx.r[7].u32) << ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1C58: 7D6B5050  subf r11, r11, r10
	ctx.r[11].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C1C5C: 7CA94B78  or r9, r5, r9
	ctx.r[9].u64 = ctx.r[5].u64 | ctx.r[9].u64;
	// 832C1C60: 7D063430  srw r6, r8, r6
	if (ctx.r[6].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[8].u32) >> ((ctx.r[6].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1C64: 7CFE2430  srw r30, r7, r4
	if (ctx.r[4].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[7].u32) >> ((ctx.r[4].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1C68: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C1C6C: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C1C70: 48000014  b 0x832c1c84
	pc = 0x832C1C84; continue 'dispatch;
	// 832C1C74: 20EB0020  subfic r7, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[7].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C1C78: 7FEB5050  subf r31, r11, r10
	ctx.r[31].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C1C7C: 7D063C30  srw r6, r8, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[8].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1C80: 7D3E5C30  srw r30, r9, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[9].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1C84: 7CC54838  and r5, r6, r9
	ctx.r[5].u64 = ctx.r[6].u64 & ctx.r[9].u64;
	// 832C1C88: 2B050000  cmplwi cr6, r5, 0
	ctx.cr[6].compare_u32(ctx.r[5].u32, 0 as u32, &mut ctx.xer);
	// 832C1C8C: 419A0184  beq cr6, 0x832c1e10
	if ctx.cr[6].eq {
	pc = 0x832C1E10; continue 'dispatch;
	}
	// 832C1C90: 8163002C  lwz r11, 0x2c(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) } as u64;
	// 832C1C94: 7F055840  cmplw cr6, r5, r11
	ctx.cr[6].compare_u32(ctx.r[5].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C1C98: 41990178  bgt cr6, 0x832c1e10
	if ctx.cr[6].gt {
	pc = 0x832C1E10; continue 'dispatch;
	}
	// 832C1C9C: 81630030  lwz r11, 0x30(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C1CA0: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832C1CA4: 7D4B2A14  add r10, r11, r5
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[5].u64;
	// 832C1CA8: 91430004  stw r10, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[10].u32 ) };
	// 832C1CAC: 91630000  stw r11, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[11].u32 ) };
	// 832C1CB0: 409A0018  bne cr6, 0x832c1cc8
	if !ctx.cr[6].eq {
	pc = 0x832C1CC8; continue 'dispatch;
	}
	// 832C1CB4: 815D0000  lwz r10, 0(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1CB8: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C1CBC: 3BE0001F  li r31, 0x1f
	ctx.r[31].s64 = 31;
	// 832C1CC0: 555EF87E  srwi r30, r10, 1
	// 832C1CC4: 48000010  b 0x832c1cd4
	pc = 0x832C1CD4; continue 'dispatch;
	// 832C1CC8: 7FCAF378  mr r10, r30
	ctx.r[10].u64 = ctx.r[30].u64;
	// 832C1CCC: 57DEF87E  srwi r30, r30, 1
	// 832C1CD0: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832C1CD4: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C1CD8: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C1CDC: 409A00C8  bne cr6, 0x832c1da4
	if !ctx.cr[6].eq {
	pc = 0x832C1DA4; continue 'dispatch;
	}
	// 832C1CE0: 80E30020  lwz r7, 0x20(r3)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C1CE4: 38830010  addi r4, r3, 0x10
	ctx.r[4].s64 = ctx.r[3].s64 + 16;
	// 832C1CE8: 80C30024  lwz r6, 0x24(r3)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(36 as u32) ) } as u64;
	// 832C1CEC: 38A5FFFF  addi r5, r5, -1
	ctx.r[5].s64 = ctx.r[5].s64 + -1;
	// 832C1CF0: 7F1F3840  cmplw cr6, r31, r7
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[7].u32, &mut ctx.xer);
	// 832C1CF4: 41980034  blt cr6, 0x832c1d28
	if ctx.cr[6].lt {
	pc = 0x832C1D28; continue 'dispatch;
	}
	// 832C1CF8: 21470020  subfic r10, r7, 0x20
	ctx.xer.ca = ctx.r[7].u32 <= 32 as u32;
	ctx.r[10].s64 = (32 as i64) - ctx.r[7].s64;
	// 832C1CFC: 7D095430  srw r9, r8, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[8].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1D00: 7D2AF038  and r10, r9, r30
	ctx.r[10].u64 = ctx.r[9].u64 & ctx.r[30].u64;
	// 832C1D04: 7D2A30AE  lbzx r9, r10, r6
	ctx.r[9].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[6].u32)) } as u64;
	// 832C1D08: 552A073E  clrlwi r10, r9, 0x1c
	ctx.r[10].u64 = ctx.r[9].u32 as u64 & 0x0000000Fu64;
	// 832C1D0C: 5529E13E  srwi r9, r9, 4
	// 832C1D10: 7FDE4C30  srw r30, r30, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1D14: 7D4A20AE  lbzx r10, r10, r4
	ctx.r[10].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[4].u32)) } as u64;
	// 832C1D18: 7FE9F850  subf r31, r9, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[9].s64;
	// 832C1D1C: 994B0000  stb r10, 0(r11)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[10].u8 ) };
	// 832C1D20: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832C1D24: 48000064  b 0x832c1d88
	pc = 0x832C1D88; continue 'dispatch;
	// 832C1D28: 7F1DD840  cmplw cr6, r29, r27
	ctx.cr[6].compare_u32(ctx.r[29].u32, ctx.r[27].u32, &mut ctx.xer);
	// 832C1D2C: 409800E4  bge cr6, 0x832c1e10
	if !ctx.cr[6].lt {
	pc = 0x832C1E10; continue 'dispatch;
	}
	// 832C1D30: 813D0000  lwz r9, 0(r29)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1D34: 21470020  subfic r10, r7, 0x20
	ctx.xer.ca = ctx.r[7].u32 <= 32 as u32;
	ctx.r[10].s64 = (32 as i64) - ctx.r[7].s64;
	// 832C1D38: 7D3AF830  slw r26, r9, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[26].u64 = 0;
	} else {
		ctx.r[26].u64 = ((ctx.r[9].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1D3C: 7D0A5430  srw r10, r8, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[8].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1D40: 7F5AF378  or r26, r26, r30
	ctx.r[26].u64 = ctx.r[26].u64 | ctx.r[30].u64;
	// 832C1D44: 7D4AD038  and r10, r10, r26
	ctx.r[10].u64 = ctx.r[10].u64 & ctx.r[26].u64;
	// 832C1D48: 7D4A30AE  lbzx r10, r10, r6
	ctx.r[10].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[6].u32)) } as u64;
	// 832C1D4C: 555A073E  clrlwi r26, r10, 0x1c
	ctx.r[26].u64 = ctx.r[10].u32 as u64 & 0x0000000Fu64;
	// 832C1D50: 554AE13E  srwi r10, r10, 4
	// 832C1D54: 7F1F5040  cmplw cr6, r31, r10
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C1D58: 7F5A20AE  lbzx r26, r26, r4
	ctx.r[26].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[26].u32.wrapping_add(ctx.r[4].u32)) } as u64;
	// 832C1D5C: 9B4B0000  stb r26, 0(r11)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[26].u8 ) };
	// 832C1D60: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832C1D64: 41980010  blt cr6, 0x832c1d74
	if ctx.cr[6].lt {
	pc = 0x832C1D74; continue 'dispatch;
	}
	// 832C1D68: 7FDE5430  srw r30, r30, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1D6C: 7FEAF850  subf r31, r10, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[10].s64;
	// 832C1D70: 48000018  b 0x832c1d88
	pc = 0x832C1D88; continue 'dispatch;
	// 832C1D74: 7FDF5050  subf r30, r31, r10
	ctx.r[30].s64 = ctx.r[10].s64 - ctx.r[31].s64;
	// 832C1D78: 7D4AF850  subf r10, r10, r31
	ctx.r[10].s64 = ctx.r[31].s64 - ctx.r[10].s64;
	// 832C1D7C: 7D3EF430  srw r30, r9, r30
	if (ctx.r[30].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[9].u32) >> ((ctx.r[30].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1D80: 3BEA0020  addi r31, r10, 0x20
	ctx.r[31].s64 = ctx.r[10].s64 + 32;
	// 832C1D84: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C1D88: 2B050000  cmplwi cr6, r5, 0
	ctx.cr[6].compare_u32(ctx.r[5].u32, 0 as u32, &mut ctx.xer);
	// 832C1D8C: 409AFF60  bne cr6, 0x832c1cec
	if !ctx.cr[6].eq {
	pc = 0x832C1CEC; continue 'dispatch;
	}
	// 832C1D90: 93BC0004  stw r29, 4(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C1D94: 93DC0000  stw r30, 0(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1D98: 93FC0008  stw r31, 8(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C1D9C: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832C1DA0: 4B9E76B0  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832C1DA4: 2B1F0004  cmplwi cr6, r31, 4
	ctx.cr[6].compare_u32(ctx.r[31].u32, 4 as u32, &mut ctx.xer);
	// 832C1DA8: 40980040  bge cr6, 0x832c1de8
	if !ctx.cr[6].lt {
	pc = 0x832C1DE8; continue 'dispatch;
	}
	// 832C1DAC: 815D0000  lwz r10, 0(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1DB0: 213F0004  subfic r9, r31, 4
	ctx.xer.ca = ctx.r[31].u32 <= 4 as u32;
	ctx.r[9].s64 = (4 as i64) - ctx.r[31].s64;
	// 832C1DB4: 7D635B78  mr r3, r11
	ctx.r[3].u64 = ctx.r[11].u64;
	// 832C1DB8: 7D48F830  slw r8, r10, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[8].u64 = 0;
	} else {
		ctx.r[8].u64 = ((ctx.r[10].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1DBC: 7D07F378  or r7, r8, r30
	ctx.r[7].u64 = ctx.r[8].u64 | ctx.r[30].u64;
	// 832C1DC0: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C1DC4: 54E4073E  clrlwi r4, r7, 0x1c
	ctx.r[4].u64 = ctx.r[7].u32 as u64 & 0x0000000Fu64;
	// 832C1DC8: 7D5E4C30  srw r30, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1DCC: 3BFF001C  addi r31, r31, 0x1c
	ctx.r[31].s64 = ctx.r[31].s64 + 28;
	// 832C1DD0: 4B9E7BE1  bl 0x82ca99b0
	ctx.lr = 0x832C1DD4;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832C1DD4: 93BC0004  stw r29, 4(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C1DD8: 93DC0000  stw r30, 0(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1DDC: 93FC0008  stw r31, 8(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C1DE0: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832C1DE4: 4B9E766C  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832C1DE8: 57C4073E  clrlwi r4, r30, 0x1c
	ctx.r[4].u64 = ctx.r[30].u32 as u64 & 0x0000000Fu64;
	// 832C1DEC: 7D635B78  mr r3, r11
	ctx.r[3].u64 = ctx.r[11].u64;
	// 832C1DF0: 57DEE13E  srwi r30, r30, 4
	// 832C1DF4: 3BFFFFFC  addi r31, r31, -4
	ctx.r[31].s64 = ctx.r[31].s64 + -4;
	// 832C1DF8: 4B9E7BB9  bl 0x82ca99b0
	ctx.lr = 0x832C1DFC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832C1DFC: 93BC0004  stw r29, 4(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C1E00: 93DC0000  stw r30, 0(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1E04: 93FC0008  stw r31, 8(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C1E08: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832C1E0C: 4B9E7644  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832C1E10: 81630030  lwz r11, 0x30(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C1E14: 394B0004  addi r10, r11, 4
	ctx.r[10].s64 = ctx.r[11].s64 + 4;
	// 832C1E18: 91430000  stw r10, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832C1E1C: 91630004  stw r11, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832C1E20: 93BC0004  stw r29, 4(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C1E24: 93DC0000  stw r30, 0(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C1E28: 93FC0008  stw r31, 8(r28)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C1E2C: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832C1E30: 4B9E7620  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
}

pub fn sub_832C1E38(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    // ---- function 0x832C1E38 size=496
	// 832C1E38: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C1E3C: 4B9E75C1  bl 0x82ca93fc
	ctx.lr = 0x832C1E40;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93FC);
	// 832C1E40: 81630000  lwz r11, 0(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1E44: 81430004  lwz r10, 4(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C1E48: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C1E4C: 409A01D8  bne cr6, 0x832c2024
	if !ctx.cr[6].eq {
	pc = 0x832C2024; continue 'dispatch;
	}
	// 832C1E50: 81430028  lwz r10, 0x28(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C1E54: 3BA0FFFF  li r29, -1
	ctx.r[29].s64 = -1;
	// 832C1E58: 81640008  lwz r11, 8(r4)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C1E5C: 81240000  lwz r9, 0(r4)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1E60: 80C40004  lwz r6, 4(r4)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C1E64: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C1E68: 40980034  bge cr6, 0x832c1e9c
	if !ctx.cr[6].lt {
	pc = 0x832C1E9C; continue 'dispatch;
	}
	// 832C1E6C: 81060000  lwz r8, 0(r6)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1E70: 7FCB5050  subf r30, r11, r10
	ctx.r[30].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C1E74: 20EA0020  subfic r7, r10, 0x20
	ctx.xer.ca = ctx.r[10].u32 <= 32 as u32;
	ctx.r[7].s64 = (32 as i64) - ctx.r[10].s64;
	// 832C1E78: 7D1F5830  slw r31, r8, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[31].u64 = 0;
	} else {
		ctx.r[31].u64 = ((ctx.r[8].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1E7C: 7D6A5850  subf r11, r10, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C1E80: 7FEA4B78  or r10, r31, r9
	ctx.r[10].u64 = ctx.r[31].u64 | ctx.r[9].u64;
	// 832C1E84: 7FA93C30  srw r9, r29, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[29].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1E88: 7D3A5038  and r26, r9, r10
	ctx.r[26].u64 = ctx.r[9].u64 & ctx.r[10].u64;
	// 832C1E8C: 7D0AF430  srw r10, r8, r30
	if (ctx.r[30].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[8].u32) >> ((ctx.r[30].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1E90: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C1E94: 38C60004  addi r6, r6, 4
	ctx.r[6].s64 = ctx.r[6].s64 + 4;
	// 832C1E98: 48000018  b 0x832c1eb0
	pc = 0x832C1EB0; continue 'dispatch;
	// 832C1E9C: 210A0020  subfic r8, r10, 0x20
	ctx.xer.ca = ctx.r[10].u32 <= 32 as u32;
	ctx.r[8].s64 = (32 as i64) - ctx.r[10].s64;
	// 832C1EA0: 7D6A5850  subf r11, r10, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C1EA4: 7FA74430  srw r7, r29, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[29].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1EA8: 7CFA4838  and r26, r7, r9
	ctx.r[26].u64 = ctx.r[7].u64 & ctx.r[9].u64;
	// 832C1EAC: 7D2A5430  srw r10, r9, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[9].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1EB0: 2B1A0000  cmplwi cr6, r26, 0
	ctx.cr[6].compare_u32(ctx.r[26].u32, 0 as u32, &mut ctx.xer);
	// 832C1EB4: 419A0154  beq cr6, 0x832c2008
	if ctx.cr[6].eq {
	pc = 0x832C2008; continue 'dispatch;
	}
	// 832C1EB8: 8123002C  lwz r9, 0x2c(r3)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) } as u64;
	// 832C1EBC: 7F1A4840  cmplw cr6, r26, r9
	ctx.cr[6].compare_u32(ctx.r[26].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832C1EC0: 41990148  bgt cr6, 0x832c2008
	if ctx.cr[6].gt {
	pc = 0x832C2008; continue 'dispatch;
	}
	// 832C1EC4: 81230030  lwz r9, 0x30(r3)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C1EC8: 3BE30010  addi r31, r3, 0x10
	ctx.r[31].s64 = ctx.r[3].s64 + 16;
	// 832C1ECC: 83830020  lwz r28, 0x20(r3)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C1ED0: 7D09D214  add r8, r9, r26
	ctx.r[8].u64 = ctx.r[9].u64 + ctx.r[26].u64;
	// 832C1ED4: 83C30024  lwz r30, 0x24(r3)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(36 as u32) ) } as u64;
	// 832C1ED8: 7D394B78  mr r25, r9
	ctx.r[25].u64 = ctx.r[9].u64;
	// 832C1EDC: 91030004  stw r8, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[8].u32 ) };
	// 832C1EE0: 91230000  stw r9, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[9].u32 ) };
	// 832C1EE4: 3B5AFFFF  addi r26, r26, -1
	ctx.r[26].s64 = ctx.r[26].s64 + -1;
	// 832C1EE8: 7F0BE040  cmplw cr6, r11, r28
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[28].u32, &mut ctx.xer);
	// 832C1EEC: 4198002C  blt cr6, 0x832c1f18
	if ctx.cr[6].lt {
	pc = 0x832C1F18; continue 'dispatch;
	}
	// 832C1EF0: 213C0020  subfic r9, r28, 0x20
	ctx.xer.ca = ctx.r[28].u32 <= 32 as u32;
	ctx.r[9].s64 = (32 as i64) - ctx.r[28].s64;
	// 832C1EF4: 7FA84C30  srw r8, r29, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[8].u64 = 0;
	} else {
		ctx.r[8].u64 = ((ctx.r[29].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1EF8: 7D075038  and r7, r8, r10
	ctx.r[7].u64 = ctx.r[8].u64 & ctx.r[10].u64;
	// 832C1EFC: 7D27F0AE  lbzx r9, r7, r30
	ctx.r[9].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[30].u32)) } as u64;
	// 832C1F00: 5527073E  clrlwi r7, r9, 0x1c
	ctx.r[7].u64 = ctx.r[9].u32 as u64 & 0x0000000Fu64;
	// 832C1F04: 5529E13E  srwi r9, r9, 4
	// 832C1F08: 7D4A4C30  srw r10, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1F0C: 7F67F8AE  lbzx r27, r7, r31
	ctx.r[27].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[31].u32)) } as u64;
	// 832C1F10: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C1F14: 4800005C  b 0x832c1f70
	pc = 0x832C1F70; continue 'dispatch;
	// 832C1F18: 7F062840  cmplw cr6, r6, r5
	ctx.cr[6].compare_u32(ctx.r[6].u32, ctx.r[5].u32, &mut ctx.xer);
	// 832C1F1C: 409800EC  bge cr6, 0x832c2008
	if !ctx.cr[6].lt {
	pc = 0x832C2008; continue 'dispatch;
	}
	// 832C1F20: 80E60000  lwz r7, 0(r6)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1F24: 213C0020  subfic r9, r28, 0x20
	ctx.xer.ca = ctx.r[28].u32 <= 32 as u32;
	ctx.r[9].s64 = (32 as i64) - ctx.r[28].s64;
	// 832C1F28: 7CFB5830  slw r27, r7, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[27].u64 = 0;
	} else {
		ctx.r[27].u64 = ((ctx.r[7].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1F2C: 7FA84C30  srw r8, r29, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[8].u64 = 0;
	} else {
		ctx.r[8].u64 = ((ctx.r[29].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1F30: 7F695378  or r9, r27, r10
	ctx.r[9].u64 = ctx.r[27].u64 | ctx.r[10].u64;
	// 832C1F34: 7D294038  and r9, r9, r8
	ctx.r[9].u64 = ctx.r[9].u64 & ctx.r[8].u64;
	// 832C1F38: 7D29F0AE  lbzx r9, r9, r30
	ctx.r[9].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[9].u32.wrapping_add(ctx.r[30].u32)) } as u64;
	// 832C1F3C: 553B073E  clrlwi r27, r9, 0x1c
	ctx.r[27].u64 = ctx.r[9].u32 as u64 & 0x0000000Fu64;
	// 832C1F40: 5529E13E  srwi r9, r9, 4
	// 832C1F44: 7F0B4840  cmplw cr6, r11, r9
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832C1F48: 7F7BF8AE  lbzx r27, r27, r31
	ctx.r[27].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[27].u32.wrapping_add(ctx.r[31].u32)) } as u64;
	// 832C1F4C: 41980010  blt cr6, 0x832c1f5c
	if ctx.cr[6].lt {
	pc = 0x832C1F5C; continue 'dispatch;
	}
	// 832C1F50: 7D4A4C30  srw r10, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1F54: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C1F58: 48000018  b 0x832c1f70
	pc = 0x832C1F70; continue 'dispatch;
	// 832C1F5C: 7D4B4850  subf r10, r11, r9
	ctx.r[10].s64 = ctx.r[9].s64 - ctx.r[11].s64;
	// 832C1F60: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C1F64: 7CEA5430  srw r10, r7, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[7].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1F68: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C1F6C: 38C60004  addi r6, r6, 4
	ctx.r[6].s64 = ctx.r[6].s64 + 4;
	// 832C1F70: 7F0BE040  cmplw cr6, r11, r28
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[28].u32, &mut ctx.xer);
	// 832C1F74: 41980024  blt cr6, 0x832c1f98
	if ctx.cr[6].lt {
	pc = 0x832C1F98; continue 'dispatch;
	}
	// 832C1F78: 7D095038  and r9, r8, r10
	ctx.r[9].u64 = ctx.r[8].u64 & ctx.r[10].u64;
	// 832C1F7C: 7D09F0AE  lbzx r8, r9, r30
	ctx.r[8].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[9].u32.wrapping_add(ctx.r[30].u32)) } as u64;
	// 832C1F80: 5507073E  clrlwi r7, r8, 0x1c
	ctx.r[7].u64 = ctx.r[8].u32 as u64 & 0x0000000Fu64;
	// 832C1F84: 5509E13E  srwi r9, r8, 4
	// 832C1F88: 7D4A4C30  srw r10, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1F8C: 7D07F8AE  lbzx r8, r7, r31
	ctx.r[8].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[31].u32)) } as u64;
	// 832C1F90: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C1F94: 4800004C  b 0x832c1fe0
	pc = 0x832C1FE0; continue 'dispatch;
	// 832C1F98: 80E60000  lwz r7, 0(r6)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C1F9C: 7CE95830  slw r9, r7, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[7].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1FA0: 7D295378  or r9, r9, r10
	ctx.r[9].u64 = ctx.r[9].u64 | ctx.r[10].u64;
	// 832C1FA4: 7D284038  and r8, r9, r8
	ctx.r[8].u64 = ctx.r[9].u64 & ctx.r[8].u64;
	// 832C1FA8: 7D28F0AE  lbzx r9, r8, r30
	ctx.r[9].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[30].u32)) } as u64;
	// 832C1FAC: 5528073E  clrlwi r8, r9, 0x1c
	ctx.r[8].u64 = ctx.r[9].u32 as u64 & 0x0000000Fu64;
	// 832C1FB0: 5529E13E  srwi r9, r9, 4
	// 832C1FB4: 7F0B4840  cmplw cr6, r11, r9
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832C1FB8: 7D08F8AE  lbzx r8, r8, r31
	ctx.r[8].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[31].u32)) } as u64;
	// 832C1FBC: 41980010  blt cr6, 0x832c1fcc
	if ctx.cr[6].lt {
	pc = 0x832C1FCC; continue 'dispatch;
	}
	// 832C1FC0: 7D4A4C30  srw r10, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1FC4: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C1FC8: 48000018  b 0x832c1fe0
	pc = 0x832C1FE0; continue 'dispatch;
	// 832C1FCC: 7D4B4850  subf r10, r11, r9
	ctx.r[10].s64 = ctx.r[9].s64 - ctx.r[11].s64;
	// 832C1FD0: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C1FD4: 7CEA5430  srw r10, r7, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[7].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C1FD8: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C1FDC: 38C60004  addi r6, r6, 4
	ctx.r[6].s64 = ctx.r[6].s64 + 4;
	// 832C1FE0: 55092036  slwi r9, r8, 4
	// 832C1FE4: 2B1A0000  cmplwi cr6, r26, 0
	ctx.cr[6].compare_u32(ctx.r[26].u32, 0 as u32, &mut ctx.xer);
	// 832C1FE8: 7D28DB78  or r8, r9, r27
	ctx.r[8].u64 = ctx.r[9].u64 | ctx.r[27].u64;
	// 832C1FEC: 99190000  stb r8, 0(r25)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[25].u32.wrapping_add(0 as u32), ctx.r[8].u8 ) };
	// 832C1FF0: 3B390001  addi r25, r25, 1
	ctx.r[25].s64 = ctx.r[25].s64 + 1;
	// 832C1FF4: 409AFEF0  bne cr6, 0x832c1ee4
	if !ctx.cr[6].eq {
	pc = 0x832C1EE4; continue 'dispatch;
	}
	// 832C1FF8: 90C40004  stw r6, 4(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(4 as u32), ctx.r[6].u32 ) };
	// 832C1FFC: 91440000  stw r10, 0(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832C2000: 91640008  stw r11, 8(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(8 as u32), ctx.r[11].u32 ) };
	// 832C2004: 4B9E7448  b 0x82ca944c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA944C);
	return;
	// 832C2008: 81230030  lwz r9, 0x30(r3)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C200C: 39090004  addi r8, r9, 4
	ctx.r[8].s64 = ctx.r[9].s64 + 4;
	// 832C2010: 91030000  stw r8, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[8].u32 ) };
	// 832C2014: 91230004  stw r9, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[9].u32 ) };
	// 832C2018: 90C40004  stw r6, 4(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(4 as u32), ctx.r[6].u32 ) };
	// 832C201C: 91440000  stw r10, 0(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832C2020: 91640008  stw r11, 8(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(8 as u32), ctx.r[11].u32 ) };
	// 832C2024: 4B9E7428  b 0x82ca944c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA944C);
	return;
}

pub fn sub_832C2028(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C2028 size=656
	// 832C2028: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C202C: 4B9E73D1  bl 0x82ca93fc
	ctx.lr = 0x832C2030;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93FC);
	// 832C2030: 9421FF70  stwu r1, -0x90(r1)
	ea = ctx.r[1].u32.wrapping_add(-144 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C2034: 81630000  lwz r11, 0(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2038: 7C9B2378  mr r27, r4
	ctx.r[27].u64 = ctx.r[4].u64;
	// 832C203C: 81430004  lwz r10, 4(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C2040: 7CBA2B78  mr r26, r5
	ctx.r[26].u64 = ctx.r[5].u64;
	// 832C2044: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C2048: 409A0268  bne cr6, 0x832c22b0
	if !ctx.cr[6].eq {
	pc = 0x832C22B0; continue 'dispatch;
	}
	// 832C204C: 81630028  lwz r11, 0x28(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C2050: 3900FFFF  li r8, -1
	ctx.r[8].s64 = -1;
	// 832C2054: 815B0008  lwz r10, 8(r27)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C2058: 813B0000  lwz r9, 0(r27)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C205C: 83BB0004  lwz r29, 4(r27)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C2060: 7F0A5840  cmplw cr6, r10, r11
	ctx.cr[6].compare_u32(ctx.r[10].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C2064: 40980030  bge cr6, 0x832c2094
	if !ctx.cr[6].lt {
	pc = 0x832C2094; continue 'dispatch;
	}
	// 832C2068: 80FD0000  lwz r7, 0(r29)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C206C: 20CB0020  subfic r6, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[6].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C2070: 7C8A5850  subf r4, r10, r11
	ctx.r[4].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C2074: 7CE55030  slw r5, r7, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[5].u64 = 0;
	} else {
		ctx.r[5].u64 = ((ctx.r[7].u32) << ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2078: 7D6B5050  subf r11, r11, r10
	ctx.r[11].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C207C: 7CA94B78  or r9, r5, r9
	ctx.r[9].u64 = ctx.r[5].u64 | ctx.r[9].u64;
	// 832C2080: 7D063430  srw r6, r8, r6
	if (ctx.r[6].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[8].u32) >> ((ctx.r[6].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2084: 7CFE2430  srw r30, r7, r4
	if (ctx.r[4].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[7].u32) >> ((ctx.r[4].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2088: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C208C: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C2090: 48000014  b 0x832c20a4
	pc = 0x832C20A4; continue 'dispatch;
	// 832C2094: 20EB0020  subfic r7, r11, 0x20
	ctx.xer.ca = ctx.r[11].u32 <= 32 as u32;
	ctx.r[7].s64 = (32 as i64) - ctx.r[11].s64;
	// 832C2098: 7FEB5050  subf r31, r11, r10
	ctx.r[31].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C209C: 7D063C30  srw r6, r8, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[8].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C20A0: 7D3E5C30  srw r30, r9, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[9].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C20A4: 7CC54838  and r5, r6, r9
	ctx.r[5].u64 = ctx.r[6].u64 & ctx.r[9].u64;
	// 832C20A8: 2B050000  cmplwi cr6, r5, 0
	ctx.cr[6].compare_u32(ctx.r[5].u32, 0 as u32, &mut ctx.xer);
	// 832C20AC: 419A01E8  beq cr6, 0x832c2294
	if ctx.cr[6].eq {
	pc = 0x832C2294; continue 'dispatch;
	}
	// 832C20B0: 8163002C  lwz r11, 0x2c(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) } as u64;
	// 832C20B4: 7F055840  cmplw cr6, r5, r11
	ctx.cr[6].compare_u32(ctx.r[5].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C20B8: 419901DC  bgt cr6, 0x832c2294
	if ctx.cr[6].gt {
	pc = 0x832C2294; continue 'dispatch;
	}
	// 832C20BC: 81430030  lwz r10, 0x30(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C20C0: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832C20C4: 7D6A2A14  add r11, r10, r5
	ctx.r[11].u64 = ctx.r[10].u64 + ctx.r[5].u64;
	// 832C20C8: 91630004  stw r11, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832C20CC: 91430000  stw r10, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832C20D0: 409A0018  bne cr6, 0x832c20e8
	if !ctx.cr[6].eq {
	pc = 0x832C20E8; continue 'dispatch;
	}
	// 832C20D4: 817D0000  lwz r11, 0(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C20D8: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C20DC: 3BE0001F  li r31, 0x1f
	ctx.r[31].s64 = 31;
	// 832C20E0: 557EF87E  srwi r30, r11, 1
	// 832C20E4: 48000010  b 0x832c20f4
	pc = 0x832C20F4; continue 'dispatch;
	// 832C20E8: 7FCBF378  mr r11, r30
	ctx.r[11].u64 = ctx.r[30].u64;
	// 832C20EC: 57DEF87E  srwi r30, r30, 1
	// 832C20F0: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832C20F4: 556B07FE  clrlwi r11, r11, 0x1f
	ctx.r[11].u64 = ctx.r[11].u32 as u64 & 0x00000001u64;
	// 832C20F8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C20FC: 409A0104  bne cr6, 0x832c2200
	if !ctx.cr[6].eq {
	pc = 0x832C2200; continue 'dispatch;
	}
	// 832C2100: 80E30020  lwz r7, 0x20(r3)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C2104: 7D5C5378  mr r28, r10
	ctx.r[28].u64 = ctx.r[10].u64;
	// 832C2108: 80C30024  lwz r6, 0x24(r3)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(36 as u32) ) } as u64;
	// 832C210C: 38830010  addi r4, r3, 0x10
	ctx.r[4].s64 = ctx.r[3].s64 + 16;
	// 832C2110: 38A5FFFF  addi r5, r5, -1
	ctx.r[5].s64 = ctx.r[5].s64 + -1;
	// 832C2114: 7F1F3840  cmplw cr6, r31, r7
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[7].u32, &mut ctx.xer);
	// 832C2118: 4198002C  blt cr6, 0x832c2144
	if ctx.cr[6].lt {
	pc = 0x832C2144; continue 'dispatch;
	}
	// 832C211C: 21670020  subfic r11, r7, 0x20
	ctx.xer.ca = ctx.r[7].u32 <= 32 as u32;
	ctx.r[11].s64 = (32 as i64) - ctx.r[7].s64;
	// 832C2120: 7D0A5C30  srw r10, r8, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[8].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2124: 7D49F038  and r9, r10, r30
	ctx.r[9].u64 = ctx.r[10].u64 & ctx.r[30].u64;
	// 832C2128: 7D6930AE  lbzx r11, r9, r6
	ctx.r[11].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[9].u32.wrapping_add(ctx.r[6].u32)) } as u64;
	// 832C212C: 556A073E  clrlwi r10, r11, 0x1c
	ctx.r[10].u64 = ctx.r[11].u32 as u64 & 0x0000000Fu64;
	// 832C2130: 556BE13E  srwi r11, r11, 4
	// 832C2134: 7FDE5C30  srw r30, r30, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2138: 7D2A20AE  lbzx r9, r10, r4
	ctx.r[9].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[4].u32)) } as u64;
	// 832C213C: 7FEBF850  subf r31, r11, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[11].s64;
	// 832C2140: 4800005C  b 0x832c219c
	pc = 0x832C219C; continue 'dispatch;
	// 832C2144: 7F1DD040  cmplw cr6, r29, r26
	ctx.cr[6].compare_u32(ctx.r[29].u32, ctx.r[26].u32, &mut ctx.xer);
	// 832C2148: 4098014C  bge cr6, 0x832c2294
	if !ctx.cr[6].lt {
	pc = 0x832C2294; continue 'dispatch;
	}
	// 832C214C: 815D0000  lwz r10, 0(r29)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2150: 21670020  subfic r11, r7, 0x20
	ctx.xer.ca = ctx.r[7].u32 <= 32 as u32;
	ctx.r[11].s64 = (32 as i64) - ctx.r[7].s64;
	// 832C2154: 7D49F830  slw r9, r10, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[10].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2158: 7D29F378  or r9, r9, r30
	ctx.r[9].u64 = ctx.r[9].u64 | ctx.r[30].u64;
	// 832C215C: 7D0B5C30  srw r11, r8, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[11].u64 = 0;
	} else {
		ctx.r[11].u64 = ((ctx.r[8].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2160: 7D6B4838  and r11, r11, r9
	ctx.r[11].u64 = ctx.r[11].u64 & ctx.r[9].u64;
	// 832C2164: 7D2B30AE  lbzx r9, r11, r6
	ctx.r[9].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[6].u32)) } as u64;
	// 832C2168: 5539073E  clrlwi r25, r9, 0x1c
	ctx.r[25].u64 = ctx.r[9].u32 as u64 & 0x0000000Fu64;
	// 832C216C: 552BE13E  srwi r11, r9, 4
	// 832C2170: 7F1F5840  cmplw cr6, r31, r11
	ctx.cr[6].compare_u32(ctx.r[31].u32, ctx.r[11].u32, &mut ctx.xer);
	// 832C2174: 7D3920AE  lbzx r9, r25, r4
	ctx.r[9].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[25].u32.wrapping_add(ctx.r[4].u32)) } as u64;
	// 832C2178: 41980010  blt cr6, 0x832c2188
	if ctx.cr[6].lt {
	pc = 0x832C2188; continue 'dispatch;
	}
	// 832C217C: 7FDE5C30  srw r30, r30, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[30].u32) >> ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2180: 7FEBF850  subf r31, r11, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[11].s64;
	// 832C2184: 48000018  b 0x832c219c
	pc = 0x832C219C; continue 'dispatch;
	// 832C2188: 7FDF5850  subf r30, r31, r11
	ctx.r[30].s64 = ctx.r[11].s64 - ctx.r[31].s64;
	// 832C218C: 7D6BF850  subf r11, r11, r31
	ctx.r[11].s64 = ctx.r[31].s64 - ctx.r[11].s64;
	// 832C2190: 7D5EF430  srw r30, r10, r30
	if (ctx.r[30].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[10].u32) >> ((ctx.r[30].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2194: 3BEB0020  addi r31, r11, 0x20
	ctx.r[31].s64 = ctx.r[11].s64 + 32;
	// 832C2198: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C219C: 2F090000  cmpwi cr6, r9, 0
	ctx.cr[6].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832C21A0: 419A003C  beq cr6, 0x832c21dc
	if ctx.cr[6].eq {
	pc = 0x832C21DC; continue 'dispatch;
	}
	// 832C21A4: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832C21A8: 409A0018  bne cr6, 0x832c21c0
	if !ctx.cr[6].eq {
	pc = 0x832C21C0; continue 'dispatch;
	}
	// 832C21AC: 817D0000  lwz r11, 0(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C21B0: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C21B4: 3BE0001F  li r31, 0x1f
	ctx.r[31].s64 = 31;
	// 832C21B8: 557EF87E  srwi r30, r11, 1
	// 832C21BC: 48000010  b 0x832c21cc
	pc = 0x832C21CC; continue 'dispatch;
	// 832C21C0: 7FCBF378  mr r11, r30
	ctx.r[11].u64 = ctx.r[30].u64;
	// 832C21C4: 57DEF87E  srwi r30, r30, 1
	// 832C21C8: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832C21CC: 556B07FE  clrlwi r11, r11, 0x1f
	ctx.r[11].u64 = ctx.r[11].u32 as u64 & 0x00000001u64;
	// 832C21D0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C21D4: 419A0008  beq cr6, 0x832c21dc
	if ctx.cr[6].eq {
	pc = 0x832C21DC; continue 'dispatch;
	}
	// 832C21D8: 7D2900D0  neg r9, r9
	ctx.r[9].s64 = -ctx.r[9].s64;
	// 832C21DC: 993C0000  stb r9, 0(r28)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[9].u8 ) };
	// 832C21E0: 2B050000  cmplwi cr6, r5, 0
	ctx.cr[6].compare_u32(ctx.r[5].u32, 0 as u32, &mut ctx.xer);
	// 832C21E4: 3B9C0001  addi r28, r28, 1
	ctx.r[28].s64 = ctx.r[28].s64 + 1;
	// 832C21E8: 409AFF28  bne cr6, 0x832c2110
	if !ctx.cr[6].eq {
	pc = 0x832C2110; continue 'dispatch;
	}
	// 832C21EC: 93BB0004  stw r29, 4(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C21F0: 93DB0000  stw r30, 0(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C21F4: 93FB0008  stw r31, 8(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C21F8: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832C21FC: 4B9E7250  b 0x82ca944c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA944C);
	return;
	// 832C2200: 2B1F0004  cmplwi cr6, r31, 4
	ctx.cr[6].compare_u32(ctx.r[31].u32, 4 as u32, &mut ctx.xer);
	// 832C2204: 40980028  bge cr6, 0x832c222c
	if !ctx.cr[6].lt {
	pc = 0x832C222C; continue 'dispatch;
	}
	// 832C2208: 817D0000  lwz r11, 0(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C220C: 213F0004  subfic r9, r31, 4
	ctx.xer.ca = ctx.r[31].u32 <= 4 as u32;
	ctx.r[9].s64 = (4 as i64) - ctx.r[31].s64;
	// 832C2210: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C2214: 7D68F830  slw r8, r11, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[8].u64 = 0;
	} else {
		ctx.r[8].u64 = ((ctx.r[11].u32) << ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2218: 7D07F378  or r7, r8, r30
	ctx.r[7].u64 = ctx.r[8].u64 | ctx.r[30].u64;
	// 832C221C: 7D7E4C30  srw r30, r11, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[11].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2220: 54E4073E  clrlwi r4, r7, 0x1c
	ctx.r[4].u64 = ctx.r[7].u32 as u64 & 0x0000000Fu64;
	// 832C2224: 3BFF001C  addi r31, r31, 0x1c
	ctx.r[31].s64 = ctx.r[31].s64 + 28;
	// 832C2228: 48000010  b 0x832c2238
	pc = 0x832C2238; continue 'dispatch;
	// 832C222C: 57C4073E  clrlwi r4, r30, 0x1c
	ctx.r[4].u64 = ctx.r[30].u32 as u64 & 0x0000000Fu64;
	// 832C2230: 57DEE13E  srwi r30, r30, 4
	// 832C2234: 3BFFFFFC  addi r31, r31, -4
	ctx.r[31].s64 = ctx.r[31].s64 + -4;
	// 832C2238: 2F040000  cmpwi cr6, r4, 0
	ctx.cr[6].compare_i32(ctx.r[4].s32, 0, &mut ctx.xer);
	// 832C223C: 419A003C  beq cr6, 0x832c2278
	if ctx.cr[6].eq {
	pc = 0x832C2278; continue 'dispatch;
	}
	// 832C2240: 2B1F0000  cmplwi cr6, r31, 0
	ctx.cr[6].compare_u32(ctx.r[31].u32, 0 as u32, &mut ctx.xer);
	// 832C2244: 409A0018  bne cr6, 0x832c225c
	if !ctx.cr[6].eq {
	pc = 0x832C225C; continue 'dispatch;
	}
	// 832C2248: 817D0000  lwz r11, 0(r29)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C224C: 3BBD0004  addi r29, r29, 4
	ctx.r[29].s64 = ctx.r[29].s64 + 4;
	// 832C2250: 3BE0001F  li r31, 0x1f
	ctx.r[31].s64 = 31;
	// 832C2254: 557EF87E  srwi r30, r11, 1
	// 832C2258: 48000010  b 0x832c2268
	pc = 0x832C2268; continue 'dispatch;
	// 832C225C: 7FCBF378  mr r11, r30
	ctx.r[11].u64 = ctx.r[30].u64;
	// 832C2260: 57DEF87E  srwi r30, r30, 1
	// 832C2264: 3BFFFFFF  addi r31, r31, -1
	ctx.r[31].s64 = ctx.r[31].s64 + -1;
	// 832C2268: 556B07FE  clrlwi r11, r11, 0x1f
	ctx.r[11].u64 = ctx.r[11].u32 as u64 & 0x00000001u64;
	// 832C226C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C2270: 419A0008  beq cr6, 0x832c2278
	if ctx.cr[6].eq {
	pc = 0x832C2278; continue 'dispatch;
	}
	// 832C2274: 7C8400D0  neg r4, r4
	ctx.r[4].s64 = -ctx.r[4].s64;
	// 832C2278: 7D435378  mr r3, r10
	ctx.r[3].u64 = ctx.r[10].u64;
	// 832C227C: 4B9E7735  bl 0x82ca99b0
	ctx.lr = 0x832C2280;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA99B0);
	// 832C2280: 93BB0004  stw r29, 4(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C2284: 93DB0000  stw r30, 0(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C2288: 93FB0008  stw r31, 8(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C228C: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832C2290: 4B9E71BC  b 0x82ca944c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA944C);
	return;
	// 832C2294: 81630030  lwz r11, 0x30(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C2298: 394B0004  addi r10, r11, 4
	ctx.r[10].s64 = ctx.r[11].s64 + 4;
	// 832C229C: 91430000  stw r10, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832C22A0: 91630004  stw r11, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832C22A4: 93BB0004  stw r29, 4(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(4 as u32), ctx.r[29].u32 ) };
	// 832C22A8: 93DB0000  stw r30, 0(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(0 as u32), ctx.r[30].u32 ) };
	// 832C22AC: 93FB0008  stw r31, 8(r27)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(8 as u32), ctx.r[31].u32 ) };
	// 832C22B0: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832C22B4: 4B9E7198  b 0x82ca944c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA944C);
	return;
}

pub fn sub_832C22B8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    // ---- function 0x832C22B8 size=864
	// 832C22B8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C22BC: 4B9E713D  bl 0x82ca93f8
	ctx.lr = 0x832C22C0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93F8);
	// 832C22C0: 81630000  lwz r11, 0(r3)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C22C4: 81430004  lwz r10, 4(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C22C8: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C22CC: 409A0344  bne cr6, 0x832c2610
	if !ctx.cr[6].eq {
	pc = 0x832C2610; continue 'dispatch;
	}
	// 832C22D0: 81430028  lwz r10, 0x28(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C22D4: 3B20FFFF  li r25, -1
	ctx.r[25].s64 = -1;
	// 832C22D8: 81640008  lwz r11, 8(r4)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C22DC: 81240000  lwz r9, 0(r4)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C22E0: 80E40004  lwz r7, 4(r4)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C22E4: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C22E8: 40980034  bge cr6, 0x832c231c
	if !ctx.cr[6].lt {
	pc = 0x832C231C; continue 'dispatch;
	}
	// 832C22EC: 81070000  lwz r8, 0(r7)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C22F0: 7FAB5050  subf r29, r11, r10
	ctx.r[29].s64 = ctx.r[10].s64 - ctx.r[11].s64;
	// 832C22F4: 20CA0020  subfic r6, r10, 0x20
	ctx.xer.ca = ctx.r[10].u32 <= 32 as u32;
	ctx.r[6].s64 = (32 as i64) - ctx.r[10].s64;
	// 832C22F8: 7D1F5830  slw r31, r8, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[31].u64 = 0;
	} else {
		ctx.r[31].u64 = ((ctx.r[8].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C22FC: 7D6A5850  subf r11, r10, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C2300: 7FEA4B78  or r10, r31, r9
	ctx.r[10].u64 = ctx.r[31].u64 | ctx.r[9].u64;
	// 832C2304: 7F293430  srw r9, r25, r6
	if (ctx.r[6].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[25].u32) >> ((ctx.r[6].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2308: 7D3E5038  and r30, r9, r10
	ctx.r[30].u64 = ctx.r[9].u64 & ctx.r[10].u64;
	// 832C230C: 7D0AEC30  srw r10, r8, r29
	if (ctx.r[29].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[8].u32) >> ((ctx.r[29].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2310: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C2314: 38E70004  addi r7, r7, 4
	ctx.r[7].s64 = ctx.r[7].s64 + 4;
	// 832C2318: 48000018  b 0x832c2330
	pc = 0x832C2330; continue 'dispatch;
	// 832C231C: 210A0020  subfic r8, r10, 0x20
	ctx.xer.ca = ctx.r[10].u32 <= 32 as u32;
	ctx.r[8].s64 = (32 as i64) - ctx.r[10].s64;
	// 832C2320: 7D6A5850  subf r11, r10, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[10].s64;
	// 832C2324: 7F264430  srw r6, r25, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[25].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2328: 7CDE4838  and r30, r6, r9
	ctx.r[30].u64 = ctx.r[6].u64 & ctx.r[9].u64;
	// 832C232C: 7D2A5430  srw r10, r9, r10
	if (ctx.r[10].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[9].u32) >> ((ctx.r[10].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2330: 2B1E0000  cmplwi cr6, r30, 0
	ctx.cr[6].compare_u32(ctx.r[30].u32, 0 as u32, &mut ctx.xer);
	// 832C2334: 419A02C0  beq cr6, 0x832c25f4
	if ctx.cr[6].eq {
	pc = 0x832C25F4; continue 'dispatch;
	}
	// 832C2338: 8123002C  lwz r9, 0x2c(r3)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) } as u64;
	// 832C233C: 5528F87E  srwi r8, r9, 1
	// 832C2340: 7F1E4040  cmplw cr6, r30, r8
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[8].u32, &mut ctx.xer);
	// 832C2344: 419902B0  bgt cr6, 0x832c25f4
	if ctx.cr[6].gt {
	pc = 0x832C25F4; continue 'dispatch;
	}
	// 832C2348: 8123000C  lwz r9, 0xc(r3)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(12 as u32) ) } as u64;
	// 832C234C: 80C30030  lwz r6, 0x30(r3)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C2350: 2F090000  cmpwi cr6, r9, 0
	ctx.cr[6].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832C2354: 81230008  lwz r9, 8(r3)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C2358: 419A00A0  beq cr6, 0x832c23f8
	if ctx.cr[6].eq {
	pc = 0x832C23F8; continue 'dispatch;
	}
	// 832C235C: 3909FFFF  addi r8, r9, -1
	ctx.r[8].s64 = ctx.r[9].s64 + -1;
	// 832C2360: 7F0B4040  cmplw cr6, r11, r8
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[8].u32, &mut ctx.xer);
	// 832C2364: 40980038  bge cr6, 0x832c239c
	if !ctx.cr[6].lt {
	pc = 0x832C239C; continue 'dispatch;
	}
	// 832C2368: 83E70000  lwz r31, 0(r7)
	ctx.r[31].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C236C: 23A90021  subfic r29, r9, 0x21
	ctx.xer.ca = ctx.r[9].u32 <= 33 as u32;
	ctx.r[29].s64 = (33 as i64) - ctx.r[9].s64;
	// 832C2370: 7D0B4850  subf r8, r11, r9
	ctx.r[8].s64 = ctx.r[9].s64 - ctx.r[11].s64;
	// 832C2374: 7FFC5830  slw r28, r31, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[28].u64 = 0;
	} else {
		ctx.r[28].u64 = ((ctx.r[31].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2378: 7F8A5378  or r10, r28, r10
	ctx.r[10].u64 = ctx.r[28].u64 | ctx.r[10].u64;
	// 832C237C: 7F3DEC30  srw r29, r25, r29
	if (ctx.r[29].u8 & 0x20) != 0 {
		ctx.r[29].u64 = 0;
	} else {
		ctx.r[29].u64 = ((ctx.r[25].u32) >> ((ctx.r[29].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2380: 3908FFFF  addi r8, r8, -1
	ctx.r[8].s64 = ctx.r[8].s64 + -1;
	// 832C2384: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C2388: 7FBB5038  and r27, r29, r10
	ctx.r[27].u64 = ctx.r[29].u64 & ctx.r[10].u64;
	// 832C238C: 7FEA4430  srw r10, r31, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[31].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2390: 396B0021  addi r11, r11, 0x21
	ctx.r[11].s64 = ctx.r[11].s64 + 33;
	// 832C2394: 38E70004  addi r7, r7, 4
	ctx.r[7].s64 = ctx.r[7].s64 + 4;
	// 832C2398: 4800001C  b 0x832c23b4
	pc = 0x832C23B4; continue 'dispatch;
	// 832C239C: 23E90021  subfic r31, r9, 0x21
	ctx.xer.ca = ctx.r[9].u32 <= 33 as u32;
	ctx.r[31].s64 = (33 as i64) - ctx.r[9].s64;
	// 832C23A0: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C23A4: 7F29FC30  srw r9, r25, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[9].u64 = 0;
	} else {
		ctx.r[9].u64 = ((ctx.r[25].u32) >> ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C23A8: 7D3B5038  and r27, r9, r10
	ctx.r[27].u64 = ctx.r[9].u64 & ctx.r[10].u64;
	// 832C23AC: 7D4A4430  srw r10, r10, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[10].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C23B0: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832C23B4: 2F1B0000  cmpwi cr6, r27, 0
	ctx.cr[6].compare_i32(ctx.r[27].s32, 0, &mut ctx.xer);
	// 832C23B8: 419A008C  beq cr6, 0x832c2444
	if ctx.cr[6].eq {
	pc = 0x832C2444; continue 'dispatch;
	}
	// 832C23BC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C23C0: 409A0018  bne cr6, 0x832c23d8
	if !ctx.cr[6].eq {
	pc = 0x832C23D8; continue 'dispatch;
	}
	// 832C23C4: 81270000  lwz r9, 0(r7)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C23C8: 38E70004  addi r7, r7, 4
	ctx.r[7].s64 = ctx.r[7].s64 + 4;
	// 832C23CC: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C23D0: 552AF87E  srwi r10, r9, 1
	// 832C23D4: 48000010  b 0x832c23e4
	pc = 0x832C23E4; continue 'dispatch;
	// 832C23D8: 7D495378  mr r9, r10
	ctx.r[9].u64 = ctx.r[10].u64;
	// 832C23DC: 554AF87E  srwi r10, r10, 1
	// 832C23E0: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C23E4: 552907FE  clrlwi r9, r9, 0x1f
	ctx.r[9].u64 = ctx.r[9].u32 as u64 & 0x00000001u64;
	// 832C23E8: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832C23EC: 419A0058  beq cr6, 0x832c2444
	if ctx.cr[6].eq {
	pc = 0x832C2444; continue 'dispatch;
	}
	// 832C23F0: 7F7B00D0  neg r27, r27
	ctx.r[27].s64 = -ctx.r[27].s64;
	// 832C23F4: 48000050  b 0x832c2444
	pc = 0x832C2444; continue 'dispatch;
	// 832C23F8: 7F0B4840  cmplw cr6, r11, r9
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832C23FC: 40980034  bge cr6, 0x832c2430
	if !ctx.cr[6].lt {
	pc = 0x832C2430; continue 'dispatch;
	}
	// 832C2400: 81070000  lwz r8, 0(r7)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2404: 23E90020  subfic r31, r9, 0x20
	ctx.xer.ca = ctx.r[9].u32 <= 32 as u32;
	ctx.r[31].s64 = (32 as i64) - ctx.r[9].s64;
	// 832C2408: 7FAB4850  subf r29, r11, r9
	ctx.r[29].s64 = ctx.r[9].s64 - ctx.r[11].s64;
	// 832C240C: 7D1C5830  slw r28, r8, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[28].u64 = 0;
	} else {
		ctx.r[28].u64 = ((ctx.r[8].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2410: 7F8A5378  or r10, r28, r10
	ctx.r[10].u64 = ctx.r[28].u64 | ctx.r[10].u64;
	// 832C2414: 7F3FFC30  srw r31, r25, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[31].u64 = 0;
	} else {
		ctx.r[31].u64 = ((ctx.r[25].u32) >> ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2418: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C241C: 7FFB5038  and r27, r31, r10
	ctx.r[27].u64 = ctx.r[31].u64 & ctx.r[10].u64;
	// 832C2420: 7D0AEC30  srw r10, r8, r29
	if (ctx.r[29].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[8].u32) >> ((ctx.r[29].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2424: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C2428: 38E70004  addi r7, r7, 4
	ctx.r[7].s64 = ctx.r[7].s64 + 4;
	// 832C242C: 48000018  b 0x832c2444
	pc = 0x832C2444; continue 'dispatch;
	// 832C2430: 21090020  subfic r8, r9, 0x20
	ctx.xer.ca = ctx.r[9].u32 <= 32 as u32;
	ctx.r[8].s64 = (32 as i64) - ctx.r[9].s64;
	// 832C2434: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C2438: 7F284430  srw r8, r25, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[8].u64 = 0;
	} else {
		ctx.r[8].u64 = ((ctx.r[25].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C243C: 7D1B5038  and r27, r8, r10
	ctx.r[27].u64 = ctx.r[8].u64 & ctx.r[10].u64;
	// 832C2440: 7D4A4C30  srw r10, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2444: 57C9083C  slwi r9, r30, 1
	// 832C2448: 90C30000  stw r6, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[6].u32 ) };
	// 832C244C: 577F043E  clrlwi r31, r27, 0x10
	ctx.r[31].u64 = ctx.r[27].u32 as u64 & 0x0000FFFFu64;
	// 832C2450: 7D293214  add r9, r9, r6
	ctx.r[9].u64 = ctx.r[9].u64 + ctx.r[6].u64;
	// 832C2454: 371EFFFF  addic. r24, r30, -1
	ctx.xer.ca = (ctx.r[30].u32 > (!(-1 as u32)));
	ctx.r[24].s64 = ctx.r[30].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[24].s32, 0, &mut ctx.xer);
	// 832C2458: B3E60000  sth r31, 0(r6)
	unsafe { crate::rt::store_u16( base as *mut u8, ctx.r[6].u32.wrapping_add(0 as u32), ctx.r[31].u16 ) };
	// 832C245C: 3B860002  addi r28, r6, 2
	ctx.r[28].s64 = ctx.r[6].s64 + 2;
	// 832C2460: 91230004  stw r9, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[9].u32 ) };
	// 832C2464: 418201A0  beq 0x832c2604
	if ctx.cr[0].eq {
	pc = 0x832C2604; continue 'dispatch;
	}
	// 832C2468: 2B180008  cmplwi cr6, r24, 8
	ctx.cr[6].compare_u32(ctx.r[24].u32, 8 as u32, &mut ctx.xer);
	// 832C246C: 3B400008  li r26, 8
	ctx.r[26].s64 = 8;
	// 832C2470: 41990008  bgt cr6, 0x832c2478
	if ctx.cr[6].gt {
	pc = 0x832C2478; continue 'dispatch;
	}
	// 832C2474: 7F1AC378  mr r26, r24
	ctx.r[26].u64 = ctx.r[24].u64;
	// 832C2478: 2B0B0004  cmplwi cr6, r11, 4
	ctx.cr[6].compare_u32(ctx.r[11].u32, 4 as u32, &mut ctx.xer);
	// 832C247C: 40980030  bge cr6, 0x832c24ac
	if !ctx.cr[6].lt {
	pc = 0x832C24AC; continue 'dispatch;
	}
	// 832C2480: 7F072840  cmplw cr6, r7, r5
	ctx.cr[6].compare_u32(ctx.r[7].u32, ctx.r[5].u32, &mut ctx.xer);
	// 832C2484: 40980170  bge cr6, 0x832c25f4
	if !ctx.cr[6].lt {
	pc = 0x832C25F4; continue 'dispatch;
	}
	// 832C2488: 81270000  lwz r9, 0(r7)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C248C: 210B0004  subfic r8, r11, 4
	ctx.xer.ca = ctx.r[11].u32 <= 4 as u32;
	ctx.r[8].s64 = (4 as i64) - ctx.r[11].s64;
	// 832C2490: 38E70004  addi r7, r7, 4
	ctx.r[7].s64 = ctx.r[7].s64 + 4;
	// 832C2494: 7D265830  slw r6, r9, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[9].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2498: 7CC65378  or r6, r6, r10
	ctx.r[6].u64 = ctx.r[6].u64 | ctx.r[10].u64;
	// 832C249C: 7D2A4430  srw r10, r9, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[9].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C24A0: 54C9073E  clrlwi r9, r6, 0x1c
	ctx.r[9].u64 = ctx.r[6].u32 as u64 & 0x0000000Fu64;
	// 832C24A4: 396B001C  addi r11, r11, 0x1c
	ctx.r[11].s64 = ctx.r[11].s64 + 28;
	// 832C24A8: 48000010  b 0x832c24b8
	pc = 0x832C24B8; continue 'dispatch;
	// 832C24AC: 5549073E  clrlwi r9, r10, 0x1c
	ctx.r[9].u64 = ctx.r[10].u32 as u64 & 0x0000000Fu64;
	// 832C24B0: 554AE13E  srwi r10, r10, 4
	// 832C24B4: 396BFFFC  addi r11, r11, -4
	ctx.r[11].s64 = ctx.r[11].s64 + -4;
	// 832C24B8: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832C24BC: 419A00B4  beq cr6, 0x832c2570
	if ctx.cr[6].eq {
	pc = 0x832C2570; continue 'dispatch;
	}
	// 832C24C0: 7F5DD378  mr r29, r26
	ctx.r[29].u64 = ctx.r[26].u64;
	// 832C24C4: 2F1A0000  cmpwi cr6, r26, 0
	ctx.cr[6].compare_i32(ctx.r[26].s32, 0, &mut ctx.xer);
	// 832C24C8: 419A0114  beq cr6, 0x832c25dc
	if ctx.cr[6].eq {
	pc = 0x832C25DC; continue 'dispatch;
	}
	// 832C24CC: 21090020  subfic r8, r9, 0x20
	ctx.xer.ca = ctx.r[9].u32 <= 32 as u32;
	ctx.r[8].s64 = (32 as i64) - ctx.r[9].s64;
	// 832C24D0: 7F3E4430  srw r30, r25, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[30].u64 = 0;
	} else {
		ctx.r[30].u64 = ((ctx.r[25].u32) >> ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C24D4: 3BBDFFFF  addi r29, r29, -1
	ctx.r[29].s64 = ctx.r[29].s64 + -1;
	// 832C24D8: 7F0B4840  cmplw cr6, r11, r9
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832C24DC: 4098002C  bge cr6, 0x832c2508
	if !ctx.cr[6].lt {
	pc = 0x832C2508; continue 'dispatch;
	}
	// 832C24E0: 80C70000  lwz r6, 0(r7)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C24E4: 7FEB4850  subf r31, r11, r9
	ctx.r[31].s64 = ctx.r[9].s64 - ctx.r[11].s64;
	// 832C24E8: 7D095850  subf r8, r9, r11
	ctx.r[8].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C24EC: 7CCB5830  slw r11, r6, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[11].u64 = 0;
	} else {
		ctx.r[11].u64 = ((ctx.r[6].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C24F0: 7D6B5378  or r11, r11, r10
	ctx.r[11].u64 = ctx.r[11].u64 | ctx.r[10].u64;
	// 832C24F4: 7CCAFC30  srw r10, r6, r31
	if (ctx.r[31].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[6].u32) >> ((ctx.r[31].u8 & 0x1F) as u32)) as u64;
	}
	// 832C24F8: 7D66F038  and r6, r11, r30
	ctx.r[6].u64 = ctx.r[11].u64 & ctx.r[30].u64;
	// 832C24FC: 39680020  addi r11, r8, 0x20
	ctx.r[11].s64 = ctx.r[8].s64 + 32;
	// 832C2500: 38E70004  addi r7, r7, 4
	ctx.r[7].s64 = ctx.r[7].s64 + 4;
	// 832C2504: 48000010  b 0x832c2514
	pc = 0x832C2514; continue 'dispatch;
	// 832C2508: 7FC65038  and r6, r30, r10
	ctx.r[6].u64 = ctx.r[30].u64 & ctx.r[10].u64;
	// 832C250C: 7D4A4C30  srw r10, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[10].u64 = 0;
	} else {
		ctx.r[10].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2510: 7D695850  subf r11, r9, r11
	ctx.r[11].s64 = ctx.r[11].s64 - ctx.r[9].s64;
	// 832C2514: 2F060000  cmpwi cr6, r6, 0
	ctx.cr[6].compare_i32(ctx.r[6].s32, 0, &mut ctx.xer);
	// 832C2518: 419A003C  beq cr6, 0x832c2554
	if ctx.cr[6].eq {
	pc = 0x832C2554; continue 'dispatch;
	}
	// 832C251C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C2520: 409A0018  bne cr6, 0x832c2538
	if !ctx.cr[6].eq {
	pc = 0x832C2538; continue 'dispatch;
	}
	// 832C2524: 81070000  lwz r8, 0(r7)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2528: 38E70004  addi r7, r7, 4
	ctx.r[7].s64 = ctx.r[7].s64 + 4;
	// 832C252C: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C2530: 550AF87E  srwi r10, r8, 1
	// 832C2534: 48000010  b 0x832c2544
	pc = 0x832C2544; continue 'dispatch;
	// 832C2538: 7D485378  mr r8, r10
	ctx.r[8].u64 = ctx.r[10].u64;
	// 832C253C: 554AF87E  srwi r10, r10, 1
	// 832C2540: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C2544: 550807FE  clrlwi r8, r8, 0x1f
	ctx.r[8].u64 = ctx.r[8].u32 as u64 & 0x00000001u64;
	// 832C2548: 2B080000  cmplwi cr6, r8, 0
	ctx.cr[6].compare_u32(ctx.r[8].u32, 0 as u32, &mut ctx.xer);
	// 832C254C: 419A0008  beq cr6, 0x832c2554
	if ctx.cr[6].eq {
	pc = 0x832C2554; continue 'dispatch;
	}
	// 832C2550: 7CC600D0  neg r6, r6
	ctx.r[6].s64 = -ctx.r[6].s64;
	// 832C2554: 7F66DA14  add r27, r6, r27
	ctx.r[27].u64 = ctx.r[6].u64 + ctx.r[27].u64;
	// 832C2558: 2F1D0000  cmpwi cr6, r29, 0
	ctx.cr[6].compare_i32(ctx.r[29].s32, 0, &mut ctx.xer);
	// 832C255C: 577F043E  clrlwi r31, r27, 0x10
	ctx.r[31].u64 = ctx.r[27].u32 as u64 & 0x0000FFFFu64;
	// 832C2560: B3FC0000  sth r31, 0(r28)
	unsafe { crate::rt::store_u16( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[31].u16 ) };
	// 832C2564: 3B9C0002  addi r28, r28, 2
	ctx.r[28].s64 = ctx.r[28].s64 + 2;
	// 832C2568: 409AFF6C  bne cr6, 0x832c24d4
	if !ctx.cr[6].eq {
	pc = 0x832C24D4; continue 'dispatch;
	}
	// 832C256C: 48000070  b 0x832c25dc
	pc = 0x832C25DC; continue 'dispatch;
	// 832C2570: 57E9043E  clrlwi r9, r31, 0x10
	ctx.r[9].u64 = ctx.r[31].u32 as u64 & 0x0000FFFFu64;
	// 832C2574: 57E8801E  slwi r8, r31, 0x10
	// 832C2578: 578607BC  rlwinm r6, r28, 0, 0x1e, 0x1e
	ctx.r[6].u64 = ctx.r[28].u32 as u64 & 0xFFFFFFFFu64;
	// 832C257C: 7D1D4B78  or r29, r8, r9
	ctx.r[29].u64 = ctx.r[8].u64 | ctx.r[9].u64;
	// 832C2580: 7F5ED378  mr r30, r26
	ctx.r[30].u64 = ctx.r[26].u64;
	// 832C2584: 7F89E378  mr r9, r28
	ctx.r[9].u64 = ctx.r[28].u64;
	// 832C2588: 2B060000  cmplwi cr6, r6, 0
	ctx.cr[6].compare_u32(ctx.r[6].u32, 0 as u32, &mut ctx.xer);
	// 832C258C: 419A0010  beq cr6, 0x832c259c
	if ctx.cr[6].eq {
	pc = 0x832C259C; continue 'dispatch;
	}
	// 832C2590: B3FC0000  sth r31, 0(r28)
	unsafe { crate::rt::store_u16( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[31].u16 ) };
	// 832C2594: 393C0002  addi r9, r28, 2
	ctx.r[9].s64 = ctx.r[28].s64 + 2;
	// 832C2598: 3BDAFFFF  addi r30, r26, -1
	ctx.r[30].s64 = ctx.r[26].s64 + -1;
	// 832C259C: 57C6F87E  srwi r6, r30, 1
	// 832C25A0: 2B060000  cmplwi cr6, r6, 0
	ctx.cr[6].compare_u32(ctx.r[6].u32, 0 as u32, &mut ctx.xer);
	// 832C25A4: 419A0020  beq cr6, 0x832c25c4
	if ctx.cr[6].eq {
	pc = 0x832C25C4; continue 'dispatch;
	}
	// 832C25A8: 7D284B78  mr r8, r9
	ctx.r[8].u64 = ctx.r[9].u64;
	// 832C25AC: 7CC903A6  mtctr r6
	ctx.ctr.u64 = ctx.r[6].u64;
	// 832C25B0: 93A80000  stw r29, 0(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[29].u32 ) };
	// 832C25B4: 39080004  addi r8, r8, 4
	ctx.r[8].s64 = ctx.r[8].s64 + 4;
	// 832C25B8: 4200FFF8  bdnz 0x832c25b0
	ctx.ctr.u64 = ctx.ctr.u64.wrapping_sub(1);
	if ctx.ctr.u32 != 0 {
			pc = 0x832C25B0; continue 'dispatch;
	}
	// 832C25BC: 54C8103A  slwi r8, r6, 2
	// 832C25C0: 7D284A14  add r9, r8, r9
	ctx.r[9].u64 = ctx.r[8].u64 + ctx.r[9].u64;
	// 832C25C4: 57C807FE  clrlwi r8, r30, 0x1f
	ctx.r[8].u64 = ctx.r[30].u32 as u64 & 0x00000001u64;
	// 832C25C8: 2B080000  cmplwi cr6, r8, 0
	ctx.cr[6].compare_u32(ctx.r[8].u32, 0 as u32, &mut ctx.xer);
	// 832C25CC: 419A0008  beq cr6, 0x832c25d4
	if ctx.cr[6].eq {
	pc = 0x832C25D4; continue 'dispatch;
	}
	// 832C25D0: B3A90000  sth r29, 0(r9)
	unsafe { crate::rt::store_u16( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.r[29].u16 ) };
	// 832C25D4: 5749083C  slwi r9, r26, 1
	// 832C25D8: 7F89E214  add r28, r9, r28
	ctx.r[28].u64 = ctx.r[9].u64 + ctx.r[28].u64;
	// 832C25DC: 7F1AC051  subf. r24, r26, r24
	ctx.r[24].s64 = ctx.r[24].s64 - ctx.r[26].s64;
	ctx.cr[0].compare_i32(ctx.r[24].s32, 0, &mut ctx.xer);
	// 832C25E0: 4082FE88  bne 0x832c2468
	if !ctx.cr[0].eq {
	pc = 0x832C2468; continue 'dispatch;
	}
	// 832C25E4: 90E40004  stw r7, 4(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(4 as u32), ctx.r[7].u32 ) };
	// 832C25E8: 91440000  stw r10, 0(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832C25EC: 91640008  stw r11, 8(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(8 as u32), ctx.r[11].u32 ) };
	// 832C25F0: 4B9E6E58  b 0x82ca9448
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9448);
	return;
	// 832C25F4: 81230030  lwz r9, 0x30(r3)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C25F8: 39090004  addi r8, r9, 4
	ctx.r[8].s64 = ctx.r[9].s64 + 4;
	// 832C25FC: 91030000  stw r8, 0(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), ctx.r[8].u32 ) };
	// 832C2600: 91230004  stw r9, 4(r3)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), ctx.r[9].u32 ) };
	// 832C2604: 90E40004  stw r7, 4(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(4 as u32), ctx.r[7].u32 ) };
	// 832C2608: 91440000  stw r10, 0(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(0 as u32), ctx.r[10].u32 ) };
	// 832C260C: 91640008  stw r11, 8(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(8 as u32), ctx.r[11].u32 ) };
	// 832C2610: 4B9E6E38  b 0x82ca9448
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9448);
	return;
}

pub fn sub_832C2618(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C2618 size=4108
	// 832C2618: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C261C: 4B9E6DB5  bl 0x82ca93d0
	ctx.lr = 0x832C2620;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93D0);
	// 832C2620: 9421FA20  stwu r1, -0x5e0(r1)
	ea = ctx.r[1].u32.wrapping_add(-1504 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C2624: 7CD93378  mr r25, r6
	ctx.r[25].u64 = ctx.r[6].u64;
	// 832C2628: 81410654  lwz r10, 0x654(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(1620 as u32) ) } as u64;
	// 832C262C: 7CFF3B78  mr r31, r7
	ctx.r[31].u64 = ctx.r[7].u64;
	// 832C2630: 82610634  lwz r19, 0x634(r1)
	ctx.r[19].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(1588 as u32) ) } as u64;
	// 832C2634: 3979FFF8  addi r11, r25, -8
	ctx.r[11].s64 = ctx.r[25].s64 + -8;
	// 832C2638: 9101006C  stw r8, 0x6c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(108 as u32), ctx.r[8].u32 ) };
	// 832C263C: 7CBA2B78  mr r26, r5
	ctx.r[26].u64 = ctx.r[5].u64;
	// 832C2640: 91010064  stw r8, 0x64(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(100 as u32), ctx.r[8].u32 ) };
	// 832C2644: 7D6BF9D6  mullw r11, r11, r31
	ctx.r[11].s64 = (ctx.r[11].s32 as i64) * (ctx.r[31].s32 as i64);
	// 832C2648: 908105FC  stw r4, 0x5fc(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(1532 as u32), ctx.r[4].u32 ) };
	// 832C264C: 93410604  stw r26, 0x604(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(1540 as u32), ctx.r[26].u32 ) };
	// 832C2650: 9321060C  stw r25, 0x60c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(1548 as u32), ctx.r[25].u32 ) };
	// 832C2654: 92610080  stw r19, 0x80(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(128 as u32), ctx.r[19].u32 ) };
	// 832C2658: 7D6B2214  add r11, r11, r4
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[4].u64;
	// 832C265C: 7D2F4B78  mr r15, r9
	ctx.r[15].u64 = ctx.r[9].u64;
	// 832C2660: 3AA00000  li r21, 0
	ctx.r[21].s64 = 0;
	// 832C2664: 57E91838  slwi r9, r31, 3
	// 832C2668: 91E10624  stw r15, 0x624(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(1572 as u32), ctx.r[15].u32 ) };
	// 832C266C: 7D6BD214  add r11, r11, r26
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[26].u64;
	// 832C2670: 92A10068  stw r21, 0x68(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(104 as u32), ctx.r[21].u32 ) };
	// 832C2674: 7C7D1B78  mr r29, r3
	ctx.r[29].u64 = ctx.r[3].u64;
	// 832C2678: 92A10060  stw r21, 0x60(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(96 as u32), ctx.r[21].u32 ) };
	// 832C267C: 7EB7AB78  mr r23, r21
	ctx.r[23].u64 = ctx.r[21].u64;
	// 832C2680: 55480420  rlwinm r8, r10, 0, 0x10, 0x10
	ctx.r[8].u64 = ctx.r[10].u32 as u64 & 0xFFFFFFFFu64;
	// 832C2684: 93A105F4  stw r29, 0x5f4(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(1524 as u32), ctx.r[29].u32 ) };
	// 832C2688: 7CFA4850  subf r7, r26, r9
	ctx.r[7].s64 = ctx.r[9].s64 - ctx.r[26].s64;
	// 832C268C: 92E10084  stw r23, 0x84(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(132 as u32), ctx.r[23].u32 ) };
	// 832C2690: 7C982378  mr r24, r4
	ctx.r[24].u64 = ctx.r[4].u64;
	// 832C2694: 93A100A8  stw r29, 0xa8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(168 as u32), ctx.r[29].u32 ) };
	// 832C2698: 38CBFFF8  addi r6, r11, -8
	ctx.r[6].s64 = ctx.r[11].s64 + -8;
	// 832C269C: 90E10134  stw r7, 0x134(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(308 as u32), ctx.r[7].u32 ) };
	// 832C26A0: 7EB6AB78  mr r22, r21
	ctx.r[22].u64 = ctx.r[21].u64;
	// 832C26A4: 93010078  stw r24, 0x78(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(120 as u32), ctx.r[24].u32 ) };
	// 832C26A8: 90C100E8  stw r6, 0xe8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(232 as u32), ctx.r[6].u32 ) };
	// 832C26AC: 2B080000  cmplwi cr6, r8, 0
	ctx.cr[6].compare_u32(ctx.r[8].u32, 0 as u32, &mut ctx.xer);
	// 832C26B0: 92C100E4  stw r22, 0xe4(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(228 as u32), ctx.r[22].u32 ) };
	// 832C26B4: 419A0010  beq cr6, 0x832c26c4
	if ctx.cr[6].eq {
	pc = 0x832C26C4; continue 'dispatch;
	}
	// 832C26B8: 3D60832C  lis r11, -0x7cd4
	ctx.r[11].s64 = -2094268416;
	// 832C26BC: 396B16B0  addi r11, r11, 0x16b0
	ctx.r[11].s64 = ctx.r[11].s64 + 5808;
	// 832C26C0: 4800000C  b 0x832c26cc
	pc = 0x832C26CC; continue 'dispatch;
	// 832C26C4: 3D60832C  lis r11, -0x7cd4
	ctx.r[11].s64 = -2094268416;
	// 832C26C8: 396B1968  addi r11, r11, 0x1968
	ctx.r[11].s64 = ctx.r[11].s64 + 6504;
	// 832C26CC: 916100EC  stw r11, 0xec(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(236 as u32), ctx.r[11].u32 ) };
	// 832C26D0: 5748E13E  srwi r8, r26, 4
	// 832C26D4: 8161064C  lwz r11, 0x64c(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(1612 as u32) ) } as u64;
	// 832C26D8: 5749E8FE  srwi r9, r26, 3
	// 832C26DC: 38A801FF  addi r5, r8, 0x1ff
	ctx.r[5].s64 = ctx.r[8].s64 + 511;
	// 832C26E0: 92A10240  stw r21, 0x240(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(576 as u32), ctx.r[21].u32 ) };
	// 832C26E4: 38C901FF  addi r6, r9, 0x1ff
	ctx.r[6].s64 = ctx.r[9].s64 + 511;
	// 832C26E8: 92A10244  stw r21, 0x244(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(580 as u32), ctx.r[21].u32 ) };
	// 832C26EC: 3BC8020F  addi r30, r8, 0x20f
	ctx.r[30].s64 = ctx.r[8].s64 + 527;
	// 832C26F0: 92A1024C  stw r21, 0x24c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(588 as u32), ctx.r[21].u32 ) };
	// 832C26F4: 54AE043E  clrlwi r14, r5, 0x10
	ctx.r[14].u64 = ctx.r[5].u32 as u64 & 0x0000FFFFu64;
	// 832C26F8: 92A10280  stw r21, 0x280(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(640 as u32), ctx.r[21].u32 ) };
	// 832C26FC: 806B0004  lwz r3, 4(r11)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C2700: 574A1832  rlwinm r10, r26, 3, 0, 0x19
	ctx.r[10].u64 = ctx.r[26].u32 as u64 & 0x1FFFFFFFu64;
	// 832C2704: 808B0000  lwz r4, 0(r11)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2708: 57470038  rlwinm r7, r26, 0, 0, 0x1c
	ctx.r[7].u64 = ctx.r[26].u32 as u64 & 0xFFFFFFFFu64;
	// 832C270C: 574800F8  rlwinm r8, r26, 0, 3, 0x1c
	ctx.r[8].u64 = ctx.r[26].u32 as u64 & 0xFFFFFFFFu64;
	// 832C2710: 838B0008  lwz r28, 8(r11)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C2714: 80AB001C  lwz r5, 0x1c(r11)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(28 as u32) ) } as u64;
	// 832C2718: 54D4043E  clrlwi r20, r6, 0x10
	ctx.r[20].u64 = ctx.r[6].u32 as u64 & 0x0000FFFFu64;
	// 832C271C: 80CB0018  lwz r6, 0x18(r11)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(24 as u32) ) } as u64;
	// 832C2720: 3908020F  addi r8, r8, 0x20f
	ctx.r[8].s64 = ctx.r[8].s64 + 527;
	// 832C2724: 394A01FF  addi r10, r10, 0x1ff
	ctx.r[10].s64 = ctx.r[10].s64 + 511;
	// 832C2728: 906102B0  stw r3, 0x2b0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(688 as u32), ctx.r[3].u32 ) };
	// 832C272C: 3A2701FF  addi r17, r7, 0x1ff
	ctx.r[17].s64 = ctx.r[7].s64 + 511;
	// 832C2730: 910100A4  stw r8, 0xa4(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(164 as u32), ctx.r[8].u32 ) };
	// 832C2734: 554A043E  clrlwi r10, r10, 0x10
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x0000FFFFu64;
	// 832C2738: 90810270  stw r4, 0x270(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(624 as u32), ctx.r[4].u32 ) };
	// 832C273C: 5623043E  clrlwi r3, r17, 0x10
	ctx.r[3].u64 = ctx.r[17].u32 as u64 & 0x0000FFFFu64;
	// 832C2740: 90A10090  stw r5, 0x90(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(144 as u32), ctx.r[5].u32 ) };
	// 832C2744: 7E840034  cntlzw r4, r20
	ctx.r[4].u64 = if ctx.r[20].u32 == 0 { 32 } else { ctx.r[20].u32.leading_zeros() as u64 };
	// 832C2748: 938100E0  stw r28, 0xe0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(224 as u32), ctx.r[28].u32 ) };
	// 832C274C: 5748F0BC  rlwinm r8, r26, 0x1e, 2, 0x1e
	ctx.r[8].u64 = ctx.r[26].u32 as u64 & 0x00000003u64;
	// 832C2750: 90C10088  stw r6, 0x88(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(136 as u32), ctx.r[6].u32 ) };
	// 832C2754: 7DDC0034  cntlzw r28, r14
	ctx.r[28].u64 = if ctx.r[14].u32 == 0 { 32 } else { ctx.r[14].u32.leading_zeros() as u64 };
	// 832C2758: 836B000C  lwz r27, 0xc(r11)
	ctx.r[27].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(12 as u32) ) } as u64;
	// 832C275C: 574518F2  rlwinm r5, r26, 3, 3, 0x19
	ctx.r[5].u64 = ctx.r[26].u32 as u64 & 0x1FFFFFFFu64;
	// 832C2760: 824B0010  lwz r18, 0x10(r11)
	ctx.r[18].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(16 as u32) ) } as u64;
	// 832C2764: 7C710034  cntlzw r17, r3
	ctx.r[17].u64 = if ctx.r[3].u32 == 0 { 32 } else { ctx.r[3].u32.leading_zeros() as u64 };
	// 832C2768: 820B0014  lwz r16, 0x14(r11)
	ctx.r[16].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(20 as u32) ) } as u64;
	// 832C276C: 7D540034  cntlzw r20, r10
	ctx.r[20].u64 = if ctx.r[10].u32 == 0 { 32 } else { ctx.r[10].u32.leading_zeros() as u64 };
	// 832C2770: 816B0020  lwz r11, 0x20(r11)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C2774: 5746E8FE  srwi r6, r26, 3
	// 832C2778: 92A10284  stw r21, 0x284(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(644 as u32), ctx.r[21].u32 ) };
	// 832C277C: 21440020  subfic r10, r4, 0x20
	ctx.xer.ca = ctx.r[4].u32 <= 32 as u32;
	ctx.r[10].s64 = (32 as i64) - ctx.r[4].s64;
	// 832C2780: 92A1028C  stw r21, 0x28c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(652 as u32), ctx.r[21].u32 ) };
	// 832C2784: 7D294214  add r9, r9, r8
	ctx.r[9].u64 = ctx.r[9].u64 + ctx.r[8].u64;
	// 832C2788: 92A100B0  stw r21, 0xb0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(176 as u32), ctx.r[21].u32 ) };
	// 832C278C: 38600008  li r3, 8
	ctx.r[3].s64 = 8;
	// 832C2790: 92A100B4  stw r21, 0xb4(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(180 as u32), ctx.r[21].u32 ) };
	// 832C2794: 211C0020  subfic r8, r28, 0x20
	ctx.xer.ca = ctx.r[28].u32 <= 32 as u32;
	ctx.r[8].s64 = (32 as i64) - ctx.r[28].s64;
	// 832C2798: 92A100BC  stw r21, 0xbc(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(188 as u32), ctx.r[21].u32 ) };
	// 832C279C: 38A5020F  addi r5, r5, 0x20f
	ctx.r[5].s64 = ctx.r[5].s64 + 527;
	// 832C27A0: 906100B8  stw r3, 0xb8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(184 as u32), ctx.r[3].u32 ) };
	// 832C27A4: 38C6020F  addi r6, r6, 0x20f
	ctx.r[6].s64 = ctx.r[6].s64 + 527;
	// 832C27A8: 90610208  stw r3, 0x208(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(520 as u32), ctx.r[3].u32 ) };
	// 832C27AC: 23940020  subfic r28, r20, 0x20
	ctx.xer.ca = ctx.r[20].u32 <= 32 as u32;
	ctx.r[28].s64 = (32 as i64) - ctx.r[20].s64;
	// 832C27B0: 910102A8  stw r8, 0x2a8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(680 as u32), ctx.r[8].u32 ) };
	// 832C27B4: 57C30036  rlwinm r3, r30, 0, 0, 0x1b
	ctx.r[3].u64 = ctx.r[30].u32 as u64 & 0xFFFFFFFFu64;
	// 832C27B8: 92A10200  stw r21, 0x200(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(512 as u32), ctx.r[21].u32 ) };
	// 832C27BC: 54A50036  rlwinm r5, r5, 0, 0, 0x1b
	ctx.r[5].u64 = ctx.r[5].u32 as u64 & 0xFFFFFFFFu64;
	// 832C27C0: 938100D8  stw r28, 0xd8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(216 as u32), ctx.r[28].u32 ) };
	// 832C27C4: 38800004  li r4, 4
	ctx.r[4].s64 = 4;
	// 832C27C8: 906102AC  stw r3, 0x2ac(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(684 as u32), ctx.r[3].u32 ) };
	// 832C27CC: 54C80036  rlwinm r8, r6, 0, 0, 0x1b
	ctx.r[8].u64 = ctx.r[6].u32 as u64 & 0xFFFFFFFFu64;
	// 832C27D0: 90A100DC  stw r5, 0xdc(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(220 as u32), ctx.r[5].u32 ) };
	// 832C27D4: 23D10020  subfic r30, r17, 0x20
	ctx.xer.ca = ctx.r[17].u32 <= 32 as u32;
	ctx.r[30].s64 = (32 as i64) - ctx.r[17].s64;
	// 832C27D8: 91410268  stw r10, 0x268(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(616 as u32), ctx.r[10].u32 ) };
	// 832C27DC: 92A10204  stw r21, 0x204(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(516 as u32), ctx.r[21].u32 ) };
	// 832C27E0: 57460836  rlwinm r6, r26, 1, 0, 0x1b
	ctx.r[6].u64 = ctx.r[26].u32 as u64 & 0x7FFFFFFFu64;
	// 832C27E4: 92A1020C  stw r21, 0x20c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(524 as u32), ctx.r[21].u32 ) };
	// 832C27E8: 55292036  slwi r9, r9, 4
	// 832C27EC: 93610230  stw r27, 0x230(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(560 as u32), ctx.r[27].u32 ) };
	// 832C27F0: 38600005  li r3, 5
	ctx.r[3].s64 = 5;
	// 832C27F4: 91410168  stw r10, 0x168(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(360 as u32), ctx.r[10].u32 ) };
	// 832C27F8: 3B800010  li r28, 0x10
	ctx.r[28].s64 = 16;
	// 832C27FC: 9101026C  stw r8, 0x26c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(620 as u32), ctx.r[8].u32 ) };
	// 832C2800: 38A0000B  li r5, 0xb
	ctx.r[5].s64 = 11;
	// 832C2804: 90810248  stw r4, 0x248(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(584 as u32), ctx.r[4].u32 ) };
	// 832C2808: 90810288  stw r4, 0x288(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(648 as u32), ctx.r[4].u32 ) };
	// 832C280C: 93C10228  stw r30, 0x228(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(552 as u32), ctx.r[30].u32 ) };
	// 832C2810: 91610098  stw r11, 0x98(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(152 as u32), ctx.r[11].u32 ) };
	// 832C2814: 574BF0FC  rlwinm r11, r26, 0x1e, 3, 0x1e
	ctx.r[11].u64 = ctx.r[26].u32 as u64 & 0x00000003u64;
	// 832C2818: 7CE73214  add r7, r7, r6
	ctx.r[7].u64 = ctx.r[7].u64 + ctx.r[6].u64;
	// 832C281C: 83C100A4  lwz r30, 0xa4(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(164 as u32) ) } as u64;
	// 832C2820: 38C901FF  addi r6, r9, 0x1ff
	ctx.r[6].s64 = ctx.r[9].s64 + 511;
	// 832C2824: 90A101C8  stw r5, 0x1c8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(456 as u32), ctx.r[5].u32 ) };
	// 832C2828: 392B040F  addi r9, r11, 0x40f
	ctx.r[9].s64 = ctx.r[11].s64 + 1039;
	// 832C282C: 90A10308  stw r5, 0x308(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(776 as u32), ctx.r[5].u32 ) };
	// 832C2830: 54EB08FC  rlwinm r11, r7, 1, 3, 0x1e
	ctx.r[11].u64 = ctx.r[7].u32 as u64 & 0x7FFFFFFFu64;
	// 832C2834: 82810088  lwz r20, 0x88(r1)
	ctx.r[20].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(136 as u32) ) } as u64;
	// 832C2838: 54C7043E  clrlwi r7, r6, 0x10
	ctx.r[7].u64 = ctx.r[6].u32 as u64 & 0x0000FFFFu64;
	// 832C283C: 90610148  stw r3, 0x148(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(328 as u32), ctx.r[3].u32 ) };
	// 832C2840: 38CB020F  addi r6, r11, 0x20f
	ctx.r[6].s64 = ctx.r[11].s64 + 527;
	// 832C2844: 90610188  stw r3, 0x188(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(392 as u32), ctx.r[3].u32 ) };
	// 832C2848: 7CE50034  cntlzw r5, r7
	ctx.r[5].u64 = if ctx.r[7].u32 == 0 { 32 } else { ctx.r[7].u32.leading_zeros() as u64 };
	// 832C284C: 90810108  stw r4, 0x108(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(264 as u32), ctx.r[4].u32 ) };
	// 832C2850: 57C70036  rlwinm r7, r30, 0, 0, 0x1b
	ctx.r[7].u64 = ctx.r[30].u32 as u64 & 0xFFFFFFFFu64;
	// 832C2854: 9101016C  stw r8, 0x16c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(364 as u32), ctx.r[8].u32 ) };
	// 832C2858: 3BC00400  li r30, 0x400
	ctx.r[30].s64 = 1024;
	// 832C285C: 9381014C  stw r28, 0x14c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(332 as u32), ctx.r[28].u32 ) };
	// 832C2860: 552B0036  rlwinm r11, r9, 0, 0, 0x1b
	ctx.r[11].u64 = ctx.r[9].u32 as u64 & 0xFFFFFFFFu64;
	// 832C2864: 9381018C  stw r28, 0x18c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(396 as u32), ctx.r[28].u32 ) };
	// 832C2868: 93C1030C  stw r30, 0x30c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(780 as u32), ctx.r[30].u32 ) };
	// 832C286C: 21250020  subfic r9, r5, 0x20
	ctx.xer.ca = ctx.r[5].u32 <= 32 as u32;
	ctx.r[9].s64 = (32 as i64) - ctx.r[5].s64;
	// 832C2870: 83C10090  lwz r30, 0x90(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(144 as u32) ) } as u64;
	// 832C2874: 54DB0036  rlwinm r27, r6, 0, 0, 0x1b
	ctx.r[27].u64 = ctx.r[6].u32 as u64 & 0xFFFFFFFFu64;
	// 832C2878: 38C10250  addi r6, r1, 0x250
	ctx.r[6].s64 = ctx.r[1].s64 + 592;
	// 832C287C: 92410170  stw r18, 0x170(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(368 as u32), ctx.r[18].u32 ) };
	// 832C2880: 38A10260  addi r5, r1, 0x260
	ctx.r[5].s64 = ctx.r[1].s64 + 608;
	// 832C2884: 914101A8  stw r10, 0x1a8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(424 as u32), ctx.r[10].u32 ) };
	// 832C2888: 38810264  addi r4, r1, 0x264
	ctx.r[4].s64 = ctx.r[1].s64 + 612;
	// 832C288C: 910101AC  stw r8, 0x1ac(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(428 as u32), ctx.r[8].u32 ) };
	// 832C2890: 38610060  addi r3, r1, 0x60
	ctx.r[3].s64 = ctx.r[1].s64 + 96;
	// 832C2894: 920101B0  stw r16, 0x1b0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(432 as u32), ctx.r[16].u32 ) };
	// 832C2898: 93C10330  stw r30, 0x330(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(816 as u32), ctx.r[30].u32 ) };
	// 832C289C: 914101E8  stw r10, 0x1e8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(488 as u32), ctx.r[10].u32 ) };
	// 832C28A0: 928101F0  stw r20, 0x1f0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(496 as u32), ctx.r[20].u32 ) };
	// 832C28A4: 83C10098  lwz r30, 0x98(r1)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(152 as u32) ) } as u64;
	// 832C28A8: 91410328  stw r10, 0x328(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(808 as u32), ctx.r[10].u32 ) };
	// 832C28AC: 916101EC  stw r11, 0x1ec(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(492 as u32), ctx.r[11].u32 ) };
	// 832C28B0: 90E1022C  stw r7, 0x22c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(556 as u32), ctx.r[7].u32 ) };
	// 832C28B4: 92A10140  stw r21, 0x140(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(320 as u32), ctx.r[21].u32 ) };
	// 832C28B8: 92A10144  stw r21, 0x144(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(324 as u32), ctx.r[21].u32 ) };
	// 832C28BC: 92A10180  stw r21, 0x180(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(384 as u32), ctx.r[21].u32 ) };
	// 832C28C0: 92A10184  stw r21, 0x184(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(388 as u32), ctx.r[21].u32 ) };
	// 832C28C4: 92A101C0  stw r21, 0x1c0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(448 as u32), ctx.r[21].u32 ) };
	// 832C28C8: 92A101C4  stw r21, 0x1c4(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(452 as u32), ctx.r[21].u32 ) };
	// 832C28CC: 92A101CC  stw r21, 0x1cc(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(460 as u32), ctx.r[21].u32 ) };
	// 832C28D0: 9161032C  stw r11, 0x32c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(812 as u32), ctx.r[11].u32 ) };
	// 832C28D4: 92A10300  stw r21, 0x300(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(768 as u32), ctx.r[21].u32 ) };
	// 832C28D8: 92A10304  stw r21, 0x304(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(772 as u32), ctx.r[21].u32 ) };
	// 832C28DC: 91210128  stw r9, 0x128(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(296 as u32), ctx.r[9].u32 ) };
	// 832C28E0: 92A10100  stw r21, 0x100(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(256 as u32), ctx.r[21].u32 ) };
	// 832C28E4: 92A10104  stw r21, 0x104(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(260 as u32), ctx.r[21].u32 ) };
	// 832C28E8: 92A1010C  stw r21, 0x10c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(268 as u32), ctx.r[21].u32 ) };
	// 832C28EC: 9361012C  stw r27, 0x12c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(300 as u32), ctx.r[27].u32 ) };
	// 832C28F0: 93C10130  stw r30, 0x130(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(304 as u32), ctx.r[30].u32 ) };
	// 832C28F4: 4BFFE2BD  bl 0x832c0bb0
	ctx.lr = 0x832C28F8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0BB0);
	// 832C28F8: 38C10290  addi r6, r1, 0x290
	ctx.r[6].s64 = ctx.r[1].s64 + 656;
	// 832C28FC: 38A102A0  addi r5, r1, 0x2a0
	ctx.r[5].s64 = ctx.r[1].s64 + 672;
	// 832C2900: 388102A4  addi r4, r1, 0x2a4
	ctx.r[4].s64 = ctx.r[1].s64 + 676;
	// 832C2904: 38610060  addi r3, r1, 0x60
	ctx.r[3].s64 = ctx.r[1].s64 + 96;
	// 832C2908: 4BFFE2A9  bl 0x832c0bb0
	ctx.lr = 0x832C290C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0BB0);
	// 832C290C: 3BC10500  addi r30, r1, 0x500
	ctx.r[30].s64 = ctx.r[1].s64 + 1280;
	// 832C2910: 3B6103C0  addi r27, r1, 0x3c0
	ctx.r[27].s64 = ctx.r[1].s64 + 960;
	// 832C2914: 7F66DB78  mr r6, r27
	ctx.r[6].u64 = ctx.r[27].u64;
	// 832C2918: 38BEFFC0  addi r5, r30, -0x40
	ctx.r[5].s64 = ctx.r[30].s64 + -64;
	// 832C291C: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832C2920: 38610060  addi r3, r1, 0x60
	ctx.r[3].s64 = ctx.r[1].s64 + 96;
	// 832C2924: 4BFFE28D  bl 0x832c0bb0
	ctx.lr = 0x832C2928;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0BB0);
	// 832C2928: 379CFFFF  addic. r28, r28, -1
	ctx.xer.ca = (ctx.r[28].u32 > (!(-1 as u32)));
	ctx.r[28].s64 = ctx.r[28].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[28].s32, 0, &mut ctx.xer);
	// 832C292C: 3B7B0010  addi r27, r27, 0x10
	ctx.r[27].s64 = ctx.r[27].s64 + 16;
	// 832C2930: 3BDE0004  addi r30, r30, 4
	ctx.r[30].s64 = ctx.r[30].s64 + 4;
	// 832C2934: 4082FFE0  bne 0x832c2914
	if !ctx.cr[0].eq {
	pc = 0x832C2914; continue 'dispatch;
	}
	// 832C2938: 38C100C0  addi r6, r1, 0xc0
	ctx.r[6].s64 = ctx.r[1].s64 + 192;
	// 832C293C: 38A100D0  addi r5, r1, 0xd0
	ctx.r[5].s64 = ctx.r[1].s64 + 208;
	// 832C2940: 388100D4  addi r4, r1, 0xd4
	ctx.r[4].s64 = ctx.r[1].s64 + 212;
	// 832C2944: 38610060  addi r3, r1, 0x60
	ctx.r[3].s64 = ctx.r[1].s64 + 96;
	// 832C2948: 4BFFE269  bl 0x832c0bb0
	ctx.lr = 0x832C294C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0BB0);
	// 832C294C: 92A10540  stw r21, 0x540(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(1344 as u32), ctx.r[21].u32 ) };
	// 832C2950: 38C10210  addi r6, r1, 0x210
	ctx.r[6].s64 = ctx.r[1].s64 + 528;
	// 832C2954: 38A10220  addi r5, r1, 0x220
	ctx.r[5].s64 = ctx.r[1].s64 + 544;
	// 832C2958: 38810224  addi r4, r1, 0x224
	ctx.r[4].s64 = ctx.r[1].s64 + 548;
	// 832C295C: 38610060  addi r3, r1, 0x60
	ctx.r[3].s64 = ctx.r[1].s64 + 96;
	// 832C2960: 4BFFE251  bl 0x832c0bb0
	ctx.lr = 0x832C2964;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0BB0);
	// 832C2964: 38C10150  addi r6, r1, 0x150
	ctx.r[6].s64 = ctx.r[1].s64 + 336;
	// 832C2968: 38A10160  addi r5, r1, 0x160
	ctx.r[5].s64 = ctx.r[1].s64 + 352;
	// 832C296C: 38810164  addi r4, r1, 0x164
	ctx.r[4].s64 = ctx.r[1].s64 + 356;
	// 832C2970: 38610060  addi r3, r1, 0x60
	ctx.r[3].s64 = ctx.r[1].s64 + 96;
	// 832C2974: 4BFFE23D  bl 0x832c0bb0
	ctx.lr = 0x832C2978;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0BB0);
	// 832C2978: 38C10190  addi r6, r1, 0x190
	ctx.r[6].s64 = ctx.r[1].s64 + 400;
	// 832C297C: 38A101A0  addi r5, r1, 0x1a0
	ctx.r[5].s64 = ctx.r[1].s64 + 416;
	// 832C2980: 388101A4  addi r4, r1, 0x1a4
	ctx.r[4].s64 = ctx.r[1].s64 + 420;
	// 832C2984: 38610060  addi r3, r1, 0x60
	ctx.r[3].s64 = ctx.r[1].s64 + 96;
	// 832C2988: 4BFFE229  bl 0x832c0bb0
	ctx.lr = 0x832C298C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0BB0);
	// 832C298C: 38C10110  addi r6, r1, 0x110
	ctx.r[6].s64 = ctx.r[1].s64 + 272;
	// 832C2990: 38A10120  addi r5, r1, 0x120
	ctx.r[5].s64 = ctx.r[1].s64 + 288;
	// 832C2994: 38810124  addi r4, r1, 0x124
	ctx.r[4].s64 = ctx.r[1].s64 + 292;
	// 832C2998: 38610060  addi r3, r1, 0x60
	ctx.r[3].s64 = ctx.r[1].s64 + 96;
	// 832C299C: 4BFFE215  bl 0x832c0bb0
	ctx.lr = 0x832C29A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C0BB0);
	// 832C29A0: 7EB1AB78  mr r17, r21
	ctx.r[17].u64 = ctx.r[21].u64;
	// 832C29A4: 2B190000  cmplwi cr6, r25, 0
	ctx.cr[6].compare_u32(ctx.r[25].u32, 0 as u32, &mut ctx.xer);
	// 832C29A8: 9221007C  stw r17, 0x7c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(124 as u32), ctx.r[17].u32 ) };
	// 832C29AC: 419A1FFC  beq cr6, 0x832c49a8
	if ctx.cr[6].eq {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C49A8);
		return;
	}
	// 832C29B0: 3D608216  lis r11, -0x7dea
	ctx.r[11].s64 = -2112487424;
	// 832C29B4: 8381063C  lwz r28, 0x63c(r1)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(1596 as u32) ) } as u64;
	// 832C29B8: 3D408216  lis r10, -0x7dea
	ctx.r[10].s64 = -2112487424;
	// 832C29BC: 3BCB6040  addi r30, r11, 0x6040
	ctx.r[30].s64 = ctx.r[11].s64 + 24640;
	// 832C29C0: 396A5C40  addi r11, r10, 0x5c40
	ctx.r[11].s64 = ctx.r[10].s64 + 23616;
	// 832C29C4: 93C10138  stw r30, 0x138(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(312 as u32), ctx.r[30].u32 ) };
	// 832C29C8: 3A800001  li r20, 1
	ctx.r[20].s64 = 1;
	// 832C29CC: 916100A4  stw r11, 0xa4(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(164 as u32), ctx.r[11].u32 ) };
	// 832C29D0: 4800000C  b 0x832c29dc
	pc = 0x832C29DC; continue 'dispatch;
	// 832C29D4: 83410604  lwz r26, 0x604(r1)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(1540 as u32) ) } as u64;
	// 832C29D8: 81E10624  lwz r15, 0x624(r1)
	ctx.r[15].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(1572 as u32) ) } as u64;
	// 832C29DC: 7DE57B78  mr r5, r15
	ctx.r[5].u64 = ctx.r[15].u64;
	// 832C29E0: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C29E4: 38610240  addi r3, r1, 0x240
	ctx.r[3].s64 = ctx.r[1].s64 + 576;
	// 832C29E8: 4BFFE9D1  bl 0x832c13b8
	ctx.lr = 0x832C29EC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C13B8);
	// 832C29EC: 7DE57B78  mr r5, r15
	ctx.r[5].u64 = ctx.r[15].u64;
	// 832C29F0: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C29F4: 38610280  addi r3, r1, 0x280
	ctx.r[3].s64 = ctx.r[1].s64 + 640;
	// 832C29F8: 4BFFE9C1  bl 0x832c13b8
	ctx.lr = 0x832C29FC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C13B8);
	// 832C29FC: 816100EC  lwz r11, 0xec(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(236 as u32) ) } as u64;
	// 832C2A00: 7DE67B78  mr r6, r15
	ctx.r[6].u64 = ctx.r[15].u64;
	// 832C2A04: 38A103C0  addi r5, r1, 0x3c0
	ctx.r[5].s64 = ctx.r[1].s64 + 960;
	// 832C2A08: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C2A0C: 386100B0  addi r3, r1, 0xb0
	ctx.r[3].s64 = ctx.r[1].s64 + 176;
	// 832C2A10: 7D6903A6  mtctr r11
	ctx.ctr.u64 = ctx.r[11].u64;
	// 832C2A14: 4E800421  bctrl
	ctx.lr = 0x832C2A18;
	crate::rt::call_indirect(ctx.ctr.u32);
	// 832C2A18: 7DE57B78  mr r5, r15
	ctx.r[5].u64 = ctx.r[15].u64;
	// 832C2A1C: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C2A20: 38610200  addi r3, r1, 0x200
	ctx.r[3].s64 = ctx.r[1].s64 + 512;
	// 832C2A24: 4BFFF415  bl 0x832c1e38
	ctx.lr = 0x832C2A28;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C1E38);
	// 832C2A28: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C2A2C: 38610140  addi r3, r1, 0x140
	ctx.r[3].s64 = ctx.r[1].s64 + 320;
	// 832C2A30: 4BFFF5F9  bl 0x832c2028
	ctx.lr = 0x832C2A34;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C2028);
	// 832C2A34: 7DE57B78  mr r5, r15
	ctx.r[5].u64 = ctx.r[15].u64;
	// 832C2A38: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C2A3C: 38610180  addi r3, r1, 0x180
	ctx.r[3].s64 = ctx.r[1].s64 + 384;
	// 832C2A40: 4BFFF5E9  bl 0x832c2028
	ctx.lr = 0x832C2A44;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C2028);
	// 832C2A44: 81410170  lwz r10, 0x170(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(368 as u32) ) } as u64;
	// 832C2A48: 7DE57B78  mr r5, r15
	ctx.r[5].u64 = ctx.r[15].u64;
	// 832C2A4C: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C2A50: 386101C0  addi r3, r1, 0x1c0
	ctx.r[3].s64 = ctx.r[1].s64 + 448;
	// 832C2A54: 81C101B0  lwz r14, 0x1b0(r1)
	ctx.r[14].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(432 as u32) ) } as u64;
	// 832C2A58: 83210180  lwz r25, 0x180(r1)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(384 as u32) ) } as u64;
	// 832C2A5C: 7D6EC850  subf r11, r14, r25
	ctx.r[11].s64 = ctx.r[25].s64 - ctx.r[14].s64;
	// 832C2A60: 7ECB5214  add r22, r11, r10
	ctx.r[22].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832C2A64: 92C10140  stw r22, 0x140(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(320 as u32), ctx.r[22].u32 ) };
	// 832C2A68: 4BFFF851  bl 0x832c22b8
	ctx.lr = 0x832C2A6C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C22B8);
	// 832C2A6C: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C2A70: 38610300  addi r3, r1, 0x300
	ctx.r[3].s64 = ctx.r[1].s64 + 768;
	// 832C2A74: 4BFFF845  bl 0x832c22b8
	ctx.lr = 0x832C2A78;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C22B8);
	// 832C2A78: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C2A7C: 38610100  addi r3, r1, 0x100
	ctx.r[3].s64 = ctx.r[1].s64 + 256;
	// 832C2A80: 4BFFF189  bl 0x832c1c08
	ctx.lr = 0x832C2A84;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C1C08);
	// 832C2A84: 7F45D378  mr r5, r26
	ctx.r[5].u64 = ctx.r[26].u64;
	// 832C2A88: 80610064  lwz r3, 0x64(r1)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(100 as u32) ) } as u64;
	// 832C2A8C: 80810060  lwz r4, 0x60(r1)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C2A90: 816100B0  lwz r11, 0xb0(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(176 as u32) ) } as u64;
	// 832C2A94: 83610200  lwz r27, 0x200(r1)
	ctx.r[27].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(512 as u32) ) } as u64;
	// 832C2A98: 824101C0  lwz r18, 0x1c0(r1)
	ctx.r[18].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(448 as u32) ) } as u64;
	// 832C2A9C: 90A100A0  stw r5, 0xa0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(160 as u32), ctx.r[5].u32 ) };
	// 832C2AA0: 83410068  lwz r26, 0x68(r1)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(104 as u32) ) } as u64;
	// 832C2AA4: 82010100  lwz r16, 0x100(r1)
	ctx.r[16].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(256 as u32) ) } as u64;
	// 832C2AA8: 4800000C  b 0x832c2ab4
	pc = 0x832C2AB4; continue 'dispatch;
	// 832C2AAC: 81C101B0  lwz r14, 0x1b0(r1)
	ctx.r[14].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(432 as u32) ) } as u64;
	// 832C2AB0: 81E10624  lwz r15, 0x624(r1)
	ctx.r[15].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(1572 as u32) ) } as u64;
	// 832C2AB4: 80C101C4  lwz r6, 0x1c4(r1)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(452 as u32) ) } as u64;
	// 832C2AB8: 7F037840  cmplw cr6, r3, r15
	ctx.cr[6].compare_u32(ctx.r[3].u32, ctx.r[15].u32, &mut ctx.xer);
	// 832C2ABC: 80E10204  lwz r7, 0x204(r1)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(516 as u32) ) } as u64;
	// 832C2AC0: 81210184  lwz r9, 0x184(r1)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(388 as u32) ) } as u64;
	// 832C2AC4: 40981E4C  bge cr6, 0x832c4910
	if !ctx.cr[6].lt {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C4910);
		return;
	}
	// 832C2AC8: 81410244  lwz r10, 0x244(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(580 as u32) ) } as u64;
	// 832C2ACC: 81010240  lwz r8, 0x240(r1)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(576 as u32) ) } as u64;
	// 832C2AD0: 7F085040  cmplw cr6, r8, r10
	ctx.cr[6].compare_u32(ctx.r[8].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C2AD4: 40981E3C  bge cr6, 0x832c4910
	if !ctx.cr[6].lt {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C4910);
		return;
	}
	// 832C2AD8: 89480000  lbz r10, 0(r8)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2ADC: 39080001  addi r8, r8, 1
	ctx.r[8].s64 = ctx.r[8].s64 + 1;
	// 832C2AE0: 91010240  stw r8, 0x240(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(576 as u32), ctx.r[8].u32 ) };
	// 832C2AE4: 2B0A0009  cmplwi cr6, r10, 9
	ctx.cr[6].compare_u32(ctx.r[10].u32, 9 as u32, &mut ctx.xer);
	// 832C2AE8: 41991E28  bgt cr6, 0x832c4910
	if ctx.cr[6].gt {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C4910);
		return;
	}
	// 832C2AEC: 3D80832C  lis r12, -0x7cd4
	ctx.r[12].s64 = -2094268416;
	// 832C2AF0: 398C2B04  addi r12, r12, 0x2b04
	ctx.r[12].s64 = ctx.r[12].s64 + 11012;
	// 832C2AF4: 5540103A  slwi r0, r10, 2
	// 832C2AF8: 7C0C002E  lwzx r0, r12, r0
	ctx.r[0].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[12].u32.wrapping_add(ctx.r[0].u32)) } as u64;
	// 832C2AFC: 7C0903A6  mtctr r0
	ctx.ctr.u64 = ctx.r[0].u64;
	// 832C2B00: 4E800420  bctr
	crate::rt::call_indirect(ctx.ctr.u32);
	return;
	// 832C2B04: 832C2B2C  lwz r25, 0x2b2c(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(11052 as u32) ) } as u64;
	// 832C2B08: 832C2BC4  lwz r25, 0x2bc4(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(11204 as u32) ) } as u64;
	// 832C2B0C: 832C3818  lwz r25, 0x3818(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(14360 as u32) ) } as u64;
	// 832C2B10: 832C3D18  lwz r25, 0x3d18(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(15640 as u32) ) } as u64;
	// 832C2B14: 832C3EDC  lwz r25, 0x3edc(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(16092 as u32) ) } as u64;
	// 832C2B18: 832C3FB0  lwz r25, 0x3fb0(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(16304 as u32) ) } as u64;
	// 832C2B1C: 832C406C  lwz r25, 0x406c(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(16492 as u32) ) } as u64;
	// 832C2B20: 832C40E8  lwz r25, 0x40e8(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(16616 as u32) ) } as u64;
	// 832C2B24: 832C4204  lwz r25, 0x4204(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(16900 as u32) ) } as u64;
	// 832C2B28: 832C4538  lwz r25, 0x4538(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(17720 as u32) ) } as u64;
	// 832C2B2C: 7D58FA14  add r10, r24, r31
	ctx.r[10].u64 = ctx.r[24].u64 + ctx.r[31].u64;
	// 832C2B30: 7C18FCAE  lfdx f0, r24, r31
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[0].u64 = unsafe { crate::rt::load_u64(base as *const u8, ctx.r[24].u32.wrapping_add(ctx.r[31].u32)) };
	// 832C2B34: 7D3DFA14  add r9, r29, r31
	ctx.r[9].u64 = ctx.r[29].u64 + ctx.r[31].u64;
	// 832C2B38: 7C1DFDAE  stfdx f0, r29, r31
	unsafe { crate::rt::store_u64(base as *mut u8, ctx.r[29].u32.wrapping_add(ctx.r[31].u32), ctx.f[0].u64) };
	// 832C2B3C: 7D4AFA14  add r10, r10, r31
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[31].u64;
	// 832C2B40: C9B80000  lfd f13, 0(r24)
	ctx.f[13].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[24].u32.wrapping_add(0 as u32) ) };
	// 832C2B44: 7D29FA14  add r9, r9, r31
	ctx.r[9].u64 = ctx.r[9].u64 + ctx.r[31].u64;
	// 832C2B48: D9BD0000  stfd f13, 0(r29)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[29].u32.wrapping_add(0 as u32), ctx.f[13].u64 ) };
	// 832C2B4C: 7D0AFA14  add r8, r10, r31
	ctx.r[8].u64 = ctx.r[10].u64 + ctx.r[31].u64;
	// 832C2B50: 7CE9FA14  add r7, r9, r31
	ctx.r[7].u64 = ctx.r[9].u64 + ctx.r[31].u64;
	// 832C2B54: 2B1C0000  cmplwi cr6, r28, 0
	ctx.cr[6].compare_u32(ctx.r[28].u32, 0 as u32, &mut ctx.xer);
	// 832C2B58: C98A0000  lfd f12, 0(r10)
	ctx.f[12].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) };
	// 832C2B5C: 7D48FA14  add r10, r8, r31
	ctx.r[10].u64 = ctx.r[8].u64 + ctx.r[31].u64;
	// 832C2B60: D9890000  stfd f12, 0(r9)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.f[12].u64 ) };
	// 832C2B64: 7D27FA14  add r9, r7, r31
	ctx.r[9].u64 = ctx.r[7].u64 + ctx.r[31].u64;
	// 832C2B68: C9680000  lfd f11, 0(r8)
	ctx.f[11].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	// 832C2B6C: 7D0AFA14  add r8, r10, r31
	ctx.r[8].u64 = ctx.r[10].u64 + ctx.r[31].u64;
	// 832C2B70: D9670000  stfd f11, 0(r7)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[7].u32.wrapping_add(0 as u32), ctx.f[11].u64 ) };
	// 832C2B74: 7CE9FA14  add r7, r9, r31
	ctx.r[7].u64 = ctx.r[9].u64 + ctx.r[31].u64;
	// 832C2B78: C94A0000  lfd f10, 0(r10)
	ctx.f[10].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) };
	// 832C2B7C: 7D48FA14  add r10, r8, r31
	ctx.r[10].u64 = ctx.r[8].u64 + ctx.r[31].u64;
	// 832C2B80: D9490000  stfd f10, 0(r9)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.f[10].u64 ) };
	// 832C2B84: 7D27FA14  add r9, r7, r31
	ctx.r[9].u64 = ctx.r[7].u64 + ctx.r[31].u64;
	// 832C2B88: C9280000  lfd f9, 0(r8)
	ctx.f[9].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	// 832C2B8C: 7D08FCAE  lfdx f8, r8, r31
	ctx.f[8].u64 = unsafe { crate::rt::load_u64(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[31].u32)) };
	// 832C2B90: D9270000  stfd f9, 0(r7)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[7].u32.wrapping_add(0 as u32), ctx.f[9].u64 ) };
	// 832C2B94: 7CEAFCAE  lfdx f7, r10, r31
	ctx.f[7].u64 = unsafe { crate::rt::load_u64(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[31].u32)) };
	// 832C2B98: 7D07FDAE  stfdx f8, r7, r31
	unsafe { crate::rt::store_u64(base as *mut u8, ctx.r[7].u32.wrapping_add(ctx.r[31].u32), ctx.f[8].u64) };
	// 832C2B9C: 7CE9FDAE  stfdx f7, r9, r31
	unsafe { crate::rt::store_u64(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[31].u32), ctx.f[7].u64) };
	// 832C2BA0: 419A0014  beq cr6, 0x832c2bb4
	if ctx.cr[6].eq {
	pc = 0x832C2BB4; continue 'dispatch;
	}
	// 832C2BA4: 56EAF87E  srwi r10, r23, 1
	// 832C2BA8: 7D2A98AE  lbzx r9, r10, r19
	ctx.r[9].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[10].u32.wrapping_add(ctx.r[19].u32)) } as u64;
	// 832C2BAC: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832C2BB0: 409A1D60  bne cr6, 0x832c4910
	if !ctx.cr[6].eq {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C4910);
		return;
	}
	// 832C2BB4: 814100E4  lwz r10, 0xe4(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(228 as u32) ) } as u64;
	// 832C2BB8: 392A0001  addi r9, r10, 1
	ctx.r[9].s64 = ctx.r[10].s64 + 1;
	// 832C2BBC: 912100E4  stw r9, 0xe4(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(228 as u32), ctx.r[9].u32 ) };
	// 832C2BC0: 48001D50  b 0x832c4910
	crate::recompiler::externs::call(&mut ctx, base, 0x832C4910);
	return;
	// 832C2BC4: 2B050010  cmplwi cr6, r5, 0x10
	ctx.cr[6].compare_u32(ctx.r[5].u32, 16 as u32, &mut ctx.xer);
	// 832C2BC8: 41981D48  blt cr6, 0x832c4910
	if ctx.cr[6].lt {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C4910);
		return;
	}
	// 832C2BCC: 562A0738  rlwinm r10, r17, 0, 0x1c, 0x1c
	ctx.r[10].u64 = ctx.r[17].u32 as u64 & 0xFFFFFFFFu64;
	// 832C2BD0: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C2BD4: 409A0C14  bne cr6, 0x832c37e8
	if !ctx.cr[6].eq {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C37E8);
		return;
	}
	// 832C2BD8: 81410284  lwz r10, 0x284(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(644 as u32) ) } as u64;
	// 832C2BDC: 81210280  lwz r9, 0x280(r1)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(640 as u32) ) } as u64;
	// 832C2BE0: 7F095040  cmplw cr6, r9, r10
	ctx.cr[6].compare_u32(ctx.r[9].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C2BE4: 40980C04  bge cr6, 0x832c37e8
	if !ctx.cr[6].lt {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C37E8);
		return;
	}
	// 832C2BE8: 89490000  lbz r10, 0(r9)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2BEC: 39290001  addi r9, r9, 1
	ctx.r[9].s64 = ctx.r[9].s64 + 1;
	// 832C2BF0: 394AFFFD  addi r10, r10, -3
	ctx.r[10].s64 = ctx.r[10].s64 + -3;
	// 832C2BF4: 91210280  stw r9, 0x280(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(640 as u32), ctx.r[9].u32 ) };
	// 832C2BF8: 2B0A0006  cmplwi cr6, r10, 6
	ctx.cr[6].compare_u32(ctx.r[10].u32, 6 as u32, &mut ctx.xer);
	// 832C2BFC: 41990BEC  bgt cr6, 0x832c37e8
	if ctx.cr[6].gt {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C37E8);
		return;
	}
	// 832C2C00: 3D80832C  lis r12, -0x7cd4
	ctx.r[12].s64 = -2094268416;
	// 832C2C04: 398C2C18  addi r12, r12, 0x2c18
	ctx.r[12].s64 = ctx.r[12].s64 + 11288;
	// 832C2C08: 5540103A  slwi r0, r10, 2
	// 832C2C0C: 7C0C002E  lwzx r0, r12, r0
	ctx.r[0].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[12].u32.wrapping_add(ctx.r[0].u32)) } as u64;
	// 832C2C10: 7C0903A6  mtctr r0
	ctx.ctr.u64 = ctx.r[0].u64;
	// 832C2C14: 4E800420  bctr
	crate::rt::call_indirect(ctx.ctr.u32);
	return;
	// 832C2C18: 832C2C34  lwz r25, 0x2c34(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(11316 as u32) ) } as u64;
	// 832C2C1C: 832C37E8  lwz r25, 0x37e8(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(14312 as u32) ) } as u64;
	// 832C2C20: 832C2FB8  lwz r25, 0x2fb8(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(12216 as u32) ) } as u64;
	// 832C2C24: 832C307C  lwz r25, 0x307c(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(12412 as u32) ) } as u64;
	// 832C2C28: 832C37E8  lwz r25, 0x37e8(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(14312 as u32) ) } as u64;
	// 832C2C2C: 832C3134  lwz r25, 0x3134(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(12596 as u32) ) } as u64;
	// 832C2C30: 832C3498  lwz r25, 0x3498(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(13464 as u32) ) } as u64;
	// 832C2C34: 8141060C  lwz r10, 0x60c(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(1548 as u32) ) } as u64;
	// 832C2C38: 7D315050  subf r9, r17, r10
	ctx.r[9].s64 = ctx.r[10].s64 - ctx.r[17].s64;
	// 832C2C3C: 2B090010  cmplwi cr6, r9, 0x10
	ctx.cr[6].compare_u32(ctx.r[9].u32, 16 as u32, &mut ctx.xer);
	// 832C2C40: 41981CD0  blt cr6, 0x832c4910
	if ctx.cr[6].lt {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C4910);
		return;
	}
	// 832C2C44: 2B1A0004  cmplwi cr6, r26, 4
	ctx.cr[6].compare_u32(ctx.r[26].u32, 4 as u32, &mut ctx.xer);
	// 832C2C48: 40980028  bge cr6, 0x832c2c70
	if !ctx.cr[6].lt {
	pc = 0x832C2C70; continue 'dispatch;
	}
	// 832C2C4C: 81430000  lwz r10, 0(r3)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2C50: 213A0004  subfic r9, r26, 4
	ctx.xer.ca = ctx.r[26].u32 <= 4 as u32;
	ctx.r[9].s64 = (4 as i64) - ctx.r[26].s64;
	// 832C2C54: 38BA001C  addi r5, r26, 0x1c
	ctx.r[5].s64 = ctx.r[26].s64 + 28;
	// 832C2C58: 7D48D030  slw r8, r10, r26
	if (ctx.r[26].u8 & 0x20) != 0 {
		ctx.r[8].u64 = 0;
	} else {
		ctx.r[8].u64 = ((ctx.r[10].u32) << ((ctx.r[26].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2C5C: 7D072378  or r7, r8, r4
	ctx.r[7].u64 = ctx.r[8].u64 | ctx.r[4].u64;
	// 832C2C60: 7D444C30  srw r4, r10, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[10].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C2C64: 54EA073E  clrlwi r10, r7, 0x1c
	ctx.r[10].u64 = ctx.r[7].u32 as u64 & 0x0000000Fu64;
	// 832C2C68: 38630004  addi r3, r3, 4
	ctx.r[3].s64 = ctx.r[3].s64 + 4;
	// 832C2C6C: 48000010  b 0x832c2c7c
	pc = 0x832C2C7C; continue 'dispatch;
	// 832C2C70: 548A073E  clrlwi r10, r4, 0x1c
	ctx.r[10].u64 = ctx.r[4].u32 as u64 & 0x0000000Fu64;
	// 832C2C74: 5484E13E  srwi r4, r4, 4
	// 832C2C78: 38BAFFFC  addi r5, r26, -4
	ctx.r[5].s64 = ctx.r[26].s64 + -4;
	// 832C2C7C: 812100B4  lwz r9, 0xb4(r1)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(180 as u32) ) } as u64;
	// 832C2C80: 554A3032  slwi r10, r10, 6
	// 832C2C84: 810100A4  lwz r8, 0xa4(r1)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(164 as u32) ) } as u64;
	// 832C2C88: 7F0B4840  cmplw cr6, r11, r9
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[9].u32, &mut ctx.xer);
	// 832C2C8C: 7CEA4214  add r7, r10, r8
	ctx.r[7].u64 = ctx.r[10].u64 + ctx.r[8].u64;
	// 832C2C90: 41980010  blt cr6, 0x832c2ca0
	if ctx.cr[6].lt {
	pc = 0x832C2CA0; continue 'dispatch;
	}
	// 832C2C94: 816100E0  lwz r11, 0xe0(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(224 as u32) ) } as u64;
	// 832C2C98: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832C2C9C: 916100B0  stw r11, 0xb0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(176 as u32), ctx.r[11].u32 ) };
	// 832C2CA0: 81410104  lwz r10, 0x104(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(260 as u32) ) } as u64;
	// 832C2CA4: 7F105040  cmplw cr6, r16, r10
	ctx.cr[6].compare_u32(ctx.r[16].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C2CA8: 4198000C  blt cr6, 0x832c2cb4
	if ctx.cr[6].lt {
	pc = 0x832C2CB4; continue 'dispatch;
	}
	// 832C2CAC: 81410130  lwz r10, 0x130(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(304 as u32) ) } as u64;
	// 832C2CB0: 3A0A0004  addi r16, r10, 4
	ctx.r[16].s64 = ctx.r[10].s64 + 4;
	// 832C2CB4: 7EA9AB78  mr r9, r21
	ctx.r[9].u64 = ctx.r[21].u64;
	// 832C2CB8: 38C00040  li r6, 0x40
	ctx.r[6].s64 = 64;
	// 832C2CBC: 89500000  lbz r10, 0(r16)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[16].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2CC0: 3A100001  addi r16, r16, 1
	ctx.r[16].s64 = ctx.r[16].s64 + 1;
	// 832C2CC4: 394A0001  addi r10, r10, 1
	ctx.r[10].s64 = ctx.r[10].s64 + 1;
	// 832C2CC8: 7F0A3000  cmpw cr6, r10, r6
	ctx.cr[6].compare_i32(ctx.r[10].s32, ctx.r[6].s32, &mut ctx.xer);
	// 832C2CCC: 419900A0  bgt cr6, 0x832c2d6c
	if ctx.cr[6].gt {
	pc = 0x832C2D6C; continue 'dispatch;
	}
	// 832C2CD0: 7CCA3050  subf r6, r10, r6
	ctx.r[6].s64 = ctx.r[6].s64 - ctx.r[10].s64;
	// 832C2CD4: 2B050000  cmplwi cr6, r5, 0
	ctx.cr[6].compare_u32(ctx.r[5].u32, 0 as u32, &mut ctx.xer);
	// 832C2CD8: 409A0018  bne cr6, 0x832c2cf0
	if !ctx.cr[6].eq {
	pc = 0x832C2CF0; continue 'dispatch;
	}
	// 832C2CDC: 81030000  lwz r8, 0(r3)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2CE0: 38630004  addi r3, r3, 4
	ctx.r[3].s64 = ctx.r[3].s64 + 4;
	// 832C2CE4: 38A0001F  li r5, 0x1f
	ctx.r[5].s64 = 31;
	// 832C2CE8: 5504F87E  srwi r4, r8, 1
	// 832C2CEC: 48000010  b 0x832c2cfc
	pc = 0x832C2CFC; continue 'dispatch;
	// 832C2CF0: 7C882378  mr r8, r4
	ctx.r[8].u64 = ctx.r[4].u64;
	// 832C2CF4: 5484F87E  srwi r4, r4, 1
	// 832C2CF8: 38A5FFFF  addi r5, r5, -1
	ctx.r[5].s64 = ctx.r[5].s64 + -1;
	// 832C2CFC: 550807FE  clrlwi r8, r8, 0x1f
	ctx.r[8].u64 = ctx.r[8].u32 as u64 & 0x00000001u64;
	// 832C2D00: 2B080000  cmplwi cr6, r8, 0
	ctx.cr[6].compare_u32(ctx.r[8].u32, 0 as u32, &mut ctx.xer);
	// 832C2D04: 419A0034  beq cr6, 0x832c2d38
	if ctx.cr[6].eq {
	pc = 0x832C2D38; continue 'dispatch;
	}
	// 832C2D08: 890B0000  lbz r8, 0(r11)
	ctx.r[8].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2D0C: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832C2D10: 2F0A0000  cmpwi cr6, r10, 0
	ctx.cr[6].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832C2D14: 916100B0  stw r11, 0xb0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(176 as u32), ctx.r[11].u32 ) };
	// 832C2D18: 419A004C  beq cr6, 0x832c2d64
	if ctx.cr[6].eq {
	pc = 0x832C2D64; continue 'dispatch;
	}
	// 832C2D1C: 7F8938AE  lbzx r28, r9, r7
	ctx.r[28].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[9].u32.wrapping_add(ctx.r[7].u32)) } as u64;
	// 832C2D20: 3B4102C0  addi r26, r1, 0x2c0
	ctx.r[26].s64 = ctx.r[1].s64 + 704;
	// 832C2D24: 354AFFFF  addic. r10, r10, -1
	ctx.xer.ca = (ctx.r[10].u32 > (!(-1 as u32)));
	ctx.r[10].s64 = ctx.r[10].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832C2D28: 39290001  addi r9, r9, 1
	ctx.r[9].s64 = ctx.r[9].s64 + 1;
	// 832C2D2C: 7D1CD1AE  stbx r8, r28, r26
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[28].u32.wrapping_add(ctx.r[26].u32), ctx.r[8].u8) };
	// 832C2D30: 4082FFEC  bne 0x832c2d1c
	if !ctx.cr[0].eq {
	pc = 0x832C2D1C; continue 'dispatch;
	}
	// 832C2D34: 48000030  b 0x832c2d64
	pc = 0x832C2D64; continue 'dispatch;
	// 832C2D38: 2F0A0000  cmpwi cr6, r10, 0
	ctx.cr[6].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832C2D3C: 419A0028  beq cr6, 0x832c2d64
	if ctx.cr[6].eq {
	pc = 0x832C2D64; continue 'dispatch;
	}
	// 832C2D40: 890B0000  lbz r8, 0(r11)
	ctx.r[8].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2D44: 3B8102C0  addi r28, r1, 0x2c0
	ctx.r[28].s64 = ctx.r[1].s64 + 704;
	// 832C2D48: 7F4938AE  lbzx r26, r9, r7
	ctx.r[26].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[9].u32.wrapping_add(ctx.r[7].u32)) } as u64;
	// 832C2D4C: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832C2D50: 354AFFFF  addic. r10, r10, -1
	ctx.xer.ca = (ctx.r[10].u32 > (!(-1 as u32)));
	ctx.r[10].s64 = ctx.r[10].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832C2D54: 39290001  addi r9, r9, 1
	ctx.r[9].s64 = ctx.r[9].s64 + 1;
	// 832C2D58: 916100B0  stw r11, 0xb0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(176 as u32), ctx.r[11].u32 ) };
	// 832C2D5C: 7D1AE1AE  stbx r8, r26, r28
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[26].u32.wrapping_add(ctx.r[28].u32), ctx.r[8].u8) };
	// 832C2D60: 4082FFE0  bne 0x832c2d40
	if !ctx.cr[0].eq {
	pc = 0x832C2D40; continue 'dispatch;
	}
	// 832C2D64: 2F060001  cmpwi cr6, r6, 1
	ctx.cr[6].compare_i32(ctx.r[6].s32, 1, &mut ctx.xer);
	// 832C2D68: 4199FF54  bgt cr6, 0x832c2cbc
	if ctx.cr[6].gt {
	pc = 0x832C2CBC; continue 'dispatch;
	}
	// 832C2D6C: 92010100  stw r16, 0x100(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(256 as u32), ctx.r[16].u32 ) };
	// 832C2D70: 2F09003F  cmpwi cr6, r9, 0x3f
	ctx.cr[6].compare_i32(ctx.r[9].s32, 63, &mut ctx.xer);
	// 832C2D74: 409A001C  bne cr6, 0x832c2d90
	if !ctx.cr[6].eq {
	pc = 0x832C2D90; continue 'dispatch;
	}
	// 832C2D78: 894B0000  lbz r10, 0(r11)
	ctx.r[10].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2D7C: 392102C0  addi r9, r1, 0x2c0
	ctx.r[9].s64 = ctx.r[1].s64 + 704;
	// 832C2D80: 8907003F  lbz r8, 0x3f(r7)
	ctx.r[8].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[7].u32.wrapping_add(63 as u32) ) } as u64;
	// 832C2D84: 396B0001  addi r11, r11, 1
	ctx.r[11].s64 = ctx.r[11].s64 + 1;
	// 832C2D88: 916100B0  stw r11, 0xb0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(176 as u32), ctx.r[11].u32 ) };
	// 832C2D8C: 7D4849AE  stbx r10, r8, r9
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[8].u32.wrapping_add(ctx.r[9].u32), ctx.r[10].u8) };
	// 832C2D90: 7FA9EB78  mr r9, r29
	ctx.r[9].u64 = ctx.r[29].u64;
	// 832C2D94: 7D1DFA14  add r8, r29, r31
	ctx.r[8].u64 = ctx.r[29].u64 + ctx.r[31].u64;
	// 832C2D98: 394102C4  addi r10, r1, 0x2c4
	ctx.r[10].s64 = ctx.r[1].s64 + 708;
	// 832C2D9C: 38E00002  li r7, 2
	ctx.r[7].s64 = 2;
	// 832C2DA0: A0CAFFFC  lhz r6, -4(r10)
	ctx.r[6].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(-4 as u32) ) } as u64;
	// 832C2DA4: 34E7FFFF  addic. r7, r7, -1
	ctx.xer.ca = (ctx.r[7].u32 > (!(-1 as u32)));
	ctx.r[7].s64 = ctx.r[7].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[7].s32, 0, &mut ctx.xer);
	// 832C2DA8: A34A0000  lhz r26, 0(r10)
	ctx.r[26].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2DAC: 54D9401E  rlwinm r25, r6, 8, 0, 0xf
	ctx.r[25].u64 = ctx.r[6].u32 as u64 & 0x00FFFFFFu64;
	// 832C2DB0: A38AFFFE  lhz r28, -2(r10)
	ctx.r[28].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(-2 as u32) ) } as u64;
	// 832C2DB4: A30A0002  lhz r24, 2(r10)
	ctx.r[24].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(2 as u32) ) } as u64;
	// 832C2DB8: 5756401E  rlwinm r22, r26, 8, 0, 0xf
	ctx.r[22].u64 = ctx.r[26].u32 as u64 & 0x00FFFFFFu64;
	// 832C2DBC: 7CD23378  mr r18, r6
	ctx.r[18].u64 = ctx.r[6].u64;
	// 832C2DC0: A2AA0004  lhz r21, 4(r10)
	ctx.r[21].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C2DC4: 7F263378  or r6, r25, r6
	ctx.r[6].u64 = ctx.r[25].u64 | ctx.r[6].u64;
	// 832C2DC8: A32A000A  lhz r25, 0xa(r10)
	ctx.r[25].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(10 as u32) ) } as u64;
	// 832C2DCC: 5797401E  rlwinm r23, r28, 8, 0, 0xf
	ctx.r[23].u64 = ctx.r[28].u32 as u64 & 0x00FFFFFFu64;
	// 832C2DD0: A26A0006  lhz r19, 6(r10)
	ctx.r[19].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(6 as u32) ) } as u64;
	// 832C2DD4: 7F4ED378  mr r14, r26
	ctx.r[14].u64 = ctx.r[26].u64;
	// 832C2DD8: A22A0008  lhz r17, 8(r10)
	ctx.r[17].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C2DDC: 5714401E  rlwinm r20, r24, 8, 0, 0xf
	ctx.r[20].u64 = ctx.r[24].u32 as u64 & 0x00FFFFFFu64;
	// 832C2DE0: A1EA000C  lhz r15, 0xc(r10)
	ctx.r[15].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(12 as u32) ) } as u64;
	// 832C2DE4: 7EDAD378  or r26, r22, r26
	ctx.r[26].u64 = ctx.r[22].u64 | ctx.r[26].u64;
	// 832C2DE8: 7F90E378  mr r16, r28
	ctx.r[16].u64 = ctx.r[28].u64;
	// 832C2DEC: 7F16C378  mr r22, r24
	ctx.r[22].u64 = ctx.r[24].u64;
	// 832C2DF0: 50D2402E  rlwimi r18, r6, 8, 0, 0x17
	ctx.r[18].u64 = (((ctx.r[6].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[18].u64 & 0xFFFFFFFF000000FF);
	// 832C2DF4: 7EFCE378  or r28, r23, r28
	ctx.r[28].u64 = ctx.r[23].u64 | ctx.r[28].u64;
	// 832C2DF8: A2EA000E  lhz r23, 0xe(r10)
	ctx.r[23].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(14 as u32) ) } as u64;
	// 832C2DFC: 7E98C378  or r24, r20, r24
	ctx.r[24].u64 = ctx.r[20].u64 | ctx.r[24].u64;
	// 832C2E00: 92490000  stw r18, 0(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.r[18].u32 ) };
	// 832C2E04: 56A6401E  rlwinm r6, r21, 8, 0, 0xf
	ctx.r[6].u64 = ctx.r[21].u32 as u64 & 0x00FFFFFFu64;
	// 832C2E08: 92480000  stw r18, 0(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[18].u32 ) };
	// 832C2E0C: 5316402E  rlwimi r22, r24, 8, 0, 0x17
	ctx.r[22].u64 = (((ctx.r[24].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[22].u64 & 0xFFFFFFFF000000FF);
	// 832C2E10: 5390402E  rlwimi r16, r28, 8, 0, 0x17
	ctx.r[16].u64 = (((ctx.r[28].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[16].u64 & 0xFFFFFFFF000000FF);
	// 832C2E14: 534E402E  rlwimi r14, r26, 8, 0, 0x17
	ctx.r[14].u64 = (((ctx.r[26].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[14].u64 & 0xFFFFFFFF000000FF);
	// 832C2E18: 92C9000C  stw r22, 0xc(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(12 as u32), ctx.r[22].u32 ) };
	// 832C2E1C: 7CC6AB78  or r6, r6, r21
	ctx.r[6].u64 = ctx.r[6].u64 | ctx.r[21].u64;
	// 832C2E20: 92090004  stw r16, 4(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), ctx.r[16].u32 ) };
	// 832C2E24: 7EB8AB78  mr r24, r21
	ctx.r[24].u64 = ctx.r[21].u64;
	// 832C2E28: 91C90008  stw r14, 8(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(8 as u32), ctx.r[14].u32 ) };
	// 832C2E2C: 5735401E  rlwinm r21, r25, 8, 0, 0xf
	ctx.r[21].u64 = ctx.r[25].u32 as u64 & 0x00FFFFFFu64;
	// 832C2E30: 92C8000C  stw r22, 0xc(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(12 as u32), ctx.r[22].u32 ) };
	// 832C2E34: 7D28FA14  add r9, r8, r31
	ctx.r[9].u64 = ctx.r[8].u64 + ctx.r[31].u64;
	// 832C2E38: 92080004  stw r16, 4(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), ctx.r[16].u32 ) };
	// 832C2E3C: 50D8402E  rlwimi r24, r6, 8, 0, 0x17
	ctx.r[24].u64 = (((ctx.r[6].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[24].u64 & 0xFFFFFFFF000000FF);
	// 832C2E40: 91C80008  stw r14, 8(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(8 as u32), ctx.r[14].u32 ) };
	// 832C2E44: 567C401E  rlwinm r28, r19, 8, 0, 0xf
	ctx.r[28].u64 = ctx.r[19].u32 as u64 & 0x00FFFFFFu64;
	// 832C2E48: 7EA6CB78  or r6, r21, r25
	ctx.r[6].u64 = ctx.r[21].u64 | ctx.r[25].u64;
	// 832C2E4C: 7F28CB78  mr r8, r25
	ctx.r[8].u64 = ctx.r[25].u64;
	// 832C2E50: 7F9C9B78  or r28, r28, r19
	ctx.r[28].u64 = ctx.r[28].u64 | ctx.r[19].u64;
	// 832C2E54: 93090000  stw r24, 0(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.r[24].u32 ) };
	// 832C2E58: 563A401E  rlwinm r26, r17, 8, 0, 0xf
	ctx.r[26].u64 = ctx.r[17].u32 as u64 & 0x00FFFFFFu64;
	// 832C2E5C: 7F09F92E  stwx r24, r9, r31
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[31].u32), ctx.r[24].u32) };
	// 832C2E60: 7E749B78  mr r20, r19
	ctx.r[20].u64 = ctx.r[19].u64;
	// 832C2E64: A30A0014  lhz r24, 0x14(r10)
	ctx.r[24].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(20 as u32) ) } as u64;
	// 832C2E68: 50C8402E  rlwimi r8, r6, 8, 0, 0x17
	ctx.r[8].u64 = (((ctx.r[6].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[8].u64 & 0xFFFFFFFF000000FF);
	// 832C2E6C: 5394402E  rlwimi r20, r28, 8, 0, 0x17
	ctx.r[20].u64 = (((ctx.r[28].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[20].u64 & 0xFFFFFFFF000000FF);
	// 832C2E70: 7F5A8B78  or r26, r26, r17
	ctx.r[26].u64 = ctx.r[26].u64 | ctx.r[17].u64;
	// 832C2E74: 7E368B78  mr r22, r17
	ctx.r[22].u64 = ctx.r[17].u64;
	// 832C2E78: 92890004  stw r20, 4(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), ctx.r[20].u32 ) };
	// 832C2E7C: 7D1C4378  mr r28, r8
	ctx.r[28].u64 = ctx.r[8].u64;
	// 832C2E80: 7D09FA14  add r8, r9, r31
	ctx.r[8].u64 = ctx.r[9].u64 + ctx.r[31].u64;
	// 832C2E84: 5356402E  rlwimi r22, r26, 8, 0, 0x17
	ctx.r[22].u64 = (((ctx.r[26].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[22].u64 & 0xFFFFFFFF000000FF);
	// 832C2E88: 55E6401E  rlwinm r6, r15, 8, 0, 0xf
	ctx.r[6].u64 = ctx.r[15].u32 as u64 & 0x00FFFFFFu64;
	// 832C2E8C: 92C90008  stw r22, 8(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(8 as u32), ctx.r[22].u32 ) };
	// 832C2E90: 7DFA7B78  mr r26, r15
	ctx.r[26].u64 = ctx.r[15].u64;
	// 832C2E94: 9389000C  stw r28, 0xc(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(12 as u32), ctx.r[28].u32 ) };
	// 832C2E98: 7CC67B78  or r6, r6, r15
	ctx.r[6].u64 = ctx.r[6].u64 | ctx.r[15].u64;
	// 832C2E9C: 92880004  stw r20, 4(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), ctx.r[20].u32 ) };
	// 832C2EA0: 7D28FA14  add r9, r8, r31
	ctx.r[9].u64 = ctx.r[8].u64 + ctx.r[31].u64;
	// 832C2EA4: 56F9401E  rlwinm r25, r23, 8, 0, 0xf
	ctx.r[25].u64 = ctx.r[23].u32 as u64 & 0x00FFFFFFu64;
	// 832C2EA8: 92C80008  stw r22, 8(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(8 as u32), ctx.r[22].u32 ) };
	// 832C2EAC: 50DA402E  rlwimi r26, r6, 8, 0, 0x17
	ctx.r[26].u64 = (((ctx.r[6].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[26].u64 & 0xFFFFFFFF000000FF);
	// 832C2EB0: 9388000C  stw r28, 0xc(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(12 as u32), ctx.r[28].u32 ) };
	// 832C2EB4: 7F3CBB78  or r28, r25, r23
	ctx.r[28].u64 = ctx.r[25].u64 | ctx.r[23].u64;
	// 832C2EB8: A2CA0016  lhz r22, 0x16(r10)
	ctx.r[22].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(22 as u32) ) } as u64;
	// 832C2EBC: 7EE6BB78  mr r6, r23
	ctx.r[6].u64 = ctx.r[23].u64;
	// 832C2EC0: 93490000  stw r26, 0(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.r[26].u32 ) };
	// 832C2EC4: 7D09FA14  add r8, r9, r31
	ctx.r[8].u64 = ctx.r[9].u64 + ctx.r[31].u64;
	// 832C2EC8: 7F49F92E  stwx r26, r9, r31
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[31].u32), ctx.r[26].u32) };
	// 832C2ECC: 5386402E  rlwimi r6, r28, 8, 0, 0x17
	ctx.r[6].u64 = (((ctx.r[28].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[6].u64 & 0xFFFFFFFF000000FF);
	// 832C2ED0: A38A0010  lhz r28, 0x10(r10)
	ctx.r[28].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(16 as u32) ) } as u64;
	// 832C2ED4: A34A0012  lhz r26, 0x12(r10)
	ctx.r[26].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(18 as u32) ) } as u64;
	// 832C2ED8: 5799401E  rlwinm r25, r28, 8, 0, 0xf
	ctx.r[25].u64 = ctx.r[28].u32 as u64 & 0x00FFFFFFu64;
	// 832C2EDC: 5757401E  rlwinm r23, r26, 8, 0, 0xf
	ctx.r[23].u64 = ctx.r[26].u32 as u64 & 0x00FFFFFFu64;
	// 832C2EE0: 7F95E378  mr r21, r28
	ctx.r[21].u64 = ctx.r[28].u64;
	// 832C2EE4: 7F3CE378  or r28, r25, r28
	ctx.r[28].u64 = ctx.r[25].u64 | ctx.r[28].u64;
	// 832C2EE8: 90C80004  stw r6, 4(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), ctx.r[6].u32 ) };
	// 832C2EEC: 7F53D378  mr r19, r26
	ctx.r[19].u64 = ctx.r[26].u64;
	// 832C2EF0: 90C90004  stw r6, 4(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), ctx.r[6].u32 ) };
	// 832C2EF4: 7EFAD378  or r26, r23, r26
	ctx.r[26].u64 = ctx.r[23].u64 | ctx.r[26].u64;
	// 832C2EF8: A32A001A  lhz r25, 0x1a(r10)
	ctx.r[25].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(26 as u32) ) } as u64;
	// 832C2EFC: 5706401E  rlwinm r6, r24, 8, 0, 0xf
	ctx.r[6].u64 = ctx.r[24].u32 as u64 & 0x00FFFFFFu64;
	// 832C2F00: A28A0018  lhz r20, 0x18(r10)
	ctx.r[20].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(24 as u32) ) } as u64;
	// 832C2F04: 5395402E  rlwimi r21, r28, 8, 0, 0x17
	ctx.r[21].u64 = (((ctx.r[28].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[21].u64 & 0xFFFFFFFF000000FF);
	// 832C2F08: 5353402E  rlwimi r19, r26, 8, 0, 0x17
	ctx.r[19].u64 = (((ctx.r[26].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[19].u64 & 0xFFFFFFFF000000FF);
	// 832C2F0C: 7CC6C378  or r6, r6, r24
	ctx.r[6].u64 = ctx.r[6].u64 | ctx.r[24].u64;
	// 832C2F10: 92A90008  stw r21, 8(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(8 as u32), ctx.r[21].u32 ) };
	// 832C2F14: 7F17C378  mr r23, r24
	ctx.r[23].u64 = ctx.r[24].u64;
	// 832C2F18: 92A80008  stw r21, 8(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(8 as u32), ctx.r[21].u32 ) };
	// 832C2F1C: 5738401E  rlwinm r24, r25, 8, 0, 0xf
	ctx.r[24].u64 = ctx.r[25].u32 as u64 & 0x00FFFFFFu64;
	// 832C2F20: 9269000C  stw r19, 0xc(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(12 as u32), ctx.r[19].u32 ) };
	// 832C2F24: 56DC401E  rlwinm r28, r22, 8, 0, 0xf
	ctx.r[28].u64 = ctx.r[22].u32 as u64 & 0x00FFFFFFu64;
	// 832C2F28: 9268000C  stw r19, 0xc(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(12 as u32), ctx.r[19].u32 ) };
	// 832C2F2C: 7D28FA14  add r9, r8, r31
	ctx.r[9].u64 = ctx.r[8].u64 + ctx.r[31].u64;
	// 832C2F30: 50D7402E  rlwimi r23, r6, 8, 0, 0x17
	ctx.r[23].u64 = (((ctx.r[6].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[23].u64 & 0xFFFFFFFF000000FF);
	// 832C2F34: 569A401E  rlwinm r26, r20, 8, 0, 0xf
	ctx.r[26].u64 = ctx.r[20].u32 as u64 & 0x00FFFFFFu64;
	// 832C2F38: 7F08CB78  or r8, r24, r25
	ctx.r[8].u64 = ctx.r[24].u64 | ctx.r[25].u64;
	// 832C2F3C: 7F26CB78  mr r6, r25
	ctx.r[6].u64 = ctx.r[25].u64;
	// 832C2F40: 7ED5B378  mr r21, r22
	ctx.r[21].u64 = ctx.r[22].u64;
	// 832C2F44: 92E90000  stw r23, 0(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.r[23].u32 ) };
	// 832C2F48: 7F9CB378  or r28, r28, r22
	ctx.r[28].u64 = ctx.r[28].u64 | ctx.r[22].u64;
	// 832C2F4C: 7EE9F92E  stwx r23, r9, r31
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[31].u32), ctx.r[23].u32) };
	// 832C2F50: 7E96A378  mr r22, r20
	ctx.r[22].u64 = ctx.r[20].u64;
	// 832C2F54: 7F5AA378  or r26, r26, r20
	ctx.r[26].u64 = ctx.r[26].u64 | ctx.r[20].u64;
	// 832C2F58: 5106402E  rlwimi r6, r8, 8, 0, 0x17
	ctx.r[6].u64 = (((ctx.r[8].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[6].u64 & 0xFFFFFFFF000000FF);
	// 832C2F5C: 7D09FA14  add r8, r9, r31
	ctx.r[8].u64 = ctx.r[9].u64 + ctx.r[31].u64;
	// 832C2F60: 5395402E  rlwimi r21, r28, 8, 0, 0x17
	ctx.r[21].u64 = (((ctx.r[28].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[21].u64 & 0xFFFFFFFF000000FF);
	// 832C2F64: 5356402E  rlwimi r22, r26, 8, 0, 0x17
	ctx.r[22].u64 = (((ctx.r[26].u32).rotate_left(8) as u64) & 0x00000000FFFFFF00) | (ctx.r[22].u64 & 0xFFFFFFFF000000FF);
	// 832C2F68: 92A90004  stw r21, 4(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), ctx.r[21].u32 ) };
	// 832C2F6C: 394A0020  addi r10, r10, 0x20
	ctx.r[10].s64 = ctx.r[10].s64 + 32;
	// 832C2F70: 92C90008  stw r22, 8(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(8 as u32), ctx.r[22].u32 ) };
	// 832C2F74: 90C9000C  stw r6, 0xc(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(12 as u32), ctx.r[6].u32 ) };
	// 832C2F78: 7D28FA14  add r9, r8, r31
	ctx.r[9].u64 = ctx.r[8].u64 + ctx.r[31].u64;
	// 832C2F7C: 92A80004  stw r21, 4(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), ctx.r[21].u32 ) };
	// 832C2F80: 92C80008  stw r22, 8(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(8 as u32), ctx.r[22].u32 ) };
	// 832C2F84: 90C8000C  stw r6, 0xc(r8)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(12 as u32), ctx.r[6].u32 ) };
	// 832C2F88: 7D09FA14  add r8, r9, r31
	ctx.r[8].u64 = ctx.r[9].u64 + ctx.r[31].u64;
	// 832C2F8C: 4082FE14  bne 0x832c2da0
	if !ctx.cr[0].eq {
	pc = 0x832C2DA0; continue 'dispatch;
	}
	// 832C2F90: 7CBA2B78  mr r26, r5
	ctx.r[26].u64 = ctx.r[5].u64;
	// 832C2F94: 83010078  lwz r24, 0x78(r1)
	ctx.r[24].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(120 as u32) ) } as u64;
	// 832C2F98: 82E10084  lwz r23, 0x84(r1)
	ctx.r[23].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(132 as u32) ) } as u64;
	// 832C2F9C: 3AA00000  li r21, 0
	ctx.r[21].s64 = 0;
	// 832C2FA0: 83210180  lwz r25, 0x180(r1)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(384 as u32) ) } as u64;
	// 832C2FA4: 82C10140  lwz r22, 0x140(r1)
	ctx.r[22].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(320 as u32) ) } as u64;
	// 832C2FA8: 90610064  stw r3, 0x64(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(100 as u32), ctx.r[3].u32 ) };
	// 832C2FAC: 90810060  stw r4, 0x60(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(96 as u32), ctx.r[4].u32 ) };
	// 832C2FB0: 93410068  stw r26, 0x68(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(104 as u32), ctx.r[26].u32 ) };
	// 832C2FB4: 48000820  b 0x832c37d4
	crate::recompiler::externs::call(&mut ctx, base, 0x832C37D4);
	return;
	// 832C2FB8: 8141060C  lwz r10, 0x60c(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(1548 as u32) ) } as u64;
	// 832C2FBC: 7D315050  subf r9, r17, r10
	ctx.r[9].s64 = ctx.r[10].s64 - ctx.r[17].s64;
	// 832C2FC0: 2B090010  cmplwi cr6, r9, 0x10
	ctx.cr[6].compare_u32(ctx.r[9].u32, 16 as u32, &mut ctx.xer);
	// 832C2FC4: 4198194C  blt cr6, 0x832c4910
	if ctx.cr[6].lt {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C4910);
		return;
	}
	// 832C2FC8: 7F123040  cmplw cr6, r18, r6
	ctx.cr[6].compare_u32(ctx.r[18].u32, ctx.r[6].u32, &mut ctx.xer);
	// 832C2FCC: 4198000C  blt cr6, 0x832c2fd8
	if ctx.cr[6].lt {
	pc = 0x832C2FD8; continue 'dispatch;
	}
	// 832C2FD0: 816101F0  lwz r11, 0x1f0(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(496 as u32) ) } as u64;
	// 832C2FD4: 3A4B0004  addi r18, r11, 4
	ctx.r[18].s64 = ctx.r[11].s64 + 4;
	// 832C2FD8: A1720000  lhz r11, 0(r18)
	ctx.r[11].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[18].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C2FDC: 3A520002  addi r18, r18, 2
	ctx.r[18].s64 = ctx.r[18].s64 + 2;
	// 832C2FE0: 38810060  addi r4, r1, 0x60
	ctx.r[4].s64 = ctx.r[1].s64 + 96;
	// 832C2FE4: 38610340  addi r3, r1, 0x340
	ctx.r[3].s64 = ctx.r[1].s64 + 832;
	// 832C2FE8: 924101C0  stw r18, 0x1c0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(448 as u32), ctx.r[18].u32 ) };
	// 832C2FEC: B1610340  sth r11, 0x340(r1)
	unsafe { crate::rt::store_u16( base as *mut u8, ctx.r[1].u32.wrapping_add(832 as u32), ctx.r[11].u16 ) };
	// 832C2FF0: 480059B9  bl 0x832c89a8
	ctx.lr = 0x832C2FF4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C89A8);
	// 832C2FF4: 81610068  lwz r11, 0x68(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(104 as u32) ) } as u64;
	// 832C2FF8: 2B0B0004  cmplwi cr6, r11, 4
	ctx.cr[6].compare_u32(ctx.r[11].u32, 4 as u32, &mut ctx.xer);
	// 832C2FFC: 4098003C  bge cr6, 0x832c3038
	if !ctx.cr[6].lt {
	pc = 0x832C3038; continue 'dispatch;
	}
	// 832C3000: 81410064  lwz r10, 0x64(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(100 as u32) ) } as u64;
	// 832C3004: 212B0004  subfic r9, r11, 4
	ctx.xer.ca = ctx.r[11].u32 <= 4 as u32;
	ctx.r[9].s64 = (4 as i64) - ctx.r[11].s64;
	// 832C3008: 81010060  lwz r8, 0x60(r1)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C300C: 38EB001C  addi r7, r11, 0x1c
	ctx.r[7].s64 = ctx.r[11].s64 + 28;
	// 832C3010: 38CA0004  addi r6, r10, 4
	ctx.r[6].s64 = ctx.r[10].s64 + 4;
	// 832C3014: 80AA0000  lwz r5, 0(r10)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C3018: 90E10068  stw r7, 0x68(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(104 as u32), ctx.r[7].u32 ) };
	// 832C301C: 7CA45830  slw r4, r5, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[4].u64 = 0;
	} else {
		ctx.r[4].u64 = ((ctx.r[5].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C3020: 90C10064  stw r6, 0x64(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(100 as u32), ctx.r[6].u32 ) };
	// 832C3024: 7CAB4C30  srw r11, r5, r9
	if (ctx.r[9].u8 & 0x20) != 0 {
		ctx.r[11].u64 = 0;
	} else {
		ctx.r[11].u64 = ((ctx.r[5].u32) >> ((ctx.r[9].u8 & 0x1F) as u32)) as u64;
	}
	// 832C3028: 7C834378  or r3, r4, r8
	ctx.r[3].u64 = ctx.r[4].u64 | ctx.r[8].u64;
	// 832C302C: 91610060  stw r11, 0x60(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(96 as u32), ctx.r[11].u32 ) };
	// 832C3030: 546B073E  clrlwi r11, r3, 0x1c
	ctx.r[11].u64 = ctx.r[3].u32 as u64 & 0x0000000Fu64;
	// 832C3034: 4800001C  b 0x832c3050
	pc = 0x832C3050; continue 'dispatch;
	// 832C3038: 81410060  lwz r10, 0x60(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C303C: 396BFFFC  addi r11, r11, -4
	ctx.r[11].s64 = ctx.r[11].s64 + -4;
	// 832C3040: 5549E13E  srwi r9, r10, 4
	// 832C3044: 91610068  stw r11, 0x68(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(104 as u32), ctx.r[11].u32 ) };
	// 832C3048: 554B073E  clrlwi r11, r10, 0x1c
	ctx.r[11].u64 = ctx.r[10].u32 as u64 & 0x0000000Fu64;
	// 832C304C: 91210060  stw r9, 0x60(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(96 as u32), ctx.r[9].u32 ) };
	// 832C3050: 556B402E  slwi r11, r11, 8
	// 832C3054: 38A10340  addi r5, r1, 0x340
	ctx.r[5].s64 = ctx.r[1].s64 + 832;
	// 832C3058: 7CCBF214  add r6, r11, r30
	ctx.r[6].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C305C: 7FE4FB78  mr r4, r31
	ctx.r[4].u64 = ctx.r[31].u64;
	// 832C3060: 7FA3EB78  mr r3, r29
	ctx.r[3].u64 = ctx.r[29].u64;
	// 832C3064: 48005255  bl 0x832c82b8
	ctx.lr = 0x832C3068;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C82B8);
	// 832C3068: 80610064  lwz r3, 0x64(r1)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(100 as u32) ) } as u64;
	// 832C306C: 80810060  lwz r4, 0x60(r1)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C3070: 816100B0  lwz r11, 0xb0(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(176 as u32) ) } as u64;
	// 832C3074: 83410068  lwz r26, 0x68(r1)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(104 as u32) ) } as u64;
	// 832C3078: 48000770  b 0x832c37e8
	crate::recompiler::externs::call(&mut ctx, base, 0x832C37E8);
	return;
	// 832C307C: 8141060C  lwz r10, 0x60c(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(1548 as u32) ) } as u64;
	// 832C3080: 7D315050  subf r9, r17, r10
	ctx.r[9].s64 = ctx.r[10].s64 - ctx.r[17].s64;
	// 832C3084: 2B090010  cmplwi cr6, r9, 0x10
	ctx.cr[6].compare_u32(ctx.r[9].u32, 16 as u32, &mut ctx.xer);
	// 832C3088: 41981888  blt cr6, 0x832c4910
	if ctx.cr[6].lt {
		crate::recompiler::externs::call(&mut ctx, base, 0x832C4910);
		return;
	}
	// 832C308C: 814100B4  lwz r10, 0xb4(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(180 as u32) ) } as u64;
	// 832C3090: 7F0B5040  cmplw cr6, r11, r10
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[10].u32, &mut ctx.xer);
	// 832C3094: 41980010  blt cr6, 0x832c30a4
	if ctx.cr[6].lt {
	pc = 0x832C30A4; continue 'dispatch;
	}
	// 832C3098: 816100E0  lwz r11, 0xe0(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(224 as u32) ) } as u64;
	// 832C309C: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832C30A0: 916100B0  stw r11, 0xb0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(176 as u32), ctx.r[11].u32 ) };
}

pub fn sub_832C49E8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C49E8 size=968
	// 832C49E8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C49EC: 4B9E49F5  bl 0x82ca93e0
	ctx.lr = 0x832C49F0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93E0);
	// 832C49F0: 3D80FFFF  lis r12, -1
	ctx.r[12].s64 = -65536;
	// 832C49F4: 618C4EC0  ori r12, r12, 0x4ec0
	ctx.r[12].u64 = ctx.r[12].u64 | 20160;
	// 832C49F8: 4B9EBC5D  bl 0x82cb0654
	ctx.lr = 0x832C49FC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CB0654);
	// 832C49FC: 7C21616E  stwux r1, r1, r12
	ea = ctx.r[1].u32.wrapping_add(ctx.r[12].u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C4A00: 7D384B78  mr r24, r9
	ctx.r[24].u64 = ctx.r[9].u64;
	// 832C4A04: 7D5C5378  mr r28, r10
	ctx.r[28].u64 = ctx.r[10].u64;
	// 832C4A08: 7CD43378  mr r20, r6
	ctx.r[20].u64 = ctx.r[6].u64;
	// 832C4A0C: 392100C0  addi r9, r1, 0xc0
	ctx.r[9].s64 = ctx.r[1].s64 + 192;
	// 832C4A10: 7CF73B78  mr r23, r7
	ctx.r[23].u64 = ctx.r[7].u64;
	// 832C4A14: 81580000  lwz r10, 0(r24)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C4A18: 7CB92B78  mr r25, r5
	ctx.r[25].u64 = ctx.r[5].u64;
	// 832C4A1C: 81780004  lwz r11, 4(r24)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C4A20: 38E100C0  addi r7, r1, 0xc0
	ctx.r[7].s64 = ctx.r[1].s64 + 192;
	// 832C4A24: 80D80008  lwz r6, 8(r24)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C4A28: 3B4100C0  addi r26, r1, 0xc0
	ctx.r[26].s64 = ctx.r[1].s64 + 192;
	// 832C4A2C: 7D6B5214  add r11, r11, r10
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832C4A30: 80B8000C  lwz r5, 0xc(r24)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(12 as u32) ) } as u64;
	// 832C4A34: 83B80010  lwz r29, 0x10(r24)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(16 as u32) ) } as u64;
	// 832C4A38: 3BC100C0  addi r30, r1, 0xc0
	ctx.r[30].s64 = ctx.r[1].s64 + 192;
	// 832C4A3C: 7ECB4A14  add r22, r11, r9
	ctx.r[22].u64 = ctx.r[11].u64 + ctx.r[9].u64;
	// 832C4A40: 81380014  lwz r9, 0x14(r24)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(20 as u32) ) } as u64;
	// 832C4A44: 7D665A14  add r11, r6, r11
	ctx.r[11].u64 = ctx.r[6].u64 + ctx.r[11].u64;
	// 832C4A48: 80D8001C  lwz r6, 0x1c(r24)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(28 as u32) ) } as u64;
	// 832C4A4C: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832C4A50: 92C10098  stw r22, 0x98(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(152 as u32), ctx.r[22].u32 ) };
	// 832C4A54: 7EAB3A14  add r21, r11, r7
	ctx.r[21].u64 = ctx.r[11].u64 + ctx.r[7].u64;
	// 832C4A58: 80F80020  lwz r7, 0x20(r24)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C4A5C: 7D655A14  add r11, r5, r11
	ctx.r[11].u64 = ctx.r[5].u64 + ctx.r[11].u64;
	// 832C4A60: 7C9B2378  mr r27, r4
	ctx.r[27].u64 = ctx.r[4].u64;
	// 832C4A64: 80980018  lwz r4, 0x18(r24)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(24 as u32) ) } as u64;
	// 832C4A68: 7F4BD214  add r26, r11, r26
	ctx.r[26].u64 = ctx.r[11].u64 + ctx.r[26].u64;
	// 832C4A6C: 92A1009C  stw r21, 0x9c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(156 as u32), ctx.r[21].u32 ) };
	// 832C4A70: 7D7D5A14  add r11, r29, r11
	ctx.r[11].u64 = ctx.r[29].u64 + ctx.r[11].u64;
	// 832C4A74: 386100C0  addi r3, r1, 0xc0
	ctx.r[3].s64 = ctx.r[1].s64 + 192;
	// 832C4A78: 934100A0  stw r26, 0xa0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(160 as u32), ctx.r[26].u32 ) };
	// 832C4A7C: 7FCBF214  add r30, r11, r30
	ctx.r[30].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C4A80: 7D695A14  add r11, r9, r11
	ctx.r[11].u64 = ctx.r[9].u64 + ctx.r[11].u64;
	// 832C4A84: 38A100C0  addi r5, r1, 0xc0
	ctx.r[5].s64 = ctx.r[1].s64 + 192;
	// 832C4A88: 93C100A4  stw r30, 0xa4(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(164 as u32), ctx.r[30].u32 ) };
	// 832C4A8C: 7C6B1A14  add r3, r11, r3
	ctx.r[3].u64 = ctx.r[11].u64 + ctx.r[3].u64;
	// 832C4A90: 7D645A14  add r11, r4, r11
	ctx.r[11].u64 = ctx.r[4].u64 + ctx.r[11].u64;
	// 832C4A94: 3BA100C0  addi r29, r1, 0xc0
	ctx.r[29].s64 = ctx.r[1].s64 + 192;
	// 832C4A98: 906100A8  stw r3, 0xa8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(168 as u32), ctx.r[3].u32 ) };
	// 832C4A9C: 7CAB2A14  add r5, r11, r5
	ctx.r[5].u64 = ctx.r[11].u64 + ctx.r[5].u64;
	// 832C4AA0: 7D665A14  add r11, r6, r11
	ctx.r[11].u64 = ctx.r[6].u64 + ctx.r[11].u64;
	// 832C4AA4: 392100C0  addi r9, r1, 0xc0
	ctx.r[9].s64 = ctx.r[1].s64 + 192;
	// 832C4AA8: 7D4AEA14  add r10, r10, r29
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[29].u64;
	// 832C4AAC: 7C675A14  add r3, r7, r11
	ctx.r[3].u64 = ctx.r[7].u64 + ctx.r[11].u64;
	// 832C4AB0: 388100C0  addi r4, r1, 0xc0
	ctx.r[4].s64 = ctx.r[1].s64 + 192;
	// 832C4AB4: 91410094  stw r10, 0x94(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(148 as u32), ctx.r[10].u32 ) };
	// 832C4AB8: 7D6B4A14  add r11, r11, r9
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[9].u64;
	// 832C4ABC: 90A100AC  stw r5, 0xac(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(172 as u32), ctx.r[5].u32 ) };
	// 832C4AC0: 90810090  stw r4, 0x90(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(144 as u32), ctx.r[4].u32 ) };
	// 832C4AC4: 3B40FFFF  li r26, -1
	ctx.r[26].s64 = -1;
	// 832C4AC8: 916100B0  stw r11, 0xb0(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(176 as u32), ctx.r[11].u32 ) };
	// 832C4ACC: 2B03B000  cmplwi cr6, r3, 0xb000
	ctx.cr[6].compare_u32(ctx.r[3].u32, 45056 as u32, &mut ctx.xer);
	// 832C4AD0: 40990010  ble cr6, 0x832c4ae0
	if !ctx.cr[6].gt {
	pc = 0x832C4AE0; continue 'dispatch;
	}
	// 832C4AD4: 38600000  li r3, 0
	ctx.r[3].s64 = 0;
	// 832C4AD8: 80210000  lwz r1, 0(r1)
	ctx.r[1].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C4ADC: 4B9E4954  b 0x82ca9430
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9430);
	return;
	// 832C4AE0: 3AC80004  addi r22, r8, 4
	ctx.r[22].s64 = ctx.r[8].s64 + 4;
	// 832C4AE4: 3AA00001  li r21, 1
	ctx.r[21].s64 = 1;
	// 832C4AE8: 2B1B0000  cmplwi cr6, r27, 0
	ctx.cr[6].compare_u32(ctx.r[27].u32, 0 as u32, &mut ctx.xer);
	// 832C4AEC: 409A0010  bne cr6, 0x832c4afc
	if !ctx.cr[6].eq {
	pc = 0x832C4AFC; continue 'dispatch;
	}
	// 832C4AF0: 3B6100B4  addi r27, r1, 0xb4
	ctx.r[27].s64 = ctx.r[1].s64 + 180;
	// 832C4AF4: 3B200000  li r25, 0
	ctx.r[25].s64 = 0;
	// 832C4AF8: 3AA00000  li r21, 0
	ctx.r[21].s64 = 0;
	// 832C4AFC: 83DF0014  lwz r30, 0x14(r31)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) } as u64;
	// 832C4B00: 7E98A378  mr r24, r20
	ctx.r[24].u64 = ctx.r[20].u64;
	// 832C4B04: 6BCB0001  xori r11, r30, 1
	ctx.r[11].u64 = ctx.r[30].u64 ^ 1;
	// 832C4B08: 7FDDF378  mr r29, r30
	ctx.r[29].u64 = ctx.r[30].u64;
	// 832C4B0C: 556A083C  slwi r10, r11, 1
	// 832C4B10: 7D4B5214  add r10, r11, r10
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832C4B14: 554A2036  slwi r10, r10, 4
	// 832C4B18: 7D2AFA14  add r9, r10, r31
	ctx.r[9].u64 = ctx.r[10].u64 + ctx.r[31].u64;
	// 832C4B1C: 8109001C  lwz r8, 0x1c(r9)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(28 as u32) ) } as u64;
	// 832C4B20: 2B080000  cmplwi cr6, r8, 0
	ctx.cr[6].compare_u32(ctx.r[8].u32, 0 as u32, &mut ctx.xer);
	// 832C4B24: 419A001C  beq cr6, 0x832c4b40
	if ctx.cr[6].eq {
	pc = 0x832C4B40; continue 'dispatch;
	}
	// 832C4B28: 578A05EE  rlwinm r10, r28, 0, 0x17, 0x17
	ctx.r[10].u64 = ctx.r[28].u32 as u64 & 0xFFFFFFFFu64;
	// 832C4B2C: 7D7D5B78  mr r29, r11
	ctx.r[29].u64 = ctx.r[11].u64;
	// 832C4B30: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C4B34: 409A000C  bne cr6, 0x832c4b40
	if !ctx.cr[6].eq {
	pc = 0x832C4B40; continue 'dispatch;
	}
	// 832C4B38: 6BCB0001  xori r11, r30, 1
	ctx.r[11].u64 = ctx.r[30].u64 ^ 1;
	// 832C4B3C: 917F0014  stw r11, 0x14(r31)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(20 as u32), ctx.r[11].u32 ) };
	// 832C4B40: 3D600000  lis r11, 0
	ctx.r[11].s64 = 0;
	// 832C4B44: 616BB194  ori r11, r11, 0xb194
	ctx.r[11].u64 = ctx.r[11].u64 | 45460;
	// 832C4B48: 7D61582E  lwzx r11, r1, r11
	ctx.r[11].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[1].u32.wrapping_add(ctx.r[11].u32)) } as u64;
	// 832C4B4C: 556A02D6  rlwinm r10, r11, 0, 0xb, 0xb
	ctx.r[10].u64 = ctx.r[11].u32 as u64 & 0xFFFFFFFFu64;
	// 832C4B50: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C4B54: 419A0088  beq cr6, 0x832c4bdc
	if ctx.cr[6].eq {
	pc = 0x832C4BDC; continue 'dispatch;
	}
	// 832C4B58: 578B02D6  rlwinm r11, r28, 0, 0xb, 0xb
	ctx.r[11].u64 = ctx.r[28].u32 as u64 & 0xFFFFFFFFu64;
	// 832C4B5C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C4B60: 419A006C  beq cr6, 0x832c4bcc
	if ctx.cr[6].eq {
	pc = 0x832C4BCC; continue 'dispatch;
	}
	// 832C4B64: 57CA083C  slwi r10, r30, 1
	// 832C4B68: 93810074  stw r28, 0x74(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(116 as u32), ctx.r[28].u32 ) };
	// 832C4B6C: 57AB083C  slwi r11, r29, 1
	// 832C4B70: 92A10064  stw r21, 0x64(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(100 as u32), ctx.r[21].u32 ) };
	// 832C4B74: 7D5E5214  add r10, r30, r10
	ctx.r[10].u64 = ctx.r[30].u64 + ctx.r[10].u64;
	// 832C4B78: 9321005C  stw r25, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[25].u32 ) };
	// 832C4B7C: 7D3D5A14  add r9, r29, r11
	ctx.r[9].u64 = ctx.r[29].u64 + ctx.r[11].u64;
	// 832C4B80: 93610054  stw r27, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[27].u32 ) };
	// 832C4B84: 554B2036  slwi r11, r10, 4
	// 832C4B88: 80DF0008  lwz r6, 8(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C4B8C: 552A2036  slwi r10, r9, 4
	// 832C4B90: 80BF0004  lwz r5, 4(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C4B94: 7D6BFA14  add r11, r11, r31
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[31].u64;
	// 832C4B98: 7C6AFA14  add r3, r10, r31
	ctx.r[3].u64 = ctx.r[10].u64 + ctx.r[31].u64;
	// 832C4B9C: 39210080  addi r9, r1, 0x80
	ctx.r[9].s64 = ctx.r[1].s64 + 128;
	// 832C4BA0: 3B410090  addi r26, r1, 0x90
	ctx.r[26].s64 = ctx.r[1].s64 + 144;
	// 832C4BA4: 9121007C  stw r9, 0x7c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(124 as u32), ctx.r[9].u32 ) };
	// 832C4BA8: 7EEABB78  mr r10, r23
	ctx.r[10].u64 = ctx.r[23].u64;
	// 832C4BAC: 7EC9B378  mr r9, r22
	ctx.r[9].u64 = ctx.r[22].u64;
	// 832C4BB0: 80EB0044  lwz r7, 0x44(r11)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(68 as u32) ) } as u64;
	// 832C4BB4: 39140004  addi r8, r20, 4
	ctx.r[8].s64 = ctx.r[20].s64 + 4;
	// 832C4BB8: 808B0040  lwz r4, 0x40(r11)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C4BBC: 80630040  lwz r3, 0x40(r3)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C4BC0: 9341006C  stw r26, 0x6c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(108 as u32), ctx.r[26].u32 ) };
	// 832C4BC4: 4BFFDA55  bl 0x832c2618
	ctx.lr = 0x832C4BC8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C2618);
	// 832C4BC8: 83410080  lwz r26, 0x80(r1)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(128 as u32) ) } as u64;
	// 832C4BCC: 81740000  lwz r11, 0(r20)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[20].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C4BD0: 7F0BA214  add r24, r11, r20
	ctx.r[24].u64 = ctx.r[11].u64 + ctx.r[20].u64;
	// 832C4BD4: 7F18A040  cmplw cr6, r24, r20
	ctx.cr[6].compare_u32(ctx.r[24].u32, ctx.r[20].u32, &mut ctx.xer);
	// 832C4BD8: 419801B4  blt cr6, 0x832c4d8c
	if ctx.cr[6].lt {
	pc = 0x832C4D8C; continue 'dispatch;
	}
	// 832C4BDC: 7F18B040  cmplw cr6, r24, r22
	ctx.cr[6].compare_u32(ctx.r[24].u32, ctx.r[22].u32, &mut ctx.xer);
	// 832C4BE0: 409801AC  bge cr6, 0x832c4d8c
	if !ctx.cr[6].lt {
	pc = 0x832C4D8C; continue 'dispatch;
	}
	// 832C4BE4: 57930420  rlwinm r19, r28, 0, 0x10, 0x10
	ctx.r[19].u64 = ctx.r[28].u32 as u64 & 0xFFFFFFFFu64;
	// 832C4BE8: 2B130000  cmplwi cr6, r19, 0
	ctx.cr[6].compare_u32(ctx.r[19].u32, 0 as u32, &mut ctx.xer);
	// 832C4BEC: 409A0008  bne cr6, 0x832c4bf4
	if !ctx.cr[6].eq {
	pc = 0x832C4BF4; continue 'dispatch;
	}
	// 832C4BF0: 3B180004  addi r24, r24, 4
	ctx.r[24].s64 = ctx.r[24].s64 + 4;
	// 832C4BF4: 578B05AC  rlwinm r11, r28, 0, 0x16, 0x16
	ctx.r[11].u64 = ctx.r[28].u32 as u64 & 0xFFFFFFFFu64;
	// 832C4BF8: 7F08C378  mr r8, r24
	ctx.r[8].u64 = ctx.r[24].u64;
	// 832C4BFC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C4C00: 409A0078  bne cr6, 0x832c4c78
	if !ctx.cr[6].eq {
	pc = 0x832C4C78; continue 'dispatch;
	}
	// 832C4C04: 57CA083C  slwi r10, r30, 1
	// 832C4C08: 9321005C  stw r25, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[25].u32 ) };
	// 832C4C0C: 57AB083C  slwi r11, r29, 1
	// 832C4C10: 93610054  stw r27, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[27].u32 ) };
	// 832C4C14: 7D5E5214  add r10, r30, r10
	ctx.r[10].u64 = ctx.r[30].u64 + ctx.r[10].u64;
	// 832C4C18: 93810074  stw r28, 0x74(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(116 as u32), ctx.r[28].u32 ) };
	// 832C4C1C: 7D3D5A14  add r9, r29, r11
	ctx.r[9].u64 = ctx.r[29].u64 + ctx.r[11].u64;
	// 832C4C20: 92A10064  stw r21, 0x64(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(100 as u32), ctx.r[21].u32 ) };
	// 832C4C24: 554B2036  slwi r11, r10, 4
	// 832C4C28: 80DF0008  lwz r6, 8(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C4C2C: 552A2036  slwi r10, r9, 4
	// 832C4C30: 80BF0004  lwz r5, 4(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C4C34: 39210080  addi r9, r1, 0x80
	ctx.r[9].s64 = ctx.r[1].s64 + 128;
	// 832C4C38: 7D6BFA14  add r11, r11, r31
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[31].u64;
	// 832C4C3C: 9121007C  stw r9, 0x7c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(124 as u32), ctx.r[9].u32 ) };
	// 832C4C40: 7C6AFA14  add r3, r10, r31
	ctx.r[3].u64 = ctx.r[10].u64 + ctx.r[31].u64;
	// 832C4C44: 3A410090  addi r18, r1, 0x90
	ctx.r[18].s64 = ctx.r[1].s64 + 144;
	// 832C4C48: 7EEABB78  mr r10, r23
	ctx.r[10].u64 = ctx.r[23].u64;
	// 832C4C4C: 7EC9B378  mr r9, r22
	ctx.r[9].u64 = ctx.r[22].u64;
	// 832C4C50: 9241006C  stw r18, 0x6c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(108 as u32), ctx.r[18].u32 ) };
	// 832C4C54: 80EB0020  lwz r7, 0x20(r11)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C4C58: 808B001C  lwz r4, 0x1c(r11)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(28 as u32) ) } as u64;
	// 832C4C5C: 8063001C  lwz r3, 0x1c(r3)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(28 as u32) ) } as u64;
	// 832C4C60: 4BFFD9B9  bl 0x832c2618
	ctx.lr = 0x832C4C64;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C2618);
	// 832C4C64: 81610080  lwz r11, 0x80(r1)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(128 as u32) ) } as u64;
	// 832C4C68: 7C681B78  mr r8, r3
	ctx.r[8].u64 = ctx.r[3].u64;
	// 832C4C6C: 7F0BD040  cmplw cr6, r11, r26
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[26].u32, &mut ctx.xer);
	// 832C4C70: 40980008  bge cr6, 0x832c4c78
	if !ctx.cr[6].lt {
	pc = 0x832C4C78; continue 'dispatch;
	}
	// 832C4C74: 7D7A5B78  mr r26, r11
	ctx.r[26].u64 = ctx.r[11].u64;
	// 832C4C78: 2B130000  cmplwi cr6, r19, 0
	ctx.cr[6].compare_u32(ctx.r[19].u32, 0 as u32, &mut ctx.xer);
	// 832C4C7C: 409A0010  bne cr6, 0x832c4c8c
	if !ctx.cr[6].eq {
	pc = 0x832C4C8C; continue 'dispatch;
	}
	// 832C4C80: 8178FFFC  lwz r11, -4(r24)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[24].u32.wrapping_add(-4 as u32) ) } as u64;
	// 832C4C84: 7D6BC214  add r11, r11, r24
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[24].u64;
	// 832C4C88: 390BFFFC  addi r8, r11, -4
	ctx.r[8].s64 = ctx.r[11].s64 + -4;
	// 832C4C8C: 578B039C  rlwinm r11, r28, 0, 0xe, 0xe
	ctx.r[11].u64 = ctx.r[28].u32 as u64 & 0xFFFFFFFFu64;
	// 832C4C90: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C4C94: 409A00F8  bne cr6, 0x832c4d8c
	if !ctx.cr[6].eq {
	pc = 0x832C4D8C; continue 'dispatch;
	}
	// 832C4C98: 7F08A040  cmplw cr6, r8, r20
	ctx.cr[6].compare_u32(ctx.r[8].u32, ctx.r[20].u32, &mut ctx.xer);
	// 832C4C9C: 419800F0  blt cr6, 0x832c4d8c
	if ctx.cr[6].lt {
	pc = 0x832C4D8C; continue 'dispatch;
	}
	// 832C4CA0: 7F08B040  cmplw cr6, r8, r22
	ctx.cr[6].compare_u32(ctx.r[8].u32, ctx.r[22].u32, &mut ctx.xer);
	// 832C4CA4: 409800E8  bge cr6, 0x832c4d8c
	if !ctx.cr[6].lt {
	pc = 0x832C4D8C; continue 'dispatch;
	}
	// 832C4CA8: 9321005C  stw r25, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[25].u32 ) };
	// 832C4CAC: 56B8083C  slwi r24, r21, 1
	// 832C4CB0: 93610054  stw r27, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[27].u32 ) };
	// 832C4CB4: 39610090  addi r11, r1, 0x90
	ctx.r[11].s64 = ctx.r[1].s64 + 144;
	// 832C4CB8: 93010064  stw r24, 0x64(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(100 as u32), ctx.r[24].u32 ) };
	// 832C4CBC: 57CA083C  slwi r10, r30, 1
	// 832C4CC0: 9161006C  stw r11, 0x6c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(108 as u32), ctx.r[11].u32 ) };
	// 832C4CC4: 57AB083C  slwi r11, r29, 1
	// 832C4CC8: 7D5E5214  add r10, r30, r10
	ctx.r[10].u64 = ctx.r[30].u64 + ctx.r[10].u64;
	// 832C4CCC: 93810074  stw r28, 0x74(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(116 as u32), ctx.r[28].u32 ) };
	// 832C4CD0: 7D3D5A14  add r9, r29, r11
	ctx.r[9].u64 = ctx.r[29].u64 + ctx.r[11].u64;
	// 832C4CD4: 80DF0010  lwz r6, 0x10(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(16 as u32) ) } as u64;
	// 832C4CD8: 554A2036  slwi r10, r10, 4
	// 832C4CDC: 80BF000C  lwz r5, 0xc(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832C4CE0: 552B2036  slwi r11, r9, 4
	// 832C4CE4: 7FCAFA14  add r30, r10, r31
	ctx.r[30].u64 = ctx.r[10].u64 + ctx.r[31].u64;
	// 832C4CE8: 7FABFA14  add r29, r11, r31
	ctx.r[29].u64 = ctx.r[11].u64 + ctx.r[31].u64;
	// 832C4CEC: 38E10080  addi r7, r1, 0x80
	ctx.r[7].s64 = ctx.r[1].s64 + 128;
	// 832C4CF0: 7EEABB78  mr r10, r23
	ctx.r[10].u64 = ctx.r[23].u64;
	// 832C4CF4: 7EC9B378  mr r9, r22
	ctx.r[9].u64 = ctx.r[22].u64;
	// 832C4CF8: 90E1007C  stw r7, 0x7c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(124 as u32), ctx.r[7].u32 ) };
	// 832C4CFC: 80FE002C  lwz r7, 0x2c(r30)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(44 as u32) ) } as u64;
	// 832C4D00: 809E0028  lwz r4, 0x28(r30)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C4D04: 807D0028  lwz r3, 0x28(r29)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(40 as u32) ) } as u64;
	// 832C4D08: 4BFFD911  bl 0x832c2618
	ctx.lr = 0x832C4D0C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C2618);
	// 832C4D0C: 80C10080  lwz r6, 0x80(r1)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(128 as u32) ) } as u64;
	// 832C4D10: 7C681B78  mr r8, r3
	ctx.r[8].u64 = ctx.r[3].u64;
	// 832C4D14: 54CB103A  slwi r11, r6, 2
	// 832C4D18: 91610080  stw r11, 0x80(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(128 as u32), ctx.r[11].u32 ) };
	// 832C4D1C: 7F0BD040  cmplw cr6, r11, r26
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[26].u32, &mut ctx.xer);
	// 832C4D20: 40980008  bge cr6, 0x832c4d28
	if !ctx.cr[6].lt {
	pc = 0x832C4D28; continue 'dispatch;
	}
	// 832C4D24: 7D7A5B78  mr r26, r11
	ctx.r[26].u64 = ctx.r[11].u64;
	// 832C4D28: 7F08A040  cmplw cr6, r8, r20
	ctx.cr[6].compare_u32(ctx.r[8].u32, ctx.r[20].u32, &mut ctx.xer);
	// 832C4D2C: 41980060  blt cr6, 0x832c4d8c
	if ctx.cr[6].lt {
	pc = 0x832C4D8C; continue 'dispatch;
	}
	// 832C4D30: 7F08B040  cmplw cr6, r8, r22
	ctx.cr[6].compare_u32(ctx.r[8].u32, ctx.r[22].u32, &mut ctx.xer);
	// 832C4D34: 40980058  bge cr6, 0x832c4d8c
	if !ctx.cr[6].lt {
	pc = 0x832C4D8C; continue 'dispatch;
	}
	// 832C4D38: 39610080  addi r11, r1, 0x80
	ctx.r[11].s64 = ctx.r[1].s64 + 128;
	// 832C4D3C: 93810074  stw r28, 0x74(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(116 as u32), ctx.r[28].u32 ) };
	// 832C4D40: 3AA10090  addi r21, r1, 0x90
	ctx.r[21].s64 = ctx.r[1].s64 + 144;
	// 832C4D44: 93010064  stw r24, 0x64(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(100 as u32), ctx.r[24].u32 ) };
	// 832C4D48: 9161007C  stw r11, 0x7c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(124 as u32), ctx.r[11].u32 ) };
	// 832C4D4C: 7EEABB78  mr r10, r23
	ctx.r[10].u64 = ctx.r[23].u64;
	// 832C4D50: 92A1006C  stw r21, 0x6c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(108 as u32), ctx.r[21].u32 ) };
	// 832C4D54: 7EC9B378  mr r9, r22
	ctx.r[9].u64 = ctx.r[22].u64;
	// 832C4D58: 9321005C  stw r25, 0x5c(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(92 as u32), ctx.r[25].u32 ) };
	// 832C4D5C: 80FE0038  lwz r7, 0x38(r30)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(56 as u32) ) } as u64;
	// 832C4D60: 80DF0010  lwz r6, 0x10(r31)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(16 as u32) ) } as u64;
	// 832C4D64: 80BF000C  lwz r5, 0xc(r31)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) } as u64;
	// 832C4D68: 809E0034  lwz r4, 0x34(r30)
	ctx.r[4].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(52 as u32) ) } as u64;
	// 832C4D6C: 807D0034  lwz r3, 0x34(r29)
	ctx.r[3].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(52 as u32) ) } as u64;
	// 832C4D70: 93610054  stw r27, 0x54(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(84 as u32), ctx.r[27].u32 ) };
	// 832C4D74: 4BFFD8A5  bl 0x832c2618
	ctx.lr = 0x832C4D78;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C2618);
	// 832C4D78: 81410080  lwz r10, 0x80(r1)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(128 as u32) ) } as u64;
	// 832C4D7C: 554B103A  slwi r11, r10, 2
	// 832C4D80: 7F0BD040  cmplw cr6, r11, r26
	ctx.cr[6].compare_u32(ctx.r[11].u32, ctx.r[26].u32, &mut ctx.xer);
	// 832C4D84: 40980008  bge cr6, 0x832c4d8c
	if !ctx.cr[6].lt {
	pc = 0x832C4D8C; continue 'dispatch;
	}
	// 832C4D88: 7D7A5B78  mr r26, r11
	ctx.r[26].u64 = ctx.r[11].u64;
	// 832C4D8C: 817F0008  lwz r11, 8(r31)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C4D90: 815F0004  lwz r10, 4(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C4D94: 7D2B51D6  mullw r9, r11, r10
	ctx.r[9].s64 = (ctx.r[11].s32 as i64) * (ctx.r[10].s32 as i64);
	// 832C4D98: 5528D1BE  srwi r8, r9, 6
	// 832C4D9C: 7CFA4050  subf r7, r26, r8
	ctx.r[7].s64 = ctx.r[8].s64 - ctx.r[26].s64;
	// 832C4DA0: 1CC70064  mulli r6, r7, 0x64
	ctx.r[6].s64 = ctx.r[7].s64 * 100;
	// 832C4DA4: 7C664396  divwu r3, r6, r8
	ctx.r[3].u32 = ctx.r[6].u32 / ctx.r[8].u32;
	// 832C4DA8: 80210000  lwz r1, 0(r1)
	ctx.r[1].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C4DAC: 4B9E4684  b 0x82ca9430
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9430);
	return;
}

pub fn sub_832C4DB0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    let mut ea: u32 = 0;
    // ---- function 0x832C4DB0 size=512
	// 832C4DB0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C4DB4: 4B9E4649  bl 0x82ca93fc
	ctx.lr = 0x832C4DB8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93FC);
	// 832C4DB8: 9421FF70  stwu r1, -0x90(r1)
	ea = ctx.r[1].u32.wrapping_add(-144 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C4DBC: 7CBA2B78  mr r26, r5
	ctx.r[26].u64 = ctx.r[5].u64;
	// 832C4DC0: 7C7F1B78  mr r31, r3
	ctx.r[31].u64 = ctx.r[3].u64;
	// 832C4DC4: 7C9B2378  mr r27, r4
	ctx.r[27].u64 = ctx.r[4].u64;
	// 832C4DC8: 7CD93378  mr r25, r6
	ctx.r[25].u64 = ctx.r[6].u64;
	// 832C4DCC: 83BA0000  lwz r29, 0(r26)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[26].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C4DD0: 57AB103A  slwi r11, r29, 2
	// 832C4DD4: 7F1F5800  cmpw cr6, r31, r11
	ctx.cr[6].compare_i32(ctx.r[31].s32, ctx.r[11].s32, &mut ctx.xer);
	// 832C4DD8: 40990028  ble cr6, 0x832c4e00
	if !ctx.cr[6].gt {
	pc = 0x832C4E00; continue 'dispatch;
	}
	// 832C4DDC: 7FFD1670  srawi r29, r31, 2
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 2) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[31].s32 >> 2) as i64;
	// 832C4DE0: 39600001  li r11, 1
	ctx.r[11].s64 = 1;
	// 832C4DE4: 93BA0000  stw r29, 0(r26)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[26].u32.wrapping_add(0 as u32), ctx.r[29].u32 ) };
	// 832C4DE8: 2F1D0002  cmpwi cr6, r29, 2
	ctx.cr[6].compare_i32(ctx.r[29].s32, 2, &mut ctx.xer);
	// 832C4DEC: 917A0004  stw r11, 4(r26)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[26].u32.wrapping_add(4 as u32), ctx.r[11].u32 ) };
	// 832C4DF0: 40990010  ble cr6, 0x832c4e00
	if !ctx.cr[6].gt {
	pc = 0x832C4E00; continue 'dispatch;
	}
	// 832C4DF4: 7F24CB78  mr r4, r25
	ctx.r[4].u64 = ctx.r[25].u64;
	// 832C4DF8: 7FA3EB78  mr r3, r29
	ctx.r[3].u64 = ctx.r[29].u64;
	// 832C4DFC: 480001B5  bl 0x832c4fb0
	ctx.lr = 0x832C4E00;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C4FB0);
	// 832C4E00: 839A0004  lwz r28, 4(r26)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[26].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C4E04: 7F1FE000  cmpw cr6, r31, r28
	ctx.cr[6].compare_i32(ctx.r[31].s32, ctx.r[28].s32, &mut ctx.xer);
	// 832C4E08: 40990024  ble cr6, 0x832c4e2c
	if !ctx.cr[6].gt {
	pc = 0x832C4E2C; continue 'dispatch;
	}
	// 832C4E0C: 7FFCFB78  mr r28, r31
	ctx.r[28].u64 = ctx.r[31].u64;
	// 832C4E10: 93FA0004  stw r31, 4(r26)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[26].u32.wrapping_add(4 as u32), ctx.r[31].u32 ) };
	// 832C4E14: 2F1F0001  cmpwi cr6, r31, 1
	ctx.cr[6].compare_i32(ctx.r[31].s32, 1, &mut ctx.xer);
	// 832C4E18: 40990014  ble cr6, 0x832c4e2c
	if !ctx.cr[6].gt {
	pc = 0x832C4E2C; continue 'dispatch;
	}
	// 832C4E1C: 57AB103A  slwi r11, r29, 2
	// 832C4E20: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C4E24: 7C8BCA14  add r4, r11, r25
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[25].u64;
	// 832C4E28: 48000379  bl 0x832c51a0
	ctx.lr = 0x832C4E2C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C51A0);
	// 832C4E2C: 57AB103A  slwi r11, r29, 2
	// 832C4E30: 7F85E378  mr r5, r28
	ctx.r[5].u64 = ctx.r[28].u64;
	// 832C4E34: 7FCBCA14  add r30, r11, r25
	ctx.r[30].u64 = ctx.r[11].u64 + ctx.r[25].u64;
	// 832C4E38: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832C4E3C: 7FC6F378  mr r6, r30
	ctx.r[6].u64 = ctx.r[30].u64;
	// 832C4E40: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C4E44: 48002F55  bl 0x832c7d98
	ctx.lr = 0x832C4E48;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7D98);
	// 832C4E48: 2F1F0004  cmpwi cr6, r31, 4
	ctx.cr[6].compare_i32(ctx.r[31].s32, 4, &mut ctx.xer);
	// 832C4E4C: 409900E0  ble cr6, 0x832c4f2c
	if !ctx.cr[6].gt {
	pc = 0x832C4F2C; continue 'dispatch;
	}
	// 832C4E50: 7F27CB78  mr r7, r25
	ctx.r[7].u64 = ctx.r[25].u64;
	// 832C4E54: 7FA6EB78  mr r6, r29
	ctx.r[6].u64 = ctx.r[29].u64;
	// 832C4E58: 38BA0008  addi r5, r26, 8
	ctx.r[5].s64 = ctx.r[26].s64 + 8;
	// 832C4E5C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C4E60: 48000431  bl 0x832c5290
	ctx.lr = 0x832C4E64;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C5290);
	// 832C4E64: 7FEB0E70  srawi r11, r31, 1
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[11].s64 = (ctx.r[31].s32 >> 1) as i64;
	// 832C4E68: 578A083C  slwi r10, r28, 1
	// 832C4E6C: 2F0B0002  cmpwi cr6, r11, 2
	ctx.cr[6].compare_i32(ctx.r[11].s32, 2, &mut ctx.xer);
	// 832C4E70: 7D4A5BD6  divw r10, r10, r11
	ctx.r[10].s32 = ctx.r[10].s32 / ctx.r[11].s32;
	// 832C4E74: 409900D4  ble cr6, 0x832c4f48
	if !ctx.cr[6].gt {
	pc = 0x832C4F48; continue 'dispatch;
	}
	// 832C4E78: 38FFFFFE  addi r7, r31, -2
	ctx.r[7].s64 = ctx.r[31].s64 + -2;
	// 832C4E7C: 390BFFFD  addi r8, r11, -3
	ctx.r[8].s64 = ctx.r[11].s64 + -3;
	// 832C4E80: 3C80820A  lis r4, -0x7df6
	ctx.r[4].s64 = -2113273856;
	// 832C4E84: 54EB103A  slwi r11, r7, 2
	// 832C4E88: 7CAA00D0  neg r5, r10
	ctx.r[5].s64 = -ctx.r[10].s64;
	// 832C4E8C: 5507F87E  srwi r7, r8, 1
	// 832C4E90: 5789103A  slwi r9, r28, 2
	// 832C4E94: 5546103A  slwi r6, r10, 2
	// 832C4E98: C00492D4  lfs f0, -0x6d2c(r4)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(-27948 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C4E9C: 395B0008  addi r10, r27, 8
	ctx.r[10].s64 = ctx.r[27].s64 + 8;
	// 832C4EA0: 7FC8F378  mr r8, r30
	ctx.r[8].u64 = ctx.r[30].u64;
	// 832C4EA4: 54A5103A  slwi r5, r5, 2
	// 832C4EA8: 7D6BDA14  add r11, r11, r27
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[27].u64;
	// 832C4EAC: 7D29F214  add r9, r9, r30
	ctx.r[9].u64 = ctx.r[9].u64 + ctx.r[30].u64;
	// 832C4EB0: 38E70001  addi r7, r7, 1
	ctx.r[7].s64 = ctx.r[7].s64 + 1;
	// 832C4EB4: 7D254A14  add r9, r5, r9
	ctx.r[9].u64 = ctx.r[5].u64 + ctx.r[9].u64;
	// 832C4EB8: C1AA0004  lfs f13, 4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C4EBC: C18B0004  lfs f12, 4(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C4EC0: 7D064214  add r8, r6, r8
	ctx.r[8].u64 = ctx.r[6].u64 + ctx.r[8].u64;
	// 832C4EC4: ED6D602A  fadds f11, f13, f12
	ctx.f[11].f64 = ((ctx.f[13].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C4EC8: C14A0000  lfs f10, 0(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C4ECC: C12B0000  lfs f9, 0(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C4ED0: 34E7FFFF  addic. r7, r7, -1
	ctx.xer.ca = (ctx.r[7].u32 > (!(-1 as u32)));
	ctx.r[7].s64 = ctx.r[7].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[7].s32, 0, &mut ctx.xer);
	// 832C4ED4: ED0A4828  fsubs f8, f10, f9
	ctx.f[8].f64 = (((ctx.f[10].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C4ED8: C0E90000  lfs f7, 0(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C4EDC: ECC03828  fsubs f6, f0, f7
	ctx.f[6].f64 = (((ctx.f[0].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C4EE0: C0A80000  lfs f5, 0(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832C4EE4: EC8B0172  fmuls f4, f11, f5
	ctx.f[4].f64 = (((ctx.f[11].f64 * ctx.f[5].f64) as f32) as f64);
	// 832C4EE8: EC6B01B2  fmuls f3, f11, f6
	ctx.f[3].f64 = (((ctx.f[11].f64 * ctx.f[6].f64) as f32) as f64);
	// 832C4EEC: EC4821B8  fmsubs f2, f8, f6, f4
	ctx.f[2].f64 = (((ctx.f[8].f64 * ctx.f[6].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C4EF0: EC28197A  fmadds f1, f8, f5, f3
	ctx.f[1].f64 = (((ctx.f[8].f64 * ctx.f[5].f64 + ctx.f[3].f64) as f32) as f64);
	// 832C4EF4: ED8A1028  fsubs f12, f10, f2
	ctx.f[12].f64 = (((ctx.f[10].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C4EF8: D18A0000  stfs f12, 0(r10)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C4EFC: ED6D0828  fsubs f11, f13, f1
	ctx.f[11].f64 = (((ctx.f[13].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C4F00: D16A0004  stfs f11, 4(r10)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C4F04: C12B0000  lfs f9, 0(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C4F08: 394A0008  addi r10, r10, 8
	ctx.r[10].s64 = ctx.r[10].s64 + 8;
	// 832C4F0C: C14B0004  lfs f10, 4(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C4F10: ECEA0828  fsubs f7, f10, f1
	ctx.f[7].f64 = (((ctx.f[10].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C4F14: ED09102A  fadds f8, f9, f2
	ctx.f[8].f64 = ((ctx.f[9].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C4F18: D10B0000  stfs f8, 0(r11)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C4F1C: D0EB0004  stfs f7, 4(r11)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C4F20: 396BFFF8  addi r11, r11, -8
	ctx.r[11].s64 = ctx.r[11].s64 + -8;
	// 832C4F24: 4082FF90  bne 0x832c4eb4
	if !ctx.cr[0].eq {
	pc = 0x832C4EB4; continue 'dispatch;
	}
	// 832C4F28: 48000020  b 0x832c4f48
	pc = 0x832C4F48; continue 'dispatch;
	// 832C4F2C: 409A001C  bne cr6, 0x832c4f48
	if !ctx.cr[6].eq {
	pc = 0x832C4F48; continue 'dispatch;
	}
	// 832C4F30: 7F27CB78  mr r7, r25
	ctx.r[7].u64 = ctx.r[25].u64;
	// 832C4F34: 7FA6EB78  mr r6, r29
	ctx.r[6].u64 = ctx.r[29].u64;
	// 832C4F38: 38BA0008  addi r5, r26, 8
	ctx.r[5].s64 = ctx.r[26].s64 + 8;
	// 832C4F3C: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832C4F40: 38600004  li r3, 4
	ctx.r[3].s64 = 4;
	// 832C4F44: 4800034D  bl 0x832c5290
	ctx.lr = 0x832C4F48;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C5290);
	// 832C4F48: C01B0000  lfs f0, 0(r27)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(0 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C4F4C: 2F1F0002  cmpwi cr6, r31, 2
	ctx.cr[6].compare_i32(ctx.r[31].s32, 2, &mut ctx.xer);
	// 832C4F50: C1BB0004  lfs f13, 4(r27)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(4 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C4F54: ED8D002A  fadds f12, f13, f0
	ctx.f[12].f64 = ((ctx.f[13].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C4F58: D19B0000  stfs f12, 0(r27)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[27].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C4F5C: EC006828  fsubs f0, f0, f13
	ctx.f[0].f64 = (((ctx.f[0].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C4F60: 40990038  ble cr6, 0x832c4f98
	if !ctx.cr[6].gt {
	pc = 0x832C4F98; continue 'dispatch;
	}
	// 832C4F64: 395FFFFD  addi r10, r31, -3
	ctx.r[10].s64 = ctx.r[31].s64 + -3;
	// 832C4F68: 397B0008  addi r11, r27, 8
	ctx.r[11].s64 = ctx.r[27].s64 + 8;
	// 832C4F6C: 554AF87E  srwi r10, r10, 1
	// 832C4F70: 394A0001  addi r10, r10, 1
	ctx.r[10].s64 = ctx.r[10].s64 + 1;
	// 832C4F74: C1AB0000  lfs f13, 0(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C4F78: 354AFFFF  addic. r10, r10, -1
	ctx.xer.ca = (ctx.r[10].u32 > (!(-1 as u32)));
	ctx.r[10].s64 = ctx.r[10].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832C4F7C: C18B0004  lfs f12, 4(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C4F80: ED6D6028  fsubs f11, f13, f12
	ctx.f[11].f64 = (((ctx.f[13].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C4F84: D16BFFFC  stfs f11, -4(r11)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C4F88: ED4C682A  fadds f10, f12, f13
	ctx.f[10].f64 = ((ctx.f[12].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C4F8C: D14B0000  stfs f10, 0(r11)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C4F90: 396B0008  addi r11, r11, 8
	ctx.r[11].s64 = ctx.r[11].s64 + 8;
	// 832C4F94: 4082FFE0  bne 0x832c4f74
	if !ctx.cr[0].eq {
	pc = 0x832C4F74; continue 'dispatch;
	}
	// 832C4F98: 57EB103A  slwi r11, r31, 2
	// 832C4F9C: 7D6BDA14  add r11, r11, r27
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[27].u64;
	// 832C4FA0: D00BFFFC  stfs f0, -4(r11)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C4FA4: 38210090  addi r1, r1, 0x90
	ctx.r[1].s64 = ctx.r[1].s64 + 144;
	// 832C4FA8: 4B9E44A4  b 0x82ca944c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA944C);
	return;
}

pub fn sub_832C4FB0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    let mut ea: u32 = 0;
    // ---- function 0x832C4FB0 size=496
	// 832C4FB0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C4FB4: 4B9E4451  bl 0x82ca9404
	ctx.lr = 0x832C4FB8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9404);
	// 832C4FB8: 3981FFD0  addi r12, r1, -0x30
	ctx.r[12].s64 = ctx.r[1].s64 + -48;
	// 832C4FBC: 4B9E8D15  bl 0x82cadcd0
	ctx.lr = 0x832C4FC0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCD0);
	// 832C4FC0: 9421FF40  stwu r1, -0xc0(r1)
	ea = ctx.r[1].u32.wrapping_add(-192 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C4FC4: 7C7F0E70  srawi r31, r3, 1
	ctx.xer.ca = (ctx.r[3].s32 < 0) && ((ctx.r[3].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[3].s32 >> 1) as i64;
	// 832C4FC8: 3D408210  lis r10, -0x7df0
	ctx.r[10].s64 = -2112880640;
	// 832C4FCC: 7FEB07B4  extsw r11, r31
	ctx.r[11].s64 = ctx.r[31].s32 as i64;
	// 832C4FD0: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832C4FD4: F9610050  std r11, 0x50(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[11].u64 ) };
	// 832C4FD8: C8010050  lfd f0, 0x50(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[0].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832C4FDC: FDA0069C  fcfid f13, f0
	ctx.f[13].f64 = (ctx.f[0].s64 as f64);
	// 832C4FE0: FD806818  frsp f12, f13
	ctx.f[12].f64 = (ctx.f[13].f64 as f32) as f64;
	// 832C4FE4: C00A0E60  lfs f0, 0xe60(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(3680 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C4FE8: EFE06024  fdivs f31, f0, f12
	ctx.f[31].f64 = ((ctx.f[0].f64 / ctx.f[12].f64) as f32) as f64;
	// 832C4FEC: EC2C07F2  fmuls f1, f12, f31
	ctx.f[1].f64 = (((ctx.f[12].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C4FF0: 4AF74EC1  bl 0x82239eb0
	ctx.lr = 0x832C4FF4;
	crate::recompiler::externs::call(&mut ctx, base, 0x82239EB0);
	// 832C4FF4: 3D60820A  lis r11, -0x7df6
	ctx.r[11].s64 = -2113273856;
	// 832C4FF8: FF800818  frsp f28, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[28].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832C4FFC: D39E0004  stfs f28, 4(r30)
	tmp.f32 = (ctx.f[28].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5000: 2F1F0004  cmpwi cr6, r31, 4
	ctx.cr[6].compare_i32(ctx.r[31].s32, 4, &mut ctx.xer);
	// 832C5004: 3B6BB7A4  addi r27, r11, -0x485c
	ctx.r[27].s64 = ctx.r[11].s64 + -18524;
	// 832C5008: C37BDCEC  lfs f27, -0x2314(r27)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(-8980 as u32) ) };
	ctx.f[27].f64 = (tmp.f32 as f64);
	// 832C500C: C3DBDB30  lfs f30, -0x24d0(r27)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(-9424 as u32) ) };
	ctx.f[30].f64 = (tmp.f32 as f64);
	// 832C5010: D37E0000  stfs f27, 0(r30)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5014: 41980034  blt cr6, 0x832c5048
	if ctx.cr[6].lt {
	pc = 0x832C5048; continue 'dispatch;
	}
	// 832C5018: C01BFCDC  lfs f0, -0x324(r27)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(-804 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C501C: EC3F0032  fmuls f1, f31, f0
	ctx.f[1].f64 = (((ctx.f[31].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C5020: 4AF74E91  bl 0x82239eb0
	ctx.lr = 0x832C5024;
	crate::recompiler::externs::call(&mut ctx, base, 0x82239EB0);
	// 832C5024: FDA00818  frsp f13, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[13].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832C5028: C01BDB24  lfs f0, -0x24dc(r27)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(-9436 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C502C: EC3F0032  fmuls f1, f31, f0
	ctx.f[1].f64 = (((ctx.f[31].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C5030: ED9E6824  fdivs f12, f30, f13
	ctx.f[12].f64 = ((ctx.f[30].f64 / ctx.f[13].f64) as f32) as f64;
	// 832C5034: D19E0008  stfs f12, 8(r30)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C5038: 4AF74E79  bl 0x82239eb0
	ctx.lr = 0x832C503C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82239EB0);
	// 832C503C: FD600818  frsp f11, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[11].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832C5040: ED5E5824  fdivs f10, f30, f11
	ctx.f[10].f64 = ((ctx.f[30].f64 / ctx.f[11].f64) as f32) as f64;
	// 832C5044: D15E000C  stfs f10, 0xc(r30)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C5048: 3B800004  li r28, 4
	ctx.r[28].s64 = 4;
	// 832C504C: 2F1F0004  cmpwi cr6, r31, 4
	ctx.cr[6].compare_i32(ctx.r[31].s32, 4, &mut ctx.xer);
	// 832C5050: 40990078  ble cr6, 0x832c50c8
	if !ctx.cr[6].gt {
	pc = 0x832C50C8; continue 'dispatch;
	}
	// 832C5054: C3BB0000  lfs f29, 0(r27)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[27].u32.wrapping_add(0 as u32) ) };
	ctx.f[29].f64 = (tmp.f32 as f64);
	// 832C5058: 3BBE0018  addi r29, r30, 0x18
	ctx.r[29].s64 = ctx.r[30].s64 + 24;
	// 832C505C: 7F8B07B4  extsw r11, r28
	ctx.r[11].s64 = ctx.r[28].s32 as i64;
	// 832C5060: F9610050  std r11, 0x50(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[11].u64 ) };
	// 832C5064: C8010050  lfd f0, 0x50(r1)
	ctx.f[0].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832C5068: FDA0069C  fcfid f13, f0
	ctx.f[13].f64 = (ctx.f[0].s64 as f64);
	// 832C506C: FD806818  frsp f12, f13
	ctx.f[12].f64 = (ctx.f[13].f64 as f32) as f64;
	// 832C5070: EF4C07F2  fmuls f26, f12, f31
	ctx.f[26].f64 = (((ctx.f[12].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C5074: FC20D090  fmr f1, f26
	ctx.f[1].f64 = ctx.f[26].f64;
	// 832C5078: 4AF74E39  bl 0x82239eb0
	ctx.lr = 0x832C507C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82239EB0);
	// 832C507C: FD600818  frsp f11, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[11].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832C5080: D17DFFF8  stfs f11, -8(r29)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(-8 as u32), tmp.u32 ) };
	// 832C5084: FC20D090  fmr f1, f26
	ctx.f[1].f64 = ctx.f[26].f64;
	// 832C5088: 4AF74F09  bl 0x82239f90
	ctx.lr = 0x832C508C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82239F90);
	// 832C508C: FD400818  frsp f10, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[10].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832C5090: D15DFFFC  stfs f10, -4(r29)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C5094: EF5A0772  fmuls f26, f26, f29
	ctx.f[26].f64 = (((ctx.f[26].f64 * ctx.f[29].f64) as f32) as f64);
	// 832C5098: FC20D090  fmr f1, f26
	ctx.f[1].f64 = ctx.f[26].f64;
	// 832C509C: 4AF74E15  bl 0x82239eb0
	ctx.lr = 0x832C50A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82239EB0);
	// 832C50A0: FD200818  frsp f9, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[9].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832C50A4: D13D0000  stfs f9, 0(r29)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C50A8: FC20D090  fmr f1, f26
	ctx.f[1].f64 = ctx.f[26].f64;
	// 832C50AC: 4AF74EE5  bl 0x82239f90
	ctx.lr = 0x832C50B0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82239F90);
	// 832C50B0: 3B9C0004  addi r28, r28, 4
	ctx.r[28].s64 = ctx.r[28].s64 + 4;
	// 832C50B4: FD000818  frsp f8, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[8].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832C50B8: D11D0004  stfs f8, 4(r29)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C50BC: 3BBD0010  addi r29, r29, 0x10
	ctx.r[29].s64 = ctx.r[29].s64 + 16;
	// 832C50C0: 7F1CF800  cmpw cr6, r28, r31
	ctx.cr[6].compare_i32(ctx.r[28].s32, ctx.r[31].s32, &mut ctx.xer);
	// 832C50C4: 4198FF98  blt cr6, 0x832c505c
	if ctx.cr[6].lt {
	pc = 0x832C505C; continue 'dispatch;
	}
	// 832C50C8: 39600000  li r11, 0
	ctx.r[11].s64 = 0;
	// 832C50CC: 2F1F0002  cmpwi cr6, r31, 2
	ctx.cr[6].compare_i32(ctx.r[31].s32, 2, &mut ctx.xer);
	// 832C50D0: 409900C0  ble cr6, 0x832c5190
	if !ctx.cr[6].gt {
	pc = 0x832C5190; continue 'dispatch;
	}
	// 832C50D4: 7D0BFA14  add r8, r11, r31
	ctx.r[8].u64 = ctx.r[11].u64 + ctx.r[31].u64;
	// 832C50D8: 7FFF0E70  srawi r31, r31, 1
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[31].s32 >> 1) as i64;
	// 832C50DC: 550A103A  slwi r10, r8, 2
	// 832C50E0: 2F1F0004  cmpwi cr6, r31, 4
	ctx.cr[6].compare_i32(ctx.r[31].s32, 4, &mut ctx.xer);
	// 832C50E4: 7D4AF214  add r10, r10, r30
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[30].u64;
	// 832C50E8: D36A0000  stfs f27, 0(r10)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C50EC: D38A0004  stfs f28, 4(r10)
	tmp.f32 = (ctx.f[28].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C50F0: 41980094  blt cr6, 0x832c5184
	if ctx.cr[6].lt {
	pc = 0x832C5184; continue 'dispatch;
	}
	// 832C50F4: 394B0006  addi r10, r11, 6
	ctx.r[10].s64 = ctx.r[11].s64 + 6;
	// 832C50F8: 392B0004  addi r9, r11, 4
	ctx.r[9].s64 = ctx.r[11].s64 + 4;
	// 832C50FC: 5547103A  slwi r7, r10, 2
	// 832C5100: 5526103A  slwi r6, r9, 2
	// 832C5104: 38A80002  addi r5, r8, 2
	ctx.r[5].s64 = ctx.r[8].s64 + 2;
	// 832C5108: 38880003  addi r4, r8, 3
	ctx.r[4].s64 = ctx.r[8].s64 + 3;
	// 832C510C: 54A3103A  slwi r3, r5, 2
	// 832C5110: 548A103A  slwi r10, r4, 2
	// 832C5114: 7C07F42E  lfsx f0, r7, r30
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[30].u32)) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C5118: 7DA6F42E  lfsx f13, r6, r30
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[6].u32.wrapping_add(ctx.r[30].u32)) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C511C: ED9E0024  fdivs f12, f30, f0
	ctx.f[12].f64 = ((ctx.f[30].f64 / ctx.f[0].f64) as f32) as f64;
	// 832C5120: ED7E6824  fdivs f11, f30, f13
	ctx.f[11].f64 = ((ctx.f[30].f64 / ctx.f[13].f64) as f32) as f64;
	// 832C5124: 7D63F52E  stfsx f11, r3, r30
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[3].u32.wrapping_add(ctx.r[30].u32), tmp.u32) };
	// 832C5128: 7D8AF52E  stfsx f12, r10, r30
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[10].u32.wrapping_add(ctx.r[30].u32), tmp.u32) };
	// 832C512C: 40990058  ble cr6, 0x832c5184
	if !ctx.cr[6].gt {
	pc = 0x832C5184; continue 'dispatch;
	}
	// 832C5130: 39480004  addi r10, r8, 4
	ctx.r[10].s64 = ctx.r[8].s64 + 4;
	// 832C5134: 392B000A  addi r9, r11, 0xa
	ctx.r[9].s64 = ctx.r[11].s64 + 10;
	// 832C5138: 38FFFFFB  addi r7, r31, -5
	ctx.r[7].s64 = ctx.r[31].s64 + -5;
	// 832C513C: 554B103A  slwi r11, r10, 2
	// 832C5140: 5529103A  slwi r9, r9, 2
	// 832C5144: 54E7F0BE  srwi r7, r7, 2
	// 832C5148: 7D4BF214  add r10, r11, r30
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C514C: 7D69F214  add r11, r9, r30
	ctx.r[11].u64 = ctx.r[9].u64 + ctx.r[30].u64;
	// 832C5150: 39270001  addi r9, r7, 1
	ctx.r[9].s64 = ctx.r[7].s64 + 1;
	// 832C5154: C00BFFFC  lfs f0, -4(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-4 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C5158: 3529FFFF  addic. r9, r9, -1
	ctx.xer.ca = (ctx.r[9].u32 > (!(-1 as u32)));
	ctx.r[9].s64 = ctx.r[9].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832C515C: C1AB0000  lfs f13, 0(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C5160: C18B0004  lfs f12, 4(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C5164: C16BFFF8  lfs f11, -8(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-8 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C5168: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C516C: D16A0000  stfs f11, 0(r10)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5170: D00A0004  stfs f0, 4(r10)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5174: D1AA0008  stfs f13, 8(r10)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C5178: D18A000C  stfs f12, 0xc(r10)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C517C: 394A0010  addi r10, r10, 0x10
	ctx.r[10].s64 = ctx.r[10].s64 + 16;
	// 832C5180: 4082FFD4  bne 0x832c5154
	if !ctx.cr[0].eq {
	pc = 0x832C5154; continue 'dispatch;
	}
	// 832C5184: 7D0B4378  mr r11, r8
	ctx.r[11].u64 = ctx.r[8].u64;
	// 832C5188: 2F1F0002  cmpwi cr6, r31, 2
	ctx.cr[6].compare_i32(ctx.r[31].s32, 2, &mut ctx.xer);
	// 832C518C: 4199FF48  bgt cr6, 0x832c50d4
	if ctx.cr[6].gt {
	pc = 0x832C50D4; continue 'dispatch;
	}
	// 832C5190: 382100C0  addi r1, r1, 0xc0
	ctx.r[1].s64 = ctx.r[1].s64 + 192;
	// 832C5194: 3981FFD0  addi r12, r1, -0x30
	ctx.r[12].s64 = ctx.r[1].s64 + -48;
	// 832C5198: 4B9E8B85  bl 0x82cadd1c
	ctx.lr = 0x832C519C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD1C);
	// 832C519C: 4B9E42B8  b 0x82ca9454
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9454);
	return;
}

pub fn sub_832C51A0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    let mut ea: u32 = 0;
    // ---- function 0x832C51A0 size=240
	// 832C51A0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C51A4: 4B9E4265  bl 0x82ca9408
	ctx.lr = 0x832C51A8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9408);
	// 832C51A8: DBA1FFC0  stfd f29, -0x40(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-64 as u32), ctx.f[29].u64 ) };
	// 832C51AC: DBC1FFC8  stfd f30, -0x38(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-56 as u32), ctx.f[30].u64 ) };
	// 832C51B0: DBE1FFD0  stfd f31, -0x30(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-48 as u32), ctx.f[31].u64 ) };
	// 832C51B4: 9421FF60  stwu r1, -0xa0(r1)
	ea = ctx.r[1].u32.wrapping_add(-160 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C51B8: 7C7E1B78  mr r30, r3
	ctx.r[30].u64 = ctx.r[3].u64;
	// 832C51BC: 3D408210  lis r10, -0x7df0
	ctx.r[10].s64 = -2112880640;
	// 832C51C0: 7FDC0E70  srawi r28, r30, 1
	ctx.xer.ca = (ctx.r[30].s32 < 0) && ((ctx.r[30].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[28].s64 = (ctx.r[30].s32 >> 1) as i64;
	// 832C51C4: 7C9D2378  mr r29, r4
	ctx.r[29].u64 = ctx.r[4].u64;
	// 832C51C8: 7F8B07B4  extsw r11, r28
	ctx.r[11].s64 = ctx.r[28].s32 as i64;
	// 832C51CC: F9610050  std r11, 0x50(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[11].u64 ) };
	// 832C51D0: C8010050  lfd f0, 0x50(r1)
	ctx.f[0].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832C51D4: FDA0069C  fcfid f13, f0
	ctx.f[13].f64 = (ctx.f[0].s64 as f64);
	// 832C51D8: C00A0E60  lfs f0, 0xe60(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(3680 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C51DC: FD806818  frsp f12, f13
	ctx.f[12].f64 = (ctx.f[13].f64 as f32) as f64;
	// 832C51E0: EFC06024  fdivs f30, f0, f12
	ctx.f[30].f64 = ((ctx.f[0].f64 / ctx.f[12].f64) as f32) as f64;
	// 832C51E4: EC2C07B2  fmuls f1, f12, f30
	ctx.f[1].f64 = (((ctx.f[12].f64 * ctx.f[30].f64) as f32) as f64);
	// 832C51E8: 4AF74CC9  bl 0x82239eb0
	ctx.lr = 0x832C51EC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82239EB0);
	// 832C51EC: FD600818  frsp f11, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[11].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832C51F0: 3D20820A  lis r9, -0x7df6
	ctx.r[9].s64 = -2113273856;
	// 832C51F4: 5788103A  slwi r8, r28, 2
	// 832C51F8: D17D0000  stfs f11, 0(r29)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C51FC: 3BE00001  li r31, 1
	ctx.r[31].s64 = 1;
	// 832C5200: 2F1C0001  cmpwi cr6, r28, 1
	ctx.cr[6].compare_i32(ctx.r[28].s32, 1, &mut ctx.xer);
	// 832C5204: C3E992D4  lfs f31, -0x6d2c(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(-27948 as u32) ) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832C5208: ED4B07F2  fmuls f10, f11, f31
	ctx.f[10].f64 = (((ctx.f[11].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C520C: 7D48ED2E  stfsx f10, r8, r29
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[8].u32.wrapping_add(ctx.r[29].u32), tmp.u32) };
	// 832C5210: 40990068  ble cr6, 0x832c5278
	if !ctx.cr[6].gt {
	pc = 0x832C5278; continue 'dispatch;
	}
	// 832C5214: 57CB103A  slwi r11, r30, 2
	// 832C5218: 3BDD0004  addi r30, r29, 4
	ctx.r[30].s64 = ctx.r[29].s64 + 4;
	// 832C521C: 7D6BEA14  add r11, r11, r29
	ctx.r[11].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832C5220: 3BABFFFC  addi r29, r11, -4
	ctx.r[29].s64 = ctx.r[11].s64 + -4;
	// 832C5224: 7FEB07B4  extsw r11, r31
	ctx.r[11].s64 = ctx.r[31].s32 as i64;
	// 832C5228: F9610050  std r11, 0x50(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(80 as u32), ctx.r[11].u64 ) };
	// 832C522C: C8010050  lfd f0, 0x50(r1)
	ctx.f[0].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(80 as u32) ) };
	// 832C5230: FDA0069C  fcfid f13, f0
	ctx.f[13].f64 = (ctx.f[0].s64 as f64);
	// 832C5234: FD806818  frsp f12, f13
	ctx.f[12].f64 = (ctx.f[13].f64 as f32) as f64;
	// 832C5238: EFAC07B2  fmuls f29, f12, f30
	ctx.f[29].f64 = (((ctx.f[12].f64 * ctx.f[30].f64) as f32) as f64);
	// 832C523C: FC20E890  fmr f1, f29
	ctx.f[1].f64 = ctx.f[29].f64;
	// 832C5240: 4AF74C71  bl 0x82239eb0
	ctx.lr = 0x832C5244;
	crate::recompiler::externs::call(&mut ctx, base, 0x82239EB0);
	// 832C5244: FD600818  frsp f11, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[11].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832C5248: FC20E890  fmr f1, f29
	ctx.f[1].f64 = ctx.f[29].f64;
	// 832C524C: ED4B07F2  fmuls f10, f11, f31
	ctx.f[10].f64 = (((ctx.f[11].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C5250: D15E0000  stfs f10, 0(r30)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5254: 4AF74D3D  bl 0x82239f90
	ctx.lr = 0x832C5258;
	crate::recompiler::externs::call(&mut ctx, base, 0x82239F90);
	// 832C5258: FD200818  frsp f9, f1
	ctx.fpscr.disable_flush_mode_unconditional();
	ctx.f[9].f64 = (ctx.f[1].f64 as f32) as f64;
	// 832C525C: 3BFF0001  addi r31, r31, 1
	ctx.r[31].s64 = ctx.r[31].s64 + 1;
	// 832C5260: 3BDE0004  addi r30, r30, 4
	ctx.r[30].s64 = ctx.r[30].s64 + 4;
	// 832C5264: 7F1FE000  cmpw cr6, r31, r28
	ctx.cr[6].compare_i32(ctx.r[31].s32, ctx.r[28].s32, &mut ctx.xer);
	// 832C5268: ED0907F2  fmuls f8, f9, f31
	ctx.f[8].f64 = (((ctx.f[9].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C526C: D11D0000  stfs f8, 0(r29)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5270: 3BBDFFFC  addi r29, r29, -4
	ctx.r[29].s64 = ctx.r[29].s64 + -4;
	// 832C5274: 4198FFB0  blt cr6, 0x832c5224
	if ctx.cr[6].lt {
	pc = 0x832C5224; continue 'dispatch;
	}
	// 832C5278: 382100A0  addi r1, r1, 0xa0
	ctx.r[1].s64 = ctx.r[1].s64 + 160;
	// 832C527C: CBA1FFC0  lfd f29, -0x40(r1)
	ctx.f[29].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-64 as u32) ) };
	// 832C5280: CBC1FFC8  lfd f30, -0x38(r1)
	ctx.f[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-56 as u32) ) };
	// 832C5284: CBE1FFD0  lfd f31, -0x30(r1)
	ctx.f[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-48 as u32) ) };
	// 832C5288: 4B9E41D0  b 0x82ca9458
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9458);
	return;
}

pub fn sub_832C5290(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    let mut ea: u32 = 0;
    // ---- function 0x832C5290 size=840
	// 832C5290: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C5294: 4B9E416D  bl 0x82ca9400
	ctx.lr = 0x832C5298;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9400);
	// 832C5298: 3981FFC8  addi r12, r1, -0x38
	ctx.r[12].s64 = ctx.r[1].s64 + -56;
	// 832C529C: 4B9E8A25  bl 0x82cadcc0
	ctx.lr = 0x832C52A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCC0);
	// 832C52A0: 9421FF20  stwu r1, -0xe0(r1)
	ea = ctx.r[1].u32.wrapping_add(-224 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C52A4: 7C7B1B78  mr r27, r3
	ctx.r[27].u64 = ctx.r[3].u64;
	// 832C52A8: 7C9F2378  mr r31, r4
	ctx.r[31].u64 = ctx.r[4].u64;
	// 832C52AC: 7CBA2B78  mr r26, r5
	ctx.r[26].u64 = ctx.r[5].u64;
	// 832C52B0: 7CDC3378  mr r28, r6
	ctx.r[28].u64 = ctx.r[6].u64;
	// 832C52B4: 7CFD3B78  mr r29, r7
	ctx.r[29].u64 = ctx.r[7].u64;
	// 832C52B8: 2F1B0020  cmpwi cr6, r27, 0x20
	ctx.cr[6].compare_i32(ctx.r[27].s32, 32, &mut ctx.xer);
	// 832C52BC: 409900F4  ble cr6, 0x832c53b0
	if !ctx.cr[6].gt {
	pc = 0x832C53B0; continue 'dispatch;
	}
	// 832C52C0: 7F7E1670  srawi r30, r27, 2
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 2) - 1)) != 0);
	ctx.r[30].s64 = (ctx.r[27].s32 >> 2) as i64;
	// 832C52C4: 7D7EE050  subf r11, r30, r28
	ctx.r[11].s64 = ctx.r[28].s64 - ctx.r[30].s64;
	// 832C52C8: 556B103A  slwi r11, r11, 2
	// 832C52CC: 7CABEA14  add r5, r11, r29
	ctx.r[5].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832C52D0: 48000829  bl 0x832c5af8
	ctx.lr = 0x832C52D4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C5AF8);
	// 832C52D4: 2F1B0200  cmpwi cr6, r27, 0x200
	ctx.cr[6].compare_i32(ctx.r[27].s32, 512, &mut ctx.xer);
	// 832C52D8: 7FA6EB78  mr r6, r29
	ctx.r[6].u64 = ctx.r[29].u64;
	// 832C52DC: 7F85E378  mr r5, r28
	ctx.r[5].u64 = ctx.r[28].u64;
	// 832C52E0: 4099007C  ble cr6, 0x832c535c
	if !ctx.cr[6].gt {
	pc = 0x832C535C; continue 'dispatch;
	}
	// 832C52E4: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832C52E8: 48000F01  bl 0x832c61e8
	ctx.lr = 0x832C52EC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C61E8);
	// 832C52EC: 57CB103A  slwi r11, r30, 2
	// 832C52F0: 7FA6EB78  mr r6, r29
	ctx.r[6].u64 = ctx.r[29].u64;
	// 832C52F4: 7F85E378  mr r5, r28
	ctx.r[5].u64 = ctx.r[28].u64;
	// 832C52F8: 7C8BFA14  add r4, r11, r31
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[31].u64;
	// 832C52FC: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832C5300: 48000FC1  bl 0x832c62c0
	ctx.lr = 0x832C5304;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C62C0);
	// 832C5304: 57CB1838  slwi r11, r30, 3
	// 832C5308: 7FA6EB78  mr r6, r29
	ctx.r[6].u64 = ctx.r[29].u64;
	// 832C530C: 7F85E378  mr r5, r28
	ctx.r[5].u64 = ctx.r[28].u64;
	// 832C5310: 7C8BFA14  add r4, r11, r31
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[31].u64;
	// 832C5314: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832C5318: 48000ED1  bl 0x832c61e8
	ctx.lr = 0x832C531C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C61E8);
	// 832C531C: 57CB083C  slwi r11, r30, 1
	// 832C5320: 7FA6EB78  mr r6, r29
	ctx.r[6].u64 = ctx.r[29].u64;
	// 832C5324: 7D7E5A14  add r11, r30, r11
	ctx.r[11].u64 = ctx.r[30].u64 + ctx.r[11].u64;
	// 832C5328: 7F85E378  mr r5, r28
	ctx.r[5].u64 = ctx.r[28].u64;
	// 832C532C: 556B103A  slwi r11, r11, 2
	// 832C5330: 7FC3F378  mr r3, r30
	ctx.r[3].u64 = ctx.r[30].u64;
	// 832C5334: 7C8BFA14  add r4, r11, r31
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[31].u64;
	// 832C5338: 48000EB1  bl 0x832c61e8
	ctx.lr = 0x832C533C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C61E8);
	// 832C533C: 7FE5FB78  mr r5, r31
	ctx.r[5].u64 = ctx.r[31].u64;
	// 832C5340: 7F44D378  mr r4, r26
	ctx.r[4].u64 = ctx.r[26].u64;
	// 832C5344: 7F63DB78  mr r3, r27
	ctx.r[3].u64 = ctx.r[27].u64;
	// 832C5348: 48000291  bl 0x832c55d8
	ctx.lr = 0x832C534C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C55D8);
	// 832C534C: 382100E0  addi r1, r1, 0xe0
	ctx.r[1].s64 = ctx.r[1].s64 + 224;
	// 832C5350: 3981FFC8  addi r12, r1, -0x38
	ctx.r[12].s64 = ctx.r[1].s64 + -56;
	// 832C5354: 4B9E89B9  bl 0x82cadd0c
	ctx.lr = 0x832C5358;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD0C);
	// 832C5358: 4B9E40F8  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832C535C: 2F1E0020  cmpwi cr6, r30, 0x20
	ctx.cr[6].compare_i32(ctx.r[30].s32, 32, &mut ctx.xer);
	// 832C5360: 7F63DB78  mr r3, r27
	ctx.r[3].u64 = ctx.r[27].u64;
	// 832C5364: 40990028  ble cr6, 0x832c538c
	if !ctx.cr[6].gt {
	pc = 0x832C538C; continue 'dispatch;
	}
	// 832C5368: 48001029  bl 0x832c6390
	ctx.lr = 0x832C536C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6390);
	// 832C536C: 7FE5FB78  mr r5, r31
	ctx.r[5].u64 = ctx.r[31].u64;
	// 832C5370: 7F44D378  mr r4, r26
	ctx.r[4].u64 = ctx.r[26].u64;
	// 832C5374: 7F63DB78  mr r3, r27
	ctx.r[3].u64 = ctx.r[27].u64;
	// 832C5378: 48000261  bl 0x832c55d8
	ctx.lr = 0x832C537C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C55D8);
	// 832C537C: 382100E0  addi r1, r1, 0xe0
	ctx.r[1].s64 = ctx.r[1].s64 + 224;
	// 832C5380: 3981FFC8  addi r12, r1, -0x38
	ctx.r[12].s64 = ctx.r[1].s64 + -56;
	// 832C5384: 4B9E8989  bl 0x82cadd0c
	ctx.lr = 0x832C5388;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD0C);
	// 832C5388: 4B9E40C8  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832C538C: 48001C45  bl 0x832c6fd0
	ctx.lr = 0x832C5390;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6FD0);
	// 832C5390: 7FE5FB78  mr r5, r31
	ctx.r[5].u64 = ctx.r[31].u64;
	// 832C5394: 7F44D378  mr r4, r26
	ctx.r[4].u64 = ctx.r[26].u64;
	// 832C5398: 7F63DB78  mr r3, r27
	ctx.r[3].u64 = ctx.r[27].u64;
	// 832C539C: 4800023D  bl 0x832c55d8
	ctx.lr = 0x832C53A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C55D8);
	// 832C53A0: 382100E0  addi r1, r1, 0xe0
	ctx.r[1].s64 = ctx.r[1].s64 + 224;
	// 832C53A4: 3981FFC8  addi r12, r1, -0x38
	ctx.r[12].s64 = ctx.r[1].s64 + -56;
	// 832C53A8: 4B9E8965  bl 0x82cadd0c
	ctx.lr = 0x832C53AC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD0C);
	// 832C53AC: 4B9E40A4  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832C53B0: 2F1B0008  cmpwi cr6, r27, 8
	ctx.cr[6].compare_i32(ctx.r[27].s32, 8, &mut ctx.xer);
	// 832C53B4: 40990148  ble cr6, 0x832c54fc
	if !ctx.cr[6].gt {
	pc = 0x832C54FC; continue 'dispatch;
	}
	// 832C53B8: 2F1B0020  cmpwi cr6, r27, 0x20
	ctx.cr[6].compare_i32(ctx.r[27].s32, 32, &mut ctx.xer);
	// 832C53BC: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C53C0: 409A00E4  bne cr6, 0x832c54a4
	if !ctx.cr[6].eq {
	pc = 0x832C54A4; continue 'dispatch;
	}
	// 832C53C4: 397CFFF8  addi r11, r28, -8
	ctx.r[11].s64 = ctx.r[28].s64 + -8;
	// 832C53C8: 556B103A  slwi r11, r11, 2
	// 832C53CC: 7C8BEA14  add r4, r11, r29
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832C53D0: 48001D49  bl 0x832c7118
	ctx.lr = 0x832C53D4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7118);
	// 832C53D4: C01F0008  lfs f0, 8(r31)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C53D8: C1BF000C  lfs f13, 0xc(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C53DC: C19F0010  lfs f12, 0x10(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(16 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C53E0: C17F0014  lfs f11, 0x14(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C53E4: C15F0018  lfs f10, 0x18(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C53E8: C13F001C  lfs f9, 0x1c(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(28 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C53EC: C11F0028  lfs f8, 0x28(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(40 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C53F0: C0FF002C  lfs f7, 0x2c(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(44 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C53F4: C0DF0038  lfs f6, 0x38(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(56 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C53F8: C0BF003C  lfs f5, 0x3c(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(60 as u32) ) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832C53FC: C09F0058  lfs f4, 0x58(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(88 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C5400: C07F005C  lfs f3, 0x5c(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(92 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C5404: C05F0040  lfs f2, 0x40(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(64 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C5408: C03F0044  lfs f1, 0x44(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(68 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C540C: D05F0008  stfs f2, 8(r31)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C5410: D03F000C  stfs f1, 0xc(r31)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C5414: D01F0040  stfs f0, 0x40(r31)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(64 as u32), tmp.u32 ) };
	// 832C5418: D1BF0044  stfs f13, 0x44(r31)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(68 as u32), tmp.u32 ) };
	// 832C541C: C3FF0020  lfs f31, 0x20(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832C5420: C3DF0024  lfs f30, 0x24(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(36 as u32) ) };
	ctx.f[30].f64 = (tmp.f32 as f64);
	// 832C5424: C3BF0060  lfs f29, 0x60(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(96 as u32) ) };
	ctx.f[29].f64 = (tmp.f32 as f64);
	// 832C5428: C39F0064  lfs f28, 0x64(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(100 as u32) ) };
	ctx.f[28].f64 = (tmp.f32 as f64);
	// 832C542C: C37F0050  lfs f27, 0x50(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(80 as u32) ) };
	ctx.f[27].f64 = (tmp.f32 as f64);
	// 832C5430: C35F0054  lfs f26, 0x54(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(84 as u32) ) };
	ctx.f[26].f64 = (tmp.f32 as f64);
	// 832C5434: C33F0070  lfs f25, 0x70(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(112 as u32) ) };
	ctx.f[25].f64 = (tmp.f32 as f64);
	// 832C5438: C31F0074  lfs f24, 0x74(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(116 as u32) ) };
	ctx.f[24].f64 = (tmp.f32 as f64);
	// 832C543C: C2FF0068  lfs f23, 0x68(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(104 as u32) ) };
	ctx.f[23].f64 = (tmp.f32 as f64);
	// 832C5440: C2DF006C  lfs f22, 0x6c(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(108 as u32) ) };
	ctx.f[22].f64 = (tmp.f32 as f64);
	// 832C5444: D3FF0010  stfs f31, 0x10(r31)
	tmp.f32 = (ctx.f[31].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(16 as u32), tmp.u32 ) };
	// 832C5448: D3DF0014  stfs f30, 0x14(r31)
	tmp.f32 = (ctx.f[30].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(20 as u32), tmp.u32 ) };
	// 832C544C: D3BF0018  stfs f29, 0x18(r31)
	tmp.f32 = (ctx.f[29].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(24 as u32), tmp.u32 ) };
	// 832C5450: D39F001C  stfs f28, 0x1c(r31)
	tmp.f32 = (ctx.f[28].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(28 as u32), tmp.u32 ) };
	// 832C5454: D19F0020  stfs f12, 0x20(r31)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(32 as u32), tmp.u32 ) };
	// 832C5458: D17F0024  stfs f11, 0x24(r31)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(36 as u32), tmp.u32 ) };
	// 832C545C: D37F0028  stfs f27, 0x28(r31)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(40 as u32), tmp.u32 ) };
	// 832C5460: D35F002C  stfs f26, 0x2c(r31)
	tmp.f32 = (ctx.f[26].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(44 as u32), tmp.u32 ) };
	// 832C5464: D33F0038  stfs f25, 0x38(r31)
	tmp.f32 = (ctx.f[25].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(56 as u32), tmp.u32 ) };
	// 832C5468: D31F003C  stfs f24, 0x3c(r31)
	tmp.f32 = (ctx.f[24].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(60 as u32), tmp.u32 ) };
	// 832C546C: D11F0050  stfs f8, 0x50(r31)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(80 as u32), tmp.u32 ) };
	// 832C5470: D0FF0054  stfs f7, 0x54(r31)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(84 as u32), tmp.u32 ) };
	// 832C5474: D2FF0058  stfs f23, 0x58(r31)
	tmp.f32 = (ctx.f[23].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(88 as u32), tmp.u32 ) };
	// 832C5478: D2DF005C  stfs f22, 0x5c(r31)
	tmp.f32 = (ctx.f[22].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(92 as u32), tmp.u32 ) };
	// 832C547C: D15F0060  stfs f10, 0x60(r31)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(96 as u32), tmp.u32 ) };
	// 832C5480: D13F0064  stfs f9, 0x64(r31)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(100 as u32), tmp.u32 ) };
	// 832C5484: D09F0068  stfs f4, 0x68(r31)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(104 as u32), tmp.u32 ) };
	// 832C5488: D07F006C  stfs f3, 0x6c(r31)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(108 as u32), tmp.u32 ) };
	// 832C548C: D0DF0070  stfs f6, 0x70(r31)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(112 as u32), tmp.u32 ) };
	// 832C5490: D0BF0074  stfs f5, 0x74(r31)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(116 as u32), tmp.u32 ) };
	// 832C5494: 382100E0  addi r1, r1, 0xe0
	ctx.r[1].s64 = ctx.r[1].s64 + 224;
	// 832C5498: 3981FFC8  addi r12, r1, -0x38
	ctx.r[12].s64 = ctx.r[1].s64 + -56;
	// 832C549C: 4B9E8871  bl 0x82cadd0c
	ctx.lr = 0x832C54A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD0C);
	// 832C54A0: 4B9E3FB0  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832C54A4: 7FA4EB78  mr r4, r29
	ctx.r[4].u64 = ctx.r[29].u64;
	// 832C54A8: 48002599  bl 0x832c7a40
	ctx.lr = 0x832C54AC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7A40);
	// 832C54AC: C01F0008  lfs f0, 8(r31)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C54B0: C1BF000C  lfs f13, 0xc(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C54B4: C19F0018  lfs f12, 0x18(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C54B8: C17F001C  lfs f11, 0x1c(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(28 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C54BC: C15F0020  lfs f10, 0x20(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(32 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C54C0: C13F0024  lfs f9, 0x24(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(36 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C54C4: C11F0030  lfs f8, 0x30(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(48 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C54C8: C0FF0034  lfs f7, 0x34(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(52 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C54CC: D15F0008  stfs f10, 8(r31)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C54D0: D13F000C  stfs f9, 0xc(r31)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C54D4: D11F0018  stfs f8, 0x18(r31)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(24 as u32), tmp.u32 ) };
	// 832C54D8: D0FF001C  stfs f7, 0x1c(r31)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(28 as u32), tmp.u32 ) };
	// 832C54DC: D01F0020  stfs f0, 0x20(r31)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(32 as u32), tmp.u32 ) };
	// 832C54E0: D1BF0024  stfs f13, 0x24(r31)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(36 as u32), tmp.u32 ) };
	// 832C54E4: D19F0030  stfs f12, 0x30(r31)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(48 as u32), tmp.u32 ) };
	// 832C54E8: D17F0034  stfs f11, 0x34(r31)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(52 as u32), tmp.u32 ) };
	// 832C54EC: 382100E0  addi r1, r1, 0xe0
	ctx.r[1].s64 = ctx.r[1].s64 + 224;
	// 832C54F0: 3981FFC8  addi r12, r1, -0x38
	ctx.r[12].s64 = ctx.r[1].s64 + -56;
	// 832C54F4: 4B9E8819  bl 0x82cadd0c
	ctx.lr = 0x832C54F8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD0C);
	// 832C54F8: 4B9E3F58  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832C54FC: 409A0094  bne cr6, 0x832c5590
	if !ctx.cr[6].eq {
	pc = 0x832C5590; continue 'dispatch;
	}
	// 832C5500: C01F0000  lfs f0, 0(r31)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C5504: C1BF0010  lfs f13, 0x10(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(16 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C5508: C11F0018  lfs f8, 0x18(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(24 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C550C: ED60682A  fadds f11, f0, f13
	ctx.f[11].f64 = ((ctx.f[0].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C5510: C0DF0008  lfs f6, 8(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C5514: ED206828  fsubs f9, f0, f13
	ctx.f[9].f64 = (((ctx.f[0].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C5518: C19F0014  lfs f12, 0x14(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(20 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C551C: EC68302A  fadds f3, f8, f6
	ctx.f[3].f64 = ((ctx.f[8].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C5520: C15F0004  lfs f10, 4(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C5524: EC264028  fsubs f1, f6, f8
	ctx.f[1].f64 = (((ctx.f[6].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C5528: C09F001C  lfs f4, 0x1c(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(28 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C552C: ECEA602A  fadds f7, f10, f12
	ctx.f[7].f64 = ((ctx.f[10].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C5530: C05F000C  lfs f2, 0xc(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C5534: ECAA6028  fsubs f5, f10, f12
	ctx.f[5].f64 = (((ctx.f[10].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C5538: EC04102A  fadds f0, f4, f2
	ctx.f[0].f64 = ((ctx.f[4].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C553C: EDA22028  fsubs f13, f2, f4
	ctx.f[13].f64 = (((ctx.f[2].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C5540: ED83582A  fadds f12, f3, f11
	ctx.f[12].f64 = ((ctx.f[3].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C5544: D19F0000  stfs f12, 0(r31)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5548: ED6B1828  fsubs f11, f11, f3
	ctx.f[11].f64 = (((ctx.f[11].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C554C: D17F0010  stfs f11, 0x10(r31)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(16 as u32), tmp.u32 ) };
	// 832C5550: ECC1282A  fadds f6, f1, f5
	ctx.f[6].f64 = ((ctx.f[1].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C5554: D0DF000C  stfs f6, 0xc(r31)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C5558: ED40382A  fadds f10, f0, f7
	ctx.f[10].f64 = ((ctx.f[0].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C555C: D15F0004  stfs f10, 4(r31)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5560: ED070028  fsubs f8, f7, f0
	ctx.f[8].f64 = (((ctx.f[7].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C5564: D11F0014  stfs f8, 0x14(r31)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(20 as u32), tmp.u32 ) };
	// 832C5568: ECE96828  fsubs f7, f9, f13
	ctx.f[7].f64 = (((ctx.f[9].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C556C: D0FF0008  stfs f7, 8(r31)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C5570: EC8D482A  fadds f4, f13, f9
	ctx.f[4].f64 = ((ctx.f[13].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C5574: D09F0018  stfs f4, 0x18(r31)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(24 as u32), tmp.u32 ) };
	// 832C5578: EC650828  fsubs f3, f5, f1
	ctx.f[3].f64 = (((ctx.f[5].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C557C: D07F001C  stfs f3, 0x1c(r31)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(28 as u32), tmp.u32 ) };
	// 832C5580: 382100E0  addi r1, r1, 0xe0
	ctx.r[1].s64 = ctx.r[1].s64 + 224;
	// 832C5584: 3981FFC8  addi r12, r1, -0x38
	ctx.r[12].s64 = ctx.r[1].s64 + -56;
	// 832C5588: 4B9E8785  bl 0x82cadd0c
	ctx.lr = 0x832C558C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD0C);
	// 832C558C: 4B9E3EC4  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
	// 832C5590: 2F1B0004  cmpwi cr6, r27, 4
	ctx.cr[6].compare_i32(ctx.r[27].s32, 4, &mut ctx.xer);
	// 832C5594: 409A0034  bne cr6, 0x832c55c8
	if !ctx.cr[6].eq {
	pc = 0x832C55C8; continue 'dispatch;
	}
	// 832C5598: C01F0000  lfs f0, 0(r31)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C559C: C1BF0008  lfs f13, 8(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(8 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C55A0: C19F0004  lfs f12, 4(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C55A4: ED606828  fsubs f11, f0, f13
	ctx.f[11].f64 = (((ctx.f[0].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C55A8: C15F000C  lfs f10, 0xc(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(12 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C55AC: ED20682A  fadds f9, f0, f13
	ctx.f[9].f64 = ((ctx.f[0].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C55B0: ED0C5028  fsubs f8, f12, f10
	ctx.f[8].f64 = (((ctx.f[12].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C55B4: D13F0000  stfs f9, 0(r31)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C55B8: ECEC502A  fadds f7, f12, f10
	ctx.f[7].f64 = ((ctx.f[12].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C55BC: D0FF0004  stfs f7, 4(r31)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C55C0: D17F0008  stfs f11, 8(r31)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C55C4: D11F000C  stfs f8, 0xc(r31)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C55C8: 382100E0  addi r1, r1, 0xe0
	ctx.r[1].s64 = ctx.r[1].s64 + 224;
	// 832C55CC: 3981FFC8  addi r12, r1, -0x38
	ctx.r[12].s64 = ctx.r[1].s64 + -56;
	// 832C55D0: 4B9E873D  bl 0x82cadd0c
	ctx.lr = 0x832C55D4;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD0C);
	// 832C55D4: 4B9E3E7C  b 0x82ca9450
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9450);
	return;
}

pub fn sub_832C55D8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    // ---- function 0x832C55D8 size=8
	// 832C55D8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C55DC: 4B9E3E11  bl 0x82ca93ec
	ctx.lr = 0x832C55E0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93EC);
}

pub fn sub_832C5AF8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    // ---- function 0x832C5AF8 size=1776
	// 832C5AF8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C5AFC: 4B9E38FD  bl 0x82ca93f8
	ctx.lr = 0x832C5B00;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93F8);
	// 832C5B00: 3981FFB8  addi r12, r1, -0x48
	ctx.r[12].s64 = ctx.r[1].s64 + -72;
	// 832C5B04: 4B9E819D  bl 0x82cadca0
	ctx.lr = 0x832C5B08;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCA0);
	// 832C5B08: 7C7E1E70  srawi r30, r3, 3
	ctx.xer.ca = (ctx.r[3].s32 < 0) && ((ctx.r[3].u32 & ((1u32 << 3) - 1)) != 0);
	ctx.r[30].s64 = (ctx.r[3].s32 >> 3) as i64;
	// 832C5B0C: C0040000  lfs f0, 0(r4)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(0 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C5B10: C1A40004  lfs f13, 4(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(4 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C5B14: 3CE0820A  lis r7, -0x7df6
	ctx.r[7].s64 = -2113273856;
	// 832C5B18: 57CA103A  slwi r10, r30, 2
	// 832C5B1C: 57CB083C  slwi r11, r30, 1
	// 832C5B20: 57C82036  slwi r8, r30, 4
	// 832C5B24: 7D2A5A14  add r9, r10, r11
	ctx.r[9].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C5B28: 7D482214  add r10, r8, r4
	ctx.r[10].u64 = ctx.r[8].u64 + ctx.r[4].u64;
	// 832C5B2C: 5529103A  slwi r9, r9, 2
	// 832C5B30: 57C81838  slwi r8, r30, 3
	// 832C5B34: 7D292214  add r9, r9, r4
	ctx.r[9].u64 = ctx.r[9].u64 + ctx.r[4].u64;
	// 832C5B38: 7D082214  add r8, r8, r4
	ctx.r[8].u64 = ctx.r[8].u64 + ctx.r[4].u64;
	// 832C5B3C: C18A0000  lfs f12, 0(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C5B40: 38C79484  addi r6, r7, -0x6b7c
	ctx.r[6].s64 = ctx.r[7].s64 + -27516;
	// 832C5B44: C16A0004  lfs f11, 4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C5B48: ED40602A  fadds f10, f0, f12
	ctx.f[10].f64 = ((ctx.f[0].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C5B4C: ECE06028  fsubs f7, f0, f12
	ctx.f[7].f64 = (((ctx.f[0].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C5B50: 3B7EFFFE  addi r27, r30, -2
	ctx.r[27].s64 = ctx.r[30].s64 + -2;
	// 832C5B54: C1090000  lfs f8, 0(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C5B58: ED2D582A  fadds f9, f13, f11
	ctx.f[9].f64 = ((ctx.f[13].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C5B5C: C0C80000  lfs f6, 0(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C5B60: ECAD5828  fsubs f5, f13, f11
	ctx.f[5].f64 = (((ctx.f[13].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C5B64: C0890004  lfs f4, 4(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(4 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C5B68: EC66402A  fadds f3, f6, f8
	ctx.f[3].f64 = ((ctx.f[6].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C5B6C: C0480004  lfs f2, 4(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(4 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C5B70: EC264028  fsubs f1, f6, f8
	ctx.f[1].f64 = (((ctx.f[6].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C5B74: EC02202A  fadds f0, f2, f4
	ctx.f[0].f64 = ((ctx.f[2].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C5B78: C186000C  lfs f12, 0xc(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(12 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C5B7C: ED622028  fsubs f11, f2, f4
	ctx.f[11].f64 = (((ctx.f[2].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C5B80: C1A79484  lfs f13, -0x6b7c(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(-27516 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C5B84: 3BA50008  addi r29, r5, 8
	ctx.r[29].s64 = ctx.r[5].s64 + 8;
	// 832C5B88: 2F1B0002  cmpwi cr6, r27, 2
	ctx.cr[6].compare_i32(ctx.r[27].s32, 2, &mut ctx.xer);
	// 832C5B8C: ED03502A  fadds f8, f3, f10
	ctx.f[8].f64 = ((ctx.f[3].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C5B90: D1040000  stfs f8, 0(r4)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5B94: ECCA1828  fsubs f6, f10, f3
	ctx.f[6].f64 = (((ctx.f[10].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C5B98: EC80482A  fadds f4, f0, f9
	ctx.f[4].f64 = ((ctx.f[0].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C5B9C: D0840004  stfs f4, 4(r4)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5BA0: EC690028  fsubs f3, f9, f0
	ctx.f[3].f64 = (((ctx.f[9].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C5BA4: D0C80000  stfs f6, 0(r8)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5BA8: D0680004  stfs f3, 4(r8)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5BAC: EC475828  fsubs f2, f7, f11
	ctx.f[2].f64 = (((ctx.f[7].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C5BB0: EC01282A  fadds f0, f1, f5
	ctx.f[0].f64 = ((ctx.f[1].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C5BB4: D04A0000  stfs f2, 0(r10)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5BB8: D00A0004  stfs f0, 4(r10)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5BBC: ED6B382A  fadds f11, f11, f7
	ctx.f[11].f64 = ((ctx.f[11].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C5BC0: ED450828  fsubs f10, f5, f1
	ctx.f[10].f64 = (((ctx.f[5].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C5BC4: D1690000  stfs f11, 0(r9)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5BC8: D1490004  stfs f10, 4(r9)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5BCC: FD006090  fmr f8, f12
	ctx.f[8].f64 = ctx.f[12].f64;
	// 832C5BD0: C0050004  lfs f0, 4(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(4 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C5BD4: FD606890  fmr f11, f13
	ctx.f[11].f64 = ctx.f[13].f64;
	// 832C5BD8: C125000C  lfs f9, 0xc(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(12 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C5BDC: C1450008  lfs f10, 8(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(8 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C5BE0: D121FF1C  stfs f9, -0xe4(r1)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-228 as u32), tmp.u32 ) };
	// 832C5BE4: D001FF20  stfs f0, -0xe0(r1)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-224 as u32), tmp.u32 ) };
	// 832C5BE8: 40990368  ble cr6, 0x832c5f50
	if !ctx.cr[6].gt {
	pc = 0x832C5F50; continue 'dispatch;
	}
	// 832C5BEC: 556A083C  slwi r10, r11, 1
	// 832C5BF0: 390B0004  addi r8, r11, 4
	ctx.r[8].s64 = ctx.r[11].s64 + 4;
	// 832C5BF4: 7CEB5214  add r7, r11, r10
	ctx.r[7].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832C5BF8: 38CBFFFC  addi r6, r11, -4
	ctx.r[6].s64 = ctx.r[11].s64 + -4;
	// 832C5BFC: 386BFFFE  addi r3, r11, -2
	ctx.r[3].s64 = ctx.r[11].s64 + -2;
	// 832C5C00: 54EA103A  slwi r10, r7, 2
	// 832C5C04: 3BEB0002  addi r31, r11, 2
	ctx.r[31].s64 = ctx.r[11].s64 + 2;
	// 832C5C08: 55692036  slwi r9, r11, 4
	// 832C5C0C: 5508103A  slwi r8, r8, 2
	// 832C5C10: 3B5BFFFD  addi r26, r27, -3
	ctx.r[26].s64 = ctx.r[27].s64 + -3;
	// 832C5C14: 54C5103A  slwi r5, r6, 2
	// 832C5C18: 547C1838  slwi r28, r3, 3
	// 832C5C1C: 7CEA2214  add r7, r10, r4
	ctx.r[7].u64 = ctx.r[10].u64 + ctx.r[4].u64;
	// 832C5C20: 57E31838  slwi r3, r31, 3
	// 832C5C24: 7CC92214  add r6, r9, r4
	ctx.r[6].u64 = ctx.r[9].u64 + ctx.r[4].u64;
	// 832C5C28: 575AF0BE  srwi r26, r26, 2
	// 832C5C2C: 7FE82214  add r31, r8, r4
	ctx.r[31].u64 = ctx.r[8].u64 + ctx.r[4].u64;
	// 832C5C30: 7D052214  add r8, r5, r4
	ctx.r[8].u64 = ctx.r[5].u64 + ctx.r[4].u64;
	// 832C5C34: 7D3C2214  add r9, r28, r4
	ctx.r[9].u64 = ctx.r[28].u64 + ctx.r[4].u64;
	// 832C5C38: 38A70010  addi r5, r7, 0x10
	ctx.r[5].s64 = ctx.r[7].s64 + 16;
	// 832C5C3C: 39440010  addi r10, r4, 0x10
	ctx.r[10].s64 = ctx.r[4].s64 + 16;
	// 832C5C40: 7C632214  add r3, r3, r4
	ctx.r[3].u64 = ctx.r[3].u64 + ctx.r[4].u64;
	// 832C5C44: 38E7FFF0  addi r7, r7, -0x10
	ctx.r[7].s64 = ctx.r[7].s64 + -16;
	// 832C5C48: 38C6FFF0  addi r6, r6, -0x10
	ctx.r[6].s64 = ctx.r[6].s64 + -16;
	// 832C5C4C: 3B9A0001  addi r28, r26, 1
	ctx.r[28].s64 = ctx.r[26].s64 + 1;
	// 832C5C50: 3BBD0010  addi r29, r29, 0x10
	ctx.r[29].s64 = ctx.r[29].s64 + 16;
	// 832C5C54: C0C5FFF8  lfs f6, -8(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(-8 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C5C58: C0A3FFFC  lfs f5, -4(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(-4 as u32) ) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832C5C5C: 379CFFFF  addic. r28, r28, -1
	ctx.xer.ca = (ctx.r[28].u32 > (!(-1 as u32)));
	ctx.r[28].s64 = ctx.r[28].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[28].s32, 0, &mut ctx.xer);
	// 832C5C60: C3DFFFF8  lfs f30, -8(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(-8 as u32) ) };
	ctx.f[30].f64 = (tmp.f32 as f64);
	// 832C5C64: C06AFFFC  lfs f3, -4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(-4 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C5C68: EF7E3028  fsubs f27, f30, f6
	ctx.f[27].f64 = (((ctx.f[30].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C5C6C: C0E5FFFC  lfs f7, -4(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(-4 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C5C70: EC432828  fsubs f2, f3, f5
	ctx.f[2].f64 = (((ctx.f[3].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C5C74: C01DFFFC  lfs f0, -4(r29)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(-4 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C5C78: ECA5182A  fadds f5, f5, f3
	ctx.f[5].f64 = ((ctx.f[5].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C5C7C: C083FFF8  lfs f4, -8(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(-8 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C5C80: EFE0682A  fadds f31, f0, f13
	ctx.f[31].f64 = ((ctx.f[0].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C5C84: C03FFFFC  lfs f1, -4(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(-4 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C5C88: ECDE302A  fadds f6, f30, f6
	ctx.f[6].f64 = ((ctx.f[30].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C5C8C: C38AFFF8  lfs f28, -8(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(-8 as u32) ) };
	ctx.f[28].f64 = (tmp.f32 as f64);
	// 832C5C90: EFA13828  fsubs f29, f1, f7
	ctx.f[29].f64 = (((ctx.f[1].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C5C94: EF5C2028  fsubs f26, f28, f4
	ctx.f[26].f64 = (((ctx.f[28].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C5C98: C2E30000  lfs f23, 0(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) };
	ctx.f[23].f64 = (tmp.f32 as f64);
	// 832C5C9C: EC21382A  fadds f1, f1, f7
	ctx.f[1].f64 = ((ctx.f[1].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C5CA0: C06A0000  lfs f3, 0(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C5CA4: C2850000  lfs f20, 0(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(0 as u32) ) };
	ctx.f[20].f64 = (tmp.f32 as f64);
	// 832C5CA8: EC84E02A  fadds f4, f4, f28
	ctx.f[4].f64 = ((ctx.f[4].f64 + ctx.f[28].f64) as f32) as f64;
	// 832C5CAC: C25F0000  lfs f18, 0(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) };
	ctx.f[18].f64 = (tmp.f32 as f64);
	// 832C5CB0: EEC3B828  fsubs f22, f3, f23
	ctx.f[22].f64 = (((ctx.f[3].f64 - ctx.f[23].f64) as f32) as f64);
	// 832C5CB4: C3250004  lfs f25, 4(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(4 as u32) ) };
	ctx.f[25].f64 = (tmp.f32 as f64);
	// 832C5CB8: EC77182A  fadds f3, f23, f3
	ctx.f[3].f64 = ((ctx.f[23].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C5CBC: C0FF0004  lfs f7, 4(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C5CC0: EDF2A02A  fadds f15, f18, f20
	ctx.f[15].f64 = ((ctx.f[18].f64 + ctx.f[20].f64) as f32) as f64;
	// 832C5CC4: C1BDFFF8  lfs f13, -8(r29)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(-8 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C5CC8: EEA7C828  fsubs f21, f7, f25
	ctx.f[21].f64 = (((ctx.f[7].f64 - ctx.f[25].f64) as f32) as f64);
	// 832C5CCC: C2630004  lfs f19, 4(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) };
	ctx.f[19].f64 = (tmp.f32 as f64);
	// 832C5CD0: ECE7C82A  fadds f7, f7, f25
	ctx.f[7].f64 = ((ctx.f[7].f64 + ctx.f[25].f64) as f32) as f64;
	// 832C5CD4: C3CA0004  lfs f30, 4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) };
	ctx.f[30].f64 = (tmp.f32 as f64);
	// 832C5CD8: EF0C682A  fadds f24, f12, f13
	ctx.f[24].f64 = ((ctx.f[12].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C5CDC: EEF3F02A  fadds f23, f19, f30
	ctx.f[23].f64 = ((ctx.f[19].f64 + ctx.f[30].f64) as f32) as f64;
	// 832C5CE0: C19D0000  lfs f12, 0(r29)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C5CE4: EFFF02B2  fmuls f31, f31, f10
	ctx.f[31].f64 = (((ctx.f[31].f64 * ctx.f[10].f64) as f32) as f64);
	// 832C5CE8: C21D0004  lfs f16, 4(r29)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(4 as u32) ) };
	ctx.f[16].f64 = (tmp.f32 as f64);
	// 832C5CEC: EF9B102A  fadds f28, f27, f2
	ctx.f[28].f64 = ((ctx.f[27].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C5CF0: D1A1FF18  stfs f13, -0xe8(r1)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-232 as u32), tmp.u32 ) };
	// 832C5CF4: EE3AE828  fsubs f17, f26, f29
	ctx.f[17].f64 = (((ctx.f[26].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C5CF8: D001FF14  stfs f0, -0xec(r1)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-236 as u32), tmp.u32 ) };
	// 832C5CFC: EFDE9828  fsubs f30, f30, f19
	ctx.f[30].f64 = (((ctx.f[30].f64 - ctx.f[19].f64) as f32) as f64);
	// 832C5D00: D181FF10  stfs f12, -0xf0(r1)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-240 as u32), tmp.u32 ) };
	// 832C5D04: EE92A028  fsubs f20, f18, f20
	ctx.f[20].f64 = (((ctx.f[18].f64 - ctx.f[20].f64) as f32) as f64);
	// 832C5D08: EDC1282A  fadds f14, f1, f5
	ctx.f[14].f64 = ((ctx.f[1].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C5D0C: D1CAFFFC  stfs f14, -4(r10)
	tmp.f32 = (ctx.f[14].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C5D10: EDC6202A  fadds f14, f6, f4
	ctx.f[14].f64 = ((ctx.f[6].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C5D14: D1CAFFF8  stfs f14, -8(r10)
	tmp.f32 = (ctx.f[14].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(-8 as u32), tmp.u32 ) };
	// 832C5D18: EDCF182A  fadds f14, f15, f3
	ctx.f[14].f64 = ((ctx.f[15].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C5D1C: D1CA0000  stfs f14, 0(r10)
	tmp.f32 = (ctx.f[14].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5D20: EF3802B2  fmuls f25, f24, f10
	ctx.f[25].f64 = (((ctx.f[24].f64 * ctx.f[10].f64) as f32) as f64);
	// 832C5D24: EDC7B82A  fadds f14, f7, f23
	ctx.f[14].f64 = ((ctx.f[7].f64 + ctx.f[23].f64) as f32) as f64;
	// 832C5D28: D1CA0004  stfs f14, 4(r10)
	tmp.f32 = (ctx.f[14].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5D2C: EC637828  fsubs f3, f3, f15
	ctx.f[3].f64 = (((ctx.f[3].f64 - ctx.f[15].f64) as f32) as f64);
	// 832C5D30: D07F0000  stfs f3, 0(r31)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5D34: EF1F0732  fmuls f24, f31, f28
	ctx.f[24].f64 = (((ctx.f[31].f64 * ctx.f[28].f64) as f32) as f64);
	// 832C5D38: ED6B8028  fsubs f11, f11, f16
	ctx.f[11].f64 = (((ctx.f[11].f64 - ctx.f[16].f64) as f32) as f64);
	// 832C5D3C: ED0C402A  fadds f8, f12, f8
	ctx.f[8].f64 = ((ctx.f[12].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C5D40: EE7F0472  fmuls f19, f31, f17
	ctx.f[19].f64 = (((ctx.f[31].f64 * ctx.f[17].f64) as f32) as f64);
	// 832C5D44: EE56A828  fsubs f18, f22, f21
	ctx.f[18].f64 = (((ctx.f[22].f64 - ctx.f[21].f64) as f32) as f64);
	// 832C5D48: EC74F02A  fadds f3, f20, f30
	ctx.f[3].f64 = ((ctx.f[20].f64 + ctx.f[30].f64) as f32) as f64;
	// 832C5D4C: ECA50828  fsubs f5, f5, f1
	ctx.f[5].f64 = (((ctx.f[5].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C5D50: D0BFFFFC  stfs f5, -4(r31)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C5D54: EC373828  fsubs f1, f23, f7
	ctx.f[1].f64 = (((ctx.f[23].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C5D58: D03F0004  stfs f1, 4(r31)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5D5C: EC843028  fsubs f4, f4, f6
	ctx.f[4].f64 = (((ctx.f[4].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C5D60: D09FFFF8  stfs f4, -8(r31)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(-8 as u32), tmp.u32 ) };
	// 832C5D64: 3BFF0010  addi r31, r31, 0x10
	ctx.r[31].s64 = ctx.r[31].s64 + 16;
	// 832C5D68: ECF9C478  fmsubs f7, f25, f17, f24
	ctx.f[7].f64 = (((ctx.f[25].f64 * ctx.f[17].f64 - ctx.f[24].f64) as f32) as f64);
	// 832C5D6C: D0E3FFF8  stfs f7, -8(r3)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(-8 as u32), tmp.u32 ) };
	// 832C5D70: EC8B0272  fmuls f4, f11, f9
	ctx.f[4].f64 = (((ctx.f[11].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C5D74: ECA80272  fmuls f5, f8, f9
	ctx.f[5].f64 = (((ctx.f[8].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C5D78: ECD99F3A  fmadds f6, f25, f28, f19
	ctx.f[6].f64 = (((ctx.f[25].f64 * ctx.f[28].f64 + ctx.f[19].f64) as f32) as f64);
	// 832C5D7C: D0C3FFFC  stfs f6, -4(r3)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C5D80: FD608050  fneg f11, f16
	ctx.f[11].u64 = ctx.f[16].u64 ^ 0x8000_0000_0000_0000u64;
	// 832C5D84: EC2004B2  fmuls f1, f0, f18
	ctx.f[1].f64 = (((ctx.f[0].f64 * ctx.f[18].f64) as f32) as f64);
	// 832C5D88: ED3DD02A  fadds f9, f29, f26
	ctx.f[9].f64 = ((ctx.f[29].f64 + ctx.f[26].f64) as f32) as f64;
	// 832C5D8C: ED15B02A  fadds f8, f21, f22
	ctx.f[8].f64 = ((ctx.f[21].f64 + ctx.f[22].f64) as f32) as f64;
	// 832C5D90: ECE000F2  fmuls f7, f0, f3
	ctx.f[7].f64 = (((ctx.f[0].f64 * ctx.f[3].f64) as f32) as f64);
	// 832C5D94: EC2D08FA  fmadds f1, f13, f3, f1
	ctx.f[1].f64 = (((ctx.f[13].f64 * ctx.f[3].f64 + ctx.f[1].f64) as f32) as f64);
	// 832C5D98: D0230004  stfs f1, 4(r3)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5D9C: EC650272  fmuls f3, f5, f9
	ctx.f[3].f64 = (((ctx.f[5].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C5DA0: 394A0010  addi r10, r10, 0x10
	ctx.r[10].s64 = ctx.r[10].s64 + 16;
	// 832C5DA4: EC240272  fmuls f1, f4, f9
	ctx.f[1].f64 = (((ctx.f[4].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C5DA8: ECC2D828  fsubs f6, f2, f27
	ctx.f[6].f64 = (((ctx.f[2].f64 - ctx.f[27].f64) as f32) as f64);
	// 832C5DAC: ED2C0232  fmuls f9, f12, f8
	ctx.f[9].f64 = (((ctx.f[12].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C5DB0: EC5EA028  fsubs f2, f30, f20
	ctx.f[2].f64 = (((ctx.f[30].f64 - ctx.f[20].f64) as f32) as f64);
	// 832C5DB4: ED0B0232  fmuls f8, f11, f8
	ctx.f[8].f64 = (((ctx.f[11].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C5DB8: ECED3CB8  fmsubs f7, f13, f18, f7
	ctx.f[7].f64 = (((ctx.f[13].f64 * ctx.f[18].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C5DBC: D0E30000  stfs f7, 0(r3)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5DC0: 38630010  addi r3, r3, 0x10
	ctx.r[3].s64 = ctx.r[3].s64 + 16;
	// 832C5DC4: EC6419BA  fmadds f3, f4, f6, f3
	ctx.f[3].f64 = (((ctx.f[4].f64 * ctx.f[6].f64 + ctx.f[3].f64) as f32) as f64);
	// 832C5DC8: D065FFF8  stfs f3, -8(r5)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[5].u32.wrapping_add(-8 as u32), tmp.u32 ) };
	// 832C5DCC: EC2509B8  fmsubs f1, f5, f6, f1
	ctx.f[1].f64 = (((ctx.f[5].f64 * ctx.f[6].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C5DD0: D025FFFC  stfs f1, -4(r5)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[5].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C5DD4: ED2B48BA  fmadds f9, f11, f2, f9
	ctx.f[9].f64 = (((ctx.f[11].f64 * ctx.f[2].f64 + ctx.f[9].f64) as f32) as f64);
	// 832C5DD8: D1250000  stfs f9, 0(r5)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[5].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5DDC: ED0C40B8  fmsubs f8, f12, f2, f8
	ctx.f[8].f64 = (((ctx.f[12].f64 * ctx.f[2].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C5DE0: D1050004  stfs f8, 4(r5)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[5].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5DE4: C0C60008  lfs f6, 8(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(8 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C5DE8: 38A50010  addi r5, r5, 0x10
	ctx.r[5].s64 = ctx.r[5].s64 + 16;
	// 832C5DEC: C066000C  lfs f3, 0xc(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(12 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C5DF0: C2C70008  lfs f22, 8(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(8 as u32) ) };
	ctx.f[22].f64 = (tmp.f32 as f64);
	// 832C5DF4: C129000C  lfs f9, 0xc(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(12 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C5DF8: C2A80008  lfs f21, 8(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(8 as u32) ) };
	ctx.f[21].f64 = (tmp.f32 as f64);
	// 832C5DFC: C0290008  lfs f1, 8(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(8 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C5E00: C1070000  lfs f8, 0(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(0 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C5E04: C3A60000  lfs f29, 0(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) };
	ctx.f[29].f64 = (tmp.f32 as f64);
	// 832C5E08: C3690000  lfs f27, 0(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) };
	ctx.f[27].f64 = (tmp.f32 as f64);
	// 832C5E0C: C3080000  lfs f24, 0(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	ctx.f[24].f64 = (tmp.f32 as f64);
	// 832C5E10: C3C70004  lfs f30, 4(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(4 as u32) ) };
	ctx.f[30].f64 = (tmp.f32 as f64);
	// 832C5E14: C3860004  lfs f28, 4(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(4 as u32) ) };
	ctx.f[28].f64 = (tmp.f32 as f64);
	// 832C5E18: C3490004  lfs f26, 4(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(4 as u32) ) };
	ctx.f[26].f64 = (tmp.f32 as f64);
	// 832C5E1C: C2E80004  lfs f23, 4(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(4 as u32) ) };
	ctx.f[23].f64 = (tmp.f32 as f64);
	// 832C5E20: C0E7000C  lfs f7, 0xc(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(12 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C5E24: C048000C  lfs f2, 0xc(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(12 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C5E28: EE423828  fsubs f18, f2, f7
	ctx.f[18].f64 = (((ctx.f[2].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C5E2C: EE213028  fsubs f17, f1, f6
	ctx.f[17].f64 = (((ctx.f[1].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C5E30: EE691828  fsubs f19, f9, f3
	ctx.f[19].f64 = (((ctx.f[9].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C5E34: EE95B028  fsubs f20, f21, f22
	ctx.f[20].f64 = (((ctx.f[21].f64 - ctx.f[22].f64) as f32) as f64);
	// 832C5E38: EC63482A  fadds f3, f3, f9
	ctx.f[3].f64 = ((ctx.f[3].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C5E3C: ECE2382A  fadds f7, f2, f7
	ctx.f[7].f64 = ((ctx.f[2].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C5E40: ED3DD82A  fadds f9, f29, f27
	ctx.f[9].f64 = ((ctx.f[29].f64 + ctx.f[27].f64) as f32) as f64;
	// 832C5E44: EC48C02A  fadds f2, f8, f24
	ctx.f[2].f64 = ((ctx.f[8].f64 + ctx.f[24].f64) as f32) as f64;
	// 832C5E48: EC26082A  fadds f1, f6, f1
	ctx.f[1].f64 = ((ctx.f[6].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C5E4C: EED5B02A  fadds f22, f21, f22
	ctx.f[22].f64 = ((ctx.f[21].f64 + ctx.f[22].f64) as f32) as f64;
	// 832C5E50: ECD7F02A  fadds f6, f23, f30
	ctx.f[6].f64 = ((ctx.f[23].f64 + ctx.f[30].f64) as f32) as f64;
	// 832C5E54: EE1CD02A  fadds f16, f28, f26
	ctx.f[16].f64 = ((ctx.f[28].f64 + ctx.f[26].f64) as f32) as f64;
	// 832C5E58: EFBBE828  fsubs f29, f27, f29
	ctx.f[29].f64 = (((ctx.f[27].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C5E5C: ED184028  fsubs f8, f24, f8
	ctx.f[8].f64 = (((ctx.f[24].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C5E60: EF9AE028  fsubs f28, f26, f28
	ctx.f[28].f64 = (((ctx.f[26].f64 - ctx.f[28].f64) as f32) as f64);
	// 832C5E64: EFD7F028  fsubs f30, f23, f30
	ctx.f[30].f64 = (((ctx.f[23].f64 - ctx.f[30].f64) as f32) as f64);
	// 832C5E68: EEB49828  fsubs f21, f20, f19
	ctx.f[21].f64 = (((ctx.f[20].f64 - ctx.f[19].f64) as f32) as f64);
	// 832C5E6C: EDF1902A  fadds f15, f17, f18
	ctx.f[15].f64 = ((ctx.f[17].f64 + ctx.f[18].f64) as f32) as f64;
	// 832C5E70: EF63382A  fadds f27, f3, f7
	ctx.f[27].f64 = ((ctx.f[3].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C5E74: D368000C  stfs f27, 0xc(r8)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C5E78: EF69102A  fadds f27, f9, f2
	ctx.f[27].f64 = ((ctx.f[9].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C5E7C: D3680000  stfs f27, 0(r8)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5E80: EF61B02A  fadds f27, f1, f22
	ctx.f[27].f64 = ((ctx.f[1].f64 + ctx.f[22].f64) as f32) as f64;
	// 832C5E84: D3680008  stfs f27, 8(r8)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C5E88: ECE71828  fsubs f7, f7, f3
	ctx.f[7].f64 = (((ctx.f[7].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C5E8C: EC624828  fsubs f3, f2, f9
	ctx.f[3].f64 = (((ctx.f[2].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C5E90: EF70302A  fadds f27, f16, f6
	ctx.f[27].f64 = ((ctx.f[16].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C5E94: D3680004  stfs f27, 4(r8)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5E98: EC468028  fsubs f2, f6, f16
	ctx.f[2].f64 = (((ctx.f[6].f64 - ctx.f[16].f64) as f32) as f64);
	// 832C5E9C: D0E9000C  stfs f7, 0xc(r9)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C5EA0: D0490004  stfs f2, 4(r9)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5EA4: EC360828  fsubs f1, f22, f1
	ctx.f[1].f64 = (((ctx.f[22].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C5EA8: D0290008  stfs f1, 8(r9)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C5EAC: ECFDF02A  fadds f7, f29, f30
	ctx.f[7].f64 = ((ctx.f[29].f64 + ctx.f[30].f64) as f32) as f64;
	// 832C5EB0: D0690000  stfs f3, 0(r9)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5EB4: ECC8E028  fsubs f6, f8, f28
	ctx.f[6].f64 = (((ctx.f[8].f64 - ctx.f[28].f64) as f32) as f64);
	// 832C5EB8: EC5C402A  fadds f2, f28, f8
	ctx.f[2].f64 = ((ctx.f[28].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C5EBC: 3929FFF0  addi r9, r9, -0x10
	ctx.r[9].s64 = ctx.r[9].s64 + -16;
	// 832C5EC0: EC390572  fmuls f1, f25, f21
	ctx.f[1].f64 = (((ctx.f[25].f64 * ctx.f[21].f64) as f32) as f64);
	// 832C5EC4: 3908FFF0  addi r8, r8, -0x10
	ctx.r[8].s64 = ctx.r[8].s64 + -16;
	// 832C5EC8: ED3903F2  fmuls f9, f25, f15
	ctx.f[9].f64 = (((ctx.f[25].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C5ECC: EC73A02A  fadds f3, f19, f20
	ctx.f[3].f64 = ((ctx.f[19].f64 + ctx.f[20].f64) as f32) as f64;
	// 832C5ED0: ED128828  fsubs f8, f18, f17
	ctx.f[8].f64 = (((ctx.f[18].f64 - ctx.f[17].f64) as f32) as f64);
	// 832C5ED4: EFDEE828  fsubs f30, f30, f29
	ctx.f[30].f64 = (((ctx.f[30].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C5ED8: EC3F0BFA  fmadds f1, f31, f15, f1
	ctx.f[1].f64 = (((ctx.f[31].f64 * ctx.f[15].f64 + ctx.f[1].f64) as f32) as f64);
	// 832C5EDC: D027000C  stfs f1, 0xc(r7)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C5EE0: ED3F4D78  fmsubs f9, f31, f21, f9
	ctx.f[9].f64 = (((ctx.f[31].f64 * ctx.f[21].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C5EE4: D1270008  stfs f9, 8(r7)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C5EE8: EC2D01F2  fmuls f1, f13, f7
	ctx.f[1].f64 = (((ctx.f[13].f64 * ctx.f[7].f64) as f32) as f64);
	// 832C5EEC: EDAD01B2  fmuls f13, f13, f6
	ctx.f[13].f64 = (((ctx.f[13].f64 * ctx.f[6].f64) as f32) as f64);
	// 832C5EF0: ED2400F2  fmuls f9, f4, f3
	ctx.f[9].f64 = (((ctx.f[4].f64 * ctx.f[3].f64) as f32) as f64);
	// 832C5EF4: EC6500F2  fmuls f3, f5, f3
	ctx.f[3].f64 = (((ctx.f[5].f64 * ctx.f[3].f64) as f32) as f64);
	// 832C5EF8: EFEB00B2  fmuls f31, f11, f2
	ctx.f[31].f64 = (((ctx.f[11].f64 * ctx.f[2].f64) as f32) as f64);
	// 832C5EFC: EC4C00B2  fmuls f2, f12, f2
	ctx.f[2].f64 = (((ctx.f[12].f64 * ctx.f[2].f64) as f32) as f64);
	// 832C5F00: EC2009B8  fmsubs f1, f0, f6, f1
	ctx.f[1].f64 = (((ctx.f[0].f64 * ctx.f[6].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C5F04: D0270000  stfs f1, 0(r7)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5F08: EC0069FA  fmadds f0, f0, f7, f13
	ctx.f[0].f64 = (((ctx.f[0].f64 * ctx.f[7].f64 + ctx.f[13].f64) as f32) as f64);
	// 832C5F0C: D0070004  stfs f0, 4(r7)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5F10: EDA54A3A  fmadds f13, f5, f8, f9
	ctx.f[13].f64 = (((ctx.f[5].f64 * ctx.f[8].f64 + ctx.f[9].f64) as f32) as f64);
	// 832C5F14: D1A60008  stfs f13, 8(r6)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[6].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C5F18: ED241A38  fmsubs f9, f4, f8, f3
	ctx.f[9].f64 = (((ctx.f[4].f64 * ctx.f[8].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C5F1C: D126000C  stfs f9, 0xc(r6)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[6].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C5F20: ED0CFFBA  fmadds f8, f12, f30, f31
	ctx.f[8].f64 = (((ctx.f[12].f64 * ctx.f[30].f64 + ctx.f[31].f64) as f32) as f64);
	// 832C5F24: D1060000  stfs f8, 0(r6)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[6].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C5F28: ECEB17B8  fmsubs f7, f11, f30, f2
	ctx.f[7].f64 = (((ctx.f[11].f64 * ctx.f[30].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C5F2C: D0E60004  stfs f7, 4(r6)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[6].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C5F30: FD006090  fmr f8, f12
	ctx.f[8].f64 = ctx.f[12].f64;
	// 832C5F34: C1A1FF14  lfs f13, -0xec(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-236 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C5F38: C181FF18  lfs f12, -0xe8(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-232 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C5F3C: 38E7FFF0  addi r7, r7, -0x10
	ctx.r[7].s64 = ctx.r[7].s64 + -16;
	// 832C5F40: C121FF1C  lfs f9, -0xe4(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-228 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C5F44: 38C6FFF0  addi r6, r6, -0x10
	ctx.r[6].s64 = ctx.r[6].s64 + -16;
	// 832C5F48: 4082FD08  bne 0x832c5c50
	if !ctx.cr[0].eq {
	pc = 0x832C5C50; continue 'dispatch;
	}
	// 832C5F4C: C001FF20  lfs f0, -0xe0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-224 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C5F50: 7CEBF214  add r7, r11, r30
	ctx.r[7].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C5F54: EDAD002A  fadds f13, f13, f0
	ctx.f[13].f64 = ((ctx.f[13].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C5F58: ECEB0028  fsubs f7, f11, f0
	ctx.f[7].f64 = (((ctx.f[11].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C5F5C: 57CA103A  slwi r10, r30, 2
	// 832C5F60: 7CC75A14  add r6, r7, r11
	ctx.r[6].u64 = ctx.r[7].u64 + ctx.r[11].u64;
	// 832C5F64: ED080028  fsubs f8, f8, f0
	ctx.f[8].f64 = (((ctx.f[8].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C5F68: ECCC002A  fadds f6, f12, f0
	ctx.f[6].f64 = ((ctx.f[12].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C5F6C: 54FA103A  slwi r26, r7, 2
	// 832C5F70: 7CA65A14  add r5, r6, r11
	ctx.r[5].u64 = ctx.r[6].u64 + ctx.r[11].u64;
	// 832C5F74: 54C9103A  slwi r9, r6, 2
	// 832C5F78: 54A8103A  slwi r8, r5, 2
	// 832C5F7C: 7D6A2214  add r11, r10, r4
	ctx.r[11].u64 = ctx.r[10].u64 + ctx.r[4].u64;
	// 832C5F80: 3BE7FFFE  addi r31, r7, -2
	ctx.r[31].s64 = ctx.r[7].s64 + -2;
	// 832C5F84: 3865FFFE  addi r3, r5, -2
	ctx.r[3].s64 = ctx.r[5].s64 + -2;
	// 832C5F88: 7D492214  add r10, r9, r4
	ctx.r[10].u64 = ctx.r[9].u64 + ctx.r[4].u64;
	// 832C5F8C: ECAD02B2  fmuls f5, f13, f10
	ctx.f[5].f64 = (((ctx.f[13].f64 * ctx.f[10].f64) as f32) as f64);
	// 832C5F90: 7D282214  add r9, r8, r4
	ctx.r[9].u64 = ctx.r[8].u64 + ctx.r[4].u64;
	// 832C5F94: EC470272  fmuls f2, f7, f9
	ctx.f[2].f64 = (((ctx.f[7].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C5F98: 3B86FFFE  addi r28, r6, -2
	ctx.r[28].s64 = ctx.r[6].s64 + -2;
	// 832C5F9C: C06BFFFC  lfs f3, -4(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-4 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C5FA0: 7D1A2214  add r8, r26, r4
	ctx.r[8].u64 = ctx.r[26].u64 + ctx.r[4].u64;
	// 832C5FA4: EC880272  fmuls f4, f8, f9
	ctx.f[4].f64 = (((ctx.f[8].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C5FA8: 5463103A  slwi r3, r3, 2
	// 832C5FAC: EC2602B2  fmuls f1, f6, f10
	ctx.f[1].f64 = (((ctx.f[6].f64 * ctx.f[10].f64) as f32) as f64);
	// 832C5FB0: 57FF103A  slwi r31, r31, 2
	// 832C5FB4: C1AAFFFC  lfs f13, -4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(-4 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C5FB8: 577D103A  slwi r29, r27, 2
	// 832C5FBC: C189FFFC  lfs f12, -4(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(-4 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C5FC0: 579C103A  slwi r28, r28, 2
	// 832C5FC4: ECC3682A  fadds f6, f3, f13
	ctx.f[6].f64 = ((ctx.f[3].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C5FC8: C168FFFC  lfs f11, -4(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(-4 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C5FCC: ED436828  fsubs f10, f3, f13
	ctx.f[10].f64 = (((ctx.f[3].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C5FD0: 7D23242E  lfsx f9, r3, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[3].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C5FD4: ED0B6028  fsubs f8, f11, f12
	ctx.f[8].f64 = (((ctx.f[11].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C5FD8: 7CFF242E  lfsx f7, r31, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[31].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C5FDC: ED8B602A  fadds f12, f11, f12
	ctx.f[12].f64 = ((ctx.f[11].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C5FE0: 7C7D242E  lfsx f3, r29, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[29].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C5FE4: EDA74828  fsubs f13, f7, f9
	ctx.f[13].f64 = (((ctx.f[7].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C5FE8: 7FFC242E  lfsx f31, r28, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[28].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832C5FEC: ED27482A  fadds f9, f7, f9
	ctx.f[9].f64 = ((ctx.f[7].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C5FF0: ED63F828  fsubs f11, f3, f31
	ctx.f[11].f64 = (((ctx.f[3].f64 - ctx.f[31].f64) as f32) as f64);
	// 832C5FF4: 3B650002  addi r27, r5, 2
	ctx.r[27].s64 = ctx.r[5].s64 + 2;
	// 832C5FF8: ECE3F82A  fadds f7, f3, f31
	ctx.f[7].f64 = ((ctx.f[3].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C5FFC: 3B270002  addi r25, r7, 2
	ctx.r[25].s64 = ctx.r[7].s64 + 2;
	// 832C6000: EC6D502A  fadds f3, f13, f10
	ctx.f[3].f64 = ((ctx.f[13].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6004: EDAA6828  fsubs f13, f10, f13
	ctx.f[13].f64 = (((ctx.f[10].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C6008: ED4B4028  fsubs f10, f11, f8
	ctx.f[10].f64 = (((ctx.f[11].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C600C: ED08582A  fadds f8, f8, f11
	ctx.f[8].f64 = ((ctx.f[8].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C6010: ED6C302A  fadds f11, f12, f6
	ctx.f[11].f64 = ((ctx.f[12].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C6014: D16BFFFC  stfs f11, -4(r11)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C6018: ED69382A  fadds f11, f9, f7
	ctx.f[11].f64 = ((ctx.f[9].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C601C: 7D7D252E  stfsx f11, r29, r4
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[29].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C6020: ECC66028  fsubs f6, f6, f12
	ctx.f[6].f64 = (((ctx.f[6].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C6024: D0C8FFFC  stfs f6, -4(r8)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C6028: ED874828  fsubs f12, f7, f9
	ctx.f[12].f64 = (((ctx.f[7].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C602C: 7D9F252E  stfsx f12, r31, r4
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[31].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C6030: ED6500F2  fmuls f11, f5, f3
	ctx.f[11].f64 = (((ctx.f[5].f64 * ctx.f[3].f64) as f32) as f64);
	// 832C6034: ED2502B2  fmuls f9, f5, f10
	ctx.f[9].f64 = (((ctx.f[5].f64 * ctx.f[10].f64) as f32) as f64);
	// 832C6038: ECE40232  fmuls f7, f4, f8
	ctx.f[7].f64 = (((ctx.f[4].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C603C: ECC20232  fmuls f6, f2, f8
	ctx.f[6].f64 = (((ctx.f[2].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C6040: ED815AB8  fmsubs f12, f1, f10, f11
	ctx.f[12].f64 = (((ctx.f[1].f64 * ctx.f[10].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C6044: 7D9C252E  stfsx f12, r28, r4
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[28].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C6048: ED6148FA  fmadds f11, f1, f3, f9
	ctx.f[11].f64 = (((ctx.f[1].f64 * ctx.f[3].f64 + ctx.f[9].f64) as f32) as f64);
	// 832C604C: D16AFFFC  stfs f11, -4(r10)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C6050: ED423B7A  fmadds f10, f2, f13, f7
	ctx.f[10].f64 = (((ctx.f[2].f64 * ctx.f[13].f64 + ctx.f[7].f64) as f32) as f64);
	// 832C6054: 7D43252E  stfsx f10, r3, r4
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[3].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C6058: ED243378  fmsubs f9, f4, f13, f6
	ctx.f[9].f64 = (((ctx.f[4].f64 * ctx.f[13].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C605C: D129FFFC  stfs f9, -4(r9)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(-4 as u32), tmp.u32 ) };
	// 832C6060: C10B0004  lfs f8, 4(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C6064: C0EA0004  lfs f7, 4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C6068: C0C90000  lfs f6, 0(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C606C: 7C7A242E  lfsx f3, r26, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[26].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C6070: C18A0000  lfs f12, 0(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C6074: C1AB0000  lfs f13, 0(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C6078: ED6D602A  fadds f11, f13, f12
	ctx.f[11].f64 = ((ctx.f[13].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C607C: EDAD6028  fsubs f13, f13, f12
	ctx.f[13].f64 = (((ctx.f[13].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C6080: C1480004  lfs f10, 4(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(4 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C6084: ED28382A  fadds f9, f8, f7
	ctx.f[9].f64 = ((ctx.f[8].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C6088: C3E90004  lfs f31, 4(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(4 as u32) ) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832C608C: ED883828  fsubs f12, f8, f7
	ctx.f[12].f64 = (((ctx.f[8].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C6090: ED033028  fsubs f8, f3, f6
	ctx.f[8].f64 = (((ctx.f[3].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C6094: ECEAF828  fsubs f7, f10, f31
	ctx.f[7].f64 = (((ctx.f[10].f64 - ctx.f[31].f64) as f32) as f64);
	// 832C6098: 3BE50003  addi r31, r5, 3
	ctx.r[31].s64 = ctx.r[5].s64 + 3;
	// 832C609C: EFC8602A  fadds f30, f8, f12
	ctx.f[30].f64 = ((ctx.f[8].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C60A0: 3BA70003  addi r29, r7, 3
	ctx.r[29].s64 = ctx.r[7].s64 + 3;
	// 832C60A4: ED8C4028  fsubs f12, f12, f8
	ctx.f[12].f64 = (((ctx.f[12].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C60A8: 387E0003  addi r3, r30, 3
	ctx.r[3].s64 = ctx.r[30].s64 + 3;
	// 832C60AC: ED03302A  fadds f8, f3, f6
	ctx.f[8].f64 = ((ctx.f[3].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C60B0: 3B060002  addi r24, r6, 2
	ctx.r[24].s64 = ctx.r[6].s64 + 2;
	// 832C60B4: ECCAF82A  fadds f6, f10, f31
	ctx.f[6].f64 = ((ctx.f[10].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C60B8: 5767103A  slwi r7, r27, 2
	// 832C60BC: FC600050  fneg f3, f0
	ctx.f[3].u64 = ctx.f[0].u64 ^ 0x8000_0000_0000_0000u64;
	// 832C60C0: 5725103A  slwi r5, r25, 2
	// 832C60C4: 3B860003  addi r28, r6, 3
	ctx.r[28].s64 = ctx.r[6].s64 + 3;
	// 832C60C8: 5463103A  slwi r3, r3, 2
	// 832C60CC: 5786103A  slwi r6, r28, 2
	// 832C60D0: 3BDE0002  addi r30, r30, 2
	ctx.r[30].s64 = ctx.r[30].s64 + 2;
	// 832C60D4: ED4D3828  fsubs f10, f13, f7
	ctx.f[10].f64 = (((ctx.f[13].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C60D8: 57FF103A  slwi r31, r31, 2
	// 832C60DC: ECE7682A  fadds f7, f7, f13
	ctx.f[7].f64 = ((ctx.f[7].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C60E0: 57BD103A  slwi r29, r29, 2
	// 832C60E4: EDA8582A  fadds f13, f8, f11
	ctx.f[13].f64 = ((ctx.f[8].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C60E8: D1AB0000  stfs f13, 0(r11)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C60EC: EDA6482A  fadds f13, f6, f9
	ctx.f[13].f64 = ((ctx.f[6].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C60F0: D1AB0004  stfs f13, 4(r11)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C60F4: ED293028  fsubs f9, f9, f6
	ctx.f[9].f64 = (((ctx.f[9].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C60F8: D1280004  stfs f9, 4(r8)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C60FC: ED6B4028  fsubs f11, f11, f8
	ctx.f[11].f64 = (((ctx.f[11].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C6100: 7D7A252E  stfsx f11, r26, r4
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[26].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C6104: 5708103A  slwi r8, r24, 2
	// 832C6108: 57CB103A  slwi r11, r30, 2
	// 832C610C: ED0AF028  fsubs f8, f10, f30
	ctx.f[8].f64 = (((ctx.f[10].f64 - ctx.f[30].f64) as f32) as f64);
	// 832C6110: EDAC382A  fadds f13, f12, f7
	ctx.f[13].f64 = ((ctx.f[12].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C6114: ECDE502A  fadds f6, f30, f10
	ctx.f[6].f64 = ((ctx.f[30].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6118: ED8C3828  fsubs f12, f12, f7
	ctx.f[12].f64 = (((ctx.f[12].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C611C: ED680032  fmuls f11, f8, f0
	ctx.f[11].f64 = (((ctx.f[8].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C6120: D16A0000  stfs f11, 0(r10)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6124: ED2D00F2  fmuls f9, f13, f3
	ctx.f[9].f64 = (((ctx.f[13].f64 * ctx.f[3].f64) as f32) as f64);
	// 832C6128: ED460032  fmuls f10, f6, f0
	ctx.f[10].f64 = (((ctx.f[6].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C612C: D14A0004  stfs f10, 4(r10)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6130: ED0C00F2  fmuls f8, f12, f3
	ctx.f[8].f64 = (((ctx.f[12].f64 * ctx.f[3].f64) as f32) as f64);
	// 832C6134: D1290000  stfs f9, 0(r9)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6138: D1090004  stfs f8, 4(r9)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C613C: 7DBD242E  lfsx f13, r29, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[29].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C6140: 7D63242E  lfsx f11, r3, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[3].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C6144: 7CE7242E  lfsx f7, r7, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C6148: 7D26242E  lfsx f9, r6, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[6].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C614C: 7C05242E  lfsx f0, r5, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[5].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C6150: ED00382A  fadds f8, f0, f7
	ctx.f[8].f64 = ((ctx.f[0].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C6154: 7C6B242E  lfsx f3, r11, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[11].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C6158: ED803828  fsubs f12, f0, f7
	ctx.f[12].f64 = (((ctx.f[0].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C615C: 7CE8242E  lfsx f7, r8, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C6160: 7CDF242E  lfsx f6, r31, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[31].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C6164: ED4D3028  fsubs f10, f13, f6
	ctx.f[10].f64 = (((ctx.f[13].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C6168: ECCD302A  fadds f6, f13, f6
	ctx.f[6].f64 = ((ctx.f[13].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C616C: EC0B4828  fsubs f0, f11, f9
	ctx.f[0].f64 = (((ctx.f[11].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C6170: EFE33828  fsubs f31, f3, f7
	ctx.f[31].f64 = (((ctx.f[3].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C6174: ECE3382A  fadds f7, f3, f7
	ctx.f[7].f64 = ((ctx.f[3].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C6178: EC6B482A  fadds f3, f11, f9
	ctx.f[3].f64 = ((ctx.f[11].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C617C: EFCC002A  fadds f30, f12, f0
	ctx.f[30].f64 = ((ctx.f[12].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C6180: EC006028  fsubs f0, f0, f12
	ctx.f[0].f64 = (((ctx.f[0].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C6184: EFBF5028  fsubs f29, f31, f10
	ctx.f[29].f64 = (((ctx.f[31].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C6188: ED4AF82A  fadds f10, f10, f31
	ctx.f[10].f64 = ((ctx.f[10].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C618C: ED28382A  fadds f9, f8, f7
	ctx.f[9].f64 = ((ctx.f[8].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C6190: 7D2B252E  stfsx f9, r11, r4
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[11].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C6194: ED274028  fsubs f9, f7, f8
	ctx.f[9].f64 = (((ctx.f[7].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C6198: ED033028  fsubs f8, f3, f6
	ctx.f[8].f64 = (((ctx.f[3].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C619C: EDA107B2  fmuls f13, f1, f30
	ctx.f[13].f64 = (((ctx.f[1].f64 * ctx.f[30].f64) as f32) as f64);
	// 832C61A0: ED810772  fmuls f12, f1, f29
	ctx.f[12].f64 = (((ctx.f[1].f64 * ctx.f[29].f64) as f32) as f64);
	// 832C61A4: ED6202B2  fmuls f11, f2, f10
	ctx.f[11].f64 = (((ctx.f[2].f64 * ctx.f[10].f64) as f32) as f64);
	// 832C61A8: ED4402B2  fmuls f10, f4, f10
	ctx.f[10].f64 = (((ctx.f[4].f64 * ctx.f[10].f64) as f32) as f64);
	// 832C61AC: EC26182A  fadds f1, f6, f3
	ctx.f[1].f64 = ((ctx.f[6].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C61B0: 7C23252E  stfsx f1, r3, r4
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[3].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C61B4: 7D25252E  stfsx f9, r5, r4
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[5].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C61B8: 7D1D252E  stfsx f8, r29, r4
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[29].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C61BC: ECE56F78  fmsubs f7, f5, f29, f13
	ctx.f[7].f64 = (((ctx.f[5].f64 * ctx.f[29].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C61C0: 7CE8252E  stfsx f7, r8, r4
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[8].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C61C4: ECC567BA  fmadds f6, f5, f30, f12
	ctx.f[6].f64 = (((ctx.f[5].f64 * ctx.f[30].f64 + ctx.f[12].f64) as f32) as f64);
	// 832C61C8: 7CC6252E  stfsx f6, r6, r4
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[6].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C61CC: ECA4583A  fmadds f5, f4, f0, f11
	ctx.f[5].f64 = (((ctx.f[4].f64 * ctx.f[0].f64 + ctx.f[11].f64) as f32) as f64);
	// 832C61D0: 7CA7252E  stfsx f5, r7, r4
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[7].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C61D4: EC825038  fmsubs f4, f2, f0, f10
	ctx.f[4].f64 = (((ctx.f[2].f64 * ctx.f[0].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C61D8: 7C9F252E  stfsx f4, r31, r4
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[31].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C61DC: 3981FFB8  addi r12, r1, -0x48
	ctx.r[12].s64 = ctx.r[1].s64 + -72;
	// 832C61E0: 4B9E7B0D  bl 0x82cadcec
	ctx.lr = 0x832C61E4;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCEC);
	// 832C61E4: 4B9E3264  b 0x82ca9448
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9448);
	return;
}

pub fn sub_832C61E8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C61E8 size=216
	// 832C61E8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C61EC: 4B9E3219  bl 0x82ca9404
	ctx.lr = 0x832C61F0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9404);
	// 832C61F0: 9421FF80  stwu r1, -0x80(r1)
	ea = ctx.r[1].u32.wrapping_add(-128 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C61F4: 7C7B1B78  mr r27, r3
	ctx.r[27].u64 = ctx.r[3].u64;
	// 832C61F8: 7CBD2B78  mr r29, r5
	ctx.r[29].u64 = ctx.r[5].u64;
	// 832C61FC: 7F7F1670  srawi r31, r27, 2
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 2) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[27].s32 >> 2) as i64;
	// 832C6200: 7CDC3378  mr r28, r6
	ctx.r[28].u64 = ctx.r[6].u64;
	// 832C6204: 57EB083C  slwi r11, r31, 1
	// 832C6208: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832C620C: 7D4BE850  subf r10, r11, r29
	ctx.r[10].s64 = ctx.r[29].s64 - ctx.r[11].s64;
	// 832C6210: 554B103A  slwi r11, r10, 2
	// 832C6214: 7CABE214  add r5, r11, r28
	ctx.r[5].u64 = ctx.r[11].u64 + ctx.r[28].u64;
	// 832C6218: 48000601  bl 0x832c6818
	ctx.lr = 0x832C621C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C621C: 2F1B0200  cmpwi cr6, r27, 0x200
	ctx.cr[6].compare_i32(ctx.r[27].s32, 512, &mut ctx.xer);
	// 832C6220: 40990084  ble cr6, 0x832c62a4
	if !ctx.cr[6].gt {
	pc = 0x832C62A4; continue 'dispatch;
	}
	// 832C6224: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832C6228: 7F86E378  mr r6, r28
	ctx.r[6].u64 = ctx.r[28].u64;
	// 832C622C: 7FA5EB78  mr r5, r29
	ctx.r[5].u64 = ctx.r[29].u64;
	// 832C6230: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6234: 4BFFFFB5  bl 0x832c61e8
	ctx.lr = 0x832C6238;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C61E8);
	// 832C6238: 57EB103A  slwi r11, r31, 2
	// 832C623C: 7F86E378  mr r6, r28
	ctx.r[6].u64 = ctx.r[28].u64;
	// 832C6240: 7C8BF214  add r4, r11, r30
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C6244: 7FA5EB78  mr r5, r29
	ctx.r[5].u64 = ctx.r[29].u64;
	// 832C6248: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C624C: 48000075  bl 0x832c62c0
	ctx.lr = 0x832C6250;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C62C0);
	// 832C6250: 57EB1838  slwi r11, r31, 3
	// 832C6254: 7F86E378  mr r6, r28
	ctx.r[6].u64 = ctx.r[28].u64;
	// 832C6258: 7C8BF214  add r4, r11, r30
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C625C: 7FA5EB78  mr r5, r29
	ctx.r[5].u64 = ctx.r[29].u64;
	// 832C6260: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6264: 4BFFFF85  bl 0x832c61e8
	ctx.lr = 0x832C6268;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C61E8);
	// 832C6268: 57EB083C  slwi r11, r31, 1
	// 832C626C: 7FFBFB78  mr r27, r31
	ctx.r[27].u64 = ctx.r[31].u64;
	// 832C6270: 7D7F5A14  add r11, r31, r11
	ctx.r[11].u64 = ctx.r[31].u64 + ctx.r[11].u64;
	// 832C6274: 7FFF1670  srawi r31, r31, 2
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 2) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[31].s32 >> 2) as i64;
	// 832C6278: 556B103A  slwi r11, r11, 2
	// 832C627C: 57EA083C  slwi r10, r31, 1
	// 832C6280: 7FCBF214  add r30, r11, r30
	ctx.r[30].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C6284: 7D2AE850  subf r9, r10, r29
	ctx.r[9].s64 = ctx.r[29].s64 - ctx.r[10].s64;
	// 832C6288: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832C628C: 552B103A  slwi r11, r9, 2
	// 832C6290: 7F63DB78  mr r3, r27
	ctx.r[3].u64 = ctx.r[27].u64;
	// 832C6294: 7CABE214  add r5, r11, r28
	ctx.r[5].u64 = ctx.r[11].u64 + ctx.r[28].u64;
	// 832C6298: 48000581  bl 0x832c6818
	ctx.lr = 0x832C629C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C629C: 2F1B0200  cmpwi cr6, r27, 0x200
	ctx.cr[6].compare_i32(ctx.r[27].s32, 512, &mut ctx.xer);
	// 832C62A0: 4199FF84  bgt cr6, 0x832c6224
	if ctx.cr[6].gt {
	pc = 0x832C6224; continue 'dispatch;
	}
	// 832C62A4: 7F86E378  mr r6, r28
	ctx.r[6].u64 = ctx.r[28].u64;
	// 832C62A8: 7FA5EB78  mr r5, r29
	ctx.r[5].u64 = ctx.r[29].u64;
	// 832C62AC: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832C62B0: 7F63DB78  mr r3, r27
	ctx.r[3].u64 = ctx.r[27].u64;
	// 832C62B4: 480000DD  bl 0x832c6390
	ctx.lr = 0x832C62B8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6390);
	// 832C62B8: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832C62BC: 4B9E3198  b 0x82ca9454
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9454);
	return;
}

pub fn sub_832C62C0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C62C0 size=208
	// 832C62C0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C62C4: 4B9E3141  bl 0x82ca9404
	ctx.lr = 0x832C62C8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9404);
	// 832C62C8: 9421FF80  stwu r1, -0x80(r1)
	ea = ctx.r[1].u32.wrapping_add(-128 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C62CC: 7C7D1B78  mr r29, r3
	ctx.r[29].u64 = ctx.r[3].u64;
	// 832C62D0: 7CBC2B78  mr r28, r5
	ctx.r[28].u64 = ctx.r[5].u64;
	// 832C62D4: 7CDB3378  mr r27, r6
	ctx.r[27].u64 = ctx.r[6].u64;
	// 832C62D8: 7D7DE050  subf r11, r29, r28
	ctx.r[11].s64 = ctx.r[28].s64 - ctx.r[29].s64;
	// 832C62DC: 7C9E2378  mr r30, r4
	ctx.r[30].u64 = ctx.r[4].u64;
	// 832C62E0: 556B103A  slwi r11, r11, 2
	// 832C62E4: 7FBF1670  srawi r31, r29, 2
	ctx.xer.ca = (ctx.r[29].s32 < 0) && ((ctx.r[29].u32 & ((1u32 << 2) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[29].s32 >> 2) as i64;
	// 832C62E8: 7CABDA14  add r5, r11, r27
	ctx.r[5].u64 = ctx.r[11].u64 + ctx.r[27].u64;
	// 832C62EC: 480008B5  bl 0x832c6ba0
	ctx.lr = 0x832C62F0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6BA0);
	// 832C62F0: 2F1D0200  cmpwi cr6, r29, 0x200
	ctx.cr[6].compare_i32(ctx.r[29].s32, 512, &mut ctx.xer);
	// 832C62F4: 40990080  ble cr6, 0x832c6374
	if !ctx.cr[6].gt {
	pc = 0x832C6374; continue 'dispatch;
	}
	// 832C62F8: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832C62FC: 7F66DB78  mr r6, r27
	ctx.r[6].u64 = ctx.r[27].u64;
	// 832C6300: 7F85E378  mr r5, r28
	ctx.r[5].u64 = ctx.r[28].u64;
	// 832C6304: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6308: 4BFFFEE1  bl 0x832c61e8
	ctx.lr = 0x832C630C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C61E8);
	// 832C630C: 57EB103A  slwi r11, r31, 2
	// 832C6310: 7F66DB78  mr r6, r27
	ctx.r[6].u64 = ctx.r[27].u64;
	// 832C6314: 7C8BF214  add r4, r11, r30
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C6318: 7F85E378  mr r5, r28
	ctx.r[5].u64 = ctx.r[28].u64;
	// 832C631C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6320: 4BFFFFA1  bl 0x832c62c0
	ctx.lr = 0x832C6324;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C62C0);
	// 832C6324: 57EB1838  slwi r11, r31, 3
	// 832C6328: 7F66DB78  mr r6, r27
	ctx.r[6].u64 = ctx.r[27].u64;
	// 832C632C: 7C8BF214  add r4, r11, r30
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C6330: 7F85E378  mr r5, r28
	ctx.r[5].u64 = ctx.r[28].u64;
	// 832C6334: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6338: 4BFFFEB1  bl 0x832c61e8
	ctx.lr = 0x832C633C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C61E8);
	// 832C633C: 57EB083C  slwi r11, r31, 1
	// 832C6340: 7FFDFB78  mr r29, r31
	ctx.r[29].u64 = ctx.r[31].u64;
	// 832C6344: 7D7F5A14  add r11, r31, r11
	ctx.r[11].u64 = ctx.r[31].u64 + ctx.r[11].u64;
	// 832C6348: 7D3DE050  subf r9, r29, r28
	ctx.r[9].s64 = ctx.r[28].s64 - ctx.r[29].s64;
	// 832C634C: 556A103A  slwi r10, r11, 2
	// 832C6350: 552B103A  slwi r11, r9, 2
	// 832C6354: 7FCAF214  add r30, r10, r30
	ctx.r[30].u64 = ctx.r[10].u64 + ctx.r[30].u64;
	// 832C6358: 7CABDA14  add r5, r11, r27
	ctx.r[5].u64 = ctx.r[11].u64 + ctx.r[27].u64;
	// 832C635C: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832C6360: 7FA3EB78  mr r3, r29
	ctx.r[3].u64 = ctx.r[29].u64;
	// 832C6364: 7FFF1670  srawi r31, r31, 2
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 2) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[31].s32 >> 2) as i64;
	// 832C6368: 48000839  bl 0x832c6ba0
	ctx.lr = 0x832C636C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6BA0);
	// 832C636C: 2F1D0200  cmpwi cr6, r29, 0x200
	ctx.cr[6].compare_i32(ctx.r[29].s32, 512, &mut ctx.xer);
	// 832C6370: 4199FF88  bgt cr6, 0x832c62f8
	if ctx.cr[6].gt {
	pc = 0x832C62F8; continue 'dispatch;
	}
	// 832C6374: 7F66DB78  mr r6, r27
	ctx.r[6].u64 = ctx.r[27].u64;
	// 832C6378: 7F85E378  mr r5, r28
	ctx.r[5].u64 = ctx.r[28].u64;
	// 832C637C: 7FC4F378  mr r4, r30
	ctx.r[4].u64 = ctx.r[30].u64;
	// 832C6380: 7FA3EB78  mr r3, r29
	ctx.r[3].u64 = ctx.r[29].u64;
	// 832C6384: 4800023D  bl 0x832c65c0
	ctx.lr = 0x832C6388;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C65C0);
	// 832C6388: 38210080  addi r1, r1, 0x80
	ctx.r[1].s64 = ctx.r[1].s64 + 128;
	// 832C638C: 4B9E30C8  b 0x82ca9454
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9454);
	return;
}

pub fn sub_832C6390(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C6390 size=560
	// 832C6390: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C6394: 4B9E304D  bl 0x82ca93e0
	ctx.lr = 0x832C6398;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93E0);
	// 832C6398: 9421FF30  stwu r1, -0xd0(r1)
	ea = ctx.r[1].u32.wrapping_add(-208 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C639C: 7C721B78  mr r18, r3
	ctx.r[18].u64 = ctx.r[3].u64;
	// 832C63A0: 7C952378  mr r21, r4
	ctx.r[21].u64 = ctx.r[4].u64;
	// 832C63A4: 7E5F1670  srawi r31, r18, 2
	ctx.xer.ca = (ctx.r[18].s32 < 0) && ((ctx.r[18].u32 & ((1u32 << 2) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[18].s32 >> 2) as i64;
	// 832C63A8: 7CB92B78  mr r25, r5
	ctx.r[25].u64 = ctx.r[5].u64;
	// 832C63AC: 7CD83378  mr r24, r6
	ctx.r[24].u64 = ctx.r[6].u64;
	// 832C63B0: 2F1F0080  cmpwi cr6, r31, 0x80
	ctx.cr[6].compare_i32(ctx.r[31].s32, 128, &mut ctx.xer);
	// 832C63B4: 409900E8  ble cr6, 0x832c649c
	if !ctx.cr[6].gt {
	pc = 0x832C649C; continue 'dispatch;
	}
	// 832C63B8: 7FF7FB78  mr r23, r31
	ctx.r[23].u64 = ctx.r[31].u64;
	// 832C63BC: 7F1F9000  cmpw cr6, r31, r18
	ctx.cr[6].compare_i32(ctx.r[31].s32, ctx.r[18].s32, &mut ctx.xer);
	// 832C63C0: 409800AC  bge cr6, 0x832c646c
	if !ctx.cr[6].lt {
	pc = 0x832C646C; continue 'dispatch;
	}
	// 832C63C4: 7FDFB850  subf r30, r31, r23
	ctx.r[30].s64 = ctx.r[23].s64 - ctx.r[31].s64;
	// 832C63C8: 7F1E9000  cmpw cr6, r30, r18
	ctx.cr[6].compare_i32(ctx.r[30].s32, ctx.r[18].s32, &mut ctx.xer);
	// 832C63CC: 40980094  bge cr6, 0x832c6460
	if !ctx.cr[6].lt {
	pc = 0x832C6460; continue 'dispatch;
	}
	// 832C63D0: 56EB083C  slwi r11, r23, 1
	// 832C63D4: 7FEA0E70  srawi r10, r31, 1
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[10].s64 = (ctx.r[31].s32 >> 1) as i64;
	// 832C63D8: 7CCBF214  add r6, r11, r30
	ctx.r[6].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C63DC: 7C8AC850  subf r4, r10, r25
	ctx.r[4].s64 = ctx.r[25].s64 - ctx.r[10].s64;
	// 832C63E0: 7CBFC850  subf r5, r31, r25
	ctx.r[5].s64 = ctx.r[25].s64 - ctx.r[31].s64;
	// 832C63E4: 7C7EBA14  add r3, r30, r23
	ctx.r[3].u64 = ctx.r[30].u64 + ctx.r[23].u64;
	// 832C63E8: 54A7103A  slwi r7, r5, 2
	// 832C63EC: 5488103A  slwi r8, r4, 2
	// 832C63F0: 57C9103A  slwi r9, r30, 2
	// 832C63F4: 54CA103A  slwi r10, r6, 2
	// 832C63F8: 546B103A  slwi r11, r3, 2
	// 832C63FC: 7E87C214  add r20, r7, r24
	ctx.r[20].u64 = ctx.r[7].u64 + ctx.r[24].u64;
	// 832C6400: 7EC8C214  add r22, r8, r24
	ctx.r[22].u64 = ctx.r[8].u64 + ctx.r[24].u64;
	// 832C6404: 56F3103A  slwi r19, r23, 2
	// 832C6408: 56FA2036  slwi r26, r23, 4
	// 832C640C: 7FA9AA14  add r29, r9, r21
	ctx.r[29].u64 = ctx.r[9].u64 + ctx.r[21].u64;
	// 832C6410: 7F6AAA14  add r27, r10, r21
	ctx.r[27].u64 = ctx.r[10].u64 + ctx.r[21].u64;
	// 832C6414: 7F8BAA14  add r28, r11, r21
	ctx.r[28].u64 = ctx.r[11].u64 + ctx.r[21].u64;
	// 832C6418: 7EC5B378  mr r5, r22
	ctx.r[5].u64 = ctx.r[22].u64;
	// 832C641C: 7FA4EB78  mr r4, r29
	ctx.r[4].u64 = ctx.r[29].u64;
	// 832C6420: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6424: 480003F5  bl 0x832c6818
	ctx.lr = 0x832C6428;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C6428: 7E85A378  mr r5, r20
	ctx.r[5].u64 = ctx.r[20].u64;
	// 832C642C: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832C6430: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6434: 4800076D  bl 0x832c6ba0
	ctx.lr = 0x832C6438;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6BA0);
	// 832C6438: 7EC5B378  mr r5, r22
	ctx.r[5].u64 = ctx.r[22].u64;
	// 832C643C: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832C6440: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6444: 480003D5  bl 0x832c6818
	ctx.lr = 0x832C6448;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C6448: 7FD3F214  add r30, r19, r30
	ctx.r[30].u64 = ctx.r[19].u64 + ctx.r[30].u64;
	// 832C644C: 7FBAEA14  add r29, r26, r29
	ctx.r[29].u64 = ctx.r[26].u64 + ctx.r[29].u64;
	// 832C6450: 7F9AE214  add r28, r26, r28
	ctx.r[28].u64 = ctx.r[26].u64 + ctx.r[28].u64;
	// 832C6454: 7F7ADA14  add r27, r26, r27
	ctx.r[27].u64 = ctx.r[26].u64 + ctx.r[27].u64;
	// 832C6458: 7F1E9000  cmpw cr6, r30, r18
	ctx.cr[6].compare_i32(ctx.r[30].s32, ctx.r[18].s32, &mut ctx.xer);
	// 832C645C: 4198FFBC  blt cr6, 0x832c6418
	if ctx.cr[6].lt {
	pc = 0x832C6418; continue 'dispatch;
	}
	// 832C6460: 56F7103A  slwi r23, r23, 2
	// 832C6464: 7F179000  cmpw cr6, r23, r18
	ctx.cr[6].compare_i32(ctx.r[23].s32, ctx.r[18].s32, &mut ctx.xer);
	// 832C6468: 4198FF5C  blt cr6, 0x832c63c4
	if ctx.cr[6].lt {
	pc = 0x832C63C4; continue 'dispatch;
	}
	// 832C646C: 7FEB0E70  srawi r11, r31, 1
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[11].s64 = (ctx.r[31].s32 >> 1) as i64;
	// 832C6470: 7D5F9050  subf r10, r31, r18
	ctx.r[10].s64 = ctx.r[18].s64 - ctx.r[31].s64;
	// 832C6474: 7D2BC850  subf r9, r11, r25
	ctx.r[9].s64 = ctx.r[25].s64 - ctx.r[11].s64;
	// 832C6478: 554A103A  slwi r10, r10, 2
	// 832C647C: 552B103A  slwi r11, r9, 2
	// 832C6480: 7C8AAA14  add r4, r10, r21
	ctx.r[4].u64 = ctx.r[10].u64 + ctx.r[21].u64;
	// 832C6484: 7CABC214  add r5, r11, r24
	ctx.r[5].u64 = ctx.r[11].u64 + ctx.r[24].u64;
	// 832C6488: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C648C: 4800038D  bl 0x832c6818
	ctx.lr = 0x832C6490;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C6490: 7FFF1670  srawi r31, r31, 2
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 2) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[31].s32 >> 2) as i64;
	// 832C6494: 2F1F0080  cmpwi cr6, r31, 0x80
	ctx.cr[6].compare_i32(ctx.r[31].s32, 128, &mut ctx.xer);
	// 832C6498: 4199FF20  bgt cr6, 0x832c63b8
	if ctx.cr[6].gt {
	pc = 0x832C63B8; continue 'dispatch;
	}
	// 832C649C: 7FF6FB78  mr r22, r31
	ctx.r[22].u64 = ctx.r[31].u64;
	// 832C64A0: 7F1F9000  cmpw cr6, r31, r18
	ctx.cr[6].compare_i32(ctx.r[31].s32, ctx.r[18].s32, &mut ctx.xer);
	// 832C64A4: 409800DC  bge cr6, 0x832c6580
	if !ctx.cr[6].lt {
	pc = 0x832C6580; continue 'dispatch;
	}
	// 832C64A8: 7FDFB050  subf r30, r31, r22
	ctx.r[30].s64 = ctx.r[22].s64 - ctx.r[31].s64;
	// 832C64AC: 7F1E9000  cmpw cr6, r30, r18
	ctx.cr[6].compare_i32(ctx.r[30].s32, ctx.r[18].s32, &mut ctx.xer);
	// 832C64B0: 409800C4  bge cr6, 0x832c6574
	if !ctx.cr[6].lt {
	pc = 0x832C6574; continue 'dispatch;
	}
	// 832C64B4: 56CB083C  slwi r11, r22, 1
	// 832C64B8: 7FEA0E70  srawi r10, r31, 1
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[10].s64 = (ctx.r[31].s32 >> 1) as i64;
	// 832C64BC: 7CCBF214  add r6, r11, r30
	ctx.r[6].u64 = ctx.r[11].u64 + ctx.r[30].u64;
	// 832C64C0: 7C8AC850  subf r4, r10, r25
	ctx.r[4].s64 = ctx.r[25].s64 - ctx.r[10].s64;
	// 832C64C4: 7CBFC850  subf r5, r31, r25
	ctx.r[5].s64 = ctx.r[25].s64 - ctx.r[31].s64;
	// 832C64C8: 7C7EB214  add r3, r30, r22
	ctx.r[3].u64 = ctx.r[30].u64 + ctx.r[22].u64;
	// 832C64CC: 54A7103A  slwi r7, r5, 2
	// 832C64D0: 5488103A  slwi r8, r4, 2
	// 832C64D4: 57C9103A  slwi r9, r30, 2
	// 832C64D8: 54CA103A  slwi r10, r6, 2
	// 832C64DC: 546B103A  slwi r11, r3, 2
	// 832C64E0: 7E87C214  add r20, r7, r24
	ctx.r[20].u64 = ctx.r[7].u64 + ctx.r[24].u64;
	// 832C64E4: 7EE8C214  add r23, r8, r24
	ctx.r[23].u64 = ctx.r[8].u64 + ctx.r[24].u64;
	// 832C64E8: 56D3103A  slwi r19, r22, 2
	// 832C64EC: 56DA2036  slwi r26, r22, 4
	// 832C64F0: 7FA9AA14  add r29, r9, r21
	ctx.r[29].u64 = ctx.r[9].u64 + ctx.r[21].u64;
	// 832C64F4: 7F6AAA14  add r27, r10, r21
	ctx.r[27].u64 = ctx.r[10].u64 + ctx.r[21].u64;
	// 832C64F8: 7F8BAA14  add r28, r11, r21
	ctx.r[28].u64 = ctx.r[11].u64 + ctx.r[21].u64;
	// 832C64FC: 7EE5BB78  mr r5, r23
	ctx.r[5].u64 = ctx.r[23].u64;
	// 832C6500: 7FA4EB78  mr r4, r29
	ctx.r[4].u64 = ctx.r[29].u64;
	// 832C6504: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6508: 48000311  bl 0x832c6818
	ctx.lr = 0x832C650C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C650C: 7F06C378  mr r6, r24
	ctx.r[6].u64 = ctx.r[24].u64;
	// 832C6510: 7F25CB78  mr r5, r25
	ctx.r[5].u64 = ctx.r[25].u64;
	// 832C6514: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6518: 48000AB9  bl 0x832c6fd0
	ctx.lr = 0x832C651C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6FD0);
	// 832C651C: 7E85A378  mr r5, r20
	ctx.r[5].u64 = ctx.r[20].u64;
	// 832C6520: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832C6524: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6528: 48000679  bl 0x832c6ba0
	ctx.lr = 0x832C652C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6BA0);
	// 832C652C: 7F06C378  mr r6, r24
	ctx.r[6].u64 = ctx.r[24].u64;
	// 832C6530: 7F25CB78  mr r5, r25
	ctx.r[5].u64 = ctx.r[25].u64;
	// 832C6534: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6538: 48000B39  bl 0x832c7070
	ctx.lr = 0x832C653C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7070);
	// 832C653C: 7EE5BB78  mr r5, r23
	ctx.r[5].u64 = ctx.r[23].u64;
	// 832C6540: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832C6544: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6548: 480002D1  bl 0x832c6818
	ctx.lr = 0x832C654C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C654C: 7F06C378  mr r6, r24
	ctx.r[6].u64 = ctx.r[24].u64;
	// 832C6550: 7F25CB78  mr r5, r25
	ctx.r[5].u64 = ctx.r[25].u64;
	// 832C6554: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6558: 48000A79  bl 0x832c6fd0
	ctx.lr = 0x832C655C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6FD0);
	// 832C655C: 7FD3F214  add r30, r19, r30
	ctx.r[30].u64 = ctx.r[19].u64 + ctx.r[30].u64;
	// 832C6560: 7FBDD214  add r29, r29, r26
	ctx.r[29].u64 = ctx.r[29].u64 + ctx.r[26].u64;
	// 832C6564: 7F9CD214  add r28, r28, r26
	ctx.r[28].u64 = ctx.r[28].u64 + ctx.r[26].u64;
	// 832C6568: 7F7BD214  add r27, r27, r26
	ctx.r[27].u64 = ctx.r[27].u64 + ctx.r[26].u64;
	// 832C656C: 7F1E9000  cmpw cr6, r30, r18
	ctx.cr[6].compare_i32(ctx.r[30].s32, ctx.r[18].s32, &mut ctx.xer);
	// 832C6570: 4198FF8C  blt cr6, 0x832c64fc
	if ctx.cr[6].lt {
	pc = 0x832C64FC; continue 'dispatch;
	}
	// 832C6574: 56D6103A  slwi r22, r22, 2
	// 832C6578: 7F169000  cmpw cr6, r22, r18
	ctx.cr[6].compare_i32(ctx.r[22].s32, ctx.r[18].s32, &mut ctx.xer);
	// 832C657C: 4198FF2C  blt cr6, 0x832c64a8
	if ctx.cr[6].lt {
	pc = 0x832C64A8; continue 'dispatch;
	}
	// 832C6580: 7FEB0E70  srawi r11, r31, 1
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[11].s64 = (ctx.r[31].s32 >> 1) as i64;
	// 832C6584: 7D5F9050  subf r10, r31, r18
	ctx.r[10].s64 = ctx.r[18].s64 - ctx.r[31].s64;
	// 832C6588: 7D2BC850  subf r9, r11, r25
	ctx.r[9].s64 = ctx.r[25].s64 - ctx.r[11].s64;
	// 832C658C: 554A103A  slwi r10, r10, 2
	// 832C6590: 552B103A  slwi r11, r9, 2
	// 832C6594: 7C8AAA14  add r4, r10, r21
	ctx.r[4].u64 = ctx.r[10].u64 + ctx.r[21].u64;
	// 832C6598: 7CABC214  add r5, r11, r24
	ctx.r[5].u64 = ctx.r[11].u64 + ctx.r[24].u64;
	// 832C659C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C65A0: 48000279  bl 0x832c6818
	ctx.lr = 0x832C65A4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C65A4: 7F06C378  mr r6, r24
	ctx.r[6].u64 = ctx.r[24].u64;
	// 832C65A8: 7F25CB78  mr r5, r25
	ctx.r[5].u64 = ctx.r[25].u64;
	// 832C65AC: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C65B0: 48000A21  bl 0x832c6fd0
	ctx.lr = 0x832C65B4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6FD0);
	// 832C65B4: 382100D0  addi r1, r1, 0xd0
	ctx.r[1].s64 = ctx.r[1].s64 + 208;
	// 832C65B8: 4B9E2E78  b 0x82ca9430
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9430);
	return;
}

pub fn sub_832C65C0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C65C0 size=600
	// 832C65C0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C65C4: 4B9E2E25  bl 0x82ca93e8
	ctx.lr = 0x832C65C8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93E8);
	// 832C65C8: 9421FF40  stwu r1, -0xc0(r1)
	ea = ctx.r[1].u32.wrapping_add(-192 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C65CC: 7C7E0E70  srawi r30, r3, 1
	ctx.xer.ca = (ctx.r[3].s32 < 0) && ((ctx.r[3].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[30].s64 = (ctx.r[3].s32 >> 1) as i64;
	// 832C65D0: 7C7F1670  srawi r31, r3, 2
	ctx.xer.ca = (ctx.r[3].s32 < 0) && ((ctx.r[3].u32 & ((1u32 << 2) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[3].s32 >> 2) as i64;
	// 832C65D4: 7C942378  mr r20, r4
	ctx.r[20].u64 = ctx.r[4].u64;
	// 832C65D8: 7CB72B78  mr r23, r5
	ctx.r[23].u64 = ctx.r[5].u64;
	// 832C65DC: 7CD63378  mr r22, r6
	ctx.r[22].u64 = ctx.r[6].u64;
	// 832C65E0: 2F1F0080  cmpwi cr6, r31, 0x80
	ctx.cr[6].compare_i32(ctx.r[31].s32, 128, &mut ctx.xer);
	// 832C65E4: 409900F8  ble cr6, 0x832c66dc
	if !ctx.cr[6].gt {
	pc = 0x832C66DC; continue 'dispatch;
	}
	// 832C65E8: 7FF8FB78  mr r24, r31
	ctx.r[24].u64 = ctx.r[31].u64;
	// 832C65EC: 7F1FF000  cmpw cr6, r31, r30
	ctx.cr[6].compare_i32(ctx.r[31].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C65F0: 409800E0  bge cr6, 0x832c66d0
	if !ctx.cr[6].lt {
	pc = 0x832C66D0; continue 'dispatch;
	}
	// 832C65F4: 7FBFC050  subf r29, r31, r24
	ctx.r[29].s64 = ctx.r[24].s64 - ctx.r[31].s64;
	// 832C65F8: 7F1DF000  cmpw cr6, r29, r30
	ctx.cr[6].compare_i32(ctx.r[29].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C65FC: 40980064  bge cr6, 0x832c6660
	if !ctx.cr[6].lt {
	pc = 0x832C6660; continue 'dispatch;
	}
	// 832C6600: 7FEB0E70  srawi r11, r31, 1
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[11].s64 = (ctx.r[31].s32 >> 1) as i64;
	// 832C6604: 7D1DF214  add r8, r29, r30
	ctx.r[8].u64 = ctx.r[29].u64 + ctx.r[30].u64;
	// 832C6608: 7CEBB850  subf r7, r11, r23
	ctx.r[7].s64 = ctx.r[23].s64 - ctx.r[11].s64;
	// 832C660C: 57AA103A  slwi r10, r29, 2
	// 832C6610: 54E9103A  slwi r9, r7, 2
	// 832C6614: 550B103A  slwi r11, r8, 2
	// 832C6618: 7F49B214  add r26, r9, r22
	ctx.r[26].u64 = ctx.r[9].u64 + ctx.r[22].u64;
	// 832C661C: 5715083C  slwi r21, r24, 1
	// 832C6620: 57191838  slwi r25, r24, 3
	// 832C6624: 7F8AA214  add r28, r10, r20
	ctx.r[28].u64 = ctx.r[10].u64 + ctx.r[20].u64;
	// 832C6628: 7F6BA214  add r27, r11, r20
	ctx.r[27].u64 = ctx.r[11].u64 + ctx.r[20].u64;
	// 832C662C: 7F45D378  mr r5, r26
	ctx.r[5].u64 = ctx.r[26].u64;
	// 832C6630: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832C6634: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6638: 480001E1  bl 0x832c6818
	ctx.lr = 0x832C663C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C663C: 7F45D378  mr r5, r26
	ctx.r[5].u64 = ctx.r[26].u64;
	// 832C6640: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832C6644: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C6648: 480001D1  bl 0x832c6818
	ctx.lr = 0x832C664C;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C664C: 7FB5EA14  add r29, r21, r29
	ctx.r[29].u64 = ctx.r[21].u64 + ctx.r[29].u64;
	// 832C6650: 7F99E214  add r28, r25, r28
	ctx.r[28].u64 = ctx.r[25].u64 + ctx.r[28].u64;
	// 832C6654: 7F79DA14  add r27, r25, r27
	ctx.r[27].u64 = ctx.r[25].u64 + ctx.r[27].u64;
	// 832C6658: 7F1DF000  cmpw cr6, r29, r30
	ctx.cr[6].compare_i32(ctx.r[29].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C665C: 4198FFD0  blt cr6, 0x832c662c
	if ctx.cr[6].lt {
	pc = 0x832C662C; continue 'dispatch;
	}
	// 832C6660: 570B083C  slwi r11, r24, 1
	// 832C6664: 7FBF5850  subf r29, r31, r11
	ctx.r[29].s64 = ctx.r[11].s64 - ctx.r[31].s64;
	// 832C6668: 7F1DF000  cmpw cr6, r29, r30
	ctx.cr[6].compare_i32(ctx.r[29].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C666C: 40980058  bge cr6, 0x832c66c4
	if !ctx.cr[6].lt {
	pc = 0x832C66C4; continue 'dispatch;
	}
	// 832C6670: 7D7FB850  subf r11, r31, r23
	ctx.r[11].s64 = ctx.r[23].s64 - ctx.r[31].s64;
	// 832C6674: 7D1DF214  add r8, r29, r30
	ctx.r[8].u64 = ctx.r[29].u64 + ctx.r[30].u64;
	// 832C6678: 5569103A  slwi r9, r11, 2
	// 832C667C: 57AA103A  slwi r10, r29, 2
	// 832C6680: 550B103A  slwi r11, r8, 2
	// 832C6684: 7CA9B214  add r5, r9, r22
	ctx.r[5].u64 = ctx.r[9].u64 + ctx.r[22].u64;
	// 832C6688: 5719103A  slwi r25, r24, 2
	// 832C668C: 571A2036  slwi r26, r24, 4
	// 832C6690: 7F8AA214  add r28, r10, r20
	ctx.r[28].u64 = ctx.r[10].u64 + ctx.r[20].u64;
	// 832C6694: 7F6BA214  add r27, r11, r20
	ctx.r[27].u64 = ctx.r[11].u64 + ctx.r[20].u64;
	// 832C6698: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832C669C: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C66A0: 48000501  bl 0x832c6ba0
	ctx.lr = 0x832C66A4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6BA0);
	// 832C66A4: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832C66A8: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C66AC: 480004F5  bl 0x832c6ba0
	ctx.lr = 0x832C66B0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6BA0);
	// 832C66B0: 7FB9EA14  add r29, r25, r29
	ctx.r[29].u64 = ctx.r[25].u64 + ctx.r[29].u64;
	// 832C66B4: 7F9AE214  add r28, r26, r28
	ctx.r[28].u64 = ctx.r[26].u64 + ctx.r[28].u64;
	// 832C66B8: 7F7ADA14  add r27, r26, r27
	ctx.r[27].u64 = ctx.r[26].u64 + ctx.r[27].u64;
	// 832C66BC: 7F1DF000  cmpw cr6, r29, r30
	ctx.cr[6].compare_i32(ctx.r[29].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C66C0: 4198FFD8  blt cr6, 0x832c6698
	if ctx.cr[6].lt {
	pc = 0x832C6698; continue 'dispatch;
	}
	// 832C66C4: 5718103A  slwi r24, r24, 2
	// 832C66C8: 7F18F000  cmpw cr6, r24, r30
	ctx.cr[6].compare_i32(ctx.r[24].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C66CC: 4198FF28  blt cr6, 0x832c65f4
	if ctx.cr[6].lt {
	pc = 0x832C65F4; continue 'dispatch;
	}
	// 832C66D0: 7FFF1670  srawi r31, r31, 2
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 2) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[31].s32 >> 2) as i64;
	// 832C66D4: 2F1F0080  cmpwi cr6, r31, 0x80
	ctx.cr[6].compare_i32(ctx.r[31].s32, 128, &mut ctx.xer);
	// 832C66D8: 4199FF10  bgt cr6, 0x832c65e8
	if ctx.cr[6].gt {
	pc = 0x832C65E8; continue 'dispatch;
	}
	// 832C66DC: 7FF8FB78  mr r24, r31
	ctx.r[24].u64 = ctx.r[31].u64;
	// 832C66E0: 7F1FF000  cmpw cr6, r31, r30
	ctx.cr[6].compare_i32(ctx.r[31].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C66E4: 40980128  bge cr6, 0x832c680c
	if !ctx.cr[6].lt {
	pc = 0x832C680C; continue 'dispatch;
	}
	// 832C66E8: 7FBFC050  subf r29, r31, r24
	ctx.r[29].s64 = ctx.r[24].s64 - ctx.r[31].s64;
	// 832C66EC: 7F1DF000  cmpw cr6, r29, r30
	ctx.cr[6].compare_i32(ctx.r[29].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C66F0: 40980084  bge cr6, 0x832c6774
	if !ctx.cr[6].lt {
	pc = 0x832C6774; continue 'dispatch;
	}
	// 832C66F4: 7FEB0E70  srawi r11, r31, 1
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[11].s64 = (ctx.r[31].s32 >> 1) as i64;
	// 832C66F8: 7D1DF214  add r8, r29, r30
	ctx.r[8].u64 = ctx.r[29].u64 + ctx.r[30].u64;
	// 832C66FC: 7CEBB850  subf r7, r11, r23
	ctx.r[7].s64 = ctx.r[23].s64 - ctx.r[11].s64;
	// 832C6700: 57AA103A  slwi r10, r29, 2
	// 832C6704: 54E9103A  slwi r9, r7, 2
	// 832C6708: 550B103A  slwi r11, r8, 2
	// 832C670C: 7F49B214  add r26, r9, r22
	ctx.r[26].u64 = ctx.r[9].u64 + ctx.r[22].u64;
	// 832C6710: 5715083C  slwi r21, r24, 1
	// 832C6714: 57191838  slwi r25, r24, 3
	// 832C6718: 7F8AA214  add r28, r10, r20
	ctx.r[28].u64 = ctx.r[10].u64 + ctx.r[20].u64;
	// 832C671C: 7F6BA214  add r27, r11, r20
	ctx.r[27].u64 = ctx.r[11].u64 + ctx.r[20].u64;
	// 832C6720: 7F45D378  mr r5, r26
	ctx.r[5].u64 = ctx.r[26].u64;
	// 832C6724: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832C6728: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C672C: 480000ED  bl 0x832c6818
	ctx.lr = 0x832C6730;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C6730: 7EC6B378  mr r6, r22
	ctx.r[6].u64 = ctx.r[22].u64;
	// 832C6734: 7EE5BB78  mr r5, r23
	ctx.r[5].u64 = ctx.r[23].u64;
	// 832C6738: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C673C: 48000895  bl 0x832c6fd0
	ctx.lr = 0x832C6740;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6FD0);
	// 832C6740: 7F45D378  mr r5, r26
	ctx.r[5].u64 = ctx.r[26].u64;
	// 832C6744: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832C6748: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C674C: 480000CD  bl 0x832c6818
	ctx.lr = 0x832C6750;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6818);
	// 832C6750: 7EC6B378  mr r6, r22
	ctx.r[6].u64 = ctx.r[22].u64;
	// 832C6754: 7EE5BB78  mr r5, r23
	ctx.r[5].u64 = ctx.r[23].u64;
	// 832C6758: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C675C: 48000875  bl 0x832c6fd0
	ctx.lr = 0x832C6760;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6FD0);
	// 832C6760: 7FB5EA14  add r29, r21, r29
	ctx.r[29].u64 = ctx.r[21].u64 + ctx.r[29].u64;
	// 832C6764: 7F9CCA14  add r28, r28, r25
	ctx.r[28].u64 = ctx.r[28].u64 + ctx.r[25].u64;
	// 832C6768: 7F7BCA14  add r27, r27, r25
	ctx.r[27].u64 = ctx.r[27].u64 + ctx.r[25].u64;
	// 832C676C: 7F1DF000  cmpw cr6, r29, r30
	ctx.cr[6].compare_i32(ctx.r[29].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C6770: 4198FFB0  blt cr6, 0x832c6720
	if ctx.cr[6].lt {
	pc = 0x832C6720; continue 'dispatch;
	}
	// 832C6774: 570B083C  slwi r11, r24, 1
	// 832C6778: 7FBF5850  subf r29, r31, r11
	ctx.r[29].s64 = ctx.r[11].s64 - ctx.r[31].s64;
	// 832C677C: 7F1DF000  cmpw cr6, r29, r30
	ctx.cr[6].compare_i32(ctx.r[29].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C6780: 40980080  bge cr6, 0x832c6800
	if !ctx.cr[6].lt {
	pc = 0x832C6800; continue 'dispatch;
	}
	// 832C6784: 7D7FB850  subf r11, r31, r23
	ctx.r[11].s64 = ctx.r[23].s64 - ctx.r[31].s64;
	// 832C6788: 7D1DF214  add r8, r29, r30
	ctx.r[8].u64 = ctx.r[29].u64 + ctx.r[30].u64;
	// 832C678C: 5569103A  slwi r9, r11, 2
	// 832C6790: 57AA103A  slwi r10, r29, 2
	// 832C6794: 550B103A  slwi r11, r8, 2
	// 832C6798: 7F49B214  add r26, r9, r22
	ctx.r[26].u64 = ctx.r[9].u64 + ctx.r[22].u64;
	// 832C679C: 5715103A  slwi r21, r24, 2
	// 832C67A0: 57192036  slwi r25, r24, 4
	// 832C67A4: 7F8AA214  add r28, r10, r20
	ctx.r[28].u64 = ctx.r[10].u64 + ctx.r[20].u64;
	// 832C67A8: 7F6BA214  add r27, r11, r20
	ctx.r[27].u64 = ctx.r[11].u64 + ctx.r[20].u64;
	// 832C67AC: 7F45D378  mr r5, r26
	ctx.r[5].u64 = ctx.r[26].u64;
	// 832C67B0: 7F84E378  mr r4, r28
	ctx.r[4].u64 = ctx.r[28].u64;
	// 832C67B4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C67B8: 480003E9  bl 0x832c6ba0
	ctx.lr = 0x832C67BC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6BA0);
	// 832C67BC: 7EC6B378  mr r6, r22
	ctx.r[6].u64 = ctx.r[22].u64;
	// 832C67C0: 7EE5BB78  mr r5, r23
	ctx.r[5].u64 = ctx.r[23].u64;
	// 832C67C4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C67C8: 480008A9  bl 0x832c7070
	ctx.lr = 0x832C67CC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7070);
	// 832C67CC: 7F45D378  mr r5, r26
	ctx.r[5].u64 = ctx.r[26].u64;
	// 832C67D0: 7F64DB78  mr r4, r27
	ctx.r[4].u64 = ctx.r[27].u64;
	// 832C67D4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C67D8: 480003C9  bl 0x832c6ba0
	ctx.lr = 0x832C67DC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C6BA0);
	// 832C67DC: 7EC6B378  mr r6, r22
	ctx.r[6].u64 = ctx.r[22].u64;
	// 832C67E0: 7EE5BB78  mr r5, r23
	ctx.r[5].u64 = ctx.r[23].u64;
	// 832C67E4: 7FE3FB78  mr r3, r31
	ctx.r[3].u64 = ctx.r[31].u64;
	// 832C67E8: 48000889  bl 0x832c7070
	ctx.lr = 0x832C67EC;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7070);
	// 832C67EC: 7FB5EA14  add r29, r21, r29
	ctx.r[29].u64 = ctx.r[21].u64 + ctx.r[29].u64;
	// 832C67F0: 7F9CCA14  add r28, r28, r25
	ctx.r[28].u64 = ctx.r[28].u64 + ctx.r[25].u64;
	// 832C67F4: 7F7BCA14  add r27, r27, r25
	ctx.r[27].u64 = ctx.r[27].u64 + ctx.r[25].u64;
	// 832C67F8: 7F1DF000  cmpw cr6, r29, r30
	ctx.cr[6].compare_i32(ctx.r[29].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C67FC: 4198FFB0  blt cr6, 0x832c67ac
	if ctx.cr[6].lt {
	pc = 0x832C67AC; continue 'dispatch;
	}
	// 832C6800: 5718103A  slwi r24, r24, 2
	// 832C6804: 7F18F000  cmpw cr6, r24, r30
	ctx.cr[6].compare_i32(ctx.r[24].s32, ctx.r[30].s32, &mut ctx.xer);
	// 832C6808: 4198FEE0  blt cr6, 0x832c66e8
	if ctx.cr[6].lt {
	pc = 0x832C66E8; continue 'dispatch;
	}
	// 832C680C: 382100C0  addi r1, r1, 0xc0
	ctx.r[1].s64 = ctx.r[1].s64 + 192;
	// 832C6810: 4B9E2C28  b 0x82ca9438
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9438);
	return;
}

pub fn sub_832C6818(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    // ---- function 0x832C6818 size=904
	// 832C6818: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C681C: 4B9E2BED  bl 0x82ca9408
	ctx.lr = 0x832C6820;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9408);
	// 832C6820: DBA1FFC0  stfd f29, -0x40(r1)
	ctx.fpscr.disable_flush_mode_unconditional();
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-64 as u32), ctx.f[29].u64 ) };
	// 832C6824: DBC1FFC8  stfd f30, -0x38(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-56 as u32), ctx.f[30].u64 ) };
	// 832C6828: DBE1FFD0  stfd f31, -0x30(r1)
	unsafe { crate::rt::store_u64( base as *mut u8, ctx.r[1].u32.wrapping_add(-48 as u32), ctx.f[31].u64 ) };
	// 832C682C: 7C7D1E70  srawi r29, r3, 3
	ctx.xer.ca = (ctx.r[3].s32 < 0) && ((ctx.r[3].u32 & ((1u32 << 3) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[3].s32 >> 3) as i64;
	// 832C6830: C0040000  lfs f0, 0(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(0 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C6834: C1A40004  lfs f13, 4(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(4 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C6838: 57AB083C  slwi r11, r29, 1
	// 832C683C: 57AA103A  slwi r10, r29, 2
	// 832C6840: 57A71838  slwi r7, r29, 3
	// 832C6844: 7D4A5A14  add r10, r10, r11
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C6848: 57A92036  slwi r9, r29, 4
	// 832C684C: 554A103A  slwi r10, r10, 2
	// 832C6850: 7D292214  add r9, r9, r4
	ctx.r[9].u64 = ctx.r[9].u64 + ctx.r[4].u64;
	// 832C6854: 7D0A2214  add r8, r10, r4
	ctx.r[8].u64 = ctx.r[10].u64 + ctx.r[4].u64;
	// 832C6858: 7D87242E  lfsx f12, r7, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C685C: 7D472214  add r10, r7, r4
	ctx.r[10].u64 = ctx.r[7].u64 + ctx.r[4].u64;
	// 832C6860: 2F1D0002  cmpwi cr6, r29, 2
	ctx.cr[6].compare_i32(ctx.r[29].s32, 2, &mut ctx.xer);
	// 832C6864: C1690000  lfs f11, 0(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C6868: C1480000  lfs f10, 0(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C686C: ED20582A  fadds f9, f0, f11
	ctx.f[9].f64 = ((ctx.f[0].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C6870: C1090004  lfs f8, 4(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(4 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C6874: ECEC502A  fadds f7, f12, f10
	ctx.f[7].f64 = ((ctx.f[12].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6878: C0C80004  lfs f6, 4(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(4 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C687C: ECAD402A  fadds f5, f13, f8
	ctx.f[5].f64 = ((ctx.f[13].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C6880: C08A0004  lfs f4, 4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C6884: EC605828  fsubs f3, f0, f11
	ctx.f[3].f64 = (((ctx.f[0].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C6888: EC44302A  fadds f2, f4, f6
	ctx.f[2].f64 = ((ctx.f[4].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C688C: EC2D4028  fsubs f1, f13, f8
	ctx.f[1].f64 = (((ctx.f[13].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C6890: EC0C5028  fsubs f0, f12, f10
	ctx.f[0].f64 = (((ctx.f[12].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C6894: EDA43028  fsubs f13, f4, f6
	ctx.f[13].f64 = (((ctx.f[4].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C6898: ED87482A  fadds f12, f7, f9
	ctx.f[12].f64 = ((ctx.f[7].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C689C: D1840000  stfs f12, 0(r4)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C68A0: ED693828  fsubs f11, f9, f7
	ctx.f[11].f64 = (((ctx.f[9].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C68A4: ED42282A  fadds f10, f2, f5
	ctx.f[10].f64 = ((ctx.f[2].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C68A8: D1440004  stfs f10, 4(r4)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C68AC: ED251028  fsubs f9, f5, f2
	ctx.f[9].f64 = (((ctx.f[5].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C68B0: 7D67252E  stfsx f11, r7, r4
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[7].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C68B4: D12A0004  stfs f9, 4(r10)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C68B8: ED00082A  fadds f8, f0, f1
	ctx.f[8].f64 = ((ctx.f[0].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C68BC: ECE36828  fsubs f7, f3, f13
	ctx.f[7].f64 = (((ctx.f[3].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C68C0: D0E90000  stfs f7, 0(r9)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C68C4: D1090004  stfs f8, 4(r9)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C68C8: ECA10028  fsubs f5, f1, f0
	ctx.f[5].f64 = (((ctx.f[1].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C68CC: ECCD182A  fadds f6, f13, f3
	ctx.f[6].f64 = ((ctx.f[13].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C68D0: D0C80000  stfs f6, 0(r8)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C68D4: D0A80004  stfs f5, 4(r8)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C68D8: C0050004  lfs f0, 4(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(4 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C68DC: 409901E4  ble cr6, 0x832c6ac0
	if !ctx.cr[6].gt {
	pc = 0x832C6AC0; continue 'dispatch;
	}
	// 832C68E0: 556A083C  slwi r10, r11, 1
	// 832C68E4: 38EB0002  addi r7, r11, 2
	ctx.r[7].s64 = ctx.r[11].s64 + 2;
	// 832C68E8: 7CCB5214  add r6, r11, r10
	ctx.r[6].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832C68EC: 386BFFFE  addi r3, r11, -2
	ctx.r[3].s64 = ctx.r[11].s64 + -2;
	// 832C68F0: 54C9103A  slwi r9, r6, 2
	// 832C68F4: 556A1838  slwi r10, r11, 3
	// 832C68F8: 7D092214  add r8, r9, r4
	ctx.r[8].u64 = ctx.r[9].u64 + ctx.r[4].u64;
	// 832C68FC: 54E9103A  slwi r9, r7, 2
	// 832C6900: 38DDFFFD  addi r6, r29, -3
	ctx.r[6].s64 = ctx.r[29].s64 + -3;
	// 832C6904: 55672036  slwi r7, r11, 4
	// 832C6908: 5463103A  slwi r3, r3, 2
	// 832C690C: 7D4A2214  add r10, r10, r4
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[4].u64;
	// 832C6910: 54DCF87E  srwi r28, r6, 1
	// 832C6914: 7CE72214  add r7, r7, r4
	ctx.r[7].u64 = ctx.r[7].u64 + ctx.r[4].u64;
	// 832C6918: 7FE92214  add r31, r9, r4
	ctx.r[31].u64 = ctx.r[9].u64 + ctx.r[4].u64;
	// 832C691C: 38C50008  addi r6, r5, 8
	ctx.r[6].s64 = ctx.r[5].s64 + 8;
	// 832C6920: 7D232214  add r9, r3, r4
	ctx.r[9].u64 = ctx.r[3].u64 + ctx.r[4].u64;
	// 832C6924: 386A0008  addi r3, r10, 8
	ctx.r[3].s64 = ctx.r[10].s64 + 8;
	// 832C6928: 38A80008  addi r5, r8, 8
	ctx.r[5].s64 = ctx.r[8].s64 + 8;
	// 832C692C: 3BC40008  addi r30, r4, 8
	ctx.r[30].s64 = ctx.r[4].s64 + 8;
	// 832C6930: 394AFFF8  addi r10, r10, -8
	ctx.r[10].s64 = ctx.r[10].s64 + -8;
	// 832C6934: 3908FFF8  addi r8, r8, -8
	ctx.r[8].s64 = ctx.r[8].s64 + -8;
	// 832C6938: 38E7FFF8  addi r7, r7, -8
	ctx.r[7].s64 = ctx.r[7].s64 + -8;
	// 832C693C: 3B9C0001  addi r28, r28, 1
	ctx.r[28].s64 = ctx.r[28].s64 + 1;
	// 832C6940: C1830004  lfs f12, 4(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C6944: 38C60010  addi r6, r6, 0x10
	ctx.r[6].s64 = ctx.r[6].s64 + 16;
	// 832C6948: C1650004  lfs f11, 4(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(4 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C694C: 379CFFFF  addic. r28, r28, -1
	ctx.xer.ca = (ctx.r[28].u32 > (!(-1 as u32)));
	ctx.r[28].s64 = ctx.r[28].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[28].s32, 0, &mut ctx.xer);
	// 832C6950: C1BE0004  lfs f13, 4(r30)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(4 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C6954: C13F0004  lfs f9, 4(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C6958: ED0C682A  fadds f8, f12, f13
	ctx.f[8].f64 = ((ctx.f[12].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C695C: C0E50000  lfs f7, 0(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(0 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C6960: EC8B482A  fadds f4, f11, f9
	ctx.f[4].f64 = ((ctx.f[11].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C6964: C0BF0000  lfs f5, 0(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832C6968: ED4D6028  fsubs f10, f13, f12
	ctx.f[10].f64 = (((ctx.f[13].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C696C: C0230000  lfs f1, 0(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C6970: EC453828  fsubs f2, f5, f7
	ctx.f[2].f64 = (((ctx.f[5].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C6974: C07E0000  lfs f3, 0(r30)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C6978: ECC95828  fsubs f6, f9, f11
	ctx.f[6].f64 = (((ctx.f[9].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C697C: ED830828  fsubs f12, f3, f1
	ctx.f[12].f64 = (((ctx.f[3].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C6980: C1660004  lfs f11, 4(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(4 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C6984: EDA7282A  fadds f13, f7, f5
	ctx.f[13].f64 = ((ctx.f[7].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C6988: C0E6FFFC  lfs f7, -4(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(-4 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C698C: FD205850  fneg f9, f11
	ctx.f[9].u64 = ctx.f[11].u64 ^ 0x8000_0000_0000_0000u64;
	// 832C6990: ECA1182A  fadds f5, f1, f3
	ctx.f[5].f64 = ((ctx.f[1].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C6994: C026FFF8  lfs f1, -8(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(-8 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C6998: C0660000  lfs f3, 0(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C699C: ED64402A  fadds f11, f4, f8
	ctx.f[11].f64 = ((ctx.f[4].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C69A0: D17E0004  stfs f11, 4(r30)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C69A4: EC882028  fsubs f4, f8, f4
	ctx.f[4].f64 = (((ctx.f[8].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C69A8: ED62502A  fadds f11, f2, f10
	ctx.f[11].f64 = ((ctx.f[2].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C69AC: ED4A1028  fsubs f10, f10, f2
	ctx.f[10].f64 = (((ctx.f[10].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C69B0: EC4C3028  fsubs f2, f12, f6
	ctx.f[2].f64 = (((ctx.f[12].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C69B4: ED86602A  fadds f12, f6, f12
	ctx.f[12].f64 = ((ctx.f[6].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C69B8: ECCD282A  fadds f6, f13, f5
	ctx.f[6].f64 = ((ctx.f[13].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C69BC: D0DE0000  stfs f6, 0(r30)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C69C0: D09F0004  stfs f4, 4(r31)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C69C4: EDA56828  fsubs f13, f5, f13
	ctx.f[13].f64 = (((ctx.f[5].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C69C8: D1BF0000  stfs f13, 0(r31)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C69CC: ED0702F2  fmuls f8, f7, f11
	ctx.f[8].f64 = (((ctx.f[7].f64 * ctx.f[11].f64) as f32) as f64);
	// 832C69D0: ECC700B2  fmuls f6, f7, f2
	ctx.f[6].f64 = (((ctx.f[7].f64 * ctx.f[2].f64) as f32) as f64);
	// 832C69D4: EC890332  fmuls f4, f9, f12
	ctx.f[4].f64 = (((ctx.f[9].f64 * ctx.f[12].f64) as f32) as f64);
	// 832C69D8: ECA30332  fmuls f5, f3, f12
	ctx.f[5].f64 = (((ctx.f[3].f64 * ctx.f[12].f64) as f32) as f64);
	// 832C69DC: EC4140B8  fmsubs f2, f1, f2, f8
	ctx.f[2].f64 = (((ctx.f[1].f64 * ctx.f[2].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C69E0: D0430000  stfs f2, 0(r3)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C69E4: EDA132FA  fmadds f13, f1, f11, f6
	ctx.f[13].f64 = (((ctx.f[1].f64 * ctx.f[11].f64 + ctx.f[6].f64) as f32) as f64);
	// 832C69E8: D1A30004  stfs f13, 4(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C69EC: ED6322B8  fmsubs f11, f3, f10, f4
	ctx.f[11].f64 = (((ctx.f[3].f64 * ctx.f[10].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C69F0: D1650004  stfs f11, 4(r5)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[5].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C69F4: ED892ABA  fmadds f12, f9, f10, f5
	ctx.f[12].f64 = (((ctx.f[9].f64 * ctx.f[10].f64 + ctx.f[5].f64) as f32) as f64);
	// 832C69F8: D1850000  stfs f12, 0(r5)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[5].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C69FC: C1090004  lfs f8, 4(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(4 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C6A00: 38A50008  addi r5, r5, 8
	ctx.r[5].s64 = ctx.r[5].s64 + 8;
	// 832C6A04: C1480004  lfs f10, 4(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(4 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C6A08: 38630008  addi r3, r3, 8
	ctx.r[3].s64 = ctx.r[3].s64 + 8;
	// 832C6A0C: C0C70004  lfs f6, 4(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(4 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C6A10: C0A80000  lfs f5, 0(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832C6A14: C08A0004  lfs f4, 4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C6A18: C0490000  lfs f2, 0(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C6A1C: C1A70000  lfs f13, 0(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(0 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C6A20: C18A0000  lfs f12, 0(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C6A24: ED6C6828  fsubs f11, f12, f13
	ctx.f[11].f64 = (((ctx.f[12].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C6A28: EFE85028  fsubs f31, f8, f10
	ctx.f[31].f64 = (((ctx.f[8].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C6A2C: ED4A402A  fadds f10, f10, f8
	ctx.f[10].f64 = ((ctx.f[10].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C6A30: ED06202A  fadds f8, f6, f4
	ctx.f[8].f64 = ((ctx.f[6].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C6A34: EFC22828  fsubs f30, f2, f5
	ctx.f[30].f64 = (((ctx.f[2].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C6A38: EFA43028  fsubs f29, f4, f6
	ctx.f[29].f64 = (((ctx.f[4].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C6A3C: ECCD602A  fadds f6, f13, f12
	ctx.f[6].f64 = ((ctx.f[13].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C6A40: ECA5102A  fadds f5, f5, f2
	ctx.f[5].f64 = ((ctx.f[5].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C6A44: EC8BF82A  fadds f4, f11, f31
	ctx.f[4].f64 = ((ctx.f[11].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C6A48: EDA8502A  fadds f13, f8, f10
	ctx.f[13].f64 = ((ctx.f[8].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6A4C: D1A90004  stfs f13, 4(r9)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6A50: ED8A4028  fsubs f12, f10, f8
	ctx.f[12].f64 = (((ctx.f[10].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C6A54: EC5EE828  fsubs f2, f30, f29
	ctx.f[2].f64 = (((ctx.f[30].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C6A58: EDBF5828  fsubs f13, f31, f11
	ctx.f[13].f64 = (((ctx.f[31].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C6A5C: ED46282A  fadds f10, f6, f5
	ctx.f[10].f64 = ((ctx.f[6].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C6A60: D1490000  stfs f10, 0(r9)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6A64: ED053028  fsubs f8, f5, f6
	ctx.f[8].f64 = (((ctx.f[5].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C6A68: D10A0000  stfs f8, 0(r10)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6A6C: D18A0004  stfs f12, 4(r10)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6A70: ECC10132  fmuls f6, f1, f4
	ctx.f[6].f64 = (((ctx.f[1].f64 * ctx.f[4].f64) as f32) as f64);
	// 832C6A74: ECA100B2  fmuls f5, f1, f2
	ctx.f[5].f64 = (((ctx.f[1].f64 * ctx.f[2].f64) as f32) as f64);
	// 832C6A78: EC3DF02A  fadds f1, f29, f30
	ctx.f[1].f64 = ((ctx.f[29].f64 + ctx.f[30].f64) as f32) as f64;
	// 832C6A7C: EC4730B8  fmsubs f2, f7, f2, f6
	ctx.f[2].f64 = (((ctx.f[7].f64 * ctx.f[2].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C6A80: D0480000  stfs f2, 0(r8)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6A84: ED690072  fmuls f11, f9, f1
	ctx.f[11].f64 = (((ctx.f[9].f64 * ctx.f[1].f64) as f32) as f64);
	// 832C6A88: 3BFF0008  addi r31, r31, 8
	ctx.r[31].s64 = ctx.r[31].s64 + 8;
	// 832C6A8C: ED430072  fmuls f10, f3, f1
	ctx.f[10].f64 = (((ctx.f[3].f64 * ctx.f[1].f64) as f32) as f64);
	// 832C6A90: 3BDE0008  addi r30, r30, 8
	ctx.r[30].s64 = ctx.r[30].s64 + 8;
	// 832C6A94: ED87293A  fmadds f12, f7, f4, f5
	ctx.f[12].f64 = (((ctx.f[7].f64 * ctx.f[4].f64 + ctx.f[5].f64) as f32) as f64);
	// 832C6A98: D1880004  stfs f12, 4(r8)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6A9C: 394AFFF8  addi r10, r10, -8
	ctx.r[10].s64 = ctx.r[10].s64 + -8;
	// 832C6AA0: 3929FFF8  addi r9, r9, -8
	ctx.r[9].s64 = ctx.r[9].s64 + -8;
	// 832C6AA4: 3908FFF8  addi r8, r8, -8
	ctx.r[8].s64 = ctx.r[8].s64 + -8;
	// 832C6AA8: ED035B7A  fmadds f8, f3, f13, f11
	ctx.f[8].f64 = (((ctx.f[3].f64 * ctx.f[13].f64 + ctx.f[11].f64) as f32) as f64);
	// 832C6AAC: D1070000  stfs f8, 0(r7)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6AB0: ECE95378  fmsubs f7, f9, f13, f10
	ctx.f[7].f64 = (((ctx.f[9].f64 * ctx.f[13].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C6AB4: D0E70004  stfs f7, 4(r7)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6AB8: 38E7FFF8  addi r7, r7, -8
	ctx.r[7].s64 = ctx.r[7].s64 + -8;
	// 832C6ABC: 4082FE84  bne 0x832c6940
	if !ctx.cr[0].eq {
	pc = 0x832C6940; continue 'dispatch;
	}
	// 832C6AC0: 7D4BEA14  add r10, r11, r29
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[29].u64;
	// 832C6AC4: FDA00050  fneg f13, f0
	ctx.f[13].u64 = ctx.f[0].u64 ^ 0x8000_0000_0000_0000u64;
	// 832C6AC8: 57A7103A  slwi r7, r29, 2
	// 832C6ACC: 7D2A5A14  add r9, r10, r11
	ctx.r[9].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C6AD0: 5546103A  slwi r6, r10, 2
	// 832C6AD4: 7D695A14  add r11, r9, r11
	ctx.r[11].u64 = ctx.r[9].u64 + ctx.r[11].u64;
	// 832C6AD8: 552A103A  slwi r10, r9, 2
	// 832C6ADC: 556B103A  slwi r11, r11, 2
	// 832C6AE0: 7D87242E  lfsx f12, r7, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[7].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C6AE4: 7D2A2214  add r9, r10, r4
	ctx.r[9].u64 = ctx.r[10].u64 + ctx.r[4].u64;
	// 832C6AE8: 7D0B2214  add r8, r11, r4
	ctx.r[8].u64 = ctx.r[11].u64 + ctx.r[4].u64;
	// 832C6AEC: 7D66242E  lfsx f11, r6, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[6].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C6AF0: 7D672214  add r11, r7, r4
	ctx.r[11].u64 = ctx.r[7].u64 + ctx.r[4].u64;
	// 832C6AF4: 7D462214  add r10, r6, r4
	ctx.r[10].u64 = ctx.r[6].u64 + ctx.r[4].u64;
	// 832C6AF8: C1490000  lfs f10, 0(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C6AFC: C0A90004  lfs f5, 4(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(4 as u32) ) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832C6B00: ED0C5028  fsubs f8, f12, f10
	ctx.f[8].f64 = (((ctx.f[12].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C6B04: C0EB0004  lfs f7, 4(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C6B08: EC8C502A  fadds f4, f12, f10
	ctx.f[4].f64 = ((ctx.f[12].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6B0C: C1280000  lfs f9, 0(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C6B10: EC472828  fsubs f2, f7, f5
	ctx.f[2].f64 = (((ctx.f[7].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C6B14: C0680004  lfs f3, 4(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(4 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C6B18: ECCB4828  fsubs f6, f11, f9
	ctx.f[6].f64 = (((ctx.f[11].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C6B1C: C02A0004  lfs f1, 4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C6B20: ED87282A  fadds f12, f7, f5
	ctx.f[12].f64 = ((ctx.f[7].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C6B24: ED411828  fsubs f10, f1, f3
	ctx.f[10].f64 = (((ctx.f[1].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C6B28: ECE1182A  fadds f7, f1, f3
	ctx.f[7].f64 = ((ctx.f[1].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C6B2C: ED2B482A  fadds f9, f11, f9
	ctx.f[9].f64 = ((ctx.f[11].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C6B30: EC623028  fsubs f3, f2, f6
	ctx.f[3].f64 = (((ctx.f[2].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C6B34: ECA6102A  fadds f5, f6, f2
	ctx.f[5].f64 = ((ctx.f[6].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C6B38: EC485028  fsubs f2, f8, f10
	ctx.f[2].f64 = (((ctx.f[8].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C6B3C: EC2A402A  fadds f1, f10, f8
	ctx.f[1].f64 = ((ctx.f[10].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C6B40: ED69202A  fadds f11, f9, f4
	ctx.f[11].f64 = ((ctx.f[9].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C6B44: 7D67252E  stfsx f11, r7, r4
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[7].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C6B48: ED47602A  fadds f10, f7, f12
	ctx.f[10].f64 = ((ctx.f[7].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C6B4C: D14B0004  stfs f10, 4(r11)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6B50: ED0C3828  fsubs f8, f12, f7
	ctx.f[8].f64 = (((ctx.f[12].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C6B54: D10A0004  stfs f8, 4(r10)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6B58: ED244828  fsubs f9, f4, f9
	ctx.f[9].f64 = (((ctx.f[4].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C6B5C: 7D26252E  stfsx f9, r6, r4
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[6].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C6B60: ECE22828  fsubs f7, f2, f5
	ctx.f[7].f64 = (((ctx.f[2].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C6B64: ECC5102A  fadds f6, f5, f2
	ctx.f[6].f64 = ((ctx.f[5].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C6B68: EC830828  fsubs f4, f3, f1
	ctx.f[4].f64 = (((ctx.f[3].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C6B6C: ECA3082A  fadds f5, f3, f1
	ctx.f[5].f64 = ((ctx.f[3].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C6B70: EC670032  fmuls f3, f7, f0
	ctx.f[3].f64 = (((ctx.f[7].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C6B74: D0690000  stfs f3, 0(r9)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6B78: EC460032  fmuls f2, f6, f0
	ctx.f[2].f64 = (((ctx.f[6].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C6B7C: D0490004  stfs f2, 4(r9)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6B80: EC040372  fmuls f0, f4, f13
	ctx.f[0].f64 = (((ctx.f[4].f64 * ctx.f[13].f64) as f32) as f64);
	// 832C6B84: D0080004  stfs f0, 4(r8)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6B88: EC250372  fmuls f1, f5, f13
	ctx.f[1].f64 = (((ctx.f[5].f64 * ctx.f[13].f64) as f32) as f64);
	// 832C6B8C: D0280000  stfs f1, 0(r8)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6B90: CBA1FFC0  lfd f29, -0x40(r1)
	ctx.f[29].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-64 as u32) ) };
	// 832C6B94: CBC1FFC8  lfd f30, -0x38(r1)
	ctx.f[30].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-56 as u32) ) };
	// 832C6B98: CBE1FFD0  lfd f31, -0x30(r1)
	ctx.f[31].u64 = unsafe { crate::rt::load_u64( base as *const u8, ctx.r[1].u32.wrapping_add(-48 as u32) ) };
	// 832C6B9C: 4B9E28BC  b 0x82ca9458
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9458);
	return;
}

pub fn sub_832C6BA0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    // ---- function 0x832C6BA0 size=1072
	// 832C6BA0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C6BA4: 4B9E2855  bl 0x82ca93f8
	ctx.lr = 0x832C6BA8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93F8);
	// 832C6BA8: 3981FFB8  addi r12, r1, -0x48
	ctx.r[12].s64 = ctx.r[1].s64 + -72;
	// 832C6BAC: 4B9E711D  bl 0x82cadcc8
	ctx.lr = 0x832C6BB0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCC8);
	// 832C6BB0: 7C7B1E70  srawi r27, r3, 3
	ctx.xer.ca = (ctx.r[3].s32 < 0) && ((ctx.r[3].u32 & ((1u32 << 3) - 1)) != 0);
	ctx.r[27].s64 = (ctx.r[3].s32 >> 3) as i64;
	// 832C6BB4: C1840004  lfs f12, 4(r4)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(4 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C6BB8: C1A40000  lfs f13, 0(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(0 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C6BBC: 576B083C  slwi r11, r27, 1
	// 832C6BC0: C0050004  lfs f0, 4(r5)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[5].u32.wrapping_add(4 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C6BC4: 576A103A  slwi r10, r27, 2
	// 832C6BC8: 57781838  slwi r24, r27, 3
	// 832C6BCC: 7D4A5A14  add r10, r10, r11
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C6BD0: 7CF82214  add r7, r24, r4
	ctx.r[7].u64 = ctx.r[24].u64 + ctx.r[4].u64;
	// 832C6BD4: 554A103A  slwi r10, r10, 2
	// 832C6BD8: 57692036  slwi r9, r27, 4
	// 832C6BDC: 7D4A2214  add r10, r10, r4
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[4].u64;
	// 832C6BE0: 7D78242E  lfsx f11, r24, r4
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[24].u32.wrapping_add(ctx.r[4].u32)) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C6BE4: 7D292214  add r9, r9, r4
	ctx.r[9].u64 = ctx.r[9].u64 + ctx.r[4].u64;
	// 832C6BE8: C1470004  lfs f10, 4(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(4 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C6BEC: 5768103A  slwi r8, r27, 2
	// 832C6BF0: 2F1B0002  cmpwi cr6, r27, 2
	ctx.cr[6].compare_i32(ctx.r[27].s32, 2, &mut ctx.xer);
	// 832C6BF4: C12A0000  lfs f9, 0(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C6BF8: C10A0004  lfs f8, 4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C6BFC: ECE9502A  fadds f7, f9, f10
	ctx.f[7].f64 = ((ctx.f[9].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6C00: ECCB4028  fsubs f6, f11, f8
	ctx.f[6].f64 = (((ctx.f[11].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C6C04: C0A90000  lfs f5, 0(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832C6C08: EC48582A  fadds f2, f8, f11
	ctx.f[2].f64 = ((ctx.f[8].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C6C0C: C0690004  lfs f3, 4(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(4 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C6C10: EC8A4828  fsubs f4, f10, f9
	ctx.f[4].f64 = (((ctx.f[10].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C6C14: ED2C2828  fsubs f9, f12, f5
	ctx.f[9].f64 = (((ctx.f[12].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C6C18: ED6C282A  fadds f11, f12, f5
	ctx.f[11].f64 = ((ctx.f[12].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C6C1C: ED43682A  fadds f10, f3, f13
	ctx.f[10].f64 = ((ctx.f[3].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C6C20: EC2D1828  fsubs f1, f13, f3
	ctx.f[1].f64 = (((ctx.f[13].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C6C24: ED063828  fsubs f8, f6, f7
	ctx.f[8].f64 = (((ctx.f[6].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C6C28: ECE7302A  fadds f7, f7, f6
	ctx.f[7].f64 = ((ctx.f[7].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C6C2C: ECC22028  fsubs f6, f2, f4
	ctx.f[6].f64 = (((ctx.f[2].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C6C30: ECA4102A  fadds f5, f4, f2
	ctx.f[5].f64 = ((ctx.f[4].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C6C34: EC880032  fmuls f4, f8, f0
	ctx.f[4].f64 = (((ctx.f[8].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C6C38: EC670032  fmuls f3, f7, f0
	ctx.f[3].f64 = (((ctx.f[7].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C6C3C: EC460032  fmuls f2, f6, f0
	ctx.f[2].f64 = (((ctx.f[6].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C6C40: EC050032  fmuls f0, f5, f0
	ctx.f[0].f64 = (((ctx.f[5].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C6C44: EDA4082A  fadds f13, f4, f1
	ctx.f[13].f64 = ((ctx.f[4].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C6C48: D1A40000  stfs f13, 0(r4)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6C4C: ED83582A  fadds f12, f3, f11
	ctx.f[12].f64 = ((ctx.f[3].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C6C50: D1840004  stfs f12, 4(r4)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6C54: ED012028  fsubs f8, f1, f4
	ctx.f[8].f64 = (((ctx.f[1].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C6C58: 7D18252E  stfsx f8, r24, r4
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32(base as *mut u8, ctx.r[24].u32.wrapping_add(ctx.r[4].u32), tmp.u32) };
	// 832C6C5C: ECEB1828  fsubs f7, f11, f3
	ctx.f[7].f64 = (((ctx.f[11].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C6C60: D0E70004  stfs f7, 4(r7)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6C64: ECC2482A  fadds f6, f2, f9
	ctx.f[6].f64 = ((ctx.f[2].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C6C68: D0C90004  stfs f6, 4(r9)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6C6C: ECAA0028  fsubs f5, f10, f0
	ctx.f[5].f64 = (((ctx.f[10].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C6C70: D0A90000  stfs f5, 0(r9)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6C74: EC80502A  fadds f4, f0, f10
	ctx.f[4].f64 = ((ctx.f[0].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6C78: D08A0000  stfs f4, 0(r10)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6C7C: EC691028  fsubs f3, f9, f2
	ctx.f[3].f64 = (((ctx.f[9].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C6C80: D06A0004  stfs f3, 4(r10)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6C84: 40990248  ble cr6, 0x832c6ecc
	if !ctx.cr[6].gt {
	pc = 0x832C6ECC; continue 'dispatch;
	}
	// 832C6C88: 556A083C  slwi r10, r11, 1
	// 832C6C8C: 38CB0002  addi r6, r11, 2
	ctx.r[6].s64 = ctx.r[11].s64 + 2;
	// 832C6C90: 7C6B5214  add r3, r11, r10
	ctx.r[3].u64 = ctx.r[11].u64 + ctx.r[10].u64;
	// 832C6C94: 3BC80002  addi r30, r8, 2
	ctx.r[30].s64 = ctx.r[8].s64 + 2;
	// 832C6C98: 5469103A  slwi r9, r3, 2
	// 832C6C9C: 386BFFFE  addi r3, r11, -2
	ctx.r[3].s64 = ctx.r[11].s64 + -2;
	// 832C6CA0: 7D092214  add r8, r9, r4
	ctx.r[8].u64 = ctx.r[9].u64 + ctx.r[4].u64;
	// 832C6CA4: 54C9103A  slwi r9, r6, 2
	// 832C6CA8: 547F103A  slwi r31, r3, 2
	// 832C6CAC: 55672036  slwi r7, r11, 4
	// 832C6CB0: 556A1838  slwi r10, r11, 3
	// 832C6CB4: 387BFFFD  addi r3, r27, -3
	ctx.r[3].s64 = ctx.r[27].s64 + -3;
	// 832C6CB8: 7F472214  add r26, r7, r4
	ctx.r[26].u64 = ctx.r[7].u64 + ctx.r[4].u64;
	// 832C6CBC: 57C6103A  slwi r6, r30, 2
	// 832C6CC0: 7D4A2214  add r10, r10, r4
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[4].u64;
	// 832C6CC4: 7FA92214  add r29, r9, r4
	ctx.r[29].u64 = ctx.r[9].u64 + ctx.r[4].u64;
	// 832C6CC8: 5479F87E  srwi r25, r3, 1
	// 832C6CCC: 7D3F2214  add r9, r31, r4
	ctx.r[9].u64 = ctx.r[31].u64 + ctx.r[4].u64;
	// 832C6CD0: 3BE80008  addi r31, r8, 8
	ctx.r[31].s64 = ctx.r[8].s64 + 8;
	// 832C6CD4: 38E8FFF8  addi r7, r8, -8
	ctx.r[7].s64 = ctx.r[8].s64 + -8;
	// 832C6CD8: 7C662A14  add r3, r6, r5
	ctx.r[3].u64 = ctx.r[6].u64 + ctx.r[5].u64;
	// 832C6CDC: 3BCA0008  addi r30, r10, 8
	ctx.r[30].s64 = ctx.r[10].s64 + 8;
	// 832C6CE0: 391AFFF8  addi r8, r26, -8
	ctx.r[8].s64 = ctx.r[26].s64 + -8;
	// 832C6CE4: 3B840008  addi r28, r4, 8
	ctx.r[28].s64 = ctx.r[4].s64 + 8;
	// 832C6CE8: 38C50008  addi r6, r5, 8
	ctx.r[6].s64 = ctx.r[5].s64 + 8;
	// 832C6CEC: 394AFFF8  addi r10, r10, -8
	ctx.r[10].s64 = ctx.r[10].s64 + -8;
	// 832C6CF0: 3B590001  addi r26, r25, 1
	ctx.r[26].s64 = ctx.r[25].s64 + 1;
	// 832C6CF4: 38C60010  addi r6, r6, 0x10
	ctx.r[6].s64 = ctx.r[6].s64 + 16;
	// 832C6CF8: C01F0004  lfs f0, 4(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(4 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C6CFC: 3863FFF0  addi r3, r3, -0x10
	ctx.r[3].s64 = ctx.r[3].s64 + -16;
	// 832C6D00: C17E0004  lfs f11, 4(r30)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(4 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C6D04: C0DD0000  lfs f6, 0(r29)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(0 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C6D08: 375AFFFF  addic. r26, r26, -1
	ctx.xer.ca = (ctx.r[26].u32 > (!(-1 as u32)));
	ctx.r[26].s64 = ctx.r[26].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[26].s32, 0, &mut ctx.xer);
	// 832C6D0C: C19C0000  lfs f12, 0(r28)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(0 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C6D10: EC660028  fsubs f3, f6, f0
	ctx.f[3].f64 = (((ctx.f[6].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C6D14: C1BF0000  lfs f13, 0(r31)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C6D18: ED2C5828  fsubs f9, f12, f11
	ctx.f[9].f64 = (((ctx.f[12].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C6D1C: C11E0000  lfs f8, 0(r30)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[30].u32.wrapping_add(0 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C6D20: ECEB602A  fadds f7, f11, f12
	ctx.f[7].f64 = ((ctx.f[11].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C6D24: C09D0004  lfs f4, 4(r29)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[29].u32.wrapping_add(4 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C6D28: ED60302A  fadds f11, f0, f6
	ctx.f[11].f64 = ((ctx.f[0].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C6D2C: C15C0004  lfs f10, 4(r28)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[28].u32.wrapping_add(4 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C6D30: EC2D202A  fadds f1, f13, f4
	ctx.f[1].f64 = ((ctx.f[13].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C6D34: C0460004  lfs f2, 4(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(4 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C6D38: ECA8502A  fadds f5, f8, f10
	ctx.f[5].f64 = ((ctx.f[8].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6D3C: C1830004  lfs f12, 4(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C6D40: FCC01050  fneg f6, f2
	ctx.f[6].u64 = ctx.f[2].u64 ^ 0x8000_0000_0000_0000u64;
	// 832C6D44: FC006050  fneg f0, f12
	ctx.f[0].u64 = ctx.f[12].u64 ^ 0x8000_0000_0000_0000u64;
	// 832C6D48: C046FFFC  lfs f2, -4(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(-4 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C6D4C: C183FFF8  lfs f12, -8(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(-8 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C6D50: EC846828  fsubs f4, f4, f13
	ctx.f[4].f64 = (((ctx.f[4].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C6D54: C3E60000  lfs f31, 0(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832C6D58: ED4A4028  fsubs f10, f10, f8
	ctx.f[10].f64 = (((ctx.f[10].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C6D5C: C1030000  lfs f8, 0(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C6D60: EF4C00F2  fmuls f26, f12, f3
	ctx.f[26].f64 = (((ctx.f[12].f64 * ctx.f[3].f64) as f32) as f64);
	// 832C6D64: EDA20272  fmuls f13, f2, f9
	ctx.f[13].f64 = (((ctx.f[2].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C6D68: C3C3FFFC  lfs f30, -4(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(-4 as u32) ) };
	ctx.f[30].f64 = (tmp.f32 as f64);
	// 832C6D6C: EFBF01F2  fmuls f29, f31, f7
	ctx.f[29].f64 = (((ctx.f[31].f64 * ctx.f[7].f64) as f32) as f64);
	// 832C6D70: C386FFF8  lfs f28, -8(r6)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(-8 as u32) ) };
	ctx.f[28].f64 = (tmp.f32 as f64);
	// 832C6D74: EF0802F2  fmuls f24, f8, f11
	ctx.f[24].f64 = (((ctx.f[8].f64 * ctx.f[11].f64) as f32) as f64);
	// 832C6D78: EF2C0072  fmuls f25, f12, f1
	ctx.f[25].f64 = (((ctx.f[12].f64 * ctx.f[1].f64) as f32) as f64);
	// 832C6D7C: EF620172  fmuls f27, f2, f5
	ctx.f[27].f64 = (((ctx.f[2].f64 * ctx.f[5].f64) as f32) as f64);
	// 832C6D80: ECE601F2  fmuls f7, f6, f7
	ctx.f[7].f64 = (((ctx.f[6].f64 * ctx.f[7].f64) as f32) as f64);
	// 832C6D84: ED6002F2  fmuls f11, f0, f11
	ctx.f[11].f64 = (((ctx.f[0].f64 * ctx.f[11].f64) as f32) as f64);
	// 832C6D88: EC3ED07A  fmadds f1, f30, f1, f26
	ctx.f[1].f64 = (((ctx.f[30].f64 * ctx.f[1].f64 + ctx.f[26].f64) as f32) as f64);
	// 832C6D8C: ECBC697A  fmadds f5, f28, f5, f13
	ctx.f[5].f64 = (((ctx.f[28].f64 * ctx.f[5].f64 + ctx.f[13].f64) as f32) as f64);
	// 832C6D90: EDA6EABA  fmadds f13, f6, f10, f29
	ctx.f[13].f64 = (((ctx.f[6].f64 * ctx.f[10].f64 + ctx.f[29].f64) as f32) as f64);
	// 832C6D94: EFA0C138  fmsubs f29, f0, f4, f24
	ctx.f[29].f64 = (((ctx.f[0].f64 * ctx.f[4].f64 - ctx.f[24].f64) as f32) as f64);
	// 832C6D98: EC7EC8F8  fmsubs f3, f30, f3, f25
	ctx.f[3].f64 = (((ctx.f[30].f64 * ctx.f[3].f64 - ctx.f[25].f64) as f32) as f64);
	// 832C6D9C: ED3CDA78  fmsubs f9, f28, f9, f27
	ctx.f[9].f64 = (((ctx.f[28].f64 * ctx.f[9].f64 - ctx.f[27].f64) as f32) as f64);
	// 832C6DA0: ED5F3AB8  fmsubs f10, f31, f10, f7
	ctx.f[10].f64 = (((ctx.f[31].f64 * ctx.f[10].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C6DA4: ECE8593A  fmadds f7, f8, f4, f11
	ctx.f[7].f64 = (((ctx.f[8].f64 * ctx.f[4].f64 + ctx.f[11].f64) as f32) as f64);
	// 832C6DA8: EC81282A  fadds f4, f1, f5
	ctx.f[4].f64 = ((ctx.f[1].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C6DAC: D09C0004  stfs f4, 4(r28)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6DB0: ECA50828  fsubs f5, f5, f1
	ctx.f[5].f64 = (((ctx.f[5].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C6DB4: ED63482A  fadds f11, f3, f9
	ctx.f[11].f64 = ((ctx.f[3].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C6DB8: D17C0000  stfs f11, 0(r28)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6DBC: EC891828  fsubs f4, f9, f3
	ctx.f[4].f64 = (((ctx.f[9].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C6DC0: D0BD0004  stfs f5, 4(r29)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6DC4: D09D0000  stfs f4, 0(r29)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[29].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6DC8: EC67682A  fadds f3, f7, f13
	ctx.f[3].f64 = ((ctx.f[7].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C6DCC: EC3D502A  fadds f1, f29, f10
	ctx.f[1].f64 = ((ctx.f[29].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6DD0: D03E0004  stfs f1, 4(r30)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6DD4: D07E0000  stfs f3, 0(r30)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[30].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6DD8: EDAD3828  fsubs f13, f13, f7
	ctx.f[13].f64 = (((ctx.f[13].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C6DDC: ED6AE828  fsubs f11, f10, f29
	ctx.f[11].f64 = (((ctx.f[10].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C6DE0: D1BF0000  stfs f13, 0(r31)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6DE4: D17F0004  stfs f11, 4(r31)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[31].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6DE8: 3BFF0008  addi r31, r31, 8
	ctx.r[31].s64 = ctx.r[31].s64 + 8;
	// 832C6DEC: C1270004  lfs f9, 4(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(4 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C6DF0: 3BDE0008  addi r30, r30, 8
	ctx.r[30].s64 = ctx.r[30].s64 + 8;
	// 832C6DF4: C1490000  lfs f10, 0(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C6DF8: 3BBD0008  addi r29, r29, 8
	ctx.r[29].s64 = ctx.r[29].s64 + 8;
	// 832C6DFC: C0A80004  lfs f5, 4(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(4 as u32) ) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832C6E00: C0EA0000  lfs f7, 0(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C6E04: C0880000  lfs f4, 0(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C6E08: C06A0004  lfs f3, 4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C6E0C: C0290004  lfs f1, 4(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(4 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C6E10: C1A70000  lfs f13, 0(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(0 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C6E14: ED6D082A  fadds f11, f13, f1
	ctx.f[11].f64 = ((ctx.f[13].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C6E18: EFAA4828  fsubs f29, f10, f9
	ctx.f[29].f64 = (((ctx.f[10].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C6E1C: ED49502A  fadds f10, f9, f10
	ctx.f[10].f64 = ((ctx.f[9].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6E20: ED216828  fsubs f9, f1, f13
	ctx.f[9].f64 = (((ctx.f[1].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C6E24: EC272828  fsubs f1, f7, f5
	ctx.f[1].f64 = (((ctx.f[7].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C6E28: ECE5382A  fadds f7, f5, f7
	ctx.f[7].f64 = ((ctx.f[5].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C6E2C: EDA4182A  fadds f13, f4, f3
	ctx.f[13].f64 = ((ctx.f[4].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C6E30: EF7E02F2  fmuls f27, f30, f11
	ctx.f[27].f64 = (((ctx.f[30].f64 * ctx.f[11].f64) as f32) as f64);
	// 832C6E34: ECBE0772  fmuls f5, f30, f29
	ctx.f[5].f64 = (((ctx.f[30].f64 * ctx.f[29].f64) as f32) as f64);
	// 832C6E38: EC832028  fsubs f4, f3, f4
	ctx.f[4].f64 = (((ctx.f[3].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C6E3C: 3B9C0008  addi r28, r28, 8
	ctx.r[28].s64 = ctx.r[28].s64 + 8;
	// 832C6E40: EFDC0072  fmuls f30, f28, f1
	ctx.f[30].f64 = (((ctx.f[28].f64 * ctx.f[1].f64) as f32) as f64);
	// 832C6E44: EC6802B2  fmuls f3, f8, f10
	ctx.f[3].f64 = (((ctx.f[8].f64 * ctx.f[10].f64) as f32) as f64);
	// 832C6E48: EFACDF78  fmsubs f29, f12, f29, f27
	ctx.f[29].f64 = (((ctx.f[12].f64 * ctx.f[29].f64 - ctx.f[27].f64) as f32) as f64);
	// 832C6E4C: EF9C0372  fmuls f28, f28, f13
	ctx.f[28].f64 = (((ctx.f[28].f64 * ctx.f[13].f64) as f32) as f64);
	// 832C6E50: EF6601F2  fmuls f27, f6, f7
	ctx.f[27].f64 = (((ctx.f[6].f64 * ctx.f[7].f64) as f32) as f64);
	// 832C6E54: ED4002B2  fmuls f10, f0, f10
	ctx.f[10].f64 = (((ctx.f[0].f64 * ctx.f[10].f64) as f32) as f64);
	// 832C6E58: ECFF01F2  fmuls f7, f31, f7
	ctx.f[7].f64 = (((ctx.f[31].f64 * ctx.f[7].f64) as f32) as f64);
	// 832C6E5C: ECAC2AFA  fmadds f5, f12, f11, f5
	ctx.f[5].f64 = (((ctx.f[12].f64 * ctx.f[11].f64 + ctx.f[5].f64) as f32) as f64);
	// 832C6E60: EDA2F37A  fmadds f13, f2, f13, f30
	ctx.f[13].f64 = (((ctx.f[2].f64 * ctx.f[13].f64 + ctx.f[30].f64) as f32) as f64);
	// 832C6E64: ED601A7A  fmadds f11, f0, f9, f3
	ctx.f[11].f64 = (((ctx.f[0].f64 * ctx.f[9].f64 + ctx.f[3].f64) as f32) as f64);
	// 832C6E68: ED82E078  fmsubs f12, f2, f1, f28
	ctx.f[12].f64 = (((ctx.f[2].f64 * ctx.f[1].f64 - ctx.f[28].f64) as f32) as f64);
	// 832C6E6C: ED485278  fmsubs f10, f8, f9, f10
	ctx.f[10].f64 = (((ctx.f[8].f64 * ctx.f[9].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C6E70: ED063938  fmsubs f8, f6, f4, f7
	ctx.f[8].f64 = (((ctx.f[6].f64 * ctx.f[4].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C6E74: ED3FD93A  fmadds f9, f31, f4, f27
	ctx.f[9].f64 = (((ctx.f[31].f64 * ctx.f[4].f64 + ctx.f[27].f64) as f32) as f64);
	// 832C6E78: ECED282A  fadds f7, f13, f5
	ctx.f[7].f64 = ((ctx.f[13].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C6E7C: D0E90004  stfs f7, 4(r9)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6E80: ECA56828  fsubs f5, f5, f13
	ctx.f[5].f64 = (((ctx.f[5].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C6E84: ECCCE82A  fadds f6, f12, f29
	ctx.f[6].f64 = ((ctx.f[12].f64 + ctx.f[29].f64) as f32) as f64;
	// 832C6E88: D0C90000  stfs f6, 0(r9)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6E8C: EC9D6028  fsubs f4, f29, f12
	ctx.f[4].f64 = (((ctx.f[29].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C6E90: D0AA0004  stfs f5, 4(r10)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6E94: D08A0000  stfs f4, 0(r10)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6E98: 394AFFF8  addi r10, r10, -8
	ctx.r[10].s64 = ctx.r[10].s64 + -8;
	// 832C6E9C: EC48502A  fadds f2, f8, f10
	ctx.f[2].f64 = ((ctx.f[8].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6EA0: D0470004  stfs f2, 4(r7)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6EA4: EC69582A  fadds f3, f9, f11
	ctx.f[3].f64 = ((ctx.f[9].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C6EA8: D0670000  stfs f3, 0(r7)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[7].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6EAC: EC2A4028  fsubs f1, f10, f8
	ctx.f[1].f64 = (((ctx.f[10].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C6EB0: D0280004  stfs f1, 4(r8)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6EB4: EC0B4828  fsubs f0, f11, f9
	ctx.f[0].f64 = (((ctx.f[11].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C6EB8: D0080000  stfs f0, 0(r8)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6EBC: 3929FFF8  addi r9, r9, -8
	ctx.r[9].s64 = ctx.r[9].s64 + -8;
	// 832C6EC0: 3908FFF8  addi r8, r8, -8
	ctx.r[8].s64 = ctx.r[8].s64 + -8;
	// 832C6EC4: 38E7FFF8  addi r7, r7, -8
	ctx.r[7].s64 = ctx.r[7].s64 + -8;
	// 832C6EC8: 4082FE2C  bne 0x832c6cf4
	if !ctx.cr[0].eq {
	pc = 0x832C6CF4; continue 'dispatch;
	}
	// 832C6ECC: 7D4BDA14  add r10, r11, r27
	ctx.r[10].u64 = ctx.r[11].u64 + ctx.r[27].u64;
	// 832C6ED0: 7C182C2E  lfsx f0, r24, r5
	tmp.u32 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[24].u32.wrapping_add(ctx.r[5].u32)) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C6ED4: 5768103A  slwi r8, r27, 2
	// 832C6ED8: 7D2A5A14  add r9, r10, r11
	ctx.r[9].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C6EDC: 5547103A  slwi r7, r10, 2
	// 832C6EE0: 7D695A14  add r11, r9, r11
	ctx.r[11].u64 = ctx.r[9].u64 + ctx.r[11].u64;
	// 832C6EE4: 552A103A  slwi r10, r9, 2
	// 832C6EE8: 556B103A  slwi r11, r11, 2
	// 832C6EEC: 7D4A2214  add r10, r10, r4
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[4].u64;
	// 832C6EF0: 7D082214  add r8, r8, r4
	ctx.r[8].u64 = ctx.r[8].u64 + ctx.r[4].u64;
	// 832C6EF4: 7D2B2214  add r9, r11, r4
	ctx.r[9].u64 = ctx.r[11].u64 + ctx.r[4].u64;
	// 832C6EF8: 7D672214  add r11, r7, r4
	ctx.r[11].u64 = ctx.r[7].u64 + ctx.r[4].u64;
	// 832C6EFC: 7CF82A14  add r7, r24, r5
	ctx.r[7].u64 = ctx.r[24].u64 + ctx.r[5].u64;
	// 832C6F00: C1AA0000  lfs f13, 0(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C6F04: C1280004  lfs f9, 4(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(4 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C6F08: C10A0004  lfs f8, 4(r10)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C6F0C: ECED482A  fadds f7, f13, f9
	ctx.f[7].f64 = ((ctx.f[13].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C6F10: C1480000  lfs f10, 0(r8)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[8].u32.wrapping_add(0 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C6F14: ED296828  fsubs f9, f9, f13
	ctx.f[9].f64 = (((ctx.f[9].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C6F18: C1890000  lfs f12, 0(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C6F1C: ECAA4028  fsubs f5, f10, f8
	ctx.f[5].f64 = (((ctx.f[10].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C6F20: C16B0000  lfs f11, 0(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C6F24: ED48502A  fadds f10, f8, f10
	ctx.f[10].f64 = ((ctx.f[8].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C6F28: C0C90004  lfs f6, 4(r9)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[9].u32.wrapping_add(4 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C6F2C: C08B0004  lfs f4, 4(r11)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C6F30: EC6B3028  fsubs f3, f11, f6
	ctx.f[3].f64 = (((ctx.f[11].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C6F34: EC44602A  fadds f2, f4, f12
	ctx.f[2].f64 = ((ctx.f[4].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C6F38: C0270004  lfs f1, 4(r7)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[7].u32.wrapping_add(4 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C6F3C: ED06582A  fadds f8, f6, f11
	ctx.f[8].f64 = ((ctx.f[6].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C6F40: ECC46028  fsubs f6, f4, f12
	ctx.f[6].f64 = (((ctx.f[4].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C6F44: EC8101F2  fmuls f4, f1, f7
	ctx.f[4].f64 = (((ctx.f[1].f64 * ctx.f[7].f64) as f32) as f64);
	// 832C6F48: EFC00272  fmuls f30, f0, f9
	ctx.f[30].f64 = (((ctx.f[0].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C6F4C: EDA10172  fmuls f13, f1, f5
	ctx.f[13].f64 = (((ctx.f[1].f64 * ctx.f[5].f64) as f32) as f64);
	// 832C6F50: EFE002B2  fmuls f31, f0, f10
	ctx.f[31].f64 = (((ctx.f[0].f64 * ctx.f[10].f64) as f32) as f64);
	// 832C6F54: ED8000F2  fmuls f12, f0, f3
	ctx.f[12].f64 = (((ctx.f[0].f64 * ctx.f[3].f64) as f32) as f64);
	// 832C6F58: ED6000B2  fmuls f11, f0, f2
	ctx.f[11].f64 = (((ctx.f[0].f64 * ctx.f[2].f64) as f32) as f64);
	// 832C6F5C: EFA10232  fmuls f29, f1, f8
	ctx.f[29].f64 = (((ctx.f[1].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C6F60: EF8101B2  fmuls f28, f1, f6
	ctx.f[28].f64 = (((ctx.f[1].f64 * ctx.f[6].f64) as f32) as f64);
	// 832C6F64: ECA02178  fmsubs f5, f0, f5, f4
	ctx.f[5].f64 = (((ctx.f[0].f64 * ctx.f[5].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C6F68: EC8069FA  fmadds f4, f0, f7, f13
	ctx.f[4].f64 = (((ctx.f[0].f64 * ctx.f[7].f64 + ctx.f[13].f64) as f32) as f64);
	// 832C6F6C: EC4160BA  fmadds f2, f1, f2, f12
	ctx.f[2].f64 = (((ctx.f[1].f64 * ctx.f[2].f64 + ctx.f[12].f64) as f32) as f64);
	// 832C6F70: EDA158F8  fmsubs f13, f1, f3, f11
	ctx.f[13].f64 = (((ctx.f[1].f64 * ctx.f[3].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C6F74: ED61F2B8  fmsubs f11, f1, f10, f30
	ctx.f[11].f64 = (((ctx.f[1].f64 * ctx.f[10].f64 - ctx.f[30].f64) as f32) as f64);
	// 832C6F78: ED81FA7A  fmadds f12, f1, f9, f31
	ctx.f[12].f64 = (((ctx.f[1].f64 * ctx.f[9].f64 + ctx.f[31].f64) as f32) as f64);
	// 832C6F7C: ED40E9BA  fmadds f10, f0, f6, f29
	ctx.f[10].f64 = (((ctx.f[0].f64 * ctx.f[6].f64 + ctx.f[29].f64) as f32) as f64);
	// 832C6F80: ED20E238  fmsubs f9, f0, f8, f28
	ctx.f[9].f64 = (((ctx.f[0].f64 * ctx.f[8].f64 - ctx.f[28].f64) as f32) as f64);
	// 832C6F84: ED02202A  fadds f8, f2, f4
	ctx.f[8].f64 = ((ctx.f[2].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C6F88: D1080004  stfs f8, 4(r8)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6F8C: ECED282A  fadds f7, f13, f5
	ctx.f[7].f64 = ((ctx.f[13].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C6F90: D0E80000  stfs f7, 0(r8)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6F94: ECC56828  fsubs f6, f5, f13
	ctx.f[6].f64 = (((ctx.f[5].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C6F98: D0CB0000  stfs f6, 0(r11)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6F9C: ECA41028  fsubs f5, f4, f2
	ctx.f[5].f64 = (((ctx.f[4].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C6FA0: D0AB0004  stfs f5, 4(r11)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6FA4: EC8C5028  fsubs f4, f12, f10
	ctx.f[4].f64 = (((ctx.f[12].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C6FA8: D08A0004  stfs f4, 4(r10)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6FAC: EC6B4828  fsubs f3, f11, f9
	ctx.f[3].f64 = (((ctx.f[11].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C6FB0: D06A0000  stfs f3, 0(r10)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6FB4: EC49582A  fadds f2, f9, f11
	ctx.f[2].f64 = ((ctx.f[9].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C6FB8: D0490000  stfs f2, 0(r9)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C6FBC: EC2A602A  fadds f1, f10, f12
	ctx.f[1].f64 = ((ctx.f[10].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C6FC0: D0290004  stfs f1, 4(r9)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C6FC4: 3981FFB8  addi r12, r1, -0x48
	ctx.r[12].s64 = ctx.r[1].s64 + -72;
	// 832C6FC8: 4B9E6D4D  bl 0x82cadd14
	ctx.lr = 0x832C6FCC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD14);
	// 832C6FCC: 4B9E247C  b 0x82ca9448
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9448);
	return;
}

pub fn sub_832C6FD0(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C6FD0 size=12
	// 832C6FD0: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C6FD4: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832C6FD8: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
}

pub fn sub_832C7070(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C7070 size=168
	// 832C7070: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C7074: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832C7078: 9421FFA0  stwu r1, -0x60(r1)
	ea = ctx.r[1].u32.wrapping_add(-96 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C707C: 7C8A2378  mr r10, r4
	ctx.r[10].u64 = ctx.r[4].u64;
	// 832C7080: 2F030080  cmpwi cr6, r3, 0x80
	ctx.cr[6].compare_i32(ctx.r[3].s32, 128, &mut ctx.xer);
	// 832C7084: 7D435378  mr r3, r10
	ctx.r[3].u64 = ctx.r[10].u64;
	// 832C7088: 409A0058  bne cr6, 0x832c70e0
	if !ctx.cr[6].eq {
	pc = 0x832C70E0; continue 'dispatch;
	}
	// 832C708C: 3965FFF8  addi r11, r5, -8
	ctx.r[11].s64 = ctx.r[5].s64 + -8;
	// 832C7090: 556B103A  slwi r11, r11, 2
	// 832C7094: 7D2B3214  add r9, r11, r6
	ctx.r[9].u64 = ctx.r[11].u64 + ctx.r[6].u64;
	// 832C7098: 7D244B78  mr r4, r9
	ctx.r[4].u64 = ctx.r[9].u64;
	// 832C709C: 4800007D  bl 0x832c7118
	ctx.lr = 0x832C70A0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7118);
	// 832C70A0: 3905FFE0  addi r8, r5, -0x20
	ctx.r[8].s64 = ctx.r[5].s64 + -32;
	// 832C70A4: 386A0080  addi r3, r10, 0x80
	ctx.r[3].s64 = ctx.r[10].s64 + 128;
	// 832C70A8: 550B103A  slwi r11, r8, 2
	// 832C70AC: 7D0B3214  add r8, r11, r6
	ctx.r[8].u64 = ctx.r[11].u64 + ctx.r[6].u64;
	// 832C70B0: 7D044378  mr r4, r8
	ctx.r[4].u64 = ctx.r[8].u64;
	// 832C70B4: 48000475  bl 0x832c7528
	ctx.lr = 0x832C70B8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7528);
	// 832C70B8: 7D244B78  mr r4, r9
	ctx.r[4].u64 = ctx.r[9].u64;
	// 832C70BC: 386A0100  addi r3, r10, 0x100
	ctx.r[3].s64 = ctx.r[10].s64 + 256;
	// 832C70C0: 48000059  bl 0x832c7118
	ctx.lr = 0x832C70C4;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7118);
	// 832C70C4: 7D044378  mr r4, r8
	ctx.r[4].u64 = ctx.r[8].u64;
	// 832C70C8: 386A0180  addi r3, r10, 0x180
	ctx.r[3].s64 = ctx.r[10].s64 + 384;
	// 832C70CC: 4800045D  bl 0x832c7528
	ctx.lr = 0x832C70D0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7528);
	// 832C70D0: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832C70D4: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832C70D8: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832C70DC: 4E800020  blr
	return;
	// 832C70E0: 3965FFF0  addi r11, r5, -0x10
	ctx.r[11].s64 = ctx.r[5].s64 + -16;
	// 832C70E4: 556B103A  slwi r11, r11, 2
	// 832C70E8: 7C8B3214  add r4, r11, r6
	ctx.r[4].u64 = ctx.r[11].u64 + ctx.r[6].u64;
	// 832C70EC: 48000955  bl 0x832c7a40
	ctx.lr = 0x832C70F0;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7A40);
	// 832C70F0: 386A0040  addi r3, r10, 0x40
	ctx.r[3].s64 = ctx.r[10].s64 + 64;
	// 832C70F4: 48000AD5  bl 0x832c7bc8
	ctx.lr = 0x832C70F8;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7BC8);
	// 832C70F8: 386A0080  addi r3, r10, 0x80
	ctx.r[3].s64 = ctx.r[10].s64 + 128;
	// 832C70FC: 48000945  bl 0x832c7a40
	ctx.lr = 0x832C7100;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7A40);
	// 832C7100: 386A00C0  addi r3, r10, 0xc0
	ctx.r[3].s64 = ctx.r[10].s64 + 192;
	// 832C7104: 48000AC5  bl 0x832c7bc8
	ctx.lr = 0x832C7108;
	crate::recompiler::externs::call(&mut ctx, base, 0x832C7BC8);
	// 832C7108: 38210060  addi r1, r1, 0x60
	ctx.r[1].s64 = ctx.r[1].s64 + 96;
	// 832C710C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832C7110: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832C7114: 4E800020  blr
	return;
}

pub fn sub_832C7118(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    // ---- function 0x832C7118 size=1040
	// 832C7118: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C711C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832C7120: 3981FFF8  addi r12, r1, -8
	ctx.r[12].s64 = ctx.r[1].s64 + -8;
	// 832C7124: 4B9E6B7D  bl 0x82cadca0
	ctx.lr = 0x832C7128;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCA0);
	// 832C7128: C0030048  lfs f0, 0x48(r3)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(72 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C712C: C1A30008  lfs f13, 8(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(8 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C7130: C1830068  lfs f12, 0x68(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(104 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C7134: ED6D0028  fsubs f11, f13, f0
	ctx.f[11].f64 = (((ctx.f[13].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C7138: C1430028  lfs f10, 0x28(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C713C: ED2D002A  fadds f9, f13, f0
	ctx.f[9].f64 = ((ctx.f[13].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C7140: C083004C  lfs f4, 0x4c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(76 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C7144: ECEA6028  fsubs f7, f10, f12
	ctx.f[7].f64 = (((ctx.f[10].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C7148: C043000C  lfs f2, 0xc(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(12 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C714C: ECAA602A  fadds f5, f10, f12
	ctx.f[5].f64 = ((ctx.f[10].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C7150: C103006C  lfs f8, 0x6c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(108 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C7154: EDA22028  fsubs f13, f2, f4
	ctx.f[13].f64 = (((ctx.f[2].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C7158: C0C3002C  lfs f6, 0x2c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C715C: ED42202A  fadds f10, f2, f4
	ctx.f[10].f64 = ((ctx.f[2].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C7160: C1840004  lfs f12, 4(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(4 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C7164: EC664028  fsubs f3, f6, f8
	ctx.f[3].f64 = (((ctx.f[6].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C7168: C0040008  lfs f0, 8(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(8 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C716C: EC26402A  fadds f1, f6, f8
	ctx.f[1].f64 = ((ctx.f[6].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C7170: ED000332  fmuls f8, f0, f12
	ctx.f[8].f64 = (((ctx.f[0].f64 * ctx.f[12].f64) as f32) as f64);
	// 832C7174: C0830000  lfs f4, 0(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C7178: C3C30020  lfs f30, 0x20(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(32 as u32) ) };
	ctx.f[30].f64 = (tmp.f32 as f64);
	// 832C717C: C0C30040  lfs f6, 0x40(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(64 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C7180: C0430060  lfs f2, 0x60(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(96 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C7184: EFE4302A  fadds f31, f4, f6
	ctx.f[31].f64 = ((ctx.f[4].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C7188: EFBE102A  fadds f29, f30, f2
	ctx.f[29].f64 = ((ctx.f[30].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C718C: C3830024  lfs f28, 0x24(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(36 as u32) ) };
	ctx.f[28].f64 = (tmp.f32 as f64);
	// 832C7190: ECC43028  fsubs f6, f4, f6
	ctx.f[6].f64 = (((ctx.f[4].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C7194: C0830064  lfs f4, 0x64(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(100 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C7198: EF07682A  fadds f24, f7, f13
	ctx.f[24].f64 = ((ctx.f[7].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C719C: C3430004  lfs f26, 4(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) };
	ctx.f[26].f64 = (tmp.f32 as f64);
	// 832C71A0: EC5E1028  fsubs f2, f30, f2
	ctx.f[2].f64 = (((ctx.f[30].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C71A4: C3630044  lfs f27, 0x44(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(68 as u32) ) };
	ctx.f[27].f64 = (tmp.f32 as f64);
	// 832C71A8: EFCB1828  fsubs f30, f11, f3
	ctx.f[30].f64 = (((ctx.f[11].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C71AC: C2E30050  lfs f23, 0x50(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(80 as u32) ) };
	ctx.f[23].f64 = (tmp.f32 as f64);
	// 832C71B0: EDAD3828  fsubs f13, f13, f7
	ctx.f[13].f64 = (((ctx.f[13].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C71B4: C0E30010  lfs f7, 0x10(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(16 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C71B8: EF3C2028  fsubs f25, f28, f4
	ctx.f[25].f64 = (((ctx.f[28].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C71BC: C2C30054  lfs f22, 0x54(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(84 as u32) ) };
	ctx.f[22].f64 = (tmp.f32 as f64);
	// 832C71C0: EC00402A  fadds f0, f0, f8
	ctx.f[0].f64 = ((ctx.f[0].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C71C4: D181FF44  stfs f12, -0xbc(r1)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-188 as u32), tmp.u32 ) };
	// 832C71C8: EEBAD82A  fadds f21, f26, f27
	ctx.f[21].f64 = ((ctx.f[26].f64 + ctx.f[27].f64) as f32) as f64;
	// 832C71CC: C2830070  lfs f20, 0x70(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(112 as u32) ) };
	ctx.f[20].f64 = (tmp.f32 as f64);
	// 832C71D0: EC9C202A  fadds f4, f28, f4
	ctx.f[4].f64 = ((ctx.f[28].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C71D4: C3830014  lfs f28, 0x14(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(20 as u32) ) };
	ctx.f[28].f64 = (tmp.f32 as f64);
	// 832C71D8: EF7AD828  fsubs f27, f26, f27
	ctx.f[27].f64 = (((ctx.f[26].f64 - ctx.f[27].f64) as f32) as f64);
	// 832C71DC: C3430030  lfs f26, 0x30(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) };
	ctx.f[26].f64 = (tmp.f32 as f64);
	// 832C71E0: EE5DF82A  fadds f18, f29, f31
	ctx.f[18].f64 = ((ctx.f[29].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C71E4: D241FF54  stfs f18, -0xac(r1)
	tmp.f32 = (ctx.f[18].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-172 as u32), tmp.u32 ) };
	// 832C71E8: EFFFE828  fsubs f31, f31, f29
	ctx.f[31].f64 = (((ctx.f[31].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C71EC: D3E1FF50  stfs f31, -0xb0(r1)
	tmp.f32 = (ctx.f[31].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-176 as u32), tmp.u32 ) };
	// 832C71F0: C3E3003C  lfs f31, 0x3c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(60 as u32) ) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832C71F4: EFB80232  fmuls f29, f24, f8
	ctx.f[29].f64 = (((ctx.f[24].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C71F8: EE3E0232  fmuls f17, f30, f8
	ctx.f[17].f64 = (((ctx.f[30].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C71FC: D3E1FF40  stfs f31, -0xc0(r1)
	tmp.f32 = (ctx.f[31].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-192 as u32), tmp.u32 ) };
	// 832C7200: EE4D0232  fmuls f18, f13, f8
	ctx.f[18].f64 = (((ctx.f[13].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C7204: C2630034  lfs f19, 0x34(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(52 as u32) ) };
	ctx.f[19].f64 = (tmp.f32 as f64);
	// 832C7208: EC63582A  fadds f3, f3, f11
	ctx.f[3].f64 = ((ctx.f[3].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C720C: C1630074  lfs f11, 0x74(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(116 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C7210: EFE6C828  fsubs f31, f6, f25
	ctx.f[31].f64 = (((ctx.f[6].f64 - ctx.f[25].f64) as f32) as f64);
	// 832C7214: C203005C  lfs f16, 0x5c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(92 as u32) ) };
	ctx.f[16].f64 = (tmp.f32 as f64);
	// 832C7218: EDAD0032  fmuls f13, f13, f0
	ctx.f[13].f64 = (((ctx.f[13].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C721C: C1E3001C  lfs f15, 0x1c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(28 as u32) ) };
	ctx.f[15].f64 = (tmp.f32 as f64);
	// 832C7220: ECD9302A  fadds f6, f25, f6
	ctx.f[6].f64 = ((ctx.f[25].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C7224: C1C3007C  lfs f14, 0x7c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(124 as u32) ) };
	ctx.f[14].f64 = (tmp.f32 as f64);
	// 832C7228: EF24A82A  fadds f25, f4, f21
	ctx.f[25].f64 = ((ctx.f[4].f64 + ctx.f[21].f64) as f32) as f64;
	// 832C722C: C1830078  lfs f12, 0x78(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(120 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C7230: EC952028  fsubs f4, f21, f4
	ctx.f[4].f64 = (((ctx.f[21].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C7234: EEA2D82A  fadds f21, f2, f27
	ctx.f[21].f64 = ((ctx.f[2].f64 + ctx.f[27].f64) as f32) as f64;
	// 832C7238: EC5B1028  fsubs f2, f27, f2
	ctx.f[2].f64 = (((ctx.f[27].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C723C: EF65482A  fadds f27, f5, f9
	ctx.f[27].f64 = ((ctx.f[5].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C7240: ED292828  fsubs f9, f9, f5
	ctx.f[9].f64 = (((ctx.f[9].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C7244: ECA1502A  fadds f5, f1, f10
	ctx.f[5].f64 = ((ctx.f[1].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C7248: EC2A0828  fsubs f1, f10, f1
	ctx.f[1].f64 = (((ctx.f[10].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C724C: ED58883A  fmadds f10, f24, f0, f17
	ctx.f[10].f64 = (((ctx.f[24].f64 * ctx.f[0].f64 + ctx.f[17].f64) as f32) as f64);
	// 832C7250: EFDEE838  fmsubs f30, f30, f0, f29
	ctx.f[30].f64 = (((ctx.f[30].f64 * ctx.f[0].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C7254: EDA36A38  fmsubs f13, f3, f8, f13
	ctx.f[13].f64 = (((ctx.f[3].f64 * ctx.f[8].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C7258: EFA7B82A  fadds f29, f7, f23
	ctx.f[29].f64 = ((ctx.f[7].f64 + ctx.f[23].f64) as f32) as f64;
	// 832C725C: EF1CB02A  fadds f24, f28, f22
	ctx.f[24].f64 = ((ctx.f[28].f64 + ctx.f[22].f64) as f32) as f64;
	// 832C7260: EC63903A  fmadds f3, f3, f0, f18
	ctx.f[3].f64 = (((ctx.f[3].f64 * ctx.f[0].f64 + ctx.f[18].f64) as f32) as f64);
	// 832C7264: ECE7B828  fsubs f7, f7, f23
	ctx.f[7].f64 = (((ctx.f[7].f64 - ctx.f[23].f64) as f32) as f64);
	// 832C7268: EF9CB028  fsubs f28, f28, f22
	ctx.f[28].f64 = (((ctx.f[28].f64 - ctx.f[22].f64) as f32) as f64);
	// 832C726C: EEFAA02A  fadds f23, f26, f20
	ctx.f[23].f64 = ((ctx.f[26].f64 + ctx.f[20].f64) as f32) as f64;
	// 832C7270: D361FF5C  stfs f27, -0xa4(r1)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-164 as u32), tmp.u32 ) };
	// 832C7274: EED3582A  fadds f22, f19, f11
	ctx.f[22].f64 = ((ctx.f[19].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C7278: D0A1FF60  stfs f5, -0xa0(r1)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-160 as u32), tmp.u32 ) };
	// 832C727C: EF5AA028  fsubs f26, f26, f20
	ctx.f[26].f64 = (((ctx.f[26].f64 - ctx.f[20].f64) as f32) as f64);
	// 832C7280: C2830038  lfs f20, 0x38(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(56 as u32) ) };
	ctx.f[20].f64 = (tmp.f32 as f64);
	// 832C7284: ED735828  fsubs f11, f19, f11
	ctx.f[11].f64 = (((ctx.f[19].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C7288: C361FF40  lfs f27, -0xc0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-192 as u32) ) };
	ctx.f[27].f64 = (tmp.f32 as f64);
	// 832C728C: EE6F8028  fsubs f19, f15, f16
	ctx.f[19].f64 = (((ctx.f[15].f64 - ctx.f[16].f64) as f32) as f64);
	// 832C7290: C2430058  lfs f18, 0x58(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(88 as u32) ) };
	ctx.f[18].f64 = (tmp.f32 as f64);
	// 832C7294: EE346028  fsubs f17, f20, f12
	ctx.f[17].f64 = (((ctx.f[20].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C7298: C0A30018  lfs f5, 0x18(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(24 as u32) ) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832C729C: D321FF58  stfs f25, -0xa8(r1)
	tmp.f32 = (ctx.f[25].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-168 as u32), tmp.u32 ) };
	// 832C72A0: EF3B7028  fsubs f25, f27, f14
	ctx.f[25].f64 = (((ctx.f[27].f64 - ctx.f[14].f64) as f32) as f64);
	// 832C72A4: D081FF4C  stfs f4, -0xb4(r1)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-180 as u32), tmp.u32 ) };
	// 832C72A8: EC859028  fsubs f4, f5, f18
	ctx.f[4].f64 = (((ctx.f[5].f64 - ctx.f[18].f64) as f32) as f64);
	// 832C72AC: D121FF40  stfs f9, -0xc0(r1)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-192 as u32), tmp.u32 ) };
	// 832C72B0: ED94602A  fadds f12, f20, f12
	ctx.f[12].f64 = ((ctx.f[20].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C72B4: D021FF48  stfs f1, -0xb8(r1)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-184 as u32), tmp.u32 ) };
	// 832C72B8: ECA5902A  fadds f5, f5, f18
	ctx.f[5].f64 = ((ctx.f[5].f64 + ctx.f[18].f64) as f32) as f64;
	// 832C72BC: EC3AE02A  fadds f1, f26, f28
	ctx.f[1].f64 = ((ctx.f[26].f64 + ctx.f[28].f64) as f32) as f64;
	// 832C72C0: ED275828  fsubs f9, f7, f11
	ctx.f[9].f64 = (((ctx.f[7].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C72C4: ECEB382A  fadds f7, f11, f7
	ctx.f[7].f64 = ((ctx.f[11].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C72C8: EF9CD028  fsubs f28, f28, f26
	ctx.f[28].f64 = (((ctx.f[28].f64 - ctx.f[26].f64) as f32) as f64);
	// 832C72CC: EF51982A  fadds f26, f17, f19
	ctx.f[26].f64 = ((ctx.f[17].f64 + ctx.f[19].f64) as f32) as f64;
	// 832C72D0: EE938828  fsubs f20, f19, f17
	ctx.f[20].f64 = (((ctx.f[19].f64 - ctx.f[17].f64) as f32) as f64);
	// 832C72D4: EE79202A  fadds f19, f25, f4
	ctx.f[19].f64 = ((ctx.f[25].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C72D8: EC84C828  fsubs f4, f4, f25
	ctx.f[4].f64 = (((ctx.f[4].f64 - ctx.f[25].f64) as f32) as f64);
	// 832C72DC: ED7B702A  fadds f11, f27, f14
	ctx.f[11].f64 = ((ctx.f[27].f64 + ctx.f[14].f64) as f32) as f64;
	// 832C72E0: EF6F802A  fadds f27, f15, f16
	ctx.f[27].f64 = ((ctx.f[15].f64 + ctx.f[16].f64) as f32) as f64;
	// 832C72E4: C1E1FF44  lfs f15, -0xbc(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-188 as u32) ) };
	ctx.f[15].f64 = (tmp.f32 as f64);
	// 832C72E8: D1E1FF44  stfs f15, -0xbc(r1)
	tmp.f32 = (ctx.f[15].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-188 as u32), tmp.u32 ) };
	// 832C72EC: EDC56028  fsubs f14, f5, f12
	ctx.f[14].f64 = (((ctx.f[5].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C72F0: EF290828  fsubs f25, f9, f1
	ctx.f[25].f64 = (((ctx.f[9].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C72F4: EC21482A  fadds f1, f1, f9
	ctx.f[1].f64 = ((ctx.f[1].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C72F8: ED3C382A  fadds f9, f28, f7
	ctx.f[9].f64 = ((ctx.f[28].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C72FC: ECFC3828  fsubs f7, f28, f7
	ctx.f[7].f64 = (((ctx.f[28].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7300: EE5A0032  fmuls f18, f26, f0
	ctx.f[18].f64 = (((ctx.f[26].f64 * ctx.f[0].f64) as f32) as f64);
	// 832C7304: EF5A0232  fmuls f26, f26, f8
	ctx.f[26].f64 = (((ctx.f[26].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C7308: EE340232  fmuls f17, f20, f8
	ctx.f[17].f64 = (((ctx.f[20].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C730C: EE130232  fmuls f16, f19, f8
	ctx.f[16].f64 = (((ctx.f[19].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C7310: EF9B5828  fsubs f28, f27, f11
	ctx.f[28].f64 = (((ctx.f[27].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C7314: ED6BD82A  fadds f11, f11, f27
	ctx.f[11].f64 = ((ctx.f[11].f64 + ctx.f[27].f64) as f32) as f64;
	// 832C7318: EF3903F2  fmuls f25, f25, f15
	ctx.f[25].f64 = (((ctx.f[25].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C731C: EC2103F2  fmuls f1, f1, f15
	ctx.f[1].f64 = (((ctx.f[1].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C7320: ED2903F2  fmuls f9, f9, f15
	ctx.f[9].f64 = (((ctx.f[9].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C7324: ECE703F2  fmuls f7, f7, f15
	ctx.f[7].f64 = (((ctx.f[7].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C7328: ED049238  fmsubs f8, f4, f8, f18
	ctx.f[8].f64 = (((ctx.f[4].f64 * ctx.f[8].f64 - ctx.f[18].f64) as f32) as f64);
	// 832C732C: EC84D03A  fmadds f4, f4, f0, f26
	ctx.f[4].f64 = (((ctx.f[4].f64 * ctx.f[0].f64 + ctx.f[26].f64) as f32) as f64);
	// 832C7330: EF538838  fmsubs f26, f19, f0, f17
	ctx.f[26].f64 = (((ctx.f[19].f64 * ctx.f[0].f64 - ctx.f[17].f64) as f32) as f64);
	// 832C7334: EC14803A  fmadds f0, f20, f0, f16
	ctx.f[0].f64 = (((ctx.f[20].f64 * ctx.f[0].f64 + ctx.f[16].f64) as f32) as f64);
	// 832C7338: EDF7E82A  fadds f15, f23, f29
	ctx.f[15].f64 = ((ctx.f[23].f64 + ctx.f[29].f64) as f32) as f64;
	// 832C733C: EE99F82A  fadds f20, f25, f31
	ctx.f[20].f64 = ((ctx.f[25].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C7340: EE61A82A  fadds f19, f1, f21
	ctx.f[19].f64 = ((ctx.f[1].f64 + ctx.f[21].f64) as f32) as f64;
	// 832C7344: EE464828  fsubs f18, f6, f9
	ctx.f[18].f64 = (((ctx.f[6].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C7348: EE27102A  fadds f17, f7, f2
	ctx.f[17].f64 = ((ctx.f[7].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C734C: ECE23828  fsubs f7, f2, f7
	ctx.f[7].f64 = (((ctx.f[2].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7350: EC48F02A  fadds f2, f8, f30
	ctx.f[2].f64 = ((ctx.f[8].f64 + ctx.f[30].f64) as f32) as f64;
	// 832C7354: ECC9302A  fadds f6, f9, f6
	ctx.f[6].f64 = ((ctx.f[9].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C7358: ED24502A  fadds f9, f4, f10
	ctx.f[9].f64 = ((ctx.f[4].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C735C: EE0DD028  fsubs f16, f13, f26
	ctx.f[16].f64 = (((ctx.f[13].f64 - ctx.f[26].f64) as f32) as f64);
	// 832C7360: EDBA682A  fadds f13, f26, f13
	ctx.f[13].f64 = ((ctx.f[26].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C7364: EF430028  fsubs f26, f3, f0
	ctx.f[26].f64 = (((ctx.f[3].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C7368: EC60182A  fadds f3, f0, f3
	ctx.f[3].f64 = ((ctx.f[0].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C736C: EC1DB828  fsubs f0, f29, f23
	ctx.f[0].f64 = (((ctx.f[29].f64 - ctx.f[23].f64) as f32) as f64);
	// 832C7370: EFB6C02A  fadds f29, f22, f24
	ctx.f[29].f64 = ((ctx.f[22].f64 + ctx.f[24].f64) as f32) as f64;
	// 832C7374: EF18B028  fsubs f24, f24, f22
	ctx.f[24].f64 = (((ctx.f[24].f64 - ctx.f[22].f64) as f32) as f64);
	// 832C7378: EF62A02A  fadds f27, f2, f20
	ctx.f[27].f64 = ((ctx.f[2].f64 + ctx.f[20].f64) as f32) as f64;
	// 832C737C: D3630040  stfs f27, 0x40(r3)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(64 as u32), tmp.u32 ) };
	// 832C7380: EF69982A  fadds f27, f9, f19
	ctx.f[27].f64 = ((ctx.f[9].f64 + ctx.f[19].f64) as f32) as f64;
	// 832C7384: D3630044  stfs f27, 0x44(r3)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(68 as u32), tmp.u32 ) };
	// 832C7388: EF70902A  fadds f27, f16, f18
	ctx.f[27].f64 = ((ctx.f[16].f64 + ctx.f[18].f64) as f32) as f64;
	// 832C738C: D3630060  stfs f27, 0x60(r3)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(96 as u32), tmp.u32 ) };
	// 832C7390: EF728028  fsubs f27, f18, f16
	ctx.f[27].f64 = (((ctx.f[18].f64 - ctx.f[16].f64) as f32) as f64);
	// 832C7394: D3630068  stfs f27, 0x68(r3)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(104 as u32), tmp.u32 ) };
	// 832C7398: EF7A382A  fadds f27, f26, f7
	ctx.f[27].f64 = ((ctx.f[26].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C739C: D3630064  stfs f27, 0x64(r3)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(100 as u32), tmp.u32 ) };
	// 832C73A0: ECE7D028  fsubs f7, f7, f26
	ctx.f[7].f64 = (((ctx.f[7].f64 - ctx.f[26].f64) as f32) as f64);
	// 832C73A4: D0E3006C  stfs f7, 0x6c(r3)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(108 as u32), tmp.u32 ) };
	// 832C73A8: ECED882A  fadds f7, f13, f17
	ctx.f[7].f64 = ((ctx.f[13].f64 + ctx.f[17].f64) as f32) as f64;
	// 832C73AC: D0E30074  stfs f7, 0x74(r3)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(116 as u32), tmp.u32 ) };
	// 832C73B0: ECF41028  fsubs f7, f20, f2
	ctx.f[7].f64 = (((ctx.f[20].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C73B4: D0E30048  stfs f7, 0x48(r3)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(72 as u32), tmp.u32 ) };
	// 832C73B8: EDB16828  fsubs f13, f17, f13
	ctx.f[13].f64 = (((ctx.f[17].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C73BC: D1A3007C  stfs f13, 0x7c(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(124 as u32), tmp.u32 ) };
	// 832C73C0: EC534828  fsubs f2, f19, f9
	ctx.f[2].f64 = (((ctx.f[19].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C73C4: D043004C  stfs f2, 0x4c(r3)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(76 as u32), tmp.u32 ) };
	// 832C73C8: EDA61828  fsubs f13, f6, f3
	ctx.f[13].f64 = (((ctx.f[6].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C73CC: C0E1FF40  lfs f7, -0xc0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-192 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C73D0: C041FF48  lfs f2, -0xb8(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-184 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C73D4: ED350828  fsubs f9, f21, f1
	ctx.f[9].f64 = (((ctx.f[21].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C73D8: ED8C282A  fadds f12, f12, f5
	ctx.f[12].f64 = ((ctx.f[12].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C73DC: D1A30070  stfs f13, 0x70(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(112 as u32), tmp.u32 ) };
	// 832C73E0: ECA7E028  fsubs f5, f7, f28
	ctx.f[5].f64 = (((ctx.f[7].f64 - ctx.f[28].f64) as f32) as f64);
	// 832C73E4: C2E1FF5C  lfs f23, -0xa4(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-164 as u32) ) };
	ctx.f[23].f64 = (tmp.f32 as f64);
	// 832C73E8: EDBC382A  fadds f13, f28, f7
	ctx.f[13].f64 = ((ctx.f[28].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C73EC: C2A1FF60  lfs f21, -0xa0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-160 as u32) ) };
	ctx.f[21].f64 = (tmp.f32 as f64);
	// 832C73F0: EC2E102A  fadds f1, f14, f2
	ctx.f[1].f64 = ((ctx.f[14].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C73F4: ECE27028  fsubs f7, f2, f14
	ctx.f[7].f64 = (((ctx.f[2].f64 - ctx.f[14].f64) as f32) as f64);
	// 832C73F8: EC8A2028  fsubs f4, f10, f4
	ctx.f[4].f64 = (((ctx.f[10].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C73FC: C141FF4C  lfs f10, -0xb4(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-180 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C7400: ED1E4028  fsubs f8, f30, f8
	ctx.f[8].f64 = (((ctx.f[30].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C7404: C3C1FF50  lfs f30, -0xb0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-176 as u32) ) };
	ctx.f[30].f64 = (tmp.f32 as f64);
	// 832C7408: EF98F02A  fadds f28, f24, f30
	ctx.f[28].f64 = ((ctx.f[24].f64 + ctx.f[30].f64) as f32) as f64;
	// 832C740C: EFDEC028  fsubs f30, f30, f24
	ctx.f[30].f64 = (((ctx.f[30].f64 - ctx.f[24].f64) as f32) as f64);
	// 832C7410: EE8BA82A  fadds f20, f11, f21
	ctx.f[20].f64 = ((ctx.f[11].f64 + ctx.f[21].f64) as f32) as f64;
	// 832C7414: EECCB82A  fadds f22, f12, f23
	ctx.f[22].f64 = ((ctx.f[12].f64 + ctx.f[23].f64) as f32) as f64;
	// 832C7418: ED976028  fsubs f12, f23, f12
	ctx.f[12].f64 = (((ctx.f[23].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C741C: EC450828  fsubs f2, f5, f1
	ctx.f[2].f64 = (((ctx.f[5].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C7420: EF4D3828  fsubs f26, f13, f7
	ctx.f[26].f64 = (((ctx.f[13].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7424: EC21282A  fadds f1, f1, f5
	ctx.f[1].f64 = ((ctx.f[1].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C7428: ECE7682A  fadds f7, f7, f13
	ctx.f[7].f64 = ((ctx.f[7].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C742C: C1A1FF44  lfs f13, -0xbc(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-188 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C7430: ECBFC828  fsubs f5, f31, f25
	ctx.f[5].f64 = (((ctx.f[31].f64 - ctx.f[25].f64) as f32) as f64);
	// 832C7434: C321FF58  lfs f25, -0xa8(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-168 as u32) ) };
	ctx.f[25].f64 = (tmp.f32 as f64);
	// 832C7438: EFEA0028  fsubs f31, f10, f0
	ctx.f[31].f64 = (((ctx.f[10].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C743C: EC00502A  fadds f0, f0, f10
	ctx.f[0].f64 = ((ctx.f[0].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C7440: C141FF54  lfs f10, -0xac(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-172 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C7444: EF6F502A  fadds f27, f15, f10
	ctx.f[27].f64 = ((ctx.f[15].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C7448: EF1DC82A  fadds f24, f29, f25
	ctx.f[24].f64 = ((ctx.f[29].f64 + ctx.f[25].f64) as f32) as f64;
	// 832C744C: ED4A7828  fsubs f10, f10, f15
	ctx.f[10].f64 = (((ctx.f[10].f64 - ctx.f[15].f64) as f32) as f64);
	// 832C7450: EC420372  fmuls f2, f2, f13
	ctx.f[2].f64 = (((ctx.f[2].f64 * ctx.f[13].f64) as f32) as f64);
	// 832C7454: EF5A0372  fmuls f26, f26, f13
	ctx.f[26].f64 = (((ctx.f[26].f64 * ctx.f[13].f64) as f32) as f64);
	// 832C7458: EC210372  fmuls f1, f1, f13
	ctx.f[1].f64 = (((ctx.f[1].f64 * ctx.f[13].f64) as f32) as f64);
	// 832C745C: ECE70372  fmuls f7, f7, f13
	ctx.f[7].f64 = (((ctx.f[7].f64 * ctx.f[13].f64) as f32) as f64);
	// 832C7460: EDB55828  fsubs f13, f21, f11
	ctx.f[13].f64 = (((ctx.f[21].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C7464: ED63302A  fadds f11, f3, f6
	ctx.f[11].f64 = ((ctx.f[3].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C7468: D1630078  stfs f11, 0x78(r3)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(120 as u32), tmp.u32 ) };
	// 832C746C: ECC8482A  fadds f6, f8, f9
	ctx.f[6].f64 = ((ctx.f[8].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C7470: D0C30054  stfs f6, 0x54(r3)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(84 as u32), tmp.u32 ) };
	// 832C7474: EC694028  fsubs f3, f9, f8
	ctx.f[3].f64 = (((ctx.f[9].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C7478: D063005C  stfs f3, 0x5c(r3)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(92 as u32), tmp.u32 ) };
	// 832C747C: ED652028  fsubs f11, f5, f4
	ctx.f[11].f64 = (((ctx.f[5].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C7480: D1630050  stfs f11, 0x50(r3)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(80 as u32), tmp.u32 ) };
	// 832C7484: ED24282A  fadds f9, f4, f5
	ctx.f[9].f64 = ((ctx.f[4].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C7488: D1230058  stfs f9, 0x58(r3)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(88 as u32), tmp.u32 ) };
	// 832C748C: ED02F02A  fadds f8, f2, f30
	ctx.f[8].f64 = ((ctx.f[2].f64 + ctx.f[30].f64) as f32) as f64;
	// 832C7490: D1030020  stfs f8, 0x20(r3)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(32 as u32), tmp.u32 ) };
	// 832C7494: ECBE1028  fsubs f5, f30, f2
	ctx.f[5].f64 = (((ctx.f[30].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C7498: D0A30028  stfs f5, 0x28(r3)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(40 as u32), tmp.u32 ) };
	// 832C749C: ECC1002A  fadds f6, f1, f0
	ctx.f[6].f64 = ((ctx.f[1].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C74A0: D0C30024  stfs f6, 0x24(r3)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(36 as u32), tmp.u32 ) };
	// 832C74A4: EC800828  fsubs f4, f0, f1
	ctx.f[4].f64 = (((ctx.f[0].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C74A8: D083002C  stfs f4, 0x2c(r3)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(44 as u32), tmp.u32 ) };
	// 832C74AC: EC5C3828  fsubs f2, f28, f7
	ctx.f[2].f64 = (((ctx.f[28].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C74B0: D0430030  stfs f2, 0x30(r3)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(48 as u32), tmp.u32 ) };
	// 832C74B4: EC27E02A  fadds f1, f7, f28
	ctx.f[1].f64 = ((ctx.f[7].f64 + ctx.f[28].f64) as f32) as f64;
	// 832C74B8: D0230038  stfs f1, 0x38(r3)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(56 as u32), tmp.u32 ) };
	// 832C74BC: EC7AF82A  fadds f3, f26, f31
	ctx.f[3].f64 = ((ctx.f[26].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C74C0: D0630034  stfs f3, 0x34(r3)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(52 as u32), tmp.u32 ) };
	// 832C74C4: EC1FD028  fsubs f0, f31, f26
	ctx.f[0].f64 = (((ctx.f[31].f64 - ctx.f[26].f64) as f32) as f64);
	// 832C74C8: D003003C  stfs f0, 0x3c(r3)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(60 as u32), tmp.u32 ) };
	// 832C74CC: ED76D82A  fadds f11, f22, f27
	ctx.f[11].f64 = ((ctx.f[22].f64 + ctx.f[27].f64) as f32) as f64;
	// 832C74D0: D1630000  stfs f11, 0(r3)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C74D4: ED34C02A  fadds f9, f20, f24
	ctx.f[9].f64 = ((ctx.f[20].f64 + ctx.f[24].f64) as f32) as f64;
	// 832C74D8: D1230004  stfs f9, 4(r3)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C74DC: ED1BB028  fsubs f8, f27, f22
	ctx.f[8].f64 = (((ctx.f[27].f64 - ctx.f[22].f64) as f32) as f64);
	// 832C74E0: D1030008  stfs f8, 8(r3)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C74E4: ECF8A028  fsubs f7, f24, f20
	ctx.f[7].f64 = (((ctx.f[24].f64 - ctx.f[20].f64) as f32) as f64);
	// 832C74E8: D0E3000C  stfs f7, 0xc(r3)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C74EC: EFB9E828  fsubs f29, f25, f29
	ctx.f[29].f64 = (((ctx.f[25].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C74F0: ECCA6828  fsubs f6, f10, f13
	ctx.f[6].f64 = (((ctx.f[10].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C74F4: ECACE82A  fadds f5, f12, f29
	ctx.f[5].f64 = ((ctx.f[12].f64 + ctx.f[29].f64) as f32) as f64;
	// 832C74F8: D0C30010  stfs f6, 0x10(r3)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(16 as u32), tmp.u32 ) };
	// 832C74FC: EC8D502A  fadds f4, f13, f10
	ctx.f[4].f64 = ((ctx.f[13].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C7500: D0A30014  stfs f5, 0x14(r3)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(20 as u32), tmp.u32 ) };
	// 832C7504: EC7D6028  fsubs f3, f29, f12
	ctx.f[3].f64 = (((ctx.f[29].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C7508: D0830018  stfs f4, 0x18(r3)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(24 as u32), tmp.u32 ) };
	// 832C750C: D063001C  stfs f3, 0x1c(r3)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(28 as u32), tmp.u32 ) };
	// 832C7510: 3981FFF8  addi r12, r1, -8
	ctx.r[12].s64 = ctx.r[1].s64 + -8;
	// 832C7514: 4B9E67D9  bl 0x82cadcec
	ctx.lr = 0x832C7518;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCEC);
	// 832C7518: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832C751C: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832C7520: 4E800020  blr
	return;
}

pub fn sub_832C7528(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    // ---- function 0x832C7528 size=1304
	// 832C7528: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C752C: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832C7530: 3981FFF8  addi r12, r1, -8
	ctx.r[12].s64 = ctx.r[1].s64 + -8;
	// 832C7534: 4B9E676D  bl 0x82cadca0
	ctx.lr = 0x832C7538;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCA0);
	// 832C7538: C0030020  lfs f0, 0x20(r3)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(32 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C753C: C1A30064  lfs f13, 0x64(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(100 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C7540: C1830060  lfs f12, 0x60(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(96 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C7544: ED606828  fsubs f11, f0, f13
	ctx.f[11].f64 = (((ctx.f[0].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C7548: C1430024  lfs f10, 0x24(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(36 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C754C: ED2D002A  fadds f9, f13, f0
	ctx.f[9].f64 = ((ctx.f[13].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C7550: ECEA602A  fadds f7, f10, f12
	ctx.f[7].f64 = ((ctx.f[10].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C7554: C1030048  lfs f8, 0x48(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(72 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C7558: ECAA6028  fsubs f5, f10, f12
	ctx.f[5].f64 = (((ctx.f[10].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C755C: C0C3000C  lfs f6, 0xc(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(12 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C7560: C0030028  lfs f0, 0x28(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C7564: EC66402A  fadds f3, f6, f8
	ctx.f[3].f64 = ((ctx.f[6].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C7568: C183006C  lfs f12, 0x6c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(108 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C756C: EC264028  fsubs f1, f6, f8
	ctx.f[1].f64 = (((ctx.f[6].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C7570: ED006028  fsubs f8, f0, f12
	ctx.f[8].f64 = (((ctx.f[0].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C7574: C0830068  lfs f4, 0x68(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(104 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C7578: C043002C  lfs f2, 0x2c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C757C: EDA2202A  fadds f13, f2, f4
	ctx.f[13].f64 = ((ctx.f[2].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C7580: C0C40018  lfs f6, 0x18(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(24 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C7584: C3640004  lfs f27, 4(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(4 as u32) ) };
	ctx.f[27].f64 = (tmp.f32 as f64);
	// 832C7588: ED422028  fsubs f10, f2, f4
	ctx.f[10].f64 = (((ctx.f[2].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C758C: C3230000  lfs f25, 0(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) };
	ctx.f[25].f64 = (tmp.f32 as f64);
	// 832C7590: C2C30004  lfs f22, 4(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) };
	ctx.f[22].f64 = (tmp.f32 as f64);
	// 832C7594: EFAB3828  fsubs f29, f11, f7
	ctx.f[29].f64 = (((ctx.f[11].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7598: C2E30040  lfs f23, 0x40(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(64 as u32) ) };
	ctx.f[23].f64 = (tmp.f32 as f64);
	// 832C759C: ED67582A  fadds f11, f7, f11
	ctx.f[11].f64 = ((ctx.f[7].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C75A0: C0440014  lfs f2, 0x14(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(20 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C75A4: EF492828  fsubs f26, f9, f5
	ctx.f[26].f64 = (((ctx.f[9].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C75A8: C0840010  lfs f4, 0x10(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(16 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C75AC: ECA5482A  fadds f5, f5, f9
	ctx.f[5].f64 = ((ctx.f[5].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C75B0: C1230044  lfs f9, 0x44(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(68 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C75B4: EE6801B2  fmuls f19, f8, f6
	ctx.f[19].f64 = (((ctx.f[8].f64 * ctx.f[6].f64) as f32) as f64);
	// 832C75B8: C3C3004C  lfs f30, 0x4c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(76 as u32) ) };
	ctx.f[30].f64 = (tmp.f32 as f64);
	// 832C75BC: EE394828  fsubs f17, f25, f9
	ctx.f[17].f64 = (((ctx.f[25].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C75C0: C3E30008  lfs f31, 8(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(8 as u32) ) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832C75C4: EDF6B82A  fadds f15, f22, f23
	ctx.f[15].f64 = ((ctx.f[22].f64 + ctx.f[23].f64) as f32) as f64;
	// 832C75C8: C0E4001C  lfs f7, 0x1c(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(28 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C75CC: EF0300B2  fmuls f24, f3, f2
	ctx.f[24].f64 = (((ctx.f[3].f64 * ctx.f[2].f64) as f32) as f64);
	// 832C75D0: C2840020  lfs f20, 0x20(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(32 as u32) ) };
	ctx.f[20].f64 = (tmp.f32 as f64);
	// 832C75D4: ED29C82A  fadds f9, f9, f25
	ctx.f[9].f64 = ((ctx.f[9].f64 + ctx.f[25].f64) as f32) as f64;
	// 832C75D8: C2440024  lfs f18, 0x24(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(36 as u32) ) };
	ctx.f[18].f64 = (tmp.f32 as f64);
	// 832C75DC: EF9FF028  fsubs f28, f31, f30
	ctx.f[28].f64 = (((ctx.f[31].f64 - ctx.f[30].f64) as f32) as f64);
	// 832C75E0: D141FF2C  stfs f10, -0xd4(r1)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-212 as u32), tmp.u32 ) };
	// 832C75E4: EFBD06F2  fmuls f29, f29, f27
	ctx.f[29].f64 = (((ctx.f[29].f64 * ctx.f[27].f64) as f32) as f64);
	// 832C75E8: D241FF28  stfs f18, -0xd8(r1)
	tmp.f32 = (ctx.f[18].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-216 as u32), tmp.u32 ) };
	// 832C75EC: ED6B06F2  fmuls f11, f11, f27
	ctx.f[11].f64 = (((ctx.f[11].f64 * ctx.f[27].f64) as f32) as f64);
	// 832C75F0: D281FF20  stfs f20, -0xe0(r1)
	tmp.f32 = (ctx.f[20].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-224 as u32), tmp.u32 ) };
	// 832C75F4: EEAD01B2  fmuls f21, f13, f6
	ctx.f[21].f64 = (((ctx.f[13].f64 * ctx.f[6].f64) as f32) as f64);
	// 832C75F8: D361FF50  stfs f27, -0xb0(r1)
	tmp.f32 = (ctx.f[27].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-176 as u32), tmp.u32 ) };
	// 832C75FC: ECA506F2  fmuls f5, f5, f27
	ctx.f[5].f64 = (((ctx.f[5].f64 * ctx.f[27].f64) as f32) as f64);
	// 832C7600: D081FF24  stfs f4, -0xdc(r1)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-220 as u32), tmp.u32 ) };
	// 832C7604: EC630132  fmuls f3, f3, f4
	ctx.f[3].f64 = (((ctx.f[3].f64 * ctx.f[4].f64) as f32) as f64);
	// 832C7608: C2030050  lfs f16, 0x50(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(80 as u32) ) };
	ctx.f[16].f64 = (tmp.f32 as f64);
	// 832C760C: EEF6B828  fsubs f23, f22, f23
	ctx.f[23].f64 = (((ctx.f[22].f64 - ctx.f[23].f64) as f32) as f64);
	// 832C7610: C1C30014  lfs f14, 0x14(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(20 as u32) ) };
	ctx.f[14].f64 = (tmp.f32 as f64);
	// 832C7614: EF5A06F2  fmuls f26, f26, f27
	ctx.f[26].f64 = (((ctx.f[26].f64 * ctx.f[27].f64) as f32) as f64);
	// 832C7618: C1430010  lfs f10, 0x10(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(16 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C761C: EDAD99FA  fmadds f13, f13, f7, f19
	ctx.f[13].f64 = (((ctx.f[13].f64 * ctx.f[7].f64 + ctx.f[19].f64) as f32) as f64);
	// 832C7620: C2430054  lfs f18, 0x54(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(84 as u32) ) };
	ctx.f[18].f64 = (tmp.f32 as f64);
	// 832C7624: EFFEF82A  fadds f31, f30, f31
	ctx.f[31].f64 = ((ctx.f[30].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C7628: C3230070  lfs f25, 0x70(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(112 as u32) ) };
	ctx.f[25].f64 = (tmp.f32 as f64);
	// 832C762C: EF1CC138  fmsubs f24, f28, f4, f24
	ctx.f[24].f64 = (((ctx.f[28].f64 * ctx.f[4].f64 - ctx.f[24].f64) as f32) as f64);
	// 832C7630: C2C30034  lfs f22, 0x34(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(52 as u32) ) };
	ctx.f[22].f64 = (tmp.f32 as f64);
	// 832C7634: EE7D882A  fadds f19, f29, f17
	ctx.f[19].f64 = ((ctx.f[29].f64 + ctx.f[17].f64) as f32) as f64;
	// 832C7638: D261FF40  stfs f19, -0xc0(r1)
	tmp.f32 = (ctx.f[19].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-192 as u32), tmp.u32 ) };
	// 832C763C: EE6B782A  fadds f19, f11, f15
	ctx.f[19].f64 = ((ctx.f[11].f64 + ctx.f[15].f64) as f32) as f64;
	// 832C7640: C2830030  lfs f20, 0x30(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) };
	ctx.f[20].f64 = (tmp.f32 as f64);
	// 832C7644: ED6F5828  fsubs f11, f15, f11
	ctx.f[11].f64 = (((ctx.f[15].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C7648: C3630074  lfs f27, 0x74(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(116 as u32) ) };
	ctx.f[27].f64 = (tmp.f32 as f64);
	// 832C764C: EDE92828  fsubs f15, f9, f5
	ctx.f[15].f64 = (((ctx.f[9].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C7650: C0830058  lfs f4, 0x58(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(88 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C7654: EC7C18BA  fmadds f3, f28, f2, f3
	ctx.f[3].f64 = (((ctx.f[28].f64 * ctx.f[2].f64 + ctx.f[3].f64) as f32) as f64);
	// 832C7658: C383001C  lfs f28, 0x1c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(28 as u32) ) };
	ctx.f[28].f64 = (tmp.f32 as f64);
	// 832C765C: ED08A9F8  fmsubs f8, f8, f7, f21
	ctx.f[8].f64 = (((ctx.f[8].f64 * ctx.f[7].f64 - ctx.f[21].f64) as f32) as f64);
	// 832C7660: C2A30018  lfs f21, 0x18(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(24 as u32) ) };
	ctx.f[21].f64 = (tmp.f32 as f64);
	// 832C7664: EFC101F2  fmuls f30, f1, f7
	ctx.f[30].f64 = (((ctx.f[1].f64 * ctx.f[7].f64) as f32) as f64);
	// 832C7668: EFB1E828  fsubs f29, f17, f29
	ctx.f[29].f64 = (((ctx.f[17].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C766C: ED25482A  fadds f9, f5, f9
	ctx.f[9].f64 = ((ctx.f[5].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C7670: EC2101B2  fmuls f1, f1, f6
	ctx.f[1].f64 = (((ctx.f[1].f64 * ctx.f[6].f64) as f32) as f64);
	// 832C7674: EE3AB82A  fadds f17, f26, f23
	ctx.f[17].f64 = ((ctx.f[26].f64 + ctx.f[23].f64) as f32) as f64;
	// 832C7678: ECB7D028  fsubs f5, f23, f26
	ctx.f[5].f64 = (((ctx.f[23].f64 - ctx.f[26].f64) as f32) as f64);
	// 832C767C: EF48C02A  fadds f26, f8, f24
	ctx.f[26].f64 = ((ctx.f[8].f64 + ctx.f[24].f64) as f32) as f64;
	// 832C7680: D121FF5C  stfs f9, -0xa4(r1)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-164 as u32), tmp.u32 ) };
	// 832C7684: ED184028  fsubs f8, f24, f8
	ctx.f[8].f64 = (((ctx.f[24].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C7688: D1E1FF54  stfs f15, -0xac(r1)
	tmp.f32 = (ctx.f[15].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-172 as u32), tmp.u32 ) };
	// 832C768C: EF0D182A  fadds f24, f13, f3
	ctx.f[24].f64 = ((ctx.f[13].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C7690: C2E3007C  lfs f23, 0x7c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(124 as u32) ) };
	ctx.f[23].f64 = (tmp.f32 as f64);
	// 832C7694: EC636828  fsubs f3, f3, f13
	ctx.f[3].f64 = (((ctx.f[3].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C7698: C1E3003C  lfs f15, 0x3c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(60 as u32) ) };
	ctx.f[15].f64 = (tmp.f32 as f64);
	// 832C769C: EDBFF1B8  fmsubs f13, f31, f6, f30
	ctx.f[13].f64 = (((ctx.f[31].f64 * ctx.f[6].f64 - ctx.f[30].f64) as f32) as f64);
	// 832C76A0: C1230078  lfs f9, 0x78(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(120 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C76A4: EC3F09FA  fmadds f1, f31, f7, f1
	ctx.f[1].f64 = (((ctx.f[31].f64 * ctx.f[7].f64 + ctx.f[1].f64) as f32) as f64);
	// 832C76A8: C3E30038  lfs f31, 0x38(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(56 as u32) ) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832C76AC: D221FF58  stfs f17, -0xa8(r1)
	tmp.f32 = (ctx.f[17].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-168 as u32), tmp.u32 ) };
	// 832C76B0: EE3C202A  fadds f17, f28, f4
	ctx.f[17].f64 = ((ctx.f[28].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C76B4: D0A1FF60  stfs f5, -0xa0(r1)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-160 as u32), tmp.u32 ) };
	// 832C76B8: ECB4D828  fsubs f5, f20, f27
	ctx.f[5].f64 = (((ctx.f[20].f64 - ctx.f[27].f64) as f32) as f64);
	// 832C76BC: D161FF4C  stfs f11, -0xb4(r1)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-180 as u32), tmp.u32 ) };
	// 832C76C0: ED7FB828  fsubs f11, f31, f23
	ctx.f[11].f64 = (((ctx.f[31].f64 - ctx.f[23].f64) as f32) as f64);
	// 832C76C4: D3A1FF48  stfs f29, -0xb8(r1)
	tmp.f32 = (ctx.f[29].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-184 as u32), tmp.u32 ) };
	// 832C76C8: EFAF482A  fadds f29, f15, f9
	ctx.f[29].f64 = ((ctx.f[15].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C76CC: EFCE802A  fadds f30, f14, f16
	ctx.f[30].f64 = ((ctx.f[14].f64 + ctx.f[16].f64) as f32) as f64;
	// 832C76D0: D341FF44  stfs f26, -0xbc(r1)
	tmp.f32 = (ctx.f[26].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-188 as u32), tmp.u32 ) };
	// 832C76D4: D301FF3C  stfs f24, -0xc4(r1)
	tmp.f32 = (ctx.f[24].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-196 as u32), tmp.u32 ) };
	// 832C76D8: EF16C82A  fadds f24, f22, f25
	ctx.f[24].f64 = ((ctx.f[22].f64 + ctx.f[25].f64) as f32) as f64;
	// 832C76DC: C343005C  lfs f26, 0x5c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(92 as u32) ) };
	ctx.f[26].f64 = (tmp.f32 as f64);
	// 832C76E0: EC0C002A  fadds f0, f12, f0
	ctx.f[0].f64 = ((ctx.f[12].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C76E4: EF36C828  fsubs f25, f22, f25
	ctx.f[25].f64 = (((ctx.f[22].f64 - ctx.f[25].f64) as f32) as f64);
	// 832C76E8: D261FF38  stfs f19, -0xc8(r1)
	tmp.f32 = (ctx.f[19].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-200 as u32), tmp.u32 ) };
	// 832C76EC: EC9C2028  fsubs f4, f28, f4
	ctx.f[4].f64 = (((ctx.f[28].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C76F0: D101FF34  stfs f8, -0xcc(r1)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-204 as u32), tmp.u32 ) };
	// 832C76F4: ED8A9028  fsubs f12, f10, f18
	ctx.f[12].f64 = (((ctx.f[10].f64 - ctx.f[18].f64) as f32) as f64);
	// 832C76F8: C101FF28  lfs f8, -0xd8(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-216 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C76FC: EED5D028  fsubs f22, f21, f26
	ctx.f[22].f64 = (((ctx.f[21].f64 - ctx.f[26].f64) as f32) as f64);
	// 832C7700: D061FF30  stfs f3, -0xd0(r1)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-208 as u32), tmp.u32 ) };
	// 832C7704: EF9AA82A  fadds f28, f26, f21
	ctx.f[28].f64 = ((ctx.f[26].f64 + ctx.f[21].f64) as f32) as f64;
	// 832C7708: C341FF20  lfs f26, -0xe0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-224 as u32) ) };
	ctx.f[26].f64 = (tmp.f32 as f64);
	// 832C770C: ED52502A  fadds f10, f18, f10
	ctx.f[10].f64 = ((ctx.f[18].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C7710: D021FF20  stfs f1, -0xe0(r1)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-224 as u32), tmp.u32 ) };
	// 832C7714: ED2F4828  fsubs f9, f15, f9
	ctx.f[9].f64 = (((ctx.f[15].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C7718: C1E1FF24  lfs f15, -0xdc(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-220 as u32) ) };
	ctx.f[15].f64 = (tmp.f32 as f64);
	// 832C771C: EE4E8028  fsubs f18, f14, f16
	ctx.f[18].f64 = (((ctx.f[14].f64 - ctx.f[16].f64) as f32) as f64);
	// 832C7720: D1A1FF24  stfs f13, -0xdc(r1)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-220 as u32), tmp.u32 ) };
	// 832C7724: EE1101F2  fmuls f16, f17, f7
	ctx.f[16].f64 = (((ctx.f[17].f64 * ctx.f[7].f64) as f32) as f64);
	// 832C7728: EEA506B2  fmuls f21, f5, f26
	ctx.f[21].f64 = (((ctx.f[5].f64 * ctx.f[26].f64) as f32) as f64);
	// 832C772C: EE3101B2  fmuls f17, f17, f6
	ctx.f[17].f64 = (((ctx.f[17].f64 * ctx.f[6].f64) as f32) as f64);
	// 832C7730: EDCB03F2  fmuls f14, f11, f15
	ctx.f[14].f64 = (((ctx.f[11].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C7734: EE7D03F2  fmuls f19, f29, f15
	ctx.f[19].f64 = (((ctx.f[29].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C7738: EFF7F82A  fadds f31, f23, f31
	ctx.f[31].f64 = ((ctx.f[23].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C773C: EEFE06B2  fmuls f23, f30, f26
	ctx.f[23].f64 = (((ctx.f[30].f64 * ctx.f[26].f64) as f32) as f64);
	// 832C7740: EF7BA02A  fadds f27, f27, f20
	ctx.f[27].f64 = ((ctx.f[27].f64 + ctx.f[20].f64) as f32) as f64;
	// 832C7744: EE8003F2  fmuls f20, f0, f15
	ctx.f[20].f64 = (((ctx.f[0].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C7748: EC0000B2  fmuls f0, f0, f2
	ctx.f[0].f64 = (((ctx.f[0].f64 * ctx.f[2].f64) as f32) as f64);
	// 832C774C: EE1681B8  fmsubs f16, f22, f6, f16
	ctx.f[16].f64 = (((ctx.f[22].f64 * ctx.f[6].f64 - ctx.f[16].f64) as f32) as f64);
	// 832C7750: EEB8AA3A  fmadds f21, f24, f8, f21
	ctx.f[21].f64 = (((ctx.f[24].f64 * ctx.f[8].f64 + ctx.f[21].f64) as f32) as f64);
	// 832C7754: EC3C03F2  fmuls f1, f28, f15
	ctx.f[1].f64 = (((ctx.f[28].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C7758: EED689FA  fmadds f22, f22, f7, f17
	ctx.f[22].f64 = (((ctx.f[22].f64 * ctx.f[7].f64 + ctx.f[17].f64) as f32) as f64);
	// 832C775C: EFBD70BA  fmadds f29, f29, f2, f14
	ctx.f[29].f64 = (((ctx.f[29].f64 * ctx.f[2].f64 + ctx.f[14].f64) as f32) as f64);
	// 832C7760: ED6B98B8  fmsubs f11, f11, f2, f19
	ctx.f[11].f64 = (((ctx.f[11].f64 * ctx.f[2].f64 - ctx.f[19].f64) as f32) as f64);
	// 832C7764: EF1806B2  fmuls f24, f24, f26
	ctx.f[24].f64 = (((ctx.f[24].f64 * ctx.f[26].f64) as f32) as f64);
	// 832C7768: EDBF01B2  fmuls f13, f31, f6
	ctx.f[13].f64 = (((ctx.f[31].f64 * ctx.f[6].f64) as f32) as f64);
	// 832C776C: EFDE0232  fmuls f30, f30, f8
	ctx.f[30].f64 = (((ctx.f[30].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C7770: EDD90232  fmuls f14, f25, f8
	ctx.f[14].f64 = (((ctx.f[25].f64 * ctx.f[8].f64) as f32) as f64);
	// 832C7774: EE6A06B2  fmuls f19, f10, f26
	ctx.f[19].f64 = (((ctx.f[10].f64 * ctx.f[26].f64) as f32) as f64);
	// 832C7778: EE3206B2  fmuls f17, f18, f26
	ctx.f[17].f64 = (((ctx.f[18].f64 * ctx.f[26].f64) as f32) as f64);
	// 832C777C: EEECBA3A  fmadds f23, f12, f8, f23
	ctx.f[23].f64 = (((ctx.f[12].f64 * ctx.f[8].f64 + ctx.f[23].f64) as f32) as f64);
	// 832C7780: EF3906B2  fmuls f25, f25, f26
	ctx.f[25].f64 = (((ctx.f[25].f64 * ctx.f[26].f64) as f32) as f64);
	// 832C7784: ECC901B2  fmuls f6, f9, f6
	ctx.f[6].f64 = (((ctx.f[9].f64 * ctx.f[6].f64) as f32) as f64);
	// 832C7788: D0C1FF28  stfs f6, -0xd8(r1)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-216 as u32), tmp.u32 ) };
	// 832C778C: C0C1FF2C  lfs f6, -0xd4(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-212 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C7790: EC6403F2  fmuls f3, f4, f15
	ctx.f[3].f64 = (((ctx.f[4].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C7794: EE86A0BA  fmadds f20, f6, f2, f20
	ctx.f[20].f64 = (((ctx.f[6].f64 * ctx.f[2].f64 + ctx.f[20].f64) as f32) as f64);
	// 832C7798: EC0603F8  fmsubs f0, f6, f15, f0
	ctx.f[0].f64 = (((ctx.f[6].f64 * ctx.f[15].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C779C: ECC5C238  fmsubs f6, f5, f8, f24
	ctx.f[6].f64 = (((ctx.f[5].f64 * ctx.f[8].f64 - ctx.f[24].f64) as f32) as f64);
	// 832C77A0: EC8408B8  fmsubs f4, f4, f2, f1
	ctx.f[4].f64 = (((ctx.f[4].f64 * ctx.f[2].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C77A4: ED8CF6B8  fmsubs f12, f12, f26, f30
	ctx.f[12].f64 = (((ctx.f[12].f64 * ctx.f[26].f64 - ctx.f[30].f64) as f32) as f64);
	// 832C77A8: ECB29A3A  fmadds f5, f18, f8, f19
	ctx.f[5].f64 = (((ctx.f[18].f64 * ctx.f[8].f64 + ctx.f[19].f64) as f32) as f64);
	// 832C77AC: ED4A8A38  fmsubs f10, f10, f8, f17
	ctx.f[10].f64 = (((ctx.f[10].f64 * ctx.f[8].f64 - ctx.f[17].f64) as f32) as f64);
	// 832C77B0: EC2969FA  fmadds f1, f9, f7, f13
	ctx.f[1].f64 = (((ctx.f[9].f64 * ctx.f[7].f64 + ctx.f[13].f64) as f32) as f64);
	// 832C77B4: EFDB76B8  fmsubs f30, f27, f26, f14
	ctx.f[30].f64 = (((ctx.f[27].f64 * ctx.f[26].f64 - ctx.f[14].f64) as f32) as f64);
	// 832C77B8: ED1BCA3A  fmadds f8, f27, f8, f25
	ctx.f[8].f64 = (((ctx.f[27].f64 * ctx.f[8].f64 + ctx.f[25].f64) as f32) as f64);
	// 832C77BC: EDB5B82A  fadds f13, f21, f23
	ctx.f[13].f64 = ((ctx.f[21].f64 + ctx.f[23].f64) as f32) as f64;
	// 832C77C0: ED36E828  fsubs f9, f22, f29
	ctx.f[9].f64 = (((ctx.f[22].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C77C4: C261FF3C  lfs f19, -0xc4(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-196 as u32) ) };
	ctx.f[19].f64 = (tmp.f32 as f64);
	// 832C77C8: EF705828  fsubs f27, f16, f11
	ctx.f[27].f64 = (((ctx.f[16].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C77CC: C1E1FF50  lfs f15, -0xb0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-176 as u32) ) };
	ctx.f[15].f64 = (tmp.f32 as f64);
	// 832C77D0: EC7C18BA  fmadds f3, f28, f2, f3
	ctx.f[3].f64 = (((ctx.f[28].f64 * ctx.f[2].f64 + ctx.f[3].f64) as f32) as f64);
	// 832C77D4: C381FF20  lfs f28, -0xe0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-224 as u32) ) };
	ctx.f[28].f64 = (tmp.f32 as f64);
	// 832C77D8: EF5C0028  fsubs f26, f28, f0
	ctx.f[26].f64 = (((ctx.f[28].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C77DC: C041FF28  lfs f2, -0xd8(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-216 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C77E0: EC00E02A  fadds f0, f0, f28
	ctx.f[0].f64 = ((ctx.f[0].f64 + ctx.f[28].f64) as f32) as f64;
	// 832C77E4: D1E1FF50  stfs f15, -0xb0(r1)
	tmp.f32 = (ctx.f[15].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-176 as u32), tmp.u32 ) };
	// 832C77E8: EF86602A  fadds f28, f6, f12
	ctx.f[28].f64 = ((ctx.f[6].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C77EC: C1C1FF58  lfs f14, -0xa8(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-168 as u32) ) };
	ctx.f[14].f64 = (tmp.f32 as f64);
	// 832C77F0: EF054028  fsubs f24, f5, f8
	ctx.f[24].f64 = (((ctx.f[5].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C77F4: ED8C3028  fsubs f12, f12, f6
	ctx.f[12].f64 = (((ctx.f[12].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C77F8: ED08282A  fadds f8, f8, f5
	ctx.f[8].f64 = ((ctx.f[8].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C77FC: D101FF58  stfs f8, -0xa8(r1)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-168 as u32), tmp.u32 ) };
	// 832C7800: ECD7A828  fsubs f6, f23, f21
	ctx.f[6].f64 = (((ctx.f[23].f64 - ctx.f[21].f64) as f32) as f64);
	// 832C7804: C2E1FF34  lfs f23, -0xcc(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-204 as u32) ) };
	ctx.f[23].f64 = (tmp.f32 as f64);
	// 832C7808: ECAB802A  fadds f5, f11, f16
	ctx.f[5].f64 = ((ctx.f[11].f64 + ctx.f[16].f64) as f32) as f64;
	// 832C780C: C161FF30  lfs f11, -0xd0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-208 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C7810: EF2AF028  fsubs f25, f10, f30
	ctx.f[25].f64 = (((ctx.f[10].f64 - ctx.f[30].f64) as f32) as f64);
	// 832C7814: EFBDB02A  fadds f29, f29, f22
	ctx.f[29].f64 = ((ctx.f[29].f64 + ctx.f[22].f64) as f32) as f64;
	// 832C7818: ED5E502A  fadds f10, f30, f10
	ctx.f[10].f64 = ((ctx.f[30].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C781C: EED74828  fsubs f22, f23, f9
	ctx.f[22].f64 = (((ctx.f[23].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C7820: EFDB582A  fadds f30, f27, f11
	ctx.f[30].f64 = ((ctx.f[27].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C7824: ED29B82A  fadds f9, f9, f23
	ctx.f[9].f64 = ((ctx.f[9].f64 + ctx.f[23].f64) as f32) as f64;
	// 832C7828: ED6BD828  fsubs f11, f11, f27
	ctx.f[11].f64 = (((ctx.f[11].f64 - ctx.f[27].f64) as f32) as f64);
	// 832C782C: ECFF11F8  fmsubs f7, f31, f7, f2
	ctx.f[7].f64 = (((ctx.f[31].f64 * ctx.f[7].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C7830: C041FF24  lfs f2, -0xdc(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-220 as u32) ) };
	ctx.f[2].f64 = (tmp.f32 as f64);
	// 832C7834: EEA1202A  fadds f21, f1, f4
	ctx.f[21].f64 = ((ctx.f[1].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C7838: EC840828  fsubs f4, f4, f1
	ctx.f[4].f64 = (((ctx.f[4].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C783C: C021FF38  lfs f1, -0xc8(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-200 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C7840: EFE2A028  fsubs f31, f2, f20
	ctx.f[31].f64 = (((ctx.f[2].f64 - ctx.f[20].f64) as f32) as f64);
	// 832C7844: EC54102A  fadds f2, f20, f2
	ctx.f[2].f64 = ((ctx.f[20].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C7848: EE8D082A  fadds f20, f13, f1
	ctx.f[20].f64 = ((ctx.f[13].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C784C: EC216828  fsubs f1, f1, f13
	ctx.f[1].f64 = (((ctx.f[1].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C7850: EE5D982A  fadds f18, f29, f19
	ctx.f[18].f64 = ((ctx.f[29].f64 + ctx.f[19].f64) as f32) as f64;
	// 832C7854: EDB3E828  fsubs f13, f19, f29
	ctx.f[13].f64 = (((ctx.f[19].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C7858: EF76F028  fsubs f27, f22, f30
	ctx.f[27].f64 = (((ctx.f[22].f64 - ctx.f[30].f64) as f32) as f64);
	// 832C785C: EFA95828  fsubs f29, f9, f11
	ctx.f[29].f64 = (((ctx.f[9].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C7860: EFDEB02A  fadds f30, f30, f22
	ctx.f[30].f64 = ((ctx.f[30].f64 + ctx.f[22].f64) as f32) as f64;
	// 832C7864: C2C1FF44  lfs f22, -0xbc(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-188 as u32) ) };
	ctx.f[22].f64 = (tmp.f32 as f64);
	// 832C7868: ED6B482A  fadds f11, f11, f9
	ctx.f[11].f64 = ((ctx.f[11].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C786C: C121FF48  lfs f9, -0xb8(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-184 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C7870: EE27182A  fadds f17, f7, f3
	ctx.f[17].f64 = ((ctx.f[7].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C7874: ECE33828  fsubs f7, f3, f7
	ctx.f[7].f64 = (((ctx.f[3].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7878: C061FF40  lfs f3, -0xc0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-192 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C787C: EE05B02A  fadds f16, f5, f22
	ctx.f[16].f64 = ((ctx.f[5].f64 + ctx.f[22].f64) as f32) as f64;
	// 832C7880: EEFC182A  fadds f23, f28, f3
	ctx.f[23].f64 = ((ctx.f[28].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C7884: EC63E028  fsubs f3, f3, f28
	ctx.f[3].f64 = (((ctx.f[3].f64 - ctx.f[28].f64) as f32) as f64);
	// 832C7888: ECB62828  fsubs f5, f22, f5
	ctx.f[5].f64 = (((ctx.f[22].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C788C: C2C1FF4C  lfs f22, -0xb4(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-180 as u32) ) };
	ctx.f[22].f64 = (tmp.f32 as f64);
	// 832C7890: EF7B03F2  fmuls f27, f27, f15
	ctx.f[27].f64 = (((ctx.f[27].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C7894: EFBD03F2  fmuls f29, f29, f15
	ctx.f[29].f64 = (((ctx.f[29].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C7898: EFDE03F2  fmuls f30, f30, f15
	ctx.f[30].f64 = (((ctx.f[30].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C789C: ED6B03F2  fmuls f11, f11, f15
	ctx.f[11].f64 = (((ctx.f[11].f64 * ctx.f[15].f64) as f32) as f64);
	// 832C78A0: EDFF8828  fsubs f15, f31, f17
	ctx.f[15].f64 = (((ctx.f[31].f64 - ctx.f[17].f64) as f32) as f64);
	// 832C78A4: EF893028  fsubs f28, f9, f6
	ctx.f[28].f64 = (((ctx.f[9].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C78A8: EFF1F82A  fadds f31, f17, f31
	ctx.f[31].f64 = ((ctx.f[17].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C78AC: EE32A02A  fadds f17, f18, f20
	ctx.f[17].f64 = ((ctx.f[18].f64 + ctx.f[20].f64) as f32) as f64;
	// 832C78B0: D2230004  stfs f17, 4(r3)
	tmp.f32 = (ctx.f[17].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C78B4: ECC6482A  fadds f6, f6, f9
	ctx.f[6].f64 = ((ctx.f[6].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C78B8: C121FF54  lfs f9, -0xac(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-172 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C78BC: EE6CB02A  fadds f19, f12, f22
	ctx.f[19].f64 = ((ctx.f[12].f64 + ctx.f[22].f64) as f32) as f64;
	// 832C78C0: D141FF54  stfs f10, -0xac(r1)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-172 as u32), tmp.u32 ) };
	// 832C78C4: EE949028  fsubs f20, f20, f18
	ctx.f[20].f64 = (((ctx.f[20].f64 - ctx.f[18].f64) as f32) as f64);
	// 832C78C8: D283000C  stfs f20, 0xc(r3)
	tmp.f32 = (ctx.f[20].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C78CC: ED966028  fsubs f12, f22, f12
	ctx.f[12].f64 = (((ctx.f[22].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C78D0: EE90B82A  fadds f20, f16, f23
	ctx.f[20].f64 = ((ctx.f[16].f64 + ctx.f[23].f64) as f32) as f64;
	// 832C78D4: D2830000  stfs f20, 0(r3)
	tmp.f32 = (ctx.f[20].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C78D8: EED9482A  fadds f22, f25, f9
	ctx.f[22].f64 = ((ctx.f[25].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C78DC: ED1AA828  fsubs f8, f26, f21
	ctx.f[8].f64 = (((ctx.f[26].f64 - ctx.f[21].f64) as f32) as f64);
	// 832C78E0: EEF78028  fsubs f23, f23, f16
	ctx.f[23].f64 = (((ctx.f[23].f64 - ctx.f[16].f64) as f32) as f64);
	// 832C78E4: D2E30008  stfs f23, 8(r3)
	tmp.f32 = (ctx.f[23].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C78E8: ED29C828  fsubs f9, f9, f25
	ctx.f[9].f64 = (((ctx.f[9].f64 - ctx.f[25].f64) as f32) as f64);
	// 832C78EC: ED58702A  fadds f10, f24, f14
	ctx.f[10].f64 = ((ctx.f[24].f64 + ctx.f[14].f64) as f32) as f64;
	// 832C78F0: EF55D02A  fadds f26, f21, f26
	ctx.f[26].f64 = ((ctx.f[21].f64 + ctx.f[26].f64) as f32) as f64;
	// 832C78F4: EF2EC028  fsubs f25, f14, f24
	ctx.f[25].f64 = (((ctx.f[14].f64 - ctx.f[24].f64) as f32) as f64);
	// 832C78F8: EF04102A  fadds f24, f4, f2
	ctx.f[24].f64 = ((ctx.f[4].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C78FC: EEA03828  fsubs f21, f0, f7
	ctx.f[21].f64 = (((ctx.f[0].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7900: EEE36828  fsubs f23, f3, f13
	ctx.f[23].f64 = (((ctx.f[3].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C7904: D2E30010  stfs f23, 0x10(r3)
	tmp.f32 = (ctx.f[23].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(16 as u32), tmp.u32 ) };
	// 832C7908: EEE5082A  fadds f23, f5, f1
	ctx.f[23].f64 = ((ctx.f[5].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C790C: EC212828  fsubs f1, f1, f5
	ctx.f[1].f64 = (((ctx.f[1].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C7910: D023001C  stfs f1, 0x1c(r3)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(28 as u32), tmp.u32 ) };
	// 832C7914: ECBE982A  fadds f5, f30, f19
	ctx.f[5].f64 = ((ctx.f[30].f64 + ctx.f[19].f64) as f32) as f64;
	// 832C7918: D0A30024  stfs f5, 0x24(r3)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(36 as u32), tmp.u32 ) };
	// 832C791C: ECACE828  fsubs f5, f12, f29
	ctx.f[5].f64 = (((ctx.f[12].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C7920: D0A3003C  stfs f5, 0x3c(r3)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(60 as u32), tmp.u32 ) };
	// 832C7924: ECA22028  fsubs f5, f2, f4
	ctx.f[5].f64 = (((ctx.f[2].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C7928: D2E30014  stfs f23, 0x14(r3)
	tmp.f32 = (ctx.f[23].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(20 as u32), tmp.u32 ) };
	// 832C792C: EC87002A  fadds f4, f7, f0
	ctx.f[4].f64 = ((ctx.f[7].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C7930: C001FF58  lfs f0, -0xa8(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-168 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C7934: EC6D182A  fadds f3, f13, f3
	ctx.f[3].f64 = ((ctx.f[13].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C7938: D0630018  stfs f3, 0x18(r3)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(24 as u32), tmp.u32 ) };
	// 832C793C: EDBBE02A  fadds f13, f27, f28
	ctx.f[13].f64 = ((ctx.f[27].f64 + ctx.f[28].f64) as f32) as f64;
	// 832C7940: D1A30020  stfs f13, 0x20(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(32 as u32), tmp.u32 ) };
	// 832C7944: EDA65828  fsubs f13, f6, f11
	ctx.f[13].f64 = (((ctx.f[6].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C7948: D1A30030  stfs f13, 0x30(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(48 as u32), tmp.u32 ) };
	// 832C794C: ED6B302A  fadds f11, f11, f6
	ctx.f[11].f64 = ((ctx.f[11].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C7950: D1630038  stfs f11, 0x38(r3)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(56 as u32), tmp.u32 ) };
	// 832C7954: EC33F028  fsubs f1, f19, f30
	ctx.f[1].f64 = (((ctx.f[19].f64 - ctx.f[30].f64) as f32) as f64);
	// 832C7958: D023002C  stfs f1, 0x2c(r3)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(44 as u32), tmp.u32 ) };
	// 832C795C: ECDD602A  fadds f6, f29, f12
	ctx.f[6].f64 = ((ctx.f[29].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C7960: D0C30034  stfs f6, 0x34(r3)
	tmp.f32 = (ctx.f[6].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(52 as u32), tmp.u32 ) };
	// 832C7964: EC7CD828  fsubs f3, f28, f27
	ctx.f[3].f64 = (((ctx.f[28].f64 - ctx.f[27].f64) as f32) as f64);
	// 832C7968: D0630028  stfs f3, 0x28(r3)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(40 as u32), tmp.u32 ) };
	// 832C796C: EC28502A  fadds f1, f8, f10
	ctx.f[1].f64 = ((ctx.f[8].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C7970: D0230044  stfs f1, 0x44(r3)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(68 as u32), tmp.u32 ) };
	// 832C7974: ED8A4028  fsubs f12, f10, f8
	ctx.f[12].f64 = (((ctx.f[10].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C7978: C021FF5C  lfs f1, -0xa4(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-164 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C797C: EC6FB02A  fadds f3, f15, f22
	ctx.f[3].f64 = ((ctx.f[15].f64 + ctx.f[22].f64) as f32) as f64;
	// 832C7980: D0630040  stfs f3, 0x40(r3)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(64 as u32), tmp.u32 ) };
	// 832C7984: ED18A828  fsubs f8, f24, f21
	ctx.f[8].f64 = (((ctx.f[24].f64 - ctx.f[21].f64) as f32) as f64);
	// 832C7988: D183004C  stfs f12, 0x4c(r3)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(76 as u32), tmp.u32 ) };
	// 832C798C: ECD5C02A  fadds f6, f21, f24
	ctx.f[6].f64 = ((ctx.f[21].f64 + ctx.f[24].f64) as f32) as f64;
	// 832C7990: C181FF60  lfs f12, -0xa0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-160 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C7994: EC652028  fsubs f3, f5, f4
	ctx.f[3].f64 = (((ctx.f[5].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C7998: EC44282A  fadds f2, f4, f5
	ctx.f[2].f64 = ((ctx.f[4].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C799C: ED49D028  fsubs f10, f9, f26
	ctx.f[10].f64 = (((ctx.f[9].f64 - ctx.f[26].f64) as f32) as f64);
	// 832C79A0: D1430050  stfs f10, 0x50(r3)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(80 as u32), tmp.u32 ) };
	// 832C79A4: ED3A482A  fadds f9, f26, f9
	ctx.f[9].f64 = ((ctx.f[26].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C79A8: D1230058  stfs f9, 0x58(r3)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(88 as u32), tmp.u32 ) };
	// 832C79AC: ED7FC82A  fadds f11, f31, f25
	ctx.f[11].f64 = ((ctx.f[31].f64 + ctx.f[25].f64) as f32) as f64;
	// 832C79B0: C121FF50  lfs f9, -0xb0(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-176 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C79B4: D1630054  stfs f11, 0x54(r3)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(84 as u32), tmp.u32 ) };
	// 832C79B8: EDB67828  fsubs f13, f22, f15
	ctx.f[13].f64 = (((ctx.f[22].f64 - ctx.f[15].f64) as f32) as f64);
	// 832C79BC: C161FF54  lfs f11, -0xac(r1)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-172 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C79C0: EC99F828  fsubs f4, f25, f31
	ctx.f[4].f64 = (((ctx.f[25].f64 - ctx.f[31].f64) as f32) as f64);
	// 832C79C4: D1A30048  stfs f13, 0x48(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(72 as u32), tmp.u32 ) };
	// 832C79C8: EDA10028  fsubs f13, f1, f0
	ctx.f[13].f64 = (((ctx.f[1].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C79CC: ED080272  fmuls f8, f8, f9
	ctx.f[8].f64 = (((ctx.f[8].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C79D0: D083005C  stfs f4, 0x5c(r3)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(92 as u32), tmp.u32 ) };
	// 832C79D4: ED4B602A  fadds f10, f11, f12
	ctx.f[10].f64 = ((ctx.f[11].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C79D8: ECE60272  fmuls f7, f6, f9
	ctx.f[7].f64 = (((ctx.f[6].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C79DC: ECC0082A  fadds f6, f0, f1
	ctx.f[6].f64 = ((ctx.f[0].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C79E0: ECAC5828  fsubs f5, f12, f11
	ctx.f[5].f64 = (((ctx.f[12].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C79E4: EC630272  fmuls f3, f3, f9
	ctx.f[3].f64 = (((ctx.f[3].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C79E8: EC420272  fmuls f2, f2, f9
	ctx.f[2].f64 = (((ctx.f[2].f64 * ctx.f[9].f64) as f32) as f64);
	// 832C79EC: EC28682A  fadds f1, f8, f13
	ctx.f[1].f64 = ((ctx.f[8].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C79F0: D0230060  stfs f1, 0x60(r3)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(96 as u32), tmp.u32 ) };
	// 832C79F4: EDAD4028  fsubs f13, f13, f8
	ctx.f[13].f64 = (((ctx.f[13].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C79F8: D1A30068  stfs f13, 0x68(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(104 as u32), tmp.u32 ) };
	// 832C79FC: EC07502A  fadds f0, f7, f10
	ctx.f[0].f64 = ((ctx.f[7].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C7A00: D0030064  stfs f0, 0x64(r3)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(100 as u32), tmp.u32 ) };
	// 832C7A04: ED8A3828  fsubs f12, f10, f7
	ctx.f[12].f64 = (((ctx.f[10].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7A08: D183006C  stfs f12, 0x6c(r3)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(108 as u32), tmp.u32 ) };
	// 832C7A0C: ED63282A  fadds f11, f3, f5
	ctx.f[11].f64 = ((ctx.f[3].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C7A10: D1630074  stfs f11, 0x74(r3)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(116 as u32), tmp.u32 ) };
	// 832C7A14: ED461028  fsubs f10, f6, f2
	ctx.f[10].f64 = (((ctx.f[6].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C7A18: D1430070  stfs f10, 0x70(r3)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(112 as u32), tmp.u32 ) };
	// 832C7A1C: ED22302A  fadds f9, f2, f6
	ctx.f[9].f64 = ((ctx.f[2].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C7A20: D1230078  stfs f9, 0x78(r3)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(120 as u32), tmp.u32 ) };
	// 832C7A24: ED051828  fsubs f8, f5, f3
	ctx.f[8].f64 = (((ctx.f[5].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C7A28: D103007C  stfs f8, 0x7c(r3)
	tmp.f32 = (ctx.f[8].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(124 as u32), tmp.u32 ) };
	// 832C7A2C: 3981FFF8  addi r12, r1, -8
	ctx.r[12].s64 = ctx.r[1].s64 + -8;
	// 832C7A30: 4B9E62BD  bl 0x82cadcec
	ctx.lr = 0x832C7A34;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCEC);
	// 832C7A34: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832C7A38: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832C7A3C: 4E800020  blr
	return;
}

pub fn sub_832C7A40(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    // ---- function 0x832C7A40 size=392
	// 832C7A40: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C7A44: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832C7A48: 3981FFF8  addi r12, r1, -8
	ctx.r[12].s64 = ctx.r[1].s64 + -8;
	// 832C7A4C: 4B9E627D  bl 0x82cadcc8
	ctx.lr = 0x832C7A50;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCC8);
	// 832C7A50: C0030028  lfs f0, 0x28(r3)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C7A54: C1830008  lfs f12, 8(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(8 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C7A58: C123003C  lfs f9, 0x3c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(60 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C7A5C: ED4C0028  fsubs f10, f12, f0
	ctx.f[10].f64 = (((ctx.f[12].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C7A60: C1A3002C  lfs f13, 0x2c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C7A64: ED0C002A  fadds f8, f12, f0
	ctx.f[8].f64 = ((ctx.f[12].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C7A68: C1630038  lfs f11, 0x38(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(56 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C7A6C: C0E3000C  lfs f7, 0xc(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(12 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C7A70: C0C30018  lfs f6, 0x18(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(24 as u32) ) };
	ctx.f[6].f64 = (tmp.f32 as f64);
	// 832C7A74: ECA76828  fsubs f5, f7, f13
	ctx.f[5].f64 = (((ctx.f[7].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C7A78: C083001C  lfs f4, 0x1c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(28 as u32) ) };
	ctx.f[4].f64 = (tmp.f32 as f64);
	// 832C7A7C: EC665828  fsubs f3, f6, f11
	ctx.f[3].f64 = (((ctx.f[6].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C7A80: EC444828  fsubs f2, f4, f9
	ctx.f[2].f64 = (((ctx.f[4].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C7A84: C1830000  lfs f12, 0(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C7A88: C0230020  lfs f1, 0x20(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(32 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C7A8C: ED66582A  fadds f11, f6, f11
	ctx.f[11].f64 = ((ctx.f[6].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C7A90: ECCC082A  fadds f6, f12, f1
	ctx.f[6].f64 = ((ctx.f[12].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C7A94: C3E30030  lfs f31, 0x30(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832C7A98: EC2C0828  fsubs f1, f12, f1
	ctx.f[1].f64 = (((ctx.f[12].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C7A9C: C1830010  lfs f12, 0x10(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(16 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C7AA0: EF8CF82A  fadds f28, f12, f31
	ctx.f[28].f64 = ((ctx.f[12].f64 + ctx.f[31].f64) as f32) as f64;
	// 832C7AA4: C0030024  lfs f0, 0x24(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(36 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C7AA8: EDA7682A  fadds f13, f7, f13
	ctx.f[13].f64 = ((ctx.f[7].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C7AAC: C0E30004  lfs f7, 4(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C7AB0: EFC7002A  fadds f30, f7, f0
	ctx.f[30].f64 = ((ctx.f[7].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C7AB4: C3A30034  lfs f29, 0x34(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(52 as u32) ) };
	ctx.f[29].f64 = (tmp.f32 as f64);
	// 832C7AB8: ECE70028  fsubs f7, f7, f0
	ctx.f[7].f64 = (((ctx.f[7].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C7ABC: C0030014  lfs f0, 0x14(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(20 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C7AC0: EF60E82A  fadds f27, f0, f29
	ctx.f[27].f64 = ((ctx.f[0].f64 + ctx.f[29].f64) as f32) as f64;
	// 832C7AC4: C3440004  lfs f26, 4(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(4 as u32) ) };
	ctx.f[26].f64 = (tmp.f32 as f64);
	// 832C7AC8: EF23282A  fadds f25, f3, f5
	ctx.f[25].f64 = ((ctx.f[3].f64 + ctx.f[5].f64) as f32) as f64;
	// 832C7ACC: EF0A1028  fsubs f24, f10, f2
	ctx.f[24].f64 = (((ctx.f[10].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C7AD0: ED42502A  fadds f10, f2, f10
	ctx.f[10].f64 = ((ctx.f[2].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C7AD4: ECA51828  fsubs f5, f5, f3
	ctx.f[5].f64 = (((ctx.f[5].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C7AD8: EC40E828  fsubs f2, f0, f29
	ctx.f[2].f64 = (((ctx.f[0].f64 - ctx.f[29].f64) as f32) as f64);
	// 832C7ADC: EC6CF828  fsubs f3, f12, f31
	ctx.f[3].f64 = (((ctx.f[12].f64 - ctx.f[31].f64) as f32) as f64);
	// 832C7AE0: EC04482A  fadds f0, f4, f9
	ctx.f[0].f64 = ((ctx.f[4].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C7AE4: ED3C302A  fadds f9, f28, f6
	ctx.f[9].f64 = ((ctx.f[28].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C7AE8: ECC6E028  fsubs f6, f6, f28
	ctx.f[6].f64 = (((ctx.f[6].f64 - ctx.f[28].f64) as f32) as f64);
	// 832C7AEC: ED8B402A  fadds f12, f11, f8
	ctx.f[12].f64 = ((ctx.f[11].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C7AF0: EC9BF02A  fadds f4, f27, f30
	ctx.f[4].f64 = ((ctx.f[27].f64 + ctx.f[30].f64) as f32) as f64;
	// 832C7AF4: EFF8C828  fsubs f31, f24, f25
	ctx.f[31].f64 = (((ctx.f[24].f64 - ctx.f[25].f64) as f32) as f64);
	// 832C7AF8: EFB9C02A  fadds f29, f25, f24
	ctx.f[29].f64 = ((ctx.f[25].f64 + ctx.f[24].f64) as f32) as f64;
	// 832C7AFC: EF85502A  fadds f28, f5, f10
	ctx.f[28].f64 = ((ctx.f[5].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C7B00: ECAA2828  fsubs f5, f10, f5
	ctx.f[5].f64 = (((ctx.f[10].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C7B04: ED411028  fsubs f10, f1, f2
	ctx.f[10].f64 = (((ctx.f[1].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C7B08: EC42082A  fadds f2, f2, f1
	ctx.f[2].f64 = ((ctx.f[2].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C7B0C: EF23382A  fadds f25, f3, f7
	ctx.f[25].f64 = ((ctx.f[3].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C7B10: EC271828  fsubs f1, f7, f3
	ctx.f[1].f64 = (((ctx.f[7].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C7B14: ECE0682A  fadds f7, f0, f13
	ctx.f[7].f64 = ((ctx.f[0].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C7B18: EC6D0028  fsubs f3, f13, f0
	ctx.f[3].f64 = (((ctx.f[13].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C7B1C: EC1F06B2  fmuls f0, f31, f26
	ctx.f[0].f64 = (((ctx.f[31].f64 * ctx.f[26].f64) as f32) as f64);
	// 832C7B20: EDBD06B2  fmuls f13, f29, f26
	ctx.f[13].f64 = (((ctx.f[29].f64 * ctx.f[26].f64) as f32) as f64);
	// 832C7B24: EFFC06B2  fmuls f31, f28, f26
	ctx.f[31].f64 = (((ctx.f[28].f64 * ctx.f[26].f64) as f32) as f64);
	// 832C7B28: ECA506B2  fmuls f5, f5, f26
	ctx.f[5].f64 = (((ctx.f[5].f64 * ctx.f[26].f64) as f32) as f64);
	// 832C7B2C: EFAC482A  fadds f29, f12, f9
	ctx.f[29].f64 = ((ctx.f[12].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C7B30: D3A30000  stfs f29, 0(r3)
	tmp.f32 = (ctx.f[29].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C7B34: ED896028  fsubs f12, f9, f12
	ctx.f[12].f64 = (((ctx.f[9].f64 - ctx.f[12].f64) as f32) as f64);
	// 832C7B38: D1830008  stfs f12, 8(r3)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C7B3C: ED085828  fsubs f8, f8, f11
	ctx.f[8].f64 = (((ctx.f[8].f64 - ctx.f[11].f64) as f32) as f64);
	// 832C7B40: ED3ED828  fsubs f9, f30, f27
	ctx.f[9].f64 = (((ctx.f[30].f64 - ctx.f[27].f64) as f32) as f64);
	// 832C7B44: ED87202A  fadds f12, f7, f4
	ctx.f[12].f64 = ((ctx.f[7].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C7B48: D1830004  stfs f12, 4(r3)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C7B4C: ED643828  fsubs f11, f4, f7
	ctx.f[11].f64 = (((ctx.f[4].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7B50: ECE0502A  fadds f7, f0, f10
	ctx.f[7].f64 = ((ctx.f[0].f64 + ctx.f[10].f64) as f32) as f64;
	// 832C7B54: D0E30020  stfs f7, 0x20(r3)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(32 as u32), tmp.u32 ) };
	// 832C7B58: EC0A0028  fsubs f0, f10, f0
	ctx.f[0].f64 = (((ctx.f[10].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C7B5C: D0030028  stfs f0, 0x28(r3)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(40 as u32), tmp.u32 ) };
	// 832C7B60: EC8DC82A  fadds f4, f13, f25
	ctx.f[4].f64 = ((ctx.f[13].f64 + ctx.f[25].f64) as f32) as f64;
	// 832C7B64: D0830024  stfs f4, 0x24(r3)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(36 as u32), tmp.u32 ) };
	// 832C7B68: ED45082A  fadds f10, f5, f1
	ctx.f[10].f64 = ((ctx.f[5].f64 + ctx.f[1].f64) as f32) as f64;
	// 832C7B6C: D1430034  stfs f10, 0x34(r3)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(52 as u32), tmp.u32 ) };
	// 832C7B70: EDB96828  fsubs f13, f25, f13
	ctx.f[13].f64 = (((ctx.f[25].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C7B74: D1A3002C  stfs f13, 0x2c(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(44 as u32), tmp.u32 ) };
	// 832C7B78: ED82F828  fsubs f12, f2, f31
	ctx.f[12].f64 = (((ctx.f[2].f64 - ctx.f[31].f64) as f32) as f64);
	// 832C7B7C: D1830030  stfs f12, 0x30(r3)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(48 as u32), tmp.u32 ) };
	// 832C7B80: ECFF102A  fadds f7, f31, f2
	ctx.f[7].f64 = ((ctx.f[31].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C7B84: D0E30038  stfs f7, 0x38(r3)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(56 as u32), tmp.u32 ) };
	// 832C7B88: ECA12828  fsubs f5, f1, f5
	ctx.f[5].f64 = (((ctx.f[1].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C7B8C: D0A3003C  stfs f5, 0x3c(r3)
	tmp.f32 = (ctx.f[5].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(60 as u32), tmp.u32 ) };
	// 832C7B90: EC861828  fsubs f4, f6, f3
	ctx.f[4].f64 = (((ctx.f[6].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C7B94: EC48482A  fadds f2, f8, f9
	ctx.f[2].f64 = ((ctx.f[8].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C7B98: D163000C  stfs f11, 0xc(r3)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C7B9C: EC23302A  fadds f1, f3, f6
	ctx.f[1].f64 = ((ctx.f[3].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C7BA0: D0830010  stfs f4, 0x10(r3)
	tmp.f32 = (ctx.f[4].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(16 as u32), tmp.u32 ) };
	// 832C7BA4: EC094028  fsubs f0, f9, f8
	ctx.f[0].f64 = (((ctx.f[9].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C7BA8: D0430014  stfs f2, 0x14(r3)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(20 as u32), tmp.u32 ) };
	// 832C7BAC: D0230018  stfs f1, 0x18(r3)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(24 as u32), tmp.u32 ) };
	// 832C7BB0: D003001C  stfs f0, 0x1c(r3)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(28 as u32), tmp.u32 ) };
	// 832C7BB4: 3981FFF8  addi r12, r1, -8
	ctx.r[12].s64 = ctx.r[1].s64 + -8;
	// 832C7BB8: 4B9E615D  bl 0x82cadd14
	ctx.lr = 0x832C7BBC;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD14);
	// 832C7BBC: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832C7BC0: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832C7BC4: 4E800020  blr
	return;
}

pub fn sub_832C7BC8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut tmp: PPCRegister = Default::default();
    // ---- function 0x832C7BC8 size=464
	// 832C7BC8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C7BCC: 9181FFF8  stw r12, -8(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(-8 as u32), ctx.r[12].u32 ) };
	// 832C7BD0: 3981FFF8  addi r12, r1, -8
	ctx.r[12].s64 = ctx.r[1].s64 + -8;
	// 832C7BD4: 4B9E60E5  bl 0x82cadcb8
	ctx.lr = 0x832C7BD8;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADCB8);
	// 832C7BD8: C1A30014  lfs f13, 0x14(r3)
	ctx.fpscr.disable_flush_mode_unconditional();
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(20 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C7BDC: C0030010  lfs f0, 0x10(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(16 as u32) ) };
	ctx.f[0].f64 = (tmp.f32 as f64);
	// 832C7BE0: C183000C  lfs f12, 0xc(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(12 as u32) ) };
	ctx.f[12].f64 = (tmp.f32 as f64);
	// 832C7BE4: C1430034  lfs f10, 0x34(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(52 as u32) ) };
	ctx.f[10].f64 = (tmp.f32 as f64);
	// 832C7BE8: C0E30028  lfs f7, 0x28(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(40 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C7BEC: ED005028  fsubs f8, f0, f10
	ctx.f[8].f64 = (((ctx.f[0].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C7BF0: C1230030  lfs f9, 0x30(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(48 as u32) ) };
	ctx.f[9].f64 = (tmp.f32 as f64);
	// 832C7BF4: EC8C382A  fadds f4, f12, f7
	ctx.f[4].f64 = ((ctx.f[12].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C7BF8: ECCD482A  fadds f6, f13, f9
	ctx.f[6].f64 = ((ctx.f[13].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C7BFC: C1630018  lfs f11, 0x18(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(24 as u32) ) };
	ctx.f[11].f64 = (tmp.f32 as f64);
	// 832C7C00: C0A3003C  lfs f5, 0x3c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(60 as u32) ) };
	ctx.f[5].f64 = (tmp.f32 as f64);
	// 832C7C04: EC0A002A  fadds f0, f10, f0
	ctx.f[0].f64 = ((ctx.f[10].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C7C08: C023001C  lfs f1, 0x1c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(28 as u32) ) };
	ctx.f[1].f64 = (tmp.f32 as f64);
	// 832C7C0C: EC4B2828  fsubs f2, f11, f5
	ctx.f[2].f64 = (((ctx.f[11].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C7C10: C0630038  lfs f3, 0x38(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(56 as u32) ) };
	ctx.f[3].f64 = (tmp.f32 as f64);
	// 832C7C14: ED2D4828  fsubs f9, f13, f9
	ctx.f[9].f64 = (((ctx.f[13].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C7C18: ED41182A  fadds f10, f1, f3
	ctx.f[10].f64 = ((ctx.f[1].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C7C1C: C1A30008  lfs f13, 8(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(8 as u32) ) };
	ctx.f[13].f64 = (tmp.f32 as f64);
	// 832C7C20: ED8C3828  fsubs f12, f12, f7
	ctx.f[12].f64 = (((ctx.f[12].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7C24: C0E3002C  lfs f7, 0x2c(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(44 as u32) ) };
	ctx.f[7].f64 = (tmp.f32 as f64);
	// 832C7C28: EC611828  fsubs f3, f1, f3
	ctx.f[3].f64 = (((ctx.f[1].f64 - ctx.f[3].f64) as f32) as f64);
	// 832C7C2C: C3C40014  lfs f30, 0x14(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(20 as u32) ) };
	ctx.f[30].f64 = (tmp.f32 as f64);
	// 832C7C30: EC2D3828  fsubs f1, f13, f7
	ctx.f[1].f64 = (((ctx.f[13].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7C34: C3E40010  lfs f31, 0x10(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(16 as u32) ) };
	ctx.f[31].f64 = (tmp.f32 as f64);
	// 832C7C38: EDA7682A  fadds f13, f7, f13
	ctx.f[13].f64 = ((ctx.f[7].f64 + ctx.f[13].f64) as f32) as f64;
	// 832C7C3C: C3A40004  lfs f29, 4(r4)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(4 as u32) ) };
	ctx.f[29].f64 = (tmp.f32 as f64);
	// 832C7C40: EF4407B2  fmuls f26, f4, f30
	ctx.f[26].f64 = (((ctx.f[4].f64 * ctx.f[30].f64) as f32) as f64);
	// 832C7C44: C3830000  lfs f28, 0(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(0 as u32) ) };
	ctx.f[28].f64 = (tmp.f32 as f64);
	// 832C7C48: ECE83028  fsubs f7, f8, f6
	ctx.f[7].f64 = (((ctx.f[8].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C7C4C: C3630004  lfs f27, 4(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(4 as u32) ) };
	ctx.f[27].f64 = (tmp.f32 as f64);
	// 832C7C50: EC8407F2  fmuls f4, f4, f31
	ctx.f[4].f64 = (((ctx.f[4].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C7C54: C3230020  lfs f25, 0x20(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(32 as u32) ) };
	ctx.f[25].f64 = (tmp.f32 as f64);
	// 832C7C58: ECC6402A  fadds f6, f6, f8
	ctx.f[6].f64 = ((ctx.f[6].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C7C5C: C1030024  lfs f8, 0x24(r3)
	tmp.u32 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[3].u32.wrapping_add(36 as u32) ) };
	ctx.f[8].f64 = (tmp.f32 as f64);
	// 832C7C60: EF0207F2  fmuls f24, f2, f31
	ctx.f[24].f64 = (((ctx.f[2].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C7C64: EEEA07F2  fmuls f23, f10, f31
	ctx.f[23].f64 = (((ctx.f[10].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C7C68: EEDC4028  fsubs f22, f28, f8
	ctx.f[22].f64 = (((ctx.f[28].f64 - ctx.f[8].f64) as f32) as f64);
	// 832C7C6C: EEBBC82A  fadds f21, f27, f25
	ctx.f[21].f64 = ((ctx.f[27].f64 + ctx.f[25].f64) as f32) as f64;
	// 832C7C70: ED65582A  fadds f11, f5, f11
	ctx.f[11].f64 = ((ctx.f[5].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C7C74: ECA04828  fsubs f5, f0, f9
	ctx.f[5].f64 = (((ctx.f[0].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C7C78: EF41D7F8  fmsubs f26, f1, f31, f26
	ctx.f[26].f64 = (((ctx.f[1].f64 * ctx.f[31].f64 - ctx.f[26].f64) as f32) as f64);
	// 832C7C7C: ECE70772  fmuls f7, f7, f29
	ctx.f[7].f64 = (((ctx.f[7].f64 * ctx.f[29].f64) as f32) as f64);
	// 832C7C80: EC8127BA  fmadds f4, f1, f30, f4
	ctx.f[4].f64 = (((ctx.f[1].f64 * ctx.f[30].f64 + ctx.f[4].f64) as f32) as f64);
	// 832C7C84: ECC60772  fmuls f6, f6, f29
	ctx.f[6].f64 = (((ctx.f[6].f64 * ctx.f[29].f64) as f32) as f64);
	// 832C7C88: EC2AC7BA  fmadds f1, f10, f30, f24
	ctx.f[1].f64 = (((ctx.f[10].f64 * ctx.f[30].f64 + ctx.f[24].f64) as f32) as f64);
	// 832C7C8C: ED42BFB8  fmsubs f10, f2, f30, f23
	ctx.f[10].f64 = (((ctx.f[2].f64 * ctx.f[30].f64 - ctx.f[23].f64) as f32) as f64);
	// 832C7C90: EC4C07F2  fmuls f2, f12, f31
	ctx.f[2].f64 = (((ctx.f[12].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C7C94: ED29002A  fadds f9, f9, f0
	ctx.f[9].f64 = ((ctx.f[9].f64 + ctx.f[0].f64) as f32) as f64;
	// 832C7C98: EC0D07F2  fmuls f0, f13, f31
	ctx.f[0].f64 = (((ctx.f[13].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C7C9C: EF0307B2  fmuls f24, f3, f30
	ctx.f[24].f64 = (((ctx.f[3].f64 * ctx.f[30].f64) as f32) as f64);
	// 832C7CA0: ED08E02A  fadds f8, f8, f28
	ctx.f[8].f64 = ((ctx.f[8].f64 + ctx.f[28].f64) as f32) as f64;
	// 832C7CA4: EEE7B02A  fadds f23, f7, f22
	ctx.f[23].f64 = ((ctx.f[7].f64 + ctx.f[22].f64) as f32) as f64;
	// 832C7CA8: ECF63828  fsubs f7, f22, f7
	ctx.f[7].f64 = (((ctx.f[22].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7CAC: EE86A82A  fadds f20, f6, f21
	ctx.f[20].f64 = ((ctx.f[6].f64 + ctx.f[21].f64) as f32) as f64;
	// 832C7CB0: EEC1202A  fadds f22, f1, f4
	ctx.f[22].f64 = ((ctx.f[1].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C7CB4: ECD53028  fsubs f6, f21, f6
	ctx.f[6].f64 = (((ctx.f[21].f64 - ctx.f[6].f64) as f32) as f64);
	// 832C7CB8: EEAAD02A  fadds f21, f10, f26
	ctx.f[21].f64 = ((ctx.f[10].f64 + ctx.f[26].f64) as f32) as f64;
	// 832C7CBC: EC840828  fsubs f4, f4, f1
	ctx.f[4].f64 = (((ctx.f[4].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C7CC0: EC3A5028  fsubs f1, f26, f10
	ctx.f[1].f64 = (((ctx.f[26].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C7CC4: EC4D17B8  fmsubs f2, f13, f30, f2
	ctx.f[2].f64 = (((ctx.f[13].f64 * ctx.f[30].f64 - ctx.f[2].f64) as f32) as f64);
	// 832C7CC8: ED4307F2  fmuls f10, f3, f31
	ctx.f[10].f64 = (((ctx.f[3].f64 * ctx.f[31].f64) as f32) as f64);
	// 832C7CCC: EC7BC828  fsubs f3, f27, f25
	ctx.f[3].f64 = (((ctx.f[27].f64 - ctx.f[25].f64) as f32) as f64);
	// 832C7CD0: ECA50772  fmuls f5, f5, f29
	ctx.f[5].f64 = (((ctx.f[5].f64 * ctx.f[29].f64) as f32) as f64);
	// 832C7CD4: ED290772  fmuls f9, f9, f29
	ctx.f[9].f64 = (((ctx.f[9].f64 * ctx.f[29].f64) as f32) as f64);
	// 832C7CD8: EDB6A02A  fadds f13, f22, f20
	ctx.f[13].f64 = ((ctx.f[22].f64 + ctx.f[20].f64) as f32) as f64;
	// 832C7CDC: D1A30004  stfs f13, 4(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(4 as u32), tmp.u32 ) };
	// 832C7CE0: EDB5B82A  fadds f13, f21, f23
	ctx.f[13].f64 = ((ctx.f[21].f64 + ctx.f[23].f64) as f32) as f64;
	// 832C7CE4: D1A30000  stfs f13, 0(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(0 as u32), tmp.u32 ) };
	// 832C7CE8: EDB7A828  fsubs f13, f23, f21
	ctx.f[13].f64 = (((ctx.f[23].f64 - ctx.f[21].f64) as f32) as f64);
	// 832C7CEC: D1A30008  stfs f13, 8(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(8 as u32), tmp.u32 ) };
	// 832C7CF0: EDB4B028  fsubs f13, f20, f22
	ctx.f[13].f64 = (((ctx.f[20].f64 - ctx.f[22].f64) as f32) as f64);
	// 832C7CF4: D1A3000C  stfs f13, 0xc(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(12 as u32), tmp.u32 ) };
	// 832C7CF8: EDA1302A  fadds f13, f1, f6
	ctx.f[13].f64 = ((ctx.f[1].f64 + ctx.f[6].f64) as f32) as f64;
	// 832C7CFC: D1A30014  stfs f13, 0x14(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(20 as u32), tmp.u32 ) };
	// 832C7D00: EDA72028  fsubs f13, f7, f4
	ctx.f[13].f64 = (((ctx.f[7].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C7D04: D1A30010  stfs f13, 0x10(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(16 as u32), tmp.u32 ) };
	// 832C7D08: ECE4382A  fadds f7, f4, f7
	ctx.f[7].f64 = ((ctx.f[4].f64 + ctx.f[7].f64) as f32) as f64;
	// 832C7D0C: D0E30018  stfs f7, 0x18(r3)
	tmp.f32 = (ctx.f[7].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(24 as u32), tmp.u32 ) };
	// 832C7D10: EC8C07BA  fmadds f4, f12, f30, f0
	ctx.f[4].f64 = (((ctx.f[12].f64 * ctx.f[30].f64 + ctx.f[0].f64) as f32) as f64);
	// 832C7D14: EC0BC7F8  fmsubs f0, f11, f31, f24
	ctx.f[0].f64 = (((ctx.f[11].f64 * ctx.f[31].f64 - ctx.f[24].f64) as f32) as f64);
	// 832C7D18: EDAB57BA  fmadds f13, f11, f30, f10
	ctx.f[13].f64 = (((ctx.f[11].f64 * ctx.f[30].f64 + ctx.f[10].f64) as f32) as f64);
	// 832C7D1C: ED884828  fsubs f12, f8, f9
	ctx.f[12].f64 = (((ctx.f[8].f64 - ctx.f[9].f64) as f32) as f64);
	// 832C7D20: ED65182A  fadds f11, f5, f3
	ctx.f[11].f64 = ((ctx.f[5].f64 + ctx.f[3].f64) as f32) as f64;
	// 832C7D24: ED29402A  fadds f9, f9, f8
	ctx.f[9].f64 = ((ctx.f[9].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C7D28: ED420028  fsubs f10, f2, f0
	ctx.f[10].f64 = (((ctx.f[2].f64 - ctx.f[0].f64) as f32) as f64);
	// 832C7D2C: ECE46828  fsubs f7, f4, f13
	ctx.f[7].f64 = (((ctx.f[4].f64 - ctx.f[13].f64) as f32) as f64);
	// 832C7D30: ED032828  fsubs f8, f3, f5
	ctx.f[8].f64 = (((ctx.f[3].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C7D34: EC8D202A  fadds f4, f13, f4
	ctx.f[4].f64 = ((ctx.f[13].f64 + ctx.f[4].f64) as f32) as f64;
	// 832C7D38: ECA0102A  fadds f5, f0, f2
	ctx.f[5].f64 = ((ctx.f[0].f64 + ctx.f[2].f64) as f32) as f64;
	// 832C7D3C: EC660828  fsubs f3, f6, f1
	ctx.f[3].f64 = (((ctx.f[6].f64 - ctx.f[1].f64) as f32) as f64);
	// 832C7D40: D063001C  stfs f3, 0x1c(r3)
	tmp.f32 = (ctx.f[3].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(28 as u32), tmp.u32 ) };
	// 832C7D44: EC4A602A  fadds f2, f10, f12
	ctx.f[2].f64 = ((ctx.f[10].f64 + ctx.f[12].f64) as f32) as f64;
	// 832C7D48: D0430020  stfs f2, 0x20(r3)
	tmp.f32 = (ctx.f[2].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(32 as u32), tmp.u32 ) };
	// 832C7D4C: EC0C5028  fsubs f0, f12, f10
	ctx.f[0].f64 = (((ctx.f[12].f64 - ctx.f[10].f64) as f32) as f64);
	// 832C7D50: D0030028  stfs f0, 0x28(r3)
	tmp.f32 = (ctx.f[0].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(40 as u32), tmp.u32 ) };
	// 832C7D54: EC27582A  fadds f1, f7, f11
	ctx.f[1].f64 = ((ctx.f[7].f64 + ctx.f[11].f64) as f32) as f64;
	// 832C7D58: D0230024  stfs f1, 0x24(r3)
	tmp.f32 = (ctx.f[1].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(36 as u32), tmp.u32 ) };
	// 832C7D5C: EDAB3828  fsubs f13, f11, f7
	ctx.f[13].f64 = (((ctx.f[11].f64 - ctx.f[7].f64) as f32) as f64);
	// 832C7D60: D1A3002C  stfs f13, 0x2c(r3)
	tmp.f32 = (ctx.f[13].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(44 as u32), tmp.u32 ) };
	// 832C7D64: ED692028  fsubs f11, f9, f4
	ctx.f[11].f64 = (((ctx.f[9].f64 - ctx.f[4].f64) as f32) as f64);
	// 832C7D68: D1630030  stfs f11, 0x30(r3)
	tmp.f32 = (ctx.f[11].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(48 as u32), tmp.u32 ) };
	// 832C7D6C: ED44482A  fadds f10, f4, f9
	ctx.f[10].f64 = ((ctx.f[4].f64 + ctx.f[9].f64) as f32) as f64;
	// 832C7D70: D1430038  stfs f10, 0x38(r3)
	tmp.f32 = (ctx.f[10].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(56 as u32), tmp.u32 ) };
	// 832C7D74: ED85402A  fadds f12, f5, f8
	ctx.f[12].f64 = ((ctx.f[5].f64 + ctx.f[8].f64) as f32) as f64;
	// 832C7D78: D1830034  stfs f12, 0x34(r3)
	tmp.f32 = (ctx.f[12].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(52 as u32), tmp.u32 ) };
	// 832C7D7C: ED282828  fsubs f9, f8, f5
	ctx.f[9].f64 = (((ctx.f[8].f64 - ctx.f[5].f64) as f32) as f64);
	// 832C7D80: D123003C  stfs f9, 0x3c(r3)
	tmp.f32 = (ctx.f[9].f64 as f32);
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[3].u32.wrapping_add(60 as u32), tmp.u32 ) };
	// 832C7D84: 3981FFF8  addi r12, r1, -8
	ctx.r[12].s64 = ctx.r[1].s64 + -8;
	// 832C7D88: 4B9E5F7D  bl 0x82cadd04
	ctx.lr = 0x832C7D8C;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CADD04);
	// 832C7D8C: 8181FFF8  lwz r12, -8(r1)
	ctx.r[12].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[1].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832C7D90: 7D8803A6  mtlr r12
	ctx.lr = ctx.r[12].u64;
	// 832C7D94: 4E800020  blr
	return;
}

pub fn sub_832C7D98(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    // ---- function 0x832C7D98 size=8
	// 832C7D98: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C7D9C: 4B9E1661  bl 0x82ca93fc
	ctx.lr = 0x832C7DA0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93FC);
}

pub fn sub_832C7F98(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    // ---- function 0x832C7F98 size=800
	// 832C7F98: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C7F9C: 4B9E1449  bl 0x82ca93e4
	ctx.lr = 0x832C7FA0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93E4);
	// 832C7FA0: 7CAA2B78  mr r10, r5
	ctx.r[10].u64 = ctx.r[5].u64;
	// 832C7FA4: 3961FE90  addi r11, r1, -0x170
	ctx.r[11].s64 = ctx.r[1].s64 + -368;
	// 832C7FA8: 3B000008  li r24, 8
	ctx.r[24].s64 = 8;
	// 832C7FAC: A10A0010  lhz r8, 0x10(r10)
	ctx.r[8].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(16 as u32) ) } as u64;
	// 832C7FB0: A12A0020  lhz r9, 0x20(r10)
	ctx.r[9].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C7FB4: A0EA0030  lhz r7, 0x30(r10)
	ctx.r[7].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C7FB8: 7D050734  extsh r5, r8
	ctx.r[5].s64 = ctx.r[8].s16 as i64;
	// 832C7FBC: 7D290734  extsh r9, r9
	ctx.r[9].s64 = ctx.r[9].s16 as i64;
	// 832C7FC0: A10A0040  lhz r8, 0x40(r10)
	ctx.r[8].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C7FC4: 7CFD0734  extsh r29, r7
	ctx.r[29].s64 = ctx.r[7].s16 as i64;
	// 832C7FC8: A0EA0050  lhz r7, 0x50(r10)
	ctx.r[7].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C7FCC: 7CBF4B78  or r31, r5, r9
	ctx.r[31].u64 = ctx.r[5].u64 | ctx.r[9].u64;
	// 832C7FD0: A3CA0060  lhz r30, 0x60(r10)
	ctx.r[30].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C7FD4: 7D080734  extsh r8, r8
	ctx.r[8].s64 = ctx.r[8].s16 as i64;
	// 832C7FD8: A36A0070  lhz r27, 0x70(r10)
	ctx.r[27].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(112 as u32) ) } as u64;
	// 832C7FDC: 7FFFEB78  or r31, r31, r29
	ctx.r[31].u64 = ctx.r[31].u64 | ctx.r[29].u64;
	// 832C7FE0: 7CFC0734  extsh r28, r7
	ctx.r[28].s64 = ctx.r[7].s16 as i64;
	// 832C7FE4: 7FFF4378  or r31, r31, r8
	ctx.r[31].u64 = ctx.r[31].u64 | ctx.r[8].u64;
	// 832C7FE8: 7FC70734  extsh r7, r30
	ctx.r[7].s64 = ctx.r[30].s16 as i64;
	// 832C7FEC: 7FFFE378  or r31, r31, r28
	ctx.r[31].u64 = ctx.r[31].u64 | ctx.r[28].u64;
	// 832C7FF0: 7F7B0734  extsh r27, r27
	ctx.r[27].s64 = ctx.r[27].s16 as i64;
	// 832C7FF4: 7FFF3B78  or r31, r31, r7
	ctx.r[31].u64 = ctx.r[31].u64 | ctx.r[7].u64;
	// 832C7FF8: 7FFFDB78  or r31, r31, r27
	ctx.r[31].u64 = ctx.r[31].u64 | ctx.r[27].u64;
	// 832C7FFC: 7FFF0734  extsh r31, r31
	ctx.r[31].s64 = ctx.r[31].s16 as i64;
	// 832C8000: 2F1F0000  cmpwi cr6, r31, 0
	ctx.cr[6].compare_i32(ctx.r[31].s32, 0, &mut ctx.xer);
	// 832C8004: 409A003C  bne cr6, 0x832c8040
	if !ctx.cr[6].eq {
	pc = 0x832C8040; continue 'dispatch;
	}
	// 832C8008: A12A0000  lhz r9, 0(r10)
	ctx.r[9].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C800C: 394A0002  addi r10, r10, 2
	ctx.r[10].s64 = ctx.r[10].s64 + 2;
	// 832C8010: 81060000  lwz r8, 0(r6)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C8014: 7D270734  extsh r7, r9
	ctx.r[7].s64 = ctx.r[9].s16 as i64;
	// 832C8018: 7CA741D6  mullw r5, r7, r8
	ctx.r[5].s64 = (ctx.r[7].s32 as i64) * (ctx.r[8].s32 as i64);
	// 832C801C: 7CA95E70  srawi r9, r5, 0xb
	ctx.xer.ca = (ctx.r[5].s32 < 0) && ((ctx.r[5].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[9].s64 = (ctx.r[5].s32 >> 11) as i64;
	// 832C8020: 912B0000  stw r9, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[9].u32 ) };
	// 832C8024: 912B0020  stw r9, 0x20(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(32 as u32), ctx.r[9].u32 ) };
	// 832C8028: 912B0040  stw r9, 0x40(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(64 as u32), ctx.r[9].u32 ) };
	// 832C802C: 912B0060  stw r9, 0x60(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(96 as u32), ctx.r[9].u32 ) };
	// 832C8030: 912B00A0  stw r9, 0xa0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(160 as u32), ctx.r[9].u32 ) };
	// 832C8034: 912B00C0  stw r9, 0xc0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(192 as u32), ctx.r[9].u32 ) };
	// 832C8038: 912B00E0  stw r9, 0xe0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(224 as u32), ctx.r[9].u32 ) };
	// 832C803C: 48000128  b 0x832c8164
	pc = 0x832C8164; continue 'dispatch;
	// 832C8040: A3EA0000  lhz r31, 0(r10)
	ctx.r[31].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C8044: 394A0002  addi r10, r10, 2
	ctx.r[10].s64 = ctx.r[10].s64 + 2;
	// 832C8048: 83C60000  lwz r30, 0(r6)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C804C: 7FFF0734  extsh r31, r31
	ctx.r[31].s64 = ctx.r[31].s16 as i64;
	// 832C8050: 83460040  lwz r26, 0x40(r6)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C8054: 83260080  lwz r25, 0x80(r6)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(128 as u32) ) } as u64;
	// 832C8058: 7FFFF1D6  mullw r31, r31, r30
	ctx.r[31].s64 = (ctx.r[31].s32 as i64) * (ctx.r[30].s32 as i64);
	// 832C805C: 82E600C0  lwz r23, 0xc0(r6)
	ctx.r[23].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(192 as u32) ) } as u64;
	// 832C8060: 82C60020  lwz r22, 0x20(r6)
	ctx.r[22].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C8064: 82A60060  lwz r21, 0x60(r6)
	ctx.r[21].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C8068: 828600A0  lwz r20, 0xa0(r6)
	ctx.r[20].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(160 as u32) ) } as u64;
	// 832C806C: 826600E0  lwz r19, 0xe0(r6)
	ctx.r[19].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(224 as u32) ) } as u64;
	// 832C8070: 7D3A49D6  mullw r9, r26, r9
	ctx.r[9].s64 = (ctx.r[26].s32 as i64) * (ctx.r[9].s32 as i64);
	// 832C8074: 7FD941D6  mullw r30, r25, r8
	ctx.r[30].s64 = (ctx.r[25].s32 as i64) * (ctx.r[8].s32 as i64);
	// 832C8078: 7FFF5E70  srawi r31, r31, 0xb
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[31].s32 >> 11) as i64;
	// 832C807C: 7CF739D6  mullw r7, r23, r7
	ctx.r[7].s64 = (ctx.r[23].s32 as i64) * (ctx.r[7].s32 as i64);
	// 832C8080: 7D285E70  srawi r8, r9, 0xb
	ctx.xer.ca = (ctx.r[9].s32 < 0) && ((ctx.r[9].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[8].s64 = (ctx.r[9].s32 >> 11) as i64;
	// 832C8084: 7FDE5E70  srawi r30, r30, 0xb
	ctx.xer.ca = (ctx.r[30].s32 < 0) && ((ctx.r[30].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[30].s64 = (ctx.r[30].s32 >> 11) as i64;
	// 832C8088: 7CE75E70  srawi r7, r7, 0xb
	ctx.xer.ca = (ctx.r[7].s32 < 0) && ((ctx.r[7].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[7].s64 = (ctx.r[7].s32 >> 11) as i64;
	// 832C808C: 7CB629D6  mullw r5, r22, r5
	ctx.r[5].s64 = (ctx.r[22].s32 as i64) * (ctx.r[5].s32 as i64);
	// 832C8090: 7D274050  subf r9, r7, r8
	ctx.r[9].s64 = ctx.r[8].s64 - ctx.r[7].s64;
	// 832C8094: 7FB5E9D6  mullw r29, r21, r29
	ctx.r[29].s64 = (ctx.r[21].s32 as i64) * (ctx.r[29].s32 as i64);
	// 832C8098: 1D290B50  mulli r9, r9, 0xb50
	ctx.r[9].s64 = ctx.r[9].s64 * 2896;
	// 832C809C: 7D395E70  srawi r25, r9, 0xb
	ctx.xer.ca = (ctx.r[9].s32 < 0) && ((ctx.r[9].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[25].s64 = (ctx.r[9].s32 >> 11) as i64;
	// 832C80A0: 7F94E1D6  mullw r28, r20, r28
	ctx.r[28].s64 = (ctx.r[20].s32 as i64) * (ctx.r[28].s32 as i64);
	// 832C80A4: 7CA95E70  srawi r9, r5, 0xb
	ctx.xer.ca = (ctx.r[5].s32 < 0) && ((ctx.r[5].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[9].s64 = (ctx.r[5].s32 >> 11) as i64;
	// 832C80A8: 7F73D9D6  mullw r27, r19, r27
	ctx.r[27].s64 = (ctx.r[19].s32 as i64) * (ctx.r[27].s32 as i64);
	// 832C80AC: 7FA55E70  srawi r5, r29, 0xb
	ctx.xer.ca = (ctx.r[29].s32 < 0) && ((ctx.r[29].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[5].s64 = (ctx.r[29].s32 >> 11) as i64;
	// 832C80B0: 7F9D5E70  srawi r29, r28, 0xb
	ctx.xer.ca = (ctx.r[28].s32 < 0) && ((ctx.r[28].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[28].s32 >> 11) as i64;
	// 832C80B4: 7F7C5E70  srawi r28, r27, 0xb
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[28].s64 = (ctx.r[27].s32 >> 11) as i64;
	// 832C80B8: 7F65E850  subf r27, r5, r29
	ctx.r[27].s64 = ctx.r[29].s64 - ctx.r[5].s64;
	// 832C80BC: 7F5C4850  subf r26, r28, r9
	ctx.r[26].s64 = ctx.r[9].s64 - ctx.r[28].s64;
	// 832C80C0: 7CBD2A14  add r5, r29, r5
	ctx.r[5].u64 = ctx.r[29].u64 + ctx.r[5].u64;
	// 832C80C4: 7FBADA14  add r29, r26, r27
	ctx.r[29].u64 = ctx.r[26].u64 + ctx.r[27].u64;
	// 832C80C8: 7D3C4A14  add r9, r28, r9
	ctx.r[9].u64 = ctx.r[28].u64 + ctx.r[9].u64;
	// 832C80CC: 1FBD0EC8  mulli r29, r29, 0xec8
	ctx.r[29].s64 = ctx.r[29].s64 * 3784;
	// 832C80D0: 1F9BEB18  mulli r28, r27, -0x14e8
	ctx.r[28].s64 = ctx.r[27].s64 * -5352;
	// 832C80D4: 7F654850  subf r27, r5, r9
	ctx.r[27].s64 = ctx.r[9].s64 - ctx.r[5].s64;
	// 832C80D8: 7D292A14  add r9, r9, r5
	ctx.r[9].u64 = ctx.r[9].u64 + ctx.r[5].u64;
	// 832C80DC: 7FBD5E70  srawi r29, r29, 0xb
	ctx.xer.ca = (ctx.r[29].s32 < 0) && ((ctx.r[29].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[29].s32 >> 11) as i64;
	// 832C80E0: 7F855E70  srawi r5, r28, 0xb
	ctx.xer.ca = (ctx.r[28].s32 < 0) && ((ctx.r[28].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[5].s64 = (ctx.r[28].s32 >> 11) as i64;
	// 832C80E4: 1F9B0B50  mulli r28, r27, 0xb50
	ctx.r[28].s64 = ctx.r[27].s64 * 2896;
	// 832C80E8: 7CA92850  subf r5, r9, r5
	ctx.r[5].s64 = ctx.r[5].s64 - ctx.r[9].s64;
	// 832C80EC: 1F7A08A9  mulli r27, r26, 0x8a9
	ctx.r[27].s64 = ctx.r[26].s64 * 2217;
	// 832C80F0: 7F9A5E70  srawi r26, r28, 0xb
	ctx.xer.ca = (ctx.r[28].s32 < 0) && ((ctx.r[28].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[26].s64 = (ctx.r[28].s32 >> 11) as i64;
	// 832C80F4: 7D074214  add r8, r7, r8
	ctx.r[8].u64 = ctx.r[7].u64 + ctx.r[8].u64;
	// 832C80F8: 7F9EF850  subf r28, r30, r31
	ctx.r[28].s64 = ctx.r[31].s64 - ctx.r[30].s64;
	// 832C80FC: 7CE5EA14  add r7, r5, r29
	ctx.r[7].u64 = ctx.r[5].u64 + ctx.r[29].u64;
	// 832C8100: 7FFEFA14  add r31, r30, r31
	ctx.r[31].u64 = ctx.r[30].u64 + ctx.r[31].u64;
	// 832C8104: 7F775E70  srawi r23, r27, 0xb
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[23].s64 = (ctx.r[27].s32 >> 11) as i64;
	// 832C8108: 7F68C850  subf r27, r8, r25
	ctx.r[27].s64 = ctx.r[25].s64 - ctx.r[8].s64;
	// 832C810C: 7FC8FA14  add r30, r8, r31
	ctx.r[30].u64 = ctx.r[8].u64 + ctx.r[31].u64;
	// 832C8110: 7CA7D050  subf r5, r7, r26
	ctx.r[5].s64 = ctx.r[26].s64 - ctx.r[7].s64;
	// 832C8114: 7F48F850  subf r26, r8, r31
	ctx.r[26].s64 = ctx.r[31].s64 - ctx.r[8].s64;
	// 832C8118: 7F3DB850  subf r25, r29, r23
	ctx.r[25].s64 = ctx.r[23].s64 - ctx.r[29].s64;
	// 832C811C: 7D1BE214  add r8, r27, r28
	ctx.r[8].u64 = ctx.r[27].u64 + ctx.r[28].u64;
	// 832C8120: 7FBBE050  subf r29, r27, r28
	ctx.r[29].s64 = ctx.r[28].s64 - ctx.r[27].s64;
	// 832C8124: 7F89F214  add r28, r9, r30
	ctx.r[28].u64 = ctx.r[9].u64 + ctx.r[30].u64;
	// 832C8128: 7D29F050  subf r9, r9, r30
	ctx.r[9].s64 = ctx.r[30].s64 - ctx.r[9].s64;
	// 832C812C: 7FC74214  add r30, r7, r8
	ctx.r[30].u64 = ctx.r[7].u64 + ctx.r[8].u64;
	// 832C8130: 938B0000  stw r28, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[28].u32 ) };
	// 832C8134: 7D074050  subf r8, r7, r8
	ctx.r[8].s64 = ctx.r[8].s64 - ctx.r[7].s64;
	// 832C8138: 912B00E0  stw r9, 0xe0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(224 as u32), ctx.r[9].u32 ) };
	// 832C813C: 7FF92A14  add r31, r25, r5
	ctx.r[31].u64 = ctx.r[25].u64 + ctx.r[5].u64;
	// 832C8140: 93CB0020  stw r30, 0x20(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(32 as u32), ctx.r[30].u32 ) };
	// 832C8144: 7CE5EA14  add r7, r5, r29
	ctx.r[7].u64 = ctx.r[5].u64 + ctx.r[29].u64;
	// 832C8148: 910B00C0  stw r8, 0xc0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(192 as u32), ctx.r[8].u32 ) };
	// 832C814C: 7CA5E850  subf r5, r5, r29
	ctx.r[5].s64 = ctx.r[29].s64 - ctx.r[5].s64;
	// 832C8150: 7D1FD050  subf r8, r31, r26
	ctx.r[8].s64 = ctx.r[26].s64 - ctx.r[31].s64;
	// 832C8154: 90EB0040  stw r7, 0x40(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(64 as u32), ctx.r[7].u32 ) };
	// 832C8158: 7D3FD214  add r9, r31, r26
	ctx.r[9].u64 = ctx.r[31].u64 + ctx.r[26].u64;
	// 832C815C: 90AB00A0  stw r5, 0xa0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(160 as u32), ctx.r[5].u32 ) };
	// 832C8160: 910B0060  stw r8, 0x60(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(96 as u32), ctx.r[8].u32 ) };
	// 832C8164: 912B0080  stw r9, 0x80(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(128 as u32), ctx.r[9].u32 ) };
	// 832C8168: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832C816C: 38C60004  addi r6, r6, 4
	ctx.r[6].s64 = ctx.r[6].s64 + 4;
	// 832C8170: 3718FFFF  addic. r24, r24, -1
	ctx.xer.ca = (ctx.r[24].u32 > (!(-1 as u32)));
	ctx.r[24].s64 = ctx.r[24].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[24].s32, 0, &mut ctx.xer);
	// 832C8174: 4181FE38  bgt 0x832c7fac
	if ctx.cr[0].gt {
	pc = 0x832C7FAC; continue 'dispatch;
	}
	// 832C8178: 39430001  addi r10, r3, 1
	ctx.r[10].s64 = ctx.r[3].s64 + 1;
	// 832C817C: 3961FEA8  addi r11, r1, -0x158
	ctx.r[11].s64 = ctx.r[1].s64 + -344;
	// 832C8180: 38A00008  li r5, 8
	ctx.r[5].s64 = 8;
	// 832C8184: 810BFFF4  lwz r8, -0xc(r11)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832C8188: 80EB0004  lwz r7, 4(r11)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C818C: 80CBFFEC  lwz r6, -0x14(r11)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-20 as u32) ) } as u64;
	// 832C8190: 812BFFFC  lwz r9, -4(r11)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-4 as u32) ) } as u64;
	// 832C8194: 83AB0000  lwz r29, 0(r11)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C8198: 7FC73050  subf r30, r7, r6
	ctx.r[30].s64 = ctx.r[6].s64 - ctx.r[7].s64;
	// 832C819C: 83EBFFF0  lwz r31, -0x10(r11)
	ctx.r[31].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-16 as u32) ) } as u64;
	// 832C81A0: 7C684850  subf r3, r8, r9
	ctx.r[3].s64 = ctx.r[9].s64 - ctx.r[8].s64;
	// 832C81A4: 7D284A14  add r9, r8, r9
	ctx.r[9].u64 = ctx.r[8].u64 + ctx.r[9].u64;
	// 832C81A8: 7F9DF850  subf r28, r29, r31
	ctx.r[28].s64 = ctx.r[31].s64 - ctx.r[29].s64;
	// 832C81AC: 7F7E1A14  add r27, r30, r3
	ctx.r[27].u64 = ctx.r[30].u64 + ctx.r[3].u64;
	// 832C81B0: 7D063A14  add r8, r6, r7
	ctx.r[8].u64 = ctx.r[6].u64 + ctx.r[7].u64;
	// 832C81B4: 80EBFFF8  lwz r7, -8(r11)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832C81B8: 1F9C0B50  mulli r28, r28, 0xb50
	ctx.r[28].s64 = ctx.r[28].s64 * 2896;
	// 832C81BC: 80CBFFE8  lwz r6, -0x18(r11)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-24 as u32) ) } as u64;
	// 832C81C0: 1F7B0EC8  mulli r27, r27, 0xec8
	ctx.r[27].s64 = ctx.r[27].s64 * 3784;
	// 832C81C4: 1F23EB18  mulli r25, r3, -0x14e8
	ctx.r[25].s64 = ctx.r[3].s64 * -5352;
	// 832C81C8: 7F494050  subf r26, r9, r8
	ctx.r[26].s64 = ctx.r[8].s64 - ctx.r[9].s64;
	// 832C81CC: 7F985E70  srawi r24, r28, 0xb
	ctx.xer.ca = (ctx.r[28].s32 < 0) && ((ctx.r[28].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[24].s64 = (ctx.r[28].s32 >> 11) as i64;
	// 832C81D0: 7F635E70  srawi r3, r27, 0xb
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[3].s64 = (ctx.r[27].s32 >> 11) as i64;
	// 832C81D4: 7D284A14  add r9, r8, r9
	ctx.r[9].u64 = ctx.r[8].u64 + ctx.r[9].u64;
	// 832C81D8: 7F3C5E70  srawi r28, r25, 0xb
	ctx.xer.ca = (ctx.r[25].s32 < 0) && ((ctx.r[25].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[28].s64 = (ctx.r[25].s32 >> 11) as i64;
	// 832C81DC: 1F7A0B50  mulli r27, r26, 0xb50
	ctx.r[27].s64 = ctx.r[26].s64 * 2896;
	// 832C81E0: 7D1DFA14  add r8, r29, r31
	ctx.r[8].u64 = ctx.r[29].u64 + ctx.r[31].u64;
	// 832C81E4: 1FDE08A9  mulli r30, r30, 0x8a9
	ctx.r[30].s64 = ctx.r[30].s64 * 2217;
	// 832C81E8: 7FA9E050  subf r29, r9, r28
	ctx.r[29].s64 = ctx.r[28].s64 - ctx.r[9].s64;
	// 832C81EC: 7FE63A14  add r31, r6, r7
	ctx.r[31].u64 = ctx.r[6].u64 + ctx.r[7].u64;
	// 832C81F0: 7F873050  subf r28, r7, r6
	ctx.r[28].s64 = ctx.r[6].s64 - ctx.r[7].s64;
	// 832C81F4: 7F7A5E70  srawi r26, r27, 0xb
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[26].s64 = (ctx.r[27].s32 >> 11) as i64;
	// 832C81F8: 7FC65E70  srawi r6, r30, 0xb
	ctx.xer.ca = (ctx.r[30].s32 < 0) && ((ctx.r[30].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[6].s64 = (ctx.r[30].s32 >> 11) as i64;
	// 832C81FC: 7F68C050  subf r27, r8, r24
	ctx.r[27].s64 = ctx.r[24].s64 - ctx.r[8].s64;
	// 832C8200: 7CFD1A14  add r7, r29, r3
	ctx.r[7].u64 = ctx.r[29].u64 + ctx.r[3].u64;
	// 832C8204: 7FC8FA14  add r30, r8, r31
	ctx.r[30].u64 = ctx.r[8].u64 + ctx.r[31].u64;
	// 832C8208: 7F233050  subf r25, r3, r6
	ctx.r[25].s64 = ctx.r[6].s64 - ctx.r[3].s64;
	// 832C820C: 7FBBE214  add r29, r27, r28
	ctx.r[29].u64 = ctx.r[27].u64 + ctx.r[28].u64;
	// 832C8210: 7CC7D050  subf r6, r7, r26
	ctx.r[6].s64 = ctx.r[26].s64 - ctx.r[7].s64;
	// 832C8214: 7C7BE050  subf r3, r27, r28
	ctx.r[3].s64 = ctx.r[28].s64 - ctx.r[27].s64;
	// 832C8218: 7F49F214  add r26, r9, r30
	ctx.r[26].u64 = ctx.r[9].u64 + ctx.r[30].u64;
	// 832C821C: 7F89F050  subf r28, r9, r30
	ctx.r[28].s64 = ctx.r[30].s64 - ctx.r[9].s64;
	// 832C8220: 7FC7EA14  add r30, r7, r29
	ctx.r[30].u64 = ctx.r[7].u64 + ctx.r[29].u64;
	// 832C8224: 7CE7E850  subf r7, r7, r29
	ctx.r[7].s64 = ctx.r[29].s64 - ctx.r[7].s64;
	// 832C8228: 7D08F850  subf r8, r8, r31
	ctx.r[8].s64 = ctx.r[31].s64 - ctx.r[8].s64;
	// 832C822C: 7D393214  add r9, r25, r6
	ctx.r[9].u64 = ctx.r[25].u64 + ctx.r[6].u64;
	// 832C8230: 3BBA007F  addi r29, r26, 0x7f
	ctx.r[29].s64 = ctx.r[26].s64 + 127;
	// 832C8234: 7FE61A14  add r31, r6, r3
	ctx.r[31].u64 = ctx.r[6].u64 + ctx.r[3].u64;
	// 832C8238: 3B9C007F  addi r28, r28, 0x7f
	ctx.r[28].s64 = ctx.r[28].s64 + 127;
	// 832C823C: 3B67007F  addi r27, r7, 0x7f
	ctx.r[27].s64 = ctx.r[7].s64 + 127;
	// 832C8240: 7CC61850  subf r6, r6, r3
	ctx.r[6].s64 = ctx.r[3].s64 - ctx.r[6].s64;
	// 832C8244: 3BDE007F  addi r30, r30, 0x7f
	ctx.r[30].s64 = ctx.r[30].s64 + 127;
	// 832C8248: 7CE94214  add r7, r9, r8
	ctx.r[7].u64 = ctx.r[9].u64 + ctx.r[8].u64;
	// 832C824C: 7FA34670  srawi r3, r29, 8
	ctx.xer.ca = (ctx.r[29].s32 < 0) && ((ctx.r[29].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[3].s64 = (ctx.r[29].s32 >> 8) as i64;
	// 832C8250: 7D294050  subf r9, r9, r8
	ctx.r[9].s64 = ctx.r[8].s64 - ctx.r[9].s64;
	// 832C8254: 7F9D4670  srawi r29, r28, 8
	ctx.xer.ca = (ctx.r[28].s32 < 0) && ((ctx.r[28].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[28].s32 >> 8) as i64;
	// 832C8258: 986AFFFF  stb r3, -1(r10)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[10].u32.wrapping_add(-1 as u32), ctx.r[3].u8 ) };
	// 832C825C: 3BFF007F  addi r31, r31, 0x7f
	ctx.r[31].s64 = ctx.r[31].s64 + 127;
	// 832C8260: 7FC84670  srawi r8, r30, 8
	ctx.xer.ca = (ctx.r[30].s32 < 0) && ((ctx.r[30].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[8].s64 = (ctx.r[30].s32 >> 8) as i64;
	// 832C8264: 9BAA0006  stb r29, 6(r10)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[10].u32.wrapping_add(6 as u32), ctx.r[29].u8 ) };
	// 832C8268: 38C6007F  addi r6, r6, 0x7f
	ctx.r[6].s64 = ctx.r[6].s64 + 127;
	// 832C826C: 7F7E4670  srawi r30, r27, 8
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[30].s64 = (ctx.r[27].s32 >> 8) as i64;
	// 832C8270: 990A0000  stb r8, 0(r10)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), ctx.r[8].u8 ) };
	// 832C8274: 38E7007F  addi r7, r7, 0x7f
	ctx.r[7].s64 = ctx.r[7].s64 + 127;
	// 832C8278: 7FFF4670  srawi r31, r31, 8
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[31].s32 >> 8) as i64;
	// 832C827C: 9BCA0005  stb r30, 5(r10)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[10].u32.wrapping_add(5 as u32), ctx.r[30].u8 ) };
	// 832C8280: 3929007F  addi r9, r9, 0x7f
	ctx.r[9].s64 = ctx.r[9].s64 + 127;
	// 832C8284: 7CC64670  srawi r6, r6, 8
	ctx.xer.ca = (ctx.r[6].s32 < 0) && ((ctx.r[6].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[6].s64 = (ctx.r[6].s32 >> 8) as i64;
	// 832C8288: 9BEA0001  stb r31, 1(r10)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[10].u32.wrapping_add(1 as u32), ctx.r[31].u8 ) };
	// 832C828C: 7CE74670  srawi r7, r7, 8
	ctx.xer.ca = (ctx.r[7].s32 < 0) && ((ctx.r[7].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[7].s64 = (ctx.r[7].s32 >> 8) as i64;
	// 832C8290: 7D294670  srawi r9, r9, 8
	ctx.xer.ca = (ctx.r[9].s32 < 0) && ((ctx.r[9].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[9].s64 = (ctx.r[9].s32 >> 8) as i64;
	// 832C8294: 98CA0004  stb r6, 4(r10)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), ctx.r[6].u8 ) };
	// 832C8298: 98EA0003  stb r7, 3(r10)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[10].u32.wrapping_add(3 as u32), ctx.r[7].u8 ) };
	// 832C829C: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C82A0: 992A0002  stb r9, 2(r10)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[10].u32.wrapping_add(2 as u32), ctx.r[9].u8 ) };
	// 832C82A4: 7D4A2214  add r10, r10, r4
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[4].u64;
	// 832C82A8: 34A5FFFF  addic. r5, r5, -1
	ctx.xer.ca = (ctx.r[5].u32 > (!(-1 as u32)));
	ctx.r[5].s64 = ctx.r[5].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[5].s32, 0, &mut ctx.xer);
	// 832C82AC: 4082FED8  bne 0x832c8184
	if !ctx.cr[0].eq {
	pc = 0x832C8184; continue 'dispatch;
	}
	// 832C82B0: 4B9E1184  b 0x82ca9434
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9434);
	return;
}

pub fn sub_832C82B8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    // ---- function 0x832C82B8 size=888
	// 832C82B8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C82BC: 4B9E1125  bl 0x82ca93e0
	ctx.lr = 0x832C82C0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93E0);
	// 832C82C0: 7CAA2B78  mr r10, r5
	ctx.r[10].u64 = ctx.r[5].u64;
	// 832C82C4: 7D232214  add r9, r3, r4
	ctx.r[9].u64 = ctx.r[3].u64 + ctx.r[4].u64;
	// 832C82C8: 5498083C  slwi r24, r4, 1
	// 832C82CC: 3961FE80  addi r11, r1, -0x180
	ctx.r[11].s64 = ctx.r[1].s64 + -384;
	// 832C82D0: 3AE00008  li r23, 8
	ctx.r[23].s64 = 8;
	// 832C82D4: A0EA0010  lhz r7, 0x10(r10)
	ctx.r[7].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(16 as u32) ) } as u64;
	// 832C82D8: A10A0020  lhz r8, 0x20(r10)
	ctx.r[8].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C82DC: A0AA0030  lhz r5, 0x30(r10)
	ctx.r[5].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C82E0: 7CE40734  extsh r4, r7
	ctx.r[4].s64 = ctx.r[7].s16 as i64;
	// 832C82E4: 7D080734  extsh r8, r8
	ctx.r[8].s64 = ctx.r[8].s16 as i64;
	// 832C82E8: A0EA0040  lhz r7, 0x40(r10)
	ctx.r[7].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C82EC: 7CBD0734  extsh r29, r5
	ctx.r[29].s64 = ctx.r[5].s16 as i64;
	// 832C82F0: A0AA0050  lhz r5, 0x50(r10)
	ctx.r[5].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C82F4: 7C9F4378  or r31, r4, r8
	ctx.r[31].u64 = ctx.r[4].u64 | ctx.r[8].u64;
	// 832C82F8: A3CA0060  lhz r30, 0x60(r10)
	ctx.r[30].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C82FC: 7CE70734  extsh r7, r7
	ctx.r[7].s64 = ctx.r[7].s16 as i64;
	// 832C8300: A36A0070  lhz r27, 0x70(r10)
	ctx.r[27].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(112 as u32) ) } as u64;
	// 832C8304: 7FFFEB78  or r31, r31, r29
	ctx.r[31].u64 = ctx.r[31].u64 | ctx.r[29].u64;
	// 832C8308: 7CBC0734  extsh r28, r5
	ctx.r[28].s64 = ctx.r[5].s16 as i64;
	// 832C830C: 7FFF3B78  or r31, r31, r7
	ctx.r[31].u64 = ctx.r[31].u64 | ctx.r[7].u64;
	// 832C8310: 7FC50734  extsh r5, r30
	ctx.r[5].s64 = ctx.r[30].s16 as i64;
	// 832C8314: 7FFFE378  or r31, r31, r28
	ctx.r[31].u64 = ctx.r[31].u64 | ctx.r[28].u64;
	// 832C8318: 7F7B0734  extsh r27, r27
	ctx.r[27].s64 = ctx.r[27].s16 as i64;
	// 832C831C: 7FFF2B78  or r31, r31, r5
	ctx.r[31].u64 = ctx.r[31].u64 | ctx.r[5].u64;
	// 832C8320: 7FFFDB78  or r31, r31, r27
	ctx.r[31].u64 = ctx.r[31].u64 | ctx.r[27].u64;
	// 832C8324: 7FFF0734  extsh r31, r31
	ctx.r[31].s64 = ctx.r[31].s16 as i64;
	// 832C8328: 2F1F0000  cmpwi cr6, r31, 0
	ctx.cr[6].compare_i32(ctx.r[31].s32, 0, &mut ctx.xer);
	// 832C832C: 409A003C  bne cr6, 0x832c8368
	if !ctx.cr[6].eq {
	pc = 0x832C8368; continue 'dispatch;
	}
	// 832C8330: A10A0000  lhz r8, 0(r10)
	ctx.r[8].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C8334: 394A0002  addi r10, r10, 2
	ctx.r[10].s64 = ctx.r[10].s64 + 2;
	// 832C8338: 80E60000  lwz r7, 0(r6)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C833C: 7D050734  extsh r5, r8
	ctx.r[5].s64 = ctx.r[8].s16 as i64;
	// 832C8340: 7C8539D6  mullw r4, r5, r7
	ctx.r[4].s64 = (ctx.r[5].s32 as i64) * (ctx.r[7].s32 as i64);
	// 832C8344: 7C885E70  srawi r8, r4, 0xb
	ctx.xer.ca = (ctx.r[4].s32 < 0) && ((ctx.r[4].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[8].s64 = (ctx.r[4].s32 >> 11) as i64;
	// 832C8348: 910B0000  stw r8, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[8].u32 ) };
	// 832C834C: 910B0020  stw r8, 0x20(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(32 as u32), ctx.r[8].u32 ) };
	// 832C8350: 910B0040  stw r8, 0x40(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(64 as u32), ctx.r[8].u32 ) };
	// 832C8354: 910B0060  stw r8, 0x60(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(96 as u32), ctx.r[8].u32 ) };
	// 832C8358: 910B00A0  stw r8, 0xa0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(160 as u32), ctx.r[8].u32 ) };
	// 832C835C: 910B00C0  stw r8, 0xc0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(192 as u32), ctx.r[8].u32 ) };
	// 832C8360: 910B00E0  stw r8, 0xe0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(224 as u32), ctx.r[8].u32 ) };
	// 832C8364: 48000128  b 0x832c848c
	pc = 0x832C848C; continue 'dispatch;
	// 832C8368: A3EA0000  lhz r31, 0(r10)
	ctx.r[31].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C836C: 394A0002  addi r10, r10, 2
	ctx.r[10].s64 = ctx.r[10].s64 + 2;
	// 832C8370: 83C60000  lwz r30, 0(r6)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C8374: 7FFF0734  extsh r31, r31
	ctx.r[31].s64 = ctx.r[31].s16 as i64;
	// 832C8378: 83460040  lwz r26, 0x40(r6)
	ctx.r[26].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C837C: 83260080  lwz r25, 0x80(r6)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(128 as u32) ) } as u64;
	// 832C8380: 7FFFF1D6  mullw r31, r31, r30
	ctx.r[31].s64 = (ctx.r[31].s32 as i64) * (ctx.r[30].s32 as i64);
	// 832C8384: 82C600C0  lwz r22, 0xc0(r6)
	ctx.r[22].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(192 as u32) ) } as u64;
	// 832C8388: 82A60020  lwz r21, 0x20(r6)
	ctx.r[21].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C838C: 82860060  lwz r20, 0x60(r6)
	ctx.r[20].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C8390: 826600A0  lwz r19, 0xa0(r6)
	ctx.r[19].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(160 as u32) ) } as u64;
	// 832C8394: 824600E0  lwz r18, 0xe0(r6)
	ctx.r[18].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[6].u32.wrapping_add(224 as u32) ) } as u64;
	// 832C8398: 7D1A41D6  mullw r8, r26, r8
	ctx.r[8].s64 = (ctx.r[26].s32 as i64) * (ctx.r[8].s32 as i64);
	// 832C839C: 7FD939D6  mullw r30, r25, r7
	ctx.r[30].s64 = (ctx.r[25].s32 as i64) * (ctx.r[7].s32 as i64);
	// 832C83A0: 7FFF5E70  srawi r31, r31, 0xb
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[31].s32 >> 11) as i64;
	// 832C83A4: 7CB629D6  mullw r5, r22, r5
	ctx.r[5].s64 = (ctx.r[22].s32 as i64) * (ctx.r[5].s32 as i64);
	// 832C83A8: 7D075E70  srawi r7, r8, 0xb
	ctx.xer.ca = (ctx.r[8].s32 < 0) && ((ctx.r[8].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[7].s64 = (ctx.r[8].s32 >> 11) as i64;
	// 832C83AC: 7FDE5E70  srawi r30, r30, 0xb
	ctx.xer.ca = (ctx.r[30].s32 < 0) && ((ctx.r[30].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[30].s64 = (ctx.r[30].s32 >> 11) as i64;
	// 832C83B0: 7CA55E70  srawi r5, r5, 0xb
	ctx.xer.ca = (ctx.r[5].s32 < 0) && ((ctx.r[5].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[5].s64 = (ctx.r[5].s32 >> 11) as i64;
	// 832C83B4: 7C9521D6  mullw r4, r21, r4
	ctx.r[4].s64 = (ctx.r[21].s32 as i64) * (ctx.r[4].s32 as i64);
	// 832C83B8: 7D053850  subf r8, r5, r7
	ctx.r[8].s64 = ctx.r[7].s64 - ctx.r[5].s64;
	// 832C83BC: 7FB4E9D6  mullw r29, r20, r29
	ctx.r[29].s64 = (ctx.r[20].s32 as i64) * (ctx.r[29].s32 as i64);
	// 832C83C0: 1D080B50  mulli r8, r8, 0xb50
	ctx.r[8].s64 = ctx.r[8].s64 * 2896;
	// 832C83C4: 7D195E70  srawi r25, r8, 0xb
	ctx.xer.ca = (ctx.r[8].s32 < 0) && ((ctx.r[8].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[25].s64 = (ctx.r[8].s32 >> 11) as i64;
	// 832C83C8: 7F93E1D6  mullw r28, r19, r28
	ctx.r[28].s64 = (ctx.r[19].s32 as i64) * (ctx.r[28].s32 as i64);
	// 832C83CC: 7C885E70  srawi r8, r4, 0xb
	ctx.xer.ca = (ctx.r[4].s32 < 0) && ((ctx.r[4].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[8].s64 = (ctx.r[4].s32 >> 11) as i64;
	// 832C83D0: 7F72D9D6  mullw r27, r18, r27
	ctx.r[27].s64 = (ctx.r[18].s32 as i64) * (ctx.r[27].s32 as i64);
	// 832C83D4: 7FA45E70  srawi r4, r29, 0xb
	ctx.xer.ca = (ctx.r[29].s32 < 0) && ((ctx.r[29].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[4].s64 = (ctx.r[29].s32 >> 11) as i64;
	// 832C83D8: 7F9D5E70  srawi r29, r28, 0xb
	ctx.xer.ca = (ctx.r[28].s32 < 0) && ((ctx.r[28].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[28].s32 >> 11) as i64;
	// 832C83DC: 7F7C5E70  srawi r28, r27, 0xb
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[28].s64 = (ctx.r[27].s32 >> 11) as i64;
	// 832C83E0: 7F64E850  subf r27, r4, r29
	ctx.r[27].s64 = ctx.r[29].s64 - ctx.r[4].s64;
	// 832C83E4: 7F5C4050  subf r26, r28, r8
	ctx.r[26].s64 = ctx.r[8].s64 - ctx.r[28].s64;
	// 832C83E8: 7C9D2214  add r4, r29, r4
	ctx.r[4].u64 = ctx.r[29].u64 + ctx.r[4].u64;
	// 832C83EC: 7FBADA14  add r29, r26, r27
	ctx.r[29].u64 = ctx.r[26].u64 + ctx.r[27].u64;
	// 832C83F0: 7D1C4214  add r8, r28, r8
	ctx.r[8].u64 = ctx.r[28].u64 + ctx.r[8].u64;
	// 832C83F4: 1FBD0EC8  mulli r29, r29, 0xec8
	ctx.r[29].s64 = ctx.r[29].s64 * 3784;
	// 832C83F8: 1F9BEB18  mulli r28, r27, -0x14e8
	ctx.r[28].s64 = ctx.r[27].s64 * -5352;
	// 832C83FC: 7F644050  subf r27, r4, r8
	ctx.r[27].s64 = ctx.r[8].s64 - ctx.r[4].s64;
	// 832C8400: 7D082214  add r8, r8, r4
	ctx.r[8].u64 = ctx.r[8].u64 + ctx.r[4].u64;
	// 832C8404: 7FBD5E70  srawi r29, r29, 0xb
	ctx.xer.ca = (ctx.r[29].s32 < 0) && ((ctx.r[29].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[29].s32 >> 11) as i64;
	// 832C8408: 7F845E70  srawi r4, r28, 0xb
	ctx.xer.ca = (ctx.r[28].s32 < 0) && ((ctx.r[28].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[4].s64 = (ctx.r[28].s32 >> 11) as i64;
	// 832C840C: 1F9B0B50  mulli r28, r27, 0xb50
	ctx.r[28].s64 = ctx.r[27].s64 * 2896;
	// 832C8410: 7C882050  subf r4, r8, r4
	ctx.r[4].s64 = ctx.r[4].s64 - ctx.r[8].s64;
	// 832C8414: 1F7A08A9  mulli r27, r26, 0x8a9
	ctx.r[27].s64 = ctx.r[26].s64 * 2217;
	// 832C8418: 7F9A5E70  srawi r26, r28, 0xb
	ctx.xer.ca = (ctx.r[28].s32 < 0) && ((ctx.r[28].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[26].s64 = (ctx.r[28].s32 >> 11) as i64;
	// 832C841C: 7CE53A14  add r7, r5, r7
	ctx.r[7].u64 = ctx.r[5].u64 + ctx.r[7].u64;
	// 832C8420: 7F9EF850  subf r28, r30, r31
	ctx.r[28].s64 = ctx.r[31].s64 - ctx.r[30].s64;
	// 832C8424: 7CA4EA14  add r5, r4, r29
	ctx.r[5].u64 = ctx.r[4].u64 + ctx.r[29].u64;
	// 832C8428: 7FFEFA14  add r31, r30, r31
	ctx.r[31].u64 = ctx.r[30].u64 + ctx.r[31].u64;
	// 832C842C: 7F765E70  srawi r22, r27, 0xb
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[22].s64 = (ctx.r[27].s32 >> 11) as i64;
	// 832C8430: 7F67C850  subf r27, r7, r25
	ctx.r[27].s64 = ctx.r[25].s64 - ctx.r[7].s64;
	// 832C8434: 7FC7FA14  add r30, r7, r31
	ctx.r[30].u64 = ctx.r[7].u64 + ctx.r[31].u64;
	// 832C8438: 7C85D050  subf r4, r5, r26
	ctx.r[4].s64 = ctx.r[26].s64 - ctx.r[5].s64;
	// 832C843C: 7F47F850  subf r26, r7, r31
	ctx.r[26].s64 = ctx.r[31].s64 - ctx.r[7].s64;
	// 832C8440: 7F3DB050  subf r25, r29, r22
	ctx.r[25].s64 = ctx.r[22].s64 - ctx.r[29].s64;
	// 832C8444: 7CFBE214  add r7, r27, r28
	ctx.r[7].u64 = ctx.r[27].u64 + ctx.r[28].u64;
	// 832C8448: 7FBBE050  subf r29, r27, r28
	ctx.r[29].s64 = ctx.r[28].s64 - ctx.r[27].s64;
	// 832C844C: 7F88F214  add r28, r8, r30
	ctx.r[28].u64 = ctx.r[8].u64 + ctx.r[30].u64;
	// 832C8450: 7D08F050  subf r8, r8, r30
	ctx.r[8].s64 = ctx.r[30].s64 - ctx.r[8].s64;
	// 832C8454: 7FC53A14  add r30, r5, r7
	ctx.r[30].u64 = ctx.r[5].u64 + ctx.r[7].u64;
	// 832C8458: 938B0000  stw r28, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[28].u32 ) };
	// 832C845C: 7CE53850  subf r7, r5, r7
	ctx.r[7].s64 = ctx.r[7].s64 - ctx.r[5].s64;
	// 832C8460: 910B00E0  stw r8, 0xe0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(224 as u32), ctx.r[8].u32 ) };
	// 832C8464: 7FF92214  add r31, r25, r4
	ctx.r[31].u64 = ctx.r[25].u64 + ctx.r[4].u64;
	// 832C8468: 93CB0020  stw r30, 0x20(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(32 as u32), ctx.r[30].u32 ) };
	// 832C846C: 7CA4EA14  add r5, r4, r29
	ctx.r[5].u64 = ctx.r[4].u64 + ctx.r[29].u64;
	// 832C8470: 90EB00C0  stw r7, 0xc0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(192 as u32), ctx.r[7].u32 ) };
	// 832C8474: 7C84E850  subf r4, r4, r29
	ctx.r[4].s64 = ctx.r[29].s64 - ctx.r[4].s64;
	// 832C8478: 7CFFD050  subf r7, r31, r26
	ctx.r[7].s64 = ctx.r[26].s64 - ctx.r[31].s64;
	// 832C847C: 90AB0040  stw r5, 0x40(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(64 as u32), ctx.r[5].u32 ) };
	// 832C8480: 7D1FD214  add r8, r31, r26
	ctx.r[8].u64 = ctx.r[31].u64 + ctx.r[26].u64;
	// 832C8484: 908B00A0  stw r4, 0xa0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(160 as u32), ctx.r[4].u32 ) };
	// 832C8488: 90EB0060  stw r7, 0x60(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(96 as u32), ctx.r[7].u32 ) };
	// 832C848C: 910B0080  stw r8, 0x80(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(128 as u32), ctx.r[8].u32 ) };
	// 832C8490: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832C8494: 38C60004  addi r6, r6, 4
	ctx.r[6].s64 = ctx.r[6].s64 + 4;
	// 832C8498: 36F7FFFF  addic. r23, r23, -1
	ctx.xer.ca = (ctx.r[23].u32 > (!(-1 as u32)));
	ctx.r[23].s64 = ctx.r[23].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[23].s32, 0, &mut ctx.xer);
	// 832C849C: 4181FE38  bgt 0x832c82d4
	if ctx.cr[0].gt {
	pc = 0x832C82D4; continue 'dispatch;
	}
	// 832C84A0: 3961FE98  addi r11, r1, -0x168
	ctx.r[11].s64 = ctx.r[1].s64 + -360;
	// 832C84A4: 39430008  addi r10, r3, 8
	ctx.r[10].s64 = ctx.r[3].s64 + 8;
	// 832C84A8: 38800008  li r4, 8
	ctx.r[4].s64 = 8;
	// 832C84AC: 80EBFFF4  lwz r7, -0xc(r11)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832C84B0: 810BFFFC  lwz r8, -4(r11)
	ctx.r[8].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-4 as u32) ) } as u64;
	// 832C84B4: 80CB0004  lwz r6, 4(r11)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C84B8: 80ABFFEC  lwz r5, -0x14(r11)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-20 as u32) ) } as u64;
	// 832C84BC: 7C674050  subf r3, r7, r8
	ctx.r[3].s64 = ctx.r[8].s64 - ctx.r[7].s64;
	// 832C84C0: 83CB0000  lwz r30, 0(r11)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C84C4: 7D074214  add r8, r7, r8
	ctx.r[8].u64 = ctx.r[7].u64 + ctx.r[8].u64;
	// 832C84C8: 83ABFFF0  lwz r29, -0x10(r11)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-16 as u32) ) } as u64;
	// 832C84CC: 7FE62850  subf r31, r6, r5
	ctx.r[31].s64 = ctx.r[5].s64 - ctx.r[6].s64;
	// 832C84D0: 838BFFE8  lwz r28, -0x18(r11)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-24 as u32) ) } as u64;
	// 832C84D4: 7CFEE850  subf r7, r30, r29
	ctx.r[7].s64 = ctx.r[29].s64 - ctx.r[30].s64;
	// 832C84D8: 7F7F1A14  add r27, r31, r3
	ctx.r[27].u64 = ctx.r[31].u64 + ctx.r[3].u64;
	// 832C84DC: 1F470B50  mulli r26, r7, 0xb50
	ctx.r[26].s64 = ctx.r[7].s64 * 2896;
	// 832C84E0: 7CE53214  add r7, r5, r6
	ctx.r[7].u64 = ctx.r[5].u64 + ctx.r[6].u64;
	// 832C84E4: 80ABFFF8  lwz r5, -8(r11)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832C84E8: 1CDB0EC8  mulli r6, r27, 0xec8
	ctx.r[6].s64 = ctx.r[27].s64 * 3784;
	// 832C84EC: 1F63EB18  mulli r27, r3, -0x14e8
	ctx.r[27].s64 = ctx.r[3].s64 * -5352;
	// 832C84F0: 7F5A5E70  srawi r26, r26, 0xb
	ctx.xer.ca = (ctx.r[26].s32 < 0) && ((ctx.r[26].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[26].s64 = (ctx.r[26].s32 >> 11) as i64;
	// 832C84F4: 7F283850  subf r25, r8, r7
	ctx.r[25].s64 = ctx.r[7].s64 - ctx.r[8].s64;
	// 832C84F8: 7CC35E70  srawi r3, r6, 0xb
	ctx.xer.ca = (ctx.r[6].s32 < 0) && ((ctx.r[6].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[3].s64 = (ctx.r[6].s32 >> 11) as i64;
	// 832C84FC: 7D074214  add r8, r7, r8
	ctx.r[8].u64 = ctx.r[7].u64 + ctx.r[8].u64;
	// 832C8500: 7F665E70  srawi r6, r27, 0xb
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[6].s64 = (ctx.r[27].s32 >> 11) as i64;
	// 832C8504: 1F790B50  mulli r27, r25, 0xb50
	ctx.r[27].s64 = ctx.r[25].s64 * 2896;
	// 832C8508: 7CFDF214  add r7, r29, r30
	ctx.r[7].u64 = ctx.r[29].u64 + ctx.r[30].u64;
	// 832C850C: 7CC83050  subf r6, r8, r6
	ctx.r[6].s64 = ctx.r[6].s64 - ctx.r[8].s64;
	// 832C8510: 1FDF08A9  mulli r30, r31, 0x8a9
	ctx.r[30].s64 = ctx.r[31].s64 * 2217;
	// 832C8514: 7F7D5E70  srawi r29, r27, 0xb
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[27].s32 >> 11) as i64;
	// 832C8518: 7CC61A14  add r6, r6, r3
	ctx.r[6].u64 = ctx.r[6].u64 + ctx.r[3].u64;
	// 832C851C: 7F67D050  subf r27, r7, r26
	ctx.r[27].s64 = ctx.r[26].s64 - ctx.r[7].s64;
	// 832C8520: 7FFC2A14  add r31, r28, r5
	ctx.r[31].u64 = ctx.r[28].u64 + ctx.r[5].u64;
	// 832C8524: 7FDA5E70  srawi r26, r30, 0xb
	ctx.xer.ca = (ctx.r[30].s32 < 0) && ((ctx.r[30].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[26].s64 = (ctx.r[30].s32 >> 11) as i64;
	// 832C8528: 7F85E050  subf r28, r5, r28
	ctx.r[28].s64 = ctx.r[28].s64 - ctx.r[5].s64;
	// 832C852C: 7CA6E850  subf r5, r6, r29
	ctx.r[5].s64 = ctx.r[29].s64 - ctx.r[6].s64;
	// 832C8530: 7FC7FA14  add r30, r7, r31
	ctx.r[30].u64 = ctx.r[7].u64 + ctx.r[31].u64;
	// 832C8534: 7C63D050  subf r3, r3, r26
	ctx.r[3].s64 = ctx.r[26].s64 - ctx.r[3].s64;
	// 832C8538: 7FBBE214  add r29, r27, r28
	ctx.r[29].u64 = ctx.r[27].u64 + ctx.r[28].u64;
	// 832C853C: 7F48F214  add r26, r8, r30
	ctx.r[26].u64 = ctx.r[8].u64 + ctx.r[30].u64;
	// 832C8540: 7FE7F850  subf r31, r7, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[7].s64;
	// 832C8544: 7F26EA14  add r25, r6, r29
	ctx.r[25].u64 = ctx.r[6].u64 + ctx.r[29].u64;
	// 832C8548: 7CE32A14  add r7, r3, r5
	ctx.r[7].u64 = ctx.r[3].u64 + ctx.r[5].u64;
	// 832C854C: 7CC6E850  subf r6, r6, r29
	ctx.r[6].s64 = ctx.r[29].s64 - ctx.r[6].s64;
	// 832C8550: 7C7BE050  subf r3, r27, r28
	ctx.r[3].s64 = ctx.r[28].s64 - ctx.r[27].s64;
	// 832C8554: 7FC8F050  subf r30, r8, r30
	ctx.r[30].s64 = ctx.r[30].s64 - ctx.r[8].s64;
	// 832C8558: 3B9A007F  addi r28, r26, 0x7f
	ctx.r[28].s64 = ctx.r[26].s64 + 127;
	// 832C855C: 7FA7FA14  add r29, r7, r31
	ctx.r[29].u64 = ctx.r[7].u64 + ctx.r[31].u64;
	// 832C8560: 3B79007F  addi r27, r25, 0x7f
	ctx.r[27].s64 = ctx.r[25].s64 + 127;
	// 832C8564: 7D051850  subf r8, r5, r3
	ctx.r[8].s64 = ctx.r[3].s64 - ctx.r[5].s64;
	// 832C8568: 38C6007F  addi r6, r6, 0x7f
	ctx.r[6].s64 = ctx.r[6].s64 + 127;
	// 832C856C: 3BDE007F  addi r30, r30, 0x7f
	ctx.r[30].s64 = ctx.r[30].s64 + 127;
	// 832C8570: 7F9C4670  srawi r28, r28, 8
	ctx.xer.ca = (ctx.r[28].s32 < 0) && ((ctx.r[28].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[28].s64 = (ctx.r[28].s32 >> 8) as i64;
	// 832C8574: 3BBD007F  addi r29, r29, 0x7f
	ctx.r[29].s64 = ctx.r[29].s64 + 127;
	// 832C8578: 7F7B4670  srawi r27, r27, 8
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[27].s64 = (ctx.r[27].s32 >> 8) as i64;
	// 832C857C: 7CDA4670  srawi r26, r6, 8
	ctx.xer.ca = (ctx.r[6].s32 < 0) && ((ctx.r[6].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[26].s64 = (ctx.r[6].s32 >> 8) as i64;
	// 832C8580: 3908007F  addi r8, r8, 0x7f
	ctx.r[8].s64 = ctx.r[8].s64 + 127;
	// 832C8584: 7FDE4670  srawi r30, r30, 8
	ctx.xer.ca = (ctx.r[30].s32 < 0) && ((ctx.r[30].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[30].s64 = (ctx.r[30].s32 >> 8) as i64;
	// 832C8588: 7CC51A14  add r6, r5, r3
	ctx.r[6].u64 = ctx.r[5].u64 + ctx.r[3].u64;
	// 832C858C: 7FBD4670  srawi r29, r29, 8
	ctx.xer.ca = (ctx.r[29].s32 < 0) && ((ctx.r[29].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[29].s32 >> 8) as i64;
	// 832C8590: 7D054670  srawi r5, r8, 8
	ctx.xer.ca = (ctx.r[8].s32 < 0) && ((ctx.r[8].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[5].s64 = (ctx.r[8].s32 >> 8) as i64;
	// 832C8594: 7D07F850  subf r8, r7, r31
	ctx.r[8].s64 = ctx.r[31].s64 - ctx.r[7].s64;
	// 832C8598: 3866007F  addi r3, r6, 0x7f
	ctx.r[3].s64 = ctx.r[6].s64 + 127;
	// 832C859C: 3908007F  addi r8, r8, 0x7f
	ctx.r[8].s64 = ctx.r[8].s64 + 127;
	// 832C85A0: 5766063E  clrlwi r6, r27, 0x18
	ctx.r[6].u64 = ctx.r[27].u32 as u64 & 0x000000FFu64;
	// 832C85A4: 5787821E  rlwinm r7, r28, 0x10, 8, 0xf
	ctx.r[7].u64 = ctx.r[28].u32 as u64 & 0x0000FFFFu64;
	// 832C85A8: 7C634670  srawi r3, r3, 8
	ctx.xer.ca = (ctx.r[3].s32 < 0) && ((ctx.r[3].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[3].s64 = (ctx.r[3].s32 >> 8) as i64;
	// 832C85AC: 575F821E  rlwinm r31, r26, 0x10, 8, 0xf
	ctx.r[31].u64 = ctx.r[26].u32 as u64 & 0x0000FFFFu64;
	// 832C85B0: 57DE063E  clrlwi r30, r30, 0x18
	ctx.r[30].u64 = ctx.r[30].u32 as u64 & 0x000000FFu64;
	// 832C85B4: 57BD821E  rlwinm r29, r29, 0x10, 8, 0xf
	ctx.r[29].u64 = ctx.r[29].u32 as u64 & 0x0000FFFFu64;
	// 832C85B8: 7D084670  srawi r8, r8, 8
	ctx.xer.ca = (ctx.r[8].s32 < 0) && ((ctx.r[8].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[8].s64 = (ctx.r[8].s32 >> 8) as i64;
	// 832C85BC: 54A5063E  clrlwi r5, r5, 0x18
	ctx.r[5].u64 = ctx.r[5].u32 as u64 & 0x000000FFu64;
	// 832C85C0: 7CE73378  or r7, r7, r6
	ctx.r[7].u64 = ctx.r[7].u64 | ctx.r[6].u64;
	// 832C85C4: 7FE6F378  or r6, r31, r30
	ctx.r[6].u64 = ctx.r[31].u64 | ctx.r[30].u64;
	// 832C85C8: 7FA52B78  or r5, r29, r5
	ctx.r[5].u64 = ctx.r[29].u64 | ctx.r[5].u64;
	// 832C85CC: 5508063E  clrlwi r8, r8, 0x18
	ctx.r[8].u64 = ctx.r[8].u32 as u64 & 0x000000FFu64;
	// 832C85D0: 5463821E  rlwinm r3, r3, 0x10, 8, 0xf
	ctx.r[3].u64 = ctx.r[3].u32 as u64 & 0x0000FFFFu64;
	// 832C85D4: 54FF402E  slwi r31, r7, 8
	// 832C85D8: 54DE402E  slwi r30, r6, 8
	// 832C85DC: 54BD402E  slwi r29, r5, 8
	// 832C85E0: 7C634378  or r3, r3, r8
	ctx.r[3].u64 = ctx.r[3].u64 | ctx.r[8].u64;
	// 832C85E4: 7FE83B78  or r8, r31, r7
	ctx.r[8].u64 = ctx.r[31].u64 | ctx.r[7].u64;
	// 832C85E8: 7FC73378  or r7, r30, r6
	ctx.r[7].u64 = ctx.r[30].u64 | ctx.r[6].u64;
	// 832C85EC: 7FA62B78  or r6, r29, r5
	ctx.r[6].u64 = ctx.r[29].u64 | ctx.r[5].u64;
	// 832C85F0: 5465402E  slwi r5, r3, 8
	// 832C85F4: 910AFFF8  stw r8, -8(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(-8 as u32), ctx.r[8].u32 ) };
	// 832C85F8: 90CA0000  stw r6, 0(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(0 as u32), ctx.r[6].u32 ) };
	// 832C85FC: 3484FFFF  addic. r4, r4, -1
	ctx.xer.ca = (ctx.r[4].u32 > (!(-1 as u32)));
	ctx.r[4].s64 = ctx.r[4].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[4].s32, 0, &mut ctx.xer);
	// 832C8600: 7CA31B78  or r3, r5, r3
	ctx.r[3].u64 = ctx.r[5].u64 | ctx.r[3].u64;
	// 832C8604: 90EA0004  stw r7, 4(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(4 as u32), ctx.r[7].u32 ) };
	// 832C8608: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C860C: 906AFFFC  stw r3, -4(r10)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[10].u32.wrapping_add(-4 as u32), ctx.r[3].u32 ) };
	// 832C8610: 7D4AC214  add r10, r10, r24
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[24].u64;
	// 832C8614: 91090000  stw r8, 0(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.r[8].u32 ) };
	// 832C8618: 90690004  stw r3, 4(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), ctx.r[3].u32 ) };
	// 832C861C: 90C90008  stw r6, 8(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(8 as u32), ctx.r[6].u32 ) };
	// 832C8620: 90E9000C  stw r7, 0xc(r9)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[9].u32.wrapping_add(12 as u32), ctx.r[7].u32 ) };
	// 832C8624: 7D384A14  add r9, r24, r9
	ctx.r[9].u64 = ctx.r[24].u64 + ctx.r[9].u64;
	// 832C8628: 4082FE84  bne 0x832c84ac
	if !ctx.cr[0].eq {
	pc = 0x832C84AC; continue 'dispatch;
	}
	// 832C862C: 4B9E0E04  b 0x82ca9430
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA9430);
	return;
}

pub fn sub_832C8630(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    // ---- function 0x832C8630 size=888
	// 832C8630: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C8634: 4B9E0DA9  bl 0x82ca93dc
	ctx.lr = 0x832C8638;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93DC);
	// 832C8638: 3D608216  lis r11, -0x7dea
	ctx.r[11].s64 = -2112487424;
	// 832C863C: 54CA402E  slwi r10, r6, 8
	// 832C8640: 396B7040  addi r11, r11, 0x7040
	ctx.r[11].s64 = ctx.r[11].s64 + 28736;
	// 832C8644: 7CA92B78  mr r9, r5
	ctx.r[9].u64 = ctx.r[5].u64;
	// 832C8648: 7D4A5A14  add r10, r10, r11
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[11].u64;
	// 832C864C: 3961FE80  addi r11, r1, -0x180
	ctx.r[11].s64 = ctx.r[1].s64 + -384;
	// 832C8650: 3AC00008  li r22, 8
	ctx.r[22].s64 = 8;
	// 832C8654: A0A90010  lhz r5, 0x10(r9)
	ctx.r[5].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[9].u32.wrapping_add(16 as u32) ) } as u64;
	// 832C8658: A0C90020  lhz r6, 0x20(r9)
	ctx.r[6].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[9].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C865C: A3E90030  lhz r31, 0x30(r9)
	ctx.r[31].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[9].u32.wrapping_add(48 as u32) ) } as u64;
	// 832C8660: 7CBE0734  extsh r30, r5
	ctx.r[30].s64 = ctx.r[5].s16 as i64;
	// 832C8664: 7CC60734  extsh r6, r6
	ctx.r[6].s64 = ctx.r[6].s16 as i64;
	// 832C8668: A0A90040  lhz r5, 0x40(r9)
	ctx.r[5].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[9].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C866C: 7FFB0734  extsh r27, r31
	ctx.r[27].s64 = ctx.r[31].s16 as i64;
	// 832C8670: A3E90050  lhz r31, 0x50(r9)
	ctx.r[31].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[9].u32.wrapping_add(80 as u32) ) } as u64;
	// 832C8674: 7FDD3378  or r29, r30, r6
	ctx.r[29].u64 = ctx.r[30].u64 | ctx.r[6].u64;
	// 832C8678: A3890060  lhz r28, 0x60(r9)
	ctx.r[28].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[9].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C867C: 7CA50734  extsh r5, r5
	ctx.r[5].s64 = ctx.r[5].s16 as i64;
	// 832C8680: A3290070  lhz r25, 0x70(r9)
	ctx.r[25].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[9].u32.wrapping_add(112 as u32) ) } as u64;
	// 832C8684: 7FBDDB78  or r29, r29, r27
	ctx.r[29].u64 = ctx.r[29].u64 | ctx.r[27].u64;
	// 832C8688: 7FFA0734  extsh r26, r31
	ctx.r[26].s64 = ctx.r[31].s16 as i64;
	// 832C868C: 7FBD2B78  or r29, r29, r5
	ctx.r[29].u64 = ctx.r[29].u64 | ctx.r[5].u64;
	// 832C8690: 7F9F0734  extsh r31, r28
	ctx.r[31].s64 = ctx.r[28].s16 as i64;
	// 832C8694: 7FBDD378  or r29, r29, r26
	ctx.r[29].u64 = ctx.r[29].u64 | ctx.r[26].u64;
	// 832C8698: 7F390734  extsh r25, r25
	ctx.r[25].s64 = ctx.r[25].s16 as i64;
	// 832C869C: 7FBDFB78  or r29, r29, r31
	ctx.r[29].u64 = ctx.r[29].u64 | ctx.r[31].u64;
	// 832C86A0: 7FBDCB78  or r29, r29, r25
	ctx.r[29].u64 = ctx.r[29].u64 | ctx.r[25].u64;
	// 832C86A4: 7FBD0734  extsh r29, r29
	ctx.r[29].s64 = ctx.r[29].s16 as i64;
	// 832C86A8: 2F1D0000  cmpwi cr6, r29, 0
	ctx.cr[6].compare_i32(ctx.r[29].s32, 0, &mut ctx.xer);
	// 832C86AC: 409A003C  bne cr6, 0x832c86e8
	if !ctx.cr[6].eq {
	pc = 0x832C86E8; continue 'dispatch;
	}
	// 832C86B0: A0C90000  lhz r6, 0(r9)
	ctx.r[6].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C86B4: 39290002  addi r9, r9, 2
	ctx.r[9].s64 = ctx.r[9].s64 + 2;
	// 832C86B8: 80AA0000  lwz r5, 0(r10)
	ctx.r[5].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C86BC: 7CC60734  extsh r6, r6
	ctx.r[6].s64 = ctx.r[6].s16 as i64;
	// 832C86C0: 7CA629D6  mullw r5, r6, r5
	ctx.r[5].s64 = (ctx.r[6].s32 as i64) * (ctx.r[5].s32 as i64);
	// 832C86C4: 7CA65E70  srawi r6, r5, 0xb
	ctx.xer.ca = (ctx.r[5].s32 < 0) && ((ctx.r[5].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[6].s64 = (ctx.r[5].s32 >> 11) as i64;
	// 832C86C8: 90CB0000  stw r6, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[6].u32 ) };
	// 832C86CC: 90CB0020  stw r6, 0x20(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(32 as u32), ctx.r[6].u32 ) };
	// 832C86D0: 90CB0040  stw r6, 0x40(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(64 as u32), ctx.r[6].u32 ) };
	// 832C86D4: 90CB0080  stw r6, 0x80(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(128 as u32), ctx.r[6].u32 ) };
	// 832C86D8: 90CB00A0  stw r6, 0xa0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(160 as u32), ctx.r[6].u32 ) };
	// 832C86DC: 90CB00C0  stw r6, 0xc0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(192 as u32), ctx.r[6].u32 ) };
	// 832C86E0: 90CB00E0  stw r6, 0xe0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(224 as u32), ctx.r[6].u32 ) };
	// 832C86E4: 48000128  b 0x832c880c
	pc = 0x832C880C; continue 'dispatch;
	// 832C86E8: A3A90000  lhz r29, 0(r9)
	ctx.r[29].u64 = unsafe { crate::rt::load_u16( base as *const u8, ctx.r[9].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C86EC: 39290002  addi r9, r9, 2
	ctx.r[9].s64 = ctx.r[9].s64 + 2;
	// 832C86F0: 838A0000  lwz r28, 0(r10)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C86F4: 7FBD0734  extsh r29, r29
	ctx.r[29].s64 = ctx.r[29].s16 as i64;
	// 832C86F8: 830A0040  lwz r24, 0x40(r10)
	ctx.r[24].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(64 as u32) ) } as u64;
	// 832C86FC: 82EA0080  lwz r23, 0x80(r10)
	ctx.r[23].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(128 as u32) ) } as u64;
	// 832C8700: 7FBDE1D6  mullw r29, r29, r28
	ctx.r[29].s64 = (ctx.r[29].s32 as i64) * (ctx.r[28].s32 as i64);
	// 832C8704: 82AA00C0  lwz r21, 0xc0(r10)
	ctx.r[21].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(192 as u32) ) } as u64;
	// 832C8708: 828A0020  lwz r20, 0x20(r10)
	ctx.r[20].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(32 as u32) ) } as u64;
	// 832C870C: 826A0060  lwz r19, 0x60(r10)
	ctx.r[19].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(96 as u32) ) } as u64;
	// 832C8710: 824A00A0  lwz r18, 0xa0(r10)
	ctx.r[18].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(160 as u32) ) } as u64;
	// 832C8714: 822A00E0  lwz r17, 0xe0(r10)
	ctx.r[17].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[10].u32.wrapping_add(224 as u32) ) } as u64;
	// 832C8718: 7CD831D6  mullw r6, r24, r6
	ctx.r[6].s64 = (ctx.r[24].s32 as i64) * (ctx.r[6].s32 as i64);
	// 832C871C: 7F9729D6  mullw r28, r23, r5
	ctx.r[28].s64 = (ctx.r[23].s32 as i64) * (ctx.r[5].s32 as i64);
	// 832C8720: 7FBD5E70  srawi r29, r29, 0xb
	ctx.xer.ca = (ctx.r[29].s32 < 0) && ((ctx.r[29].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[29].s32 >> 11) as i64;
	// 832C8724: 7FF5F9D6  mullw r31, r21, r31
	ctx.r[31].s64 = (ctx.r[21].s32 as i64) * (ctx.r[31].s32 as i64);
	// 832C8728: 7CC55E70  srawi r5, r6, 0xb
	ctx.xer.ca = (ctx.r[6].s32 < 0) && ((ctx.r[6].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[5].s64 = (ctx.r[6].s32 >> 11) as i64;
	// 832C872C: 7F9C5E70  srawi r28, r28, 0xb
	ctx.xer.ca = (ctx.r[28].s32 < 0) && ((ctx.r[28].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[28].s64 = (ctx.r[28].s32 >> 11) as i64;
	// 832C8730: 7FFF5E70  srawi r31, r31, 0xb
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[31].s32 >> 11) as i64;
	// 832C8734: 7CD4F1D6  mullw r6, r20, r30
	ctx.r[6].s64 = (ctx.r[20].s32 as i64) * (ctx.r[30].s32 as i64);
	// 832C8738: 7FDF2850  subf r30, r31, r5
	ctx.r[30].s64 = ctx.r[5].s64 - ctx.r[31].s64;
	// 832C873C: 7F73D9D6  mullw r27, r19, r27
	ctx.r[27].s64 = (ctx.r[19].s32 as i64) * (ctx.r[27].s32 as i64);
	// 832C8740: 1FDE0B50  mulli r30, r30, 0xb50
	ctx.r[30].s64 = ctx.r[30].s64 * 2896;
	// 832C8744: 7FD75E70  srawi r23, r30, 0xb
	ctx.xer.ca = (ctx.r[30].s32 < 0) && ((ctx.r[30].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[23].s64 = (ctx.r[30].s32 >> 11) as i64;
	// 832C8748: 7F52D1D6  mullw r26, r18, r26
	ctx.r[26].s64 = (ctx.r[18].s32 as i64) * (ctx.r[26].s32 as i64);
	// 832C874C: 7CC65E70  srawi r6, r6, 0xb
	ctx.xer.ca = (ctx.r[6].s32 < 0) && ((ctx.r[6].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[6].s64 = (ctx.r[6].s32 >> 11) as i64;
	// 832C8750: 7F31C9D6  mullw r25, r17, r25
	ctx.r[25].s64 = (ctx.r[17].s32 as i64) * (ctx.r[25].s32 as i64);
	// 832C8754: 7F7E5E70  srawi r30, r27, 0xb
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[30].s64 = (ctx.r[27].s32 >> 11) as i64;
	// 832C8758: 7F5B5E70  srawi r27, r26, 0xb
	ctx.xer.ca = (ctx.r[26].s32 < 0) && ((ctx.r[26].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[27].s64 = (ctx.r[26].s32 >> 11) as i64;
	// 832C875C: 7F3A5E70  srawi r26, r25, 0xb
	ctx.xer.ca = (ctx.r[25].s32 < 0) && ((ctx.r[25].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[26].s64 = (ctx.r[25].s32 >> 11) as i64;
	// 832C8760: 7F3ED850  subf r25, r30, r27
	ctx.r[25].s64 = ctx.r[27].s64 - ctx.r[30].s64;
	// 832C8764: 7F1A3050  subf r24, r26, r6
	ctx.r[24].s64 = ctx.r[6].s64 - ctx.r[26].s64;
	// 832C8768: 7FDBF214  add r30, r27, r30
	ctx.r[30].u64 = ctx.r[27].u64 + ctx.r[30].u64;
	// 832C876C: 7F78CA14  add r27, r24, r25
	ctx.r[27].u64 = ctx.r[24].u64 + ctx.r[25].u64;
	// 832C8770: 7CDA3214  add r6, r26, r6
	ctx.r[6].u64 = ctx.r[26].u64 + ctx.r[6].u64;
	// 832C8774: 1F7B0EC8  mulli r27, r27, 0xec8
	ctx.r[27].s64 = ctx.r[27].s64 * 3784;
	// 832C8778: 1F59EB18  mulli r26, r25, -0x14e8
	ctx.r[26].s64 = ctx.r[25].s64 * -5352;
	// 832C877C: 7F3E3050  subf r25, r30, r6
	ctx.r[25].s64 = ctx.r[6].s64 - ctx.r[30].s64;
	// 832C8780: 7CC6F214  add r6, r6, r30
	ctx.r[6].u64 = ctx.r[6].u64 + ctx.r[30].u64;
	// 832C8784: 7F7B5E70  srawi r27, r27, 0xb
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[27].s64 = (ctx.r[27].s32 >> 11) as i64;
	// 832C8788: 7F5E5E70  srawi r30, r26, 0xb
	ctx.xer.ca = (ctx.r[26].s32 < 0) && ((ctx.r[26].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[30].s64 = (ctx.r[26].s32 >> 11) as i64;
	// 832C878C: 1F590B50  mulli r26, r25, 0xb50
	ctx.r[26].s64 = ctx.r[25].s64 * 2896;
	// 832C8790: 7FC6F050  subf r30, r6, r30
	ctx.r[30].s64 = ctx.r[30].s64 - ctx.r[6].s64;
	// 832C8794: 1F3808A9  mulli r25, r24, 0x8a9
	ctx.r[25].s64 = ctx.r[24].s64 * 2217;
	// 832C8798: 7F585E70  srawi r24, r26, 0xb
	ctx.xer.ca = (ctx.r[26].s32 < 0) && ((ctx.r[26].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[24].s64 = (ctx.r[26].s32 >> 11) as i64;
	// 832C879C: 7CBF2A14  add r5, r31, r5
	ctx.r[5].u64 = ctx.r[31].u64 + ctx.r[5].u64;
	// 832C87A0: 7F5CE850  subf r26, r28, r29
	ctx.r[26].s64 = ctx.r[29].s64 - ctx.r[28].s64;
	// 832C87A4: 7FFEDA14  add r31, r30, r27
	ctx.r[31].u64 = ctx.r[30].u64 + ctx.r[27].u64;
	// 832C87A8: 7FBCEA14  add r29, r28, r29
	ctx.r[29].u64 = ctx.r[28].u64 + ctx.r[29].u64;
	// 832C87AC: 7F355E70  srawi r21, r25, 0xb
	ctx.xer.ca = (ctx.r[25].s32 < 0) && ((ctx.r[25].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[21].s64 = (ctx.r[25].s32 >> 11) as i64;
	// 832C87B0: 7F25B850  subf r25, r5, r23
	ctx.r[25].s64 = ctx.r[23].s64 - ctx.r[5].s64;
	// 832C87B4: 7F85EA14  add r28, r5, r29
	ctx.r[28].u64 = ctx.r[5].u64 + ctx.r[29].u64;
	// 832C87B8: 7FDFC050  subf r30, r31, r24
	ctx.r[30].s64 = ctx.r[24].s64 - ctx.r[31].s64;
	// 832C87BC: 7F05E850  subf r24, r5, r29
	ctx.r[24].s64 = ctx.r[29].s64 - ctx.r[5].s64;
	// 832C87C0: 7EFBA850  subf r23, r27, r21
	ctx.r[23].s64 = ctx.r[21].s64 - ctx.r[27].s64;
	// 832C87C4: 7CB9D214  add r5, r25, r26
	ctx.r[5].u64 = ctx.r[25].u64 + ctx.r[26].u64;
	// 832C87C8: 7F79D050  subf r27, r25, r26
	ctx.r[27].s64 = ctx.r[26].s64 - ctx.r[25].s64;
	// 832C87CC: 7F46E214  add r26, r6, r28
	ctx.r[26].u64 = ctx.r[6].u64 + ctx.r[28].u64;
	// 832C87D0: 7CC6E050  subf r6, r6, r28
	ctx.r[6].s64 = ctx.r[28].s64 - ctx.r[6].s64;
	// 832C87D4: 7F9F2A14  add r28, r31, r5
	ctx.r[28].u64 = ctx.r[31].u64 + ctx.r[5].u64;
	// 832C87D8: 934B0000  stw r26, 0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(0 as u32), ctx.r[26].u32 ) };
	// 832C87DC: 7CBF2850  subf r5, r31, r5
	ctx.r[5].s64 = ctx.r[5].s64 - ctx.r[31].s64;
	// 832C87E0: 90CB00E0  stw r6, 0xe0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(224 as u32), ctx.r[6].u32 ) };
	// 832C87E4: 7FB7F214  add r29, r23, r30
	ctx.r[29].u64 = ctx.r[23].u64 + ctx.r[30].u64;
	// 832C87E8: 938B0020  stw r28, 0x20(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(32 as u32), ctx.r[28].u32 ) };
	// 832C87EC: 7CDEDA14  add r6, r30, r27
	ctx.r[6].u64 = ctx.r[30].u64 + ctx.r[27].u64;
	// 832C87F0: 90AB00C0  stw r5, 0xc0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(192 as u32), ctx.r[5].u32 ) };
	// 832C87F4: 7FFED850  subf r31, r30, r27
	ctx.r[31].s64 = ctx.r[27].s64 - ctx.r[30].s64;
	// 832C87F8: 7CBDC214  add r5, r29, r24
	ctx.r[5].u64 = ctx.r[29].u64 + ctx.r[24].u64;
	// 832C87FC: 90CB0040  stw r6, 0x40(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(64 as u32), ctx.r[6].u32 ) };
	// 832C8800: 7CDDC050  subf r6, r29, r24
	ctx.r[6].s64 = ctx.r[24].s64 - ctx.r[29].s64;
	// 832C8804: 93EB00A0  stw r31, 0xa0(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(160 as u32), ctx.r[31].u32 ) };
	// 832C8808: 90AB0080  stw r5, 0x80(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(128 as u32), ctx.r[5].u32 ) };
	// 832C880C: 90CB0060  stw r6, 0x60(r11)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[11].u32.wrapping_add(96 as u32), ctx.r[6].u32 ) };
	// 832C8810: 396B0004  addi r11, r11, 4
	ctx.r[11].s64 = ctx.r[11].s64 + 4;
	// 832C8814: 394A0004  addi r10, r10, 4
	ctx.r[10].s64 = ctx.r[10].s64 + 4;
	// 832C8818: 36D6FFFF  addic. r22, r22, -1
	ctx.xer.ca = (ctx.r[22].u32 > (!(-1 as u32)));
	ctx.r[22].s64 = ctx.r[22].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[22].s32, 0, &mut ctx.xer);
	// 832C881C: 4181FE38  bgt 0x832c8654
	if ctx.cr[0].gt {
	pc = 0x832C8654; continue 'dispatch;
	}
	// 832C8820: 39230001  addi r9, r3, 1
	ctx.r[9].s64 = ctx.r[3].s64 + 1;
	// 832C8824: 39470001  addi r10, r7, 1
	ctx.r[10].s64 = ctx.r[7].s64 + 1;
	// 832C8828: 3961FE98  addi r11, r1, -0x168
	ctx.r[11].s64 = ctx.r[1].s64 + -360;
	// 832C882C: 38600008  li r3, 8
	ctx.r[3].s64 = 8;
	// 832C8830: 80EBFFFC  lwz r7, -4(r11)
	ctx.r[7].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-4 as u32) ) } as u64;
	// 832C8834: 80CBFFF4  lwz r6, -0xc(r11)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-12 as u32) ) } as u64;
	// 832C8838: 83CBFFEC  lwz r30, -0x14(r11)
	ctx.r[30].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-20 as u32) ) } as u64;
	// 832C883C: 83EB0004  lwz r31, 4(r11)
	ctx.r[31].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C8840: 7F463850  subf r26, r6, r7
	ctx.r[26].s64 = ctx.r[7].s64 - ctx.r[6].s64;
	// 832C8844: 7CA63A14  add r5, r6, r7
	ctx.r[5].u64 = ctx.r[6].u64 + ctx.r[7].u64;
	// 832C8848: 838BFFF8  lwz r28, -8(r11)
	ctx.r[28].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-8 as u32) ) } as u64;
	// 832C884C: 80CB0000  lwz r6, 0(r11)
	ctx.r[6].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C8850: 7F3FF050  subf r25, r31, r30
	ctx.r[25].s64 = ctx.r[30].s64 - ctx.r[31].s64;
	// 832C8854: 83ABFFF0  lwz r29, -0x10(r11)
	ctx.r[29].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-16 as u32) ) } as u64;
	// 832C8858: 7FFEFA14  add r31, r30, r31
	ctx.r[31].u64 = ctx.r[30].u64 + ctx.r[31].u64;
	// 832C885C: 836BFFE8  lwz r27, -0x18(r11)
	ctx.r[27].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[11].u32.wrapping_add(-24 as u32) ) } as u64;
	// 832C8860: 7F19D214  add r24, r25, r26
	ctx.r[24].u64 = ctx.r[25].u64 + ctx.r[26].u64;
	// 832C8864: 7CFD3214  add r7, r29, r6
	ctx.r[7].u64 = ctx.r[29].u64 + ctx.r[6].u64;
	// 832C8868: 8AEAFFFF  lbz r23, -1(r10)
	ctx.r[23].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(-1 as u32) ) } as u64;
	// 832C886C: 7FDBE214  add r30, r27, r28
	ctx.r[30].u64 = ctx.r[27].u64 + ctx.r[28].u64;
	// 832C8870: 7EC6E850  subf r22, r6, r29
	ctx.r[22].s64 = ctx.r[29].s64 - ctx.r[6].s64;
	// 832C8874: 7CDF2A14  add r6, r31, r5
	ctx.r[6].u64 = ctx.r[31].u64 + ctx.r[5].u64;
	// 832C8878: 7FA7F214  add r29, r7, r30
	ctx.r[29].u64 = ctx.r[7].u64 + ctx.r[30].u64;
	// 832C887C: 7FE5F850  subf r31, r5, r31
	ctx.r[31].s64 = ctx.r[31].s64 - ctx.r[5].s64;
	// 832C8880: 1ED60B50  mulli r22, r22, 0xb50
	ctx.r[22].s64 = ctx.r[22].s64 * 2896;
	// 832C8884: 1F180EC8  mulli r24, r24, 0xec8
	ctx.r[24].s64 = ctx.r[24].s64 * 3784;
	// 832C8888: 7CA6EA14  add r5, r6, r29
	ctx.r[5].u64 = ctx.r[6].u64 + ctx.r[29].u64;
	// 832C888C: 1F5AEB18  mulli r26, r26, -0x14e8
	ctx.r[26].s64 = ctx.r[26].s64 * -5352;
	// 832C8890: 1EBF0B50  mulli r21, r31, 0xb50
	ctx.r[21].s64 = ctx.r[31].s64 * 2896;
	// 832C8894: 7ED65E70  srawi r22, r22, 0xb
	ctx.xer.ca = (ctx.r[22].s32 < 0) && ((ctx.r[22].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[22].s64 = (ctx.r[22].s32 >> 11) as i64;
	// 832C8898: 7F1F5E70  srawi r31, r24, 0xb
	ctx.xer.ca = (ctx.r[24].s32 < 0) && ((ctx.r[24].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[24].s32 >> 11) as i64;
	// 832C889C: 1F3908A9  mulli r25, r25, 0x8a9
	ctx.r[25].s64 = ctx.r[25].s64 * 2217;
	// 832C88A0: 38A5007F  addi r5, r5, 0x7f
	ctx.r[5].s64 = ctx.r[5].s64 + 127;
	// 832C88A4: 7F585E70  srawi r24, r26, 0xb
	ctx.xer.ca = (ctx.r[26].s32 < 0) && ((ctx.r[26].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[24].s64 = (ctx.r[26].s32 >> 11) as i64;
	// 832C88A8: 7EB55E70  srawi r21, r21, 0xb
	ctx.xer.ca = (ctx.r[21].s32 < 0) && ((ctx.r[21].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[21].s64 = (ctx.r[21].s32 >> 11) as i64;
	// 832C88AC: 7F345E70  srawi r20, r25, 0xb
	ctx.xer.ca = (ctx.r[25].s32 < 0) && ((ctx.r[25].u32 & ((1u32 << 11) - 1)) != 0);
	ctx.r[20].s64 = (ctx.r[25].s32 >> 11) as i64;
	// 832C88B0: 7CB94670  srawi r25, r5, 8
	ctx.xer.ca = (ctx.r[5].s32 < 0) && ((ctx.r[5].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[25].s64 = (ctx.r[5].s32 >> 8) as i64;
	// 832C88B4: 7CA6E850  subf r5, r6, r29
	ctx.r[5].s64 = ctx.r[29].s64 - ctx.r[6].s64;
	// 832C88B8: 7FB9BA14  add r29, r25, r23
	ctx.r[29].u64 = ctx.r[25].u64 + ctx.r[23].u64;
	// 832C88BC: 7F7CD850  subf r27, r28, r27
	ctx.r[27].s64 = ctx.r[27].s64 - ctx.r[28].s64;
	// 832C88C0: 9BA9FFFF  stb r29, -1(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(-1 as u32), ctx.r[29].u8 ) };
	// 832C88C4: 7F87B050  subf r28, r7, r22
	ctx.r[28].s64 = ctx.r[22].s64 - ctx.r[7].s64;
	// 832C88C8: 7CC6C050  subf r6, r6, r24
	ctx.r[6].s64 = ctx.r[24].s64 - ctx.r[6].s64;
	// 832C88CC: 7FBCDA14  add r29, r28, r27
	ctx.r[29].u64 = ctx.r[28].u64 + ctx.r[27].u64;
	// 832C88D0: 7CC6FA14  add r6, r6, r31
	ctx.r[6].u64 = ctx.r[6].u64 + ctx.r[31].u64;
	// 832C88D4: 3B25007F  addi r25, r5, 0x7f
	ctx.r[25].s64 = ctx.r[5].s64 + 127;
	// 832C88D8: 7CA6A850  subf r5, r6, r21
	ctx.r[5].s64 = ctx.r[21].s64 - ctx.r[6].s64;
	// 832C88DC: 7F46EA14  add r26, r6, r29
	ctx.r[26].u64 = ctx.r[6].u64 + ctx.r[29].u64;
	// 832C88E0: 7CC6E850  subf r6, r6, r29
	ctx.r[6].s64 = ctx.r[29].s64 - ctx.r[6].s64;
	// 832C88E4: 7F394670  srawi r25, r25, 8
	ctx.xer.ca = (ctx.r[25].s32 < 0) && ((ctx.r[25].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[25].s64 = (ctx.r[25].s32 >> 8) as i64;
	// 832C88E8: 3BA6007F  addi r29, r6, 0x7f
	ctx.r[29].s64 = ctx.r[6].s64 + 127;
	// 832C88EC: 7CDCD850  subf r6, r28, r27
	ctx.r[6].s64 = ctx.r[27].s64 - ctx.r[28].s64;
	// 832C88F0: 7F7FA050  subf r27, r31, r20
	ctx.r[27].s64 = ctx.r[20].s64 - ctx.r[31].s64;
	// 832C88F4: 7FE53214  add r31, r5, r6
	ctx.r[31].u64 = ctx.r[5].u64 + ctx.r[6].u64;
	// 832C88F8: 7F853050  subf r28, r5, r6
	ctx.r[28].s64 = ctx.r[6].s64 - ctx.r[5].s64;
	// 832C88FC: 7CC7F050  subf r6, r7, r30
	ctx.r[6].s64 = ctx.r[30].s64 - ctx.r[7].s64;
	// 832C8900: 7CFB2A14  add r7, r27, r5
	ctx.r[7].u64 = ctx.r[27].u64 + ctx.r[5].u64;
	// 832C8904: 38BA007F  addi r5, r26, 0x7f
	ctx.r[5].s64 = ctx.r[26].s64 + 127;
	// 832C8908: 3BFF007F  addi r31, r31, 0x7f
	ctx.r[31].s64 = ctx.r[31].s64 + 127;
	// 832C890C: 7CBA4670  srawi r26, r5, 8
	ctx.xer.ca = (ctx.r[5].s32 < 0) && ((ctx.r[5].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[26].s64 = (ctx.r[5].s32 >> 8) as i64;
	// 832C8910: 7FBD4670  srawi r29, r29, 8
	ctx.xer.ca = (ctx.r[29].s32 < 0) && ((ctx.r[29].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[29].s64 = (ctx.r[29].s32 >> 8) as i64;
	// 832C8914: 88AA0006  lbz r5, 6(r10)
	ctx.r[5].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(6 as u32) ) } as u64;
	// 832C8918: 7FFF4670  srawi r31, r31, 8
	ctx.xer.ca = (ctx.r[31].s32 < 0) && ((ctx.r[31].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[31].s64 = (ctx.r[31].s32 >> 8) as i64;
	// 832C891C: 3BDC007F  addi r30, r28, 0x7f
	ctx.r[30].s64 = ctx.r[28].s64 + 127;
	// 832C8920: 7CB92A14  add r5, r25, r5
	ctx.r[5].u64 = ctx.r[25].u64 + ctx.r[5].u64;
	// 832C8924: 98A90006  stb r5, 6(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(6 as u32), ctx.r[5].u8 ) };
	// 832C8928: 88AA0000  lbz r5, 0(r10)
	ctx.r[5].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C892C: 7CBA2A14  add r5, r26, r5
	ctx.r[5].u64 = ctx.r[26].u64 + ctx.r[5].u64;
	// 832C8930: 98A90000  stb r5, 0(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(0 as u32), ctx.r[5].u8 ) };
	// 832C8934: 88AA0005  lbz r5, 5(r10)
	ctx.r[5].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(5 as u32) ) } as u64;
	// 832C8938: 7CBD2A14  add r5, r29, r5
	ctx.r[5].u64 = ctx.r[29].u64 + ctx.r[5].u64;
	// 832C893C: 98A90005  stb r5, 5(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(5 as u32), ctx.r[5].u8 ) };
	// 832C8940: 88AA0001  lbz r5, 1(r10)
	ctx.r[5].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(1 as u32) ) } as u64;
	// 832C8944: 7CBF2A14  add r5, r31, r5
	ctx.r[5].u64 = ctx.r[31].u64 + ctx.r[5].u64;
	// 832C8948: 98A90001  stb r5, 1(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(1 as u32), ctx.r[5].u8 ) };
	// 832C894C: 7FC54670  srawi r5, r30, 8
	ctx.xer.ca = (ctx.r[30].s32 < 0) && ((ctx.r[30].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[5].s64 = (ctx.r[30].s32 >> 8) as i64;
	// 832C8950: 8BEA0004  lbz r31, 4(r10)
	ctx.r[31].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C8954: 7FC73050  subf r30, r7, r6
	ctx.r[30].s64 = ctx.r[6].s64 - ctx.r[7].s64;
	// 832C8958: 7CA5FA14  add r5, r5, r31
	ctx.r[5].u64 = ctx.r[5].u64 + ctx.r[31].u64;
	// 832C895C: 7CE73214  add r7, r7, r6
	ctx.r[7].u64 = ctx.r[7].u64 + ctx.r[6].u64;
	// 832C8960: 7CA62B78  mr r6, r5
	ctx.r[6].u64 = ctx.r[5].u64;
	// 832C8964: 3BDE007F  addi r30, r30, 0x7f
	ctx.r[30].s64 = ctx.r[30].s64 + 127;
	// 832C8968: 98C90004  stb r6, 4(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(4 as u32), ctx.r[6].u8 ) };
	// 832C896C: 38E7007F  addi r7, r7, 0x7f
	ctx.r[7].s64 = ctx.r[7].s64 + 127;
	// 832C8970: 7FC54670  srawi r5, r30, 8
	ctx.xer.ca = (ctx.r[30].s32 < 0) && ((ctx.r[30].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[5].s64 = (ctx.r[30].s32 >> 8) as i64;
	// 832C8974: 88CA0002  lbz r6, 2(r10)
	ctx.r[6].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(2 as u32) ) } as u64;
	// 832C8978: 7CE74670  srawi r7, r7, 8
	ctx.xer.ca = (ctx.r[7].s32 < 0) && ((ctx.r[7].u32 & ((1u32 << 8) - 1)) != 0);
	ctx.r[7].s64 = (ctx.r[7].s32 >> 8) as i64;
	// 832C897C: 3463FFFF  addic. r3, r3, -1
	ctx.xer.ca = (ctx.r[3].u32 > (!(-1 as u32)));
	ctx.r[3].s64 = ctx.r[3].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[3].s32, 0, &mut ctx.xer);
	// 832C8980: 7CA53214  add r5, r5, r6
	ctx.r[5].u64 = ctx.r[5].u64 + ctx.r[6].u64;
	// 832C8984: 396B0020  addi r11, r11, 0x20
	ctx.r[11].s64 = ctx.r[11].s64 + 32;
	// 832C8988: 98A90002  stb r5, 2(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(2 as u32), ctx.r[5].u8 ) };
	// 832C898C: 88CA0003  lbz r6, 3(r10)
	ctx.r[6].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[10].u32.wrapping_add(3 as u32) ) } as u64;
	// 832C8990: 7CE73214  add r7, r7, r6
	ctx.r[7].u64 = ctx.r[7].u64 + ctx.r[6].u64;
	// 832C8994: 98E90003  stb r7, 3(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(3 as u32), ctx.r[7].u8 ) };
	// 832C8998: 7D4A4214  add r10, r10, r8
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[8].u64;
	// 832C899C: 7D292214  add r9, r9, r4
	ctx.r[9].u64 = ctx.r[9].u64 + ctx.r[4].u64;
	// 832C89A0: 4082FE90  bne 0x832c8830
	if !ctx.cr[0].eq {
	pc = 0x832C8830; continue 'dispatch;
	}
	// 832C89A4: 4B9E0A88  b 0x82ca942c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA942C);
	return;
}

pub fn sub_832C89A8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    // ---- function 0x832C89A8 size=8
	// 832C89A8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C89AC: 4B9E0A25  bl 0x82ca93d0
	ctx.lr = 0x832C89B0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93D0);
}

pub fn sub_832C9488(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    // ---- function 0x832C9488 size=1312
	// 832C9488: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C948C: 4B9DFF61  bl 0x82ca93ec
	ctx.lr = 0x832C9490;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93EC);
	// 832C9490: 81640008  lwz r11, 8(r4)
	ctx.r[11].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(8 as u32) ) } as u64;
	// 832C9494: 3B200000  li r25, 0
	ctx.r[25].s64 = 0;
	// 832C9498: 81440000  lwz r10, 0(r4)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C949C: 83E40004  lwz r31, 4(r4)
	ctx.r[31].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[4].u32.wrapping_add(4 as u32) ) } as u64;
	// 832C94A0: 7F3ECB78  mr r30, r25
	ctx.r[30].u64 = ctx.r[25].u64;
	// 832C94A4: 2B0B0003  cmplwi cr6, r11, 3
	ctx.cr[6].compare_u32(ctx.r[11].u32, 3 as u32, &mut ctx.xer);
	// 832C94A8: 40980030  bge cr6, 0x832c94d8
	if !ctx.cr[6].lt {
	pc = 0x832C94D8; continue 'dispatch;
	}
	// 832C94AC: 813F0000  lwz r9, 0(r31)
	ctx.r[9].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C94B0: 5548063E  clrlwi r8, r10, 0x18
	ctx.r[8].u64 = ctx.r[10].u32 as u64 & 0x000000FFu64;
	// 832C94B4: 20EB0003  subfic r7, r11, 3
	ctx.xer.ca = ctx.r[11].u32 <= 3 as u32;
	ctx.r[7].s64 = (3 as i64) - ctx.r[11].s64;
	// 832C94B8: 7D265830  slw r6, r9, r11
	if (ctx.r[11].u8 & 0x20) != 0 {
		ctx.r[6].u64 = 0;
	} else {
		ctx.r[6].u64 = ((ctx.r[9].u32) << ((ctx.r[11].u8 & 0x1F) as u32)) as u64;
	}
	// 832C94BC: 54CA063E  clrlwi r10, r6, 0x18
	ctx.r[10].u64 = ctx.r[6].u32 as u64 & 0x000000FFu64;
	// 832C94C0: 7D273C30  srw r7, r9, r7
	if (ctx.r[7].u8 & 0x20) != 0 {
		ctx.r[7].u64 = 0;
	} else {
		ctx.r[7].u64 = ((ctx.r[9].u32) >> ((ctx.r[7].u8 & 0x1F) as u32)) as u64;
	}
	// 832C94C4: 7D494378  or r9, r10, r8
	ctx.r[9].u64 = ctx.r[10].u64 | ctx.r[8].u64;
	// 832C94C8: 396B001D  addi r11, r11, 0x1d
	ctx.r[11].s64 = ctx.r[11].s64 + 29;
	// 832C94CC: 5529077E  clrlwi r9, r9, 0x1d
	ctx.r[9].u64 = ctx.r[9].u32 as u64 & 0x00000007u64;
	// 832C94D0: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C94D4: 48000010  b 0x832c94e4
	pc = 0x832C94E4; continue 'dispatch;
	// 832C94D8: 5549077E  clrlwi r9, r10, 0x1d
	ctx.r[9].u64 = ctx.r[10].u32 as u64 & 0x00000007u64;
	// 832C94DC: 5547E8FE  srwi r7, r10, 3
	// 832C94E0: 396BFFFD  addi r11, r11, -3
	ctx.r[11].s64 = ctx.r[11].s64 + -3;
	// 832C94E4: 38C00010  li r6, 0x10
	ctx.r[6].s64 = 16;
	// 832C94E8: 35490001  addic. r10, r9, 1
	ctx.xer.ca = (ctx.r[9].u32 > (!(1 as u32)));
	ctx.r[10].s64 = ctx.r[9].s64 + 1;
	ctx.cr[0].compare_i32(ctx.r[10].s32, 0, &mut ctx.xer);
	// 832C94EC: 3BA00060  li r29, 0x60
	ctx.r[29].s64 = 96;
	// 832C94F0: 98C1FF14  stb r6, -0xec(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(-236 as u32), ctx.r[6].u8 ) };
	// 832C94F4: 3B6000B0  li r27, 0xb0
	ctx.r[27].s64 = 176;
	// 832C94F8: 39200001  li r9, 1
	ctx.r[9].s64 = 1;
	// 832C94FC: 9BA1FF15  stb r29, -0xeb(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(-235 as u32), ctx.r[29].u8 ) };
	// 832C9500: 390AFFFF  addi r8, r10, -1
	ctx.r[8].s64 = ctx.r[10].s64 + -1;
	// 832C9504: 9B61FF16  stb r27, -0xea(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(-234 as u32), ctx.r[27].u8 ) };
	// 832C9508: 38C00002  li r6, 2
	ctx.r[6].s64 = 2;
	// 832C950C: 3B81FF14  addi r28, r1, -0xec
	ctx.r[28].s64 = ctx.r[1].s64 + -236;
	// 832C9510: 98C1FF17  stb r6, -0xe9(r1)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[1].u32.wrapping_add(-233 as u32), ctx.r[6].u8 ) };
	// 832C9514: 3B41FF18  addi r26, r1, -0xe8
	ctx.r[26].s64 = ctx.r[1].s64 + -232;
	// 832C9518: 7F3DCB78  mr r29, r25
	ctx.r[29].u64 = ctx.r[25].u64;
	// 832C951C: 7D3B4030  slw r27, r9, r8
	if (ctx.r[8].u8 & 0x20) != 0 {
		ctx.r[27].u64 = 0;
	} else {
		ctx.r[27].u64 = ((ctx.r[9].u32) << ((ctx.r[8].u8 & 0x1F) as u32)) as u64;
	}
	// 832C9520: 7D585378  mr r24, r10
	ctx.r[24].u64 = ctx.r[10].u64;
	// 832C9524: 41820470  beq 0x832c9994
	if ctx.cr[0].eq {
	pc = 0x832C9994; continue 'dispatch;
	}
	// 832C9528: 7F26CB78  mr r6, r25
	ctx.r[6].u64 = ctx.r[25].u64;
	// 832C952C: 2F1D0000  cmpwi cr6, r29, 0
	ctx.cr[6].compare_i32(ctx.r[29].s32, 0, &mut ctx.xer);
	// 832C9530: 40990078  ble cr6, 0x832c95a8
	if !ctx.cr[6].gt {
	pc = 0x832C95A8; continue 'dispatch;
	}
	// 832C9534: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C9538: 409A0018  bne cr6, 0x832c9550
	if !ctx.cr[6].eq {
	pc = 0x832C9550; continue 'dispatch;
	}
	// 832C953C: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C9540: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C9544: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C9548: 5547F87E  srwi r7, r10, 1
	// 832C954C: 48000010  b 0x832c955c
	pc = 0x832C955C; continue 'dispatch;
	// 832C9550: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C9554: 54E7F87E  srwi r7, r7, 1
	// 832C9558: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C955C: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C9560: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C9564: 419A0038  beq cr6, 0x832c959c
	if ctx.cr[6].eq {
	pc = 0x832C959C; continue 'dispatch;
	}
	// 832C9568: 3941FF60  addi r10, r1, -0xa0
	ctx.r[10].s64 = ctx.r[1].s64 + -160;
	// 832C956C: 7D0650AE  lbzx r8, r6, r10
	ctx.r[8].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[6].u32.wrapping_add(ctx.r[10].u32)) } as u64;
	// 832C9570: 7D5B00D0  neg r10, r27
	ctx.r[10].s64 = -ctx.r[27].s64;
	// 832C9574: 7D2818AE  lbzx r9, r8, r3
	ctx.r[9].u64 = unsafe { crate::rt::load_u8(base as *const u8, ctx.r[8].u32.wrapping_add(ctx.r[3].u32)) } as u64;
	// 832C9578: 7D290774  extsb r9, r9
	ctx.r[9].s64 = ctx.r[9].s8 as i64;
	// 832C957C: 2F090000  cmpwi cr6, r9, 0
	ctx.cr[6].compare_i32(ctx.r[9].s32, 0, &mut ctx.xer);
	// 832C9580: 41980008  blt cr6, 0x832c9588
	if ctx.cr[6].lt {
	pc = 0x832C9588; continue 'dispatch;
	}
	// 832C9584: 7F6ADB78  mr r10, r27
	ctx.r[10].u64 = ctx.r[27].u64;
	// 832C9588: 7D4A4A14  add r10, r10, r9
	ctx.r[10].u64 = ctx.r[10].u64 + ctx.r[9].u64;
	// 832C958C: 7F1E2840  cmplw cr6, r30, r5
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[5].u32, &mut ctx.xer);
	// 832C9590: 3BDE0001  addi r30, r30, 1
	ctx.r[30].s64 = ctx.r[30].s64 + 1;
	// 832C9594: 7D4819AE  stbx r10, r8, r3
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[8].u32.wrapping_add(ctx.r[3].u32), ctx.r[10].u8) };
	// 832C9598: 419A03FC  beq cr6, 0x832c9994
	if ctx.cr[6].eq {
	pc = 0x832C9994; continue 'dispatch;
	}
	// 832C959C: 38C60001  addi r6, r6, 1
	ctx.r[6].s64 = ctx.r[6].s64 + 1;
	// 832C95A0: 7F06E800  cmpw cr6, r6, r29
	ctx.cr[6].compare_i32(ctx.r[6].s32, ctx.r[29].s32, &mut ctx.xer);
	// 832C95A4: 4198FF90  blt cr6, 0x832c9534
	if ctx.cr[6].lt {
	pc = 0x832C9534; continue 'dispatch;
	}
	// 832C95A8: 7F86E378  mr r6, r28
	ctx.r[6].u64 = ctx.r[28].u64;
	// 832C95AC: 7F1CD040  cmplw cr6, r28, r26
	ctx.cr[6].compare_u32(ctx.r[28].u32, ctx.r[26].u32, &mut ctx.xer);
	// 832C95B0: 409803D8  bge cr6, 0x832c9988
	if !ctx.cr[6].lt {
	pc = 0x832C9988; continue 'dispatch;
	}
	// 832C95B4: 3941FF60  addi r10, r1, -0xa0
	ctx.r[10].s64 = ctx.r[1].s64 + -160;
	// 832C95B8: 7D1D5214  add r8, r29, r10
	ctx.r[8].u64 = ctx.r[29].u64 + ctx.r[10].u64;
	// 832C95BC: 89260000  lbz r9, 0(r6)
	ctx.r[9].u64 = unsafe { crate::rt::load_u8( base as *const u8, ctx.r[6].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C95C0: 2B090000  cmplwi cr6, r9, 0
	ctx.cr[6].compare_u32(ctx.r[9].u32, 0 as u32, &mut ctx.xer);
	// 832C95C4: 419A03B8  beq cr6, 0x832c997c
	if ctx.cr[6].eq {
	pc = 0x832C997C; continue 'dispatch;
	}
	// 832C95C8: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C95CC: 409A0018  bne cr6, 0x832c95e4
	if !ctx.cr[6].eq {
	pc = 0x832C95E4; continue 'dispatch;
	}
	// 832C95D0: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C95D4: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C95D8: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C95DC: 5547F87E  srwi r7, r10, 1
	// 832C95E0: 48000010  b 0x832c95f0
	pc = 0x832C95F0; continue 'dispatch;
	// 832C95E4: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C95E8: 54E7F87E  srwi r7, r7, 1
	// 832C95EC: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C95F0: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C95F4: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C95F8: 419A0384  beq cr6, 0x832c997c
	if ctx.cr[6].eq {
	pc = 0x832C997C; continue 'dispatch;
	}
	// 832C95FC: 552A07BE  clrlwi r10, r9, 0x1e
	ctx.r[10].u64 = ctx.r[9].u32 as u64 & 0x00000003u64;
	// 832C9600: 2B0A0003  cmplwi cr6, r10, 3
	ctx.cr[6].compare_u32(ctx.r[10].u32, 3 as u32, &mut ctx.xer);
	// 832C9604: 41990378  bgt cr6, 0x832c997c
	if ctx.cr[6].gt {
	pc = 0x832C997C; continue 'dispatch;
	}
	// 832C9608: 3D80832D  lis r12, -0x7cd3
	ctx.r[12].s64 = -2094202880;
	// 832C960C: 398C9620  addi r12, r12, -0x69e0
	ctx.r[12].s64 = ctx.r[12].s64 + -27104;
	// 832C9610: 5540103A  slwi r0, r10, 2
	// 832C9614: 7C0C002E  lwzx r0, r12, r0
	ctx.r[0].u64 = unsafe { crate::rt::load_u32(base as *const u8, ctx.r[12].u32.wrapping_add(ctx.r[0].u32)) } as u64;
	// 832C9618: 7C0903A6  mtctr r0
	ctx.ctr.u64 = ctx.r[0].u64;
	// 832C961C: 4E800420  bctr
	crate::rt::call_indirect(ctx.ctr.u32);
	return;
	// 832C9620: 832C9630  lwz r25, -0x69d0(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(-27088 as u32) ) } as u64;
	// 832C9624: 832C9644  lwz r25, -0x69bc(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(-27068 as u32) ) } as u64;
	// 832C9628: 832C9680  lwz r25, -0x6980(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(-27008 as u32) ) } as u64;
	// 832C962C: 832C991C  lwz r25, -0x66e4(r12)
	ctx.r[25].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[12].u32.wrapping_add(-26340 as u32) ) } as u64;
	// 832C9630: 5529F0BE  srwi r9, r9, 2
	// 832C9634: 552A103A  slwi r10, r9, 2
	// 832C9638: 394A0011  addi r10, r10, 0x11
	ctx.r[10].s64 = ctx.r[10].s64 + 17;
	// 832C963C: 99460000  stb r10, 0(r6)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[6].u32.wrapping_add(0 as u32), ctx.r[10].u8 ) };
	// 832C9640: 4800004C  b 0x832c968c
	pc = 0x832C968C; continue 'dispatch;
	// 832C9644: 7D2A4B78  mr r10, r9
	ctx.r[10].u64 = ctx.r[9].u64;
	// 832C9648: 393A0001  addi r9, r26, 1
	ctx.r[9].s64 = ctx.r[26].s64 + 1;
	// 832C964C: 554A003A  rlwinm r10, r10, 0, 0, 0x1d
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0xFFFFFFFFu64;
	// 832C9650: 3AEA0002  addi r23, r10, 2
	ctx.r[23].s64 = ctx.r[10].s64 + 2;
	// 832C9654: 3ACA0012  addi r22, r10, 0x12
	ctx.r[22].s64 = ctx.r[10].s64 + 18;
	// 832C9658: 3AAA0022  addi r21, r10, 0x22
	ctx.r[21].s64 = ctx.r[10].s64 + 34;
	// 832C965C: 9AE60000  stb r23, 0(r6)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[6].u32.wrapping_add(0 as u32), ctx.r[23].u8 ) };
	// 832C9660: 394A0032  addi r10, r10, 0x32
	ctx.r[10].s64 = ctx.r[10].s64 + 50;
	// 832C9664: 9ADA0000  stb r22, 0(r26)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[26].u32.wrapping_add(0 as u32), ctx.r[22].u8 ) };
	// 832C9668: 9ABA0001  stb r21, 1(r26)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[26].u32.wrapping_add(1 as u32), ctx.r[21].u8 ) };
	// 832C966C: 7D575378  mr r23, r10
	ctx.r[23].u64 = ctx.r[10].u64;
	// 832C9670: 39490001  addi r10, r9, 1
	ctx.r[10].s64 = ctx.r[9].s64 + 1;
	// 832C9674: 9AE90001  stb r23, 1(r9)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[9].u32.wrapping_add(1 as u32), ctx.r[23].u8 ) };
	// 832C9678: 3B4A0001  addi r26, r10, 1
	ctx.r[26].s64 = ctx.r[10].s64 + 1;
	// 832C967C: 48000304  b 0x832c9980
	pc = 0x832C9980; continue 'dispatch;
	// 832C9680: 9B260000  stb r25, 0(r6)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[6].u32.wrapping_add(0 as u32), ctx.r[25].u8 ) };
	// 832C9684: 5529F0BE  srwi r9, r9, 2
	// 832C9688: 38C60001  addi r6, r6, 1
	ctx.r[6].s64 = ctx.r[6].s64 + 1;
	// 832C968C: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C9690: 409A0018  bne cr6, 0x832c96a8
	if !ctx.cr[6].eq {
	pc = 0x832C96A8; continue 'dispatch;
	}
	// 832C9694: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C9698: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C969C: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C96A0: 5547F87E  srwi r7, r10, 1
	// 832C96A4: 48000010  b 0x832c96b4
	pc = 0x832C96B4; continue 'dispatch;
	// 832C96A8: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C96AC: 54E7F87E  srwi r7, r7, 1
	// 832C96B0: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C96B4: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C96B8: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C96BC: 419A0018  beq cr6, 0x832c96d4
	if ctx.cr[6].eq {
	pc = 0x832C96D4; continue 'dispatch;
	}
	// 832C96C0: 552A103A  slwi r10, r9, 2
	// 832C96C4: 3B9CFFFF  addi r28, r28, -1
	ctx.r[28].s64 = ctx.r[28].s64 + -1;
	// 832C96C8: 394A0003  addi r10, r10, 3
	ctx.r[10].s64 = ctx.r[10].s64 + 3;
	// 832C96CC: 995C0000  stb r10, 0(r28)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[10].u8 ) };
	// 832C96D0: 4800005C  b 0x832c972c
	pc = 0x832C972C; continue 'dispatch;
	// 832C96D4: 99280000  stb r9, 0(r8)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[9].u8 ) };
	// 832C96D8: 3BBD0001  addi r29, r29, 1
	ctx.r[29].s64 = ctx.r[29].s64 + 1;
	// 832C96DC: 39080001  addi r8, r8, 1
	ctx.r[8].s64 = ctx.r[8].s64 + 1;
	// 832C96E0: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C96E4: 409A0018  bne cr6, 0x832c96fc
	if !ctx.cr[6].eq {
	pc = 0x832C96FC; continue 'dispatch;
	}
	// 832C96E8: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C96EC: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C96F0: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C96F4: 5547F87E  srwi r7, r10, 1
	// 832C96F8: 48000010  b 0x832c9708
	pc = 0x832C9708; continue 'dispatch;
	// 832C96FC: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C9700: 54E7F87E  srwi r7, r7, 1
	// 832C9704: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C9708: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C970C: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C9710: 7D5B00D0  neg r10, r27
	ctx.r[10].s64 = -ctx.r[27].s64;
	// 832C9714: 409A0008  bne cr6, 0x832c971c
	if !ctx.cr[6].eq {
	pc = 0x832C971C; continue 'dispatch;
	}
	// 832C9718: 7F6ADB78  mr r10, r27
	ctx.r[10].u64 = ctx.r[27].u64;
	// 832C971C: 7F1E2840  cmplw cr6, r30, r5
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[5].u32, &mut ctx.xer);
	// 832C9720: 7D4919AE  stbx r10, r9, r3
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[3].u32), ctx.r[10].u8) };
	// 832C9724: 3BDE0001  addi r30, r30, 1
	ctx.r[30].s64 = ctx.r[30].s64 + 1;
	// 832C9728: 419A026C  beq cr6, 0x832c9994
	if ctx.cr[6].eq {
	pc = 0x832C9994; continue 'dispatch;
	}
	// 832C972C: 39290001  addi r9, r9, 1
	ctx.r[9].s64 = ctx.r[9].s64 + 1;
	// 832C9730: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C9734: 409A0018  bne cr6, 0x832c974c
	if !ctx.cr[6].eq {
	pc = 0x832C974C; continue 'dispatch;
	}
	// 832C9738: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C973C: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C9740: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C9744: 5547F87E  srwi r7, r10, 1
	// 832C9748: 48000010  b 0x832c9758
	pc = 0x832C9758; continue 'dispatch;
	// 832C974C: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C9750: 54E7F87E  srwi r7, r7, 1
	// 832C9754: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C9758: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C975C: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C9760: 419A0018  beq cr6, 0x832c9778
	if ctx.cr[6].eq {
	pc = 0x832C9778; continue 'dispatch;
	}
	// 832C9764: 552A103A  slwi r10, r9, 2
	// 832C9768: 3B9CFFFF  addi r28, r28, -1
	ctx.r[28].s64 = ctx.r[28].s64 + -1;
	// 832C976C: 394A0003  addi r10, r10, 3
	ctx.r[10].s64 = ctx.r[10].s64 + 3;
	// 832C9770: 995C0000  stb r10, 0(r28)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[10].u8 ) };
	// 832C9774: 4800005C  b 0x832c97d0
	pc = 0x832C97D0; continue 'dispatch;
	// 832C9778: 99280000  stb r9, 0(r8)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[9].u8 ) };
	// 832C977C: 3BBD0001  addi r29, r29, 1
	ctx.r[29].s64 = ctx.r[29].s64 + 1;
	// 832C9780: 39080001  addi r8, r8, 1
	ctx.r[8].s64 = ctx.r[8].s64 + 1;
	// 832C9784: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C9788: 409A0018  bne cr6, 0x832c97a0
	if !ctx.cr[6].eq {
	pc = 0x832C97A0; continue 'dispatch;
	}
	// 832C978C: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C9790: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C9794: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C9798: 5547F87E  srwi r7, r10, 1
	// 832C979C: 48000010  b 0x832c97ac
	pc = 0x832C97AC; continue 'dispatch;
	// 832C97A0: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C97A4: 54E7F87E  srwi r7, r7, 1
	// 832C97A8: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C97AC: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C97B0: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C97B4: 7D5B00D0  neg r10, r27
	ctx.r[10].s64 = -ctx.r[27].s64;
	// 832C97B8: 409A0008  bne cr6, 0x832c97c0
	if !ctx.cr[6].eq {
	pc = 0x832C97C0; continue 'dispatch;
	}
	// 832C97BC: 7F6ADB78  mr r10, r27
	ctx.r[10].u64 = ctx.r[27].u64;
	// 832C97C0: 7F1E2840  cmplw cr6, r30, r5
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[5].u32, &mut ctx.xer);
	// 832C97C4: 7D4919AE  stbx r10, r9, r3
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[3].u32), ctx.r[10].u8) };
	// 832C97C8: 3BDE0001  addi r30, r30, 1
	ctx.r[30].s64 = ctx.r[30].s64 + 1;
	// 832C97CC: 419A01C8  beq cr6, 0x832c9994
	if ctx.cr[6].eq {
	pc = 0x832C9994; continue 'dispatch;
	}
	// 832C97D0: 39290001  addi r9, r9, 1
	ctx.r[9].s64 = ctx.r[9].s64 + 1;
	// 832C97D4: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C97D8: 409A0018  bne cr6, 0x832c97f0
	if !ctx.cr[6].eq {
	pc = 0x832C97F0; continue 'dispatch;
	}
	// 832C97DC: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C97E0: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C97E4: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C97E8: 5547F87E  srwi r7, r10, 1
	// 832C97EC: 48000010  b 0x832c97fc
	pc = 0x832C97FC; continue 'dispatch;
	// 832C97F0: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C97F4: 54E7F87E  srwi r7, r7, 1
	// 832C97F8: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C97FC: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C9800: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C9804: 419A0018  beq cr6, 0x832c981c
	if ctx.cr[6].eq {
	pc = 0x832C981C; continue 'dispatch;
	}
	// 832C9808: 552A103A  slwi r10, r9, 2
	// 832C980C: 3B9CFFFF  addi r28, r28, -1
	ctx.r[28].s64 = ctx.r[28].s64 + -1;
	// 832C9810: 394A0003  addi r10, r10, 3
	ctx.r[10].s64 = ctx.r[10].s64 + 3;
	// 832C9814: 995C0000  stb r10, 0(r28)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[10].u8 ) };
	// 832C9818: 4800005C  b 0x832c9874
	pc = 0x832C9874; continue 'dispatch;
	// 832C981C: 99280000  stb r9, 0(r8)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[9].u8 ) };
	// 832C9820: 3BBD0001  addi r29, r29, 1
	ctx.r[29].s64 = ctx.r[29].s64 + 1;
	// 832C9824: 39080001  addi r8, r8, 1
	ctx.r[8].s64 = ctx.r[8].s64 + 1;
	// 832C9828: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C982C: 409A0018  bne cr6, 0x832c9844
	if !ctx.cr[6].eq {
	pc = 0x832C9844; continue 'dispatch;
	}
	// 832C9830: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C9834: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C9838: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C983C: 5547F87E  srwi r7, r10, 1
	// 832C9840: 48000010  b 0x832c9850
	pc = 0x832C9850; continue 'dispatch;
	// 832C9844: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C9848: 54E7F87E  srwi r7, r7, 1
	// 832C984C: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C9850: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C9854: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C9858: 7D5B00D0  neg r10, r27
	ctx.r[10].s64 = -ctx.r[27].s64;
	// 832C985C: 409A0008  bne cr6, 0x832c9864
	if !ctx.cr[6].eq {
	pc = 0x832C9864; continue 'dispatch;
	}
	// 832C9860: 7F6ADB78  mr r10, r27
	ctx.r[10].u64 = ctx.r[27].u64;
	// 832C9864: 7F1E2840  cmplw cr6, r30, r5
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[5].u32, &mut ctx.xer);
	// 832C9868: 7D4919AE  stbx r10, r9, r3
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[3].u32), ctx.r[10].u8) };
	// 832C986C: 3BDE0001  addi r30, r30, 1
	ctx.r[30].s64 = ctx.r[30].s64 + 1;
	// 832C9870: 419A0124  beq cr6, 0x832c9994
	if ctx.cr[6].eq {
	pc = 0x832C9994; continue 'dispatch;
	}
	// 832C9874: 39290001  addi r9, r9, 1
	ctx.r[9].s64 = ctx.r[9].s64 + 1;
	// 832C9878: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C987C: 409A0018  bne cr6, 0x832c9894
	if !ctx.cr[6].eq {
	pc = 0x832C9894; continue 'dispatch;
	}
	// 832C9880: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C9884: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C9888: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C988C: 5547F87E  srwi r7, r10, 1
	// 832C9890: 48000010  b 0x832c98a0
	pc = 0x832C98A0; continue 'dispatch;
	// 832C9894: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C9898: 54E7F87E  srwi r7, r7, 1
	// 832C989C: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C98A0: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C98A4: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C98A8: 419A0018  beq cr6, 0x832c98c0
	if ctx.cr[6].eq {
	pc = 0x832C98C0; continue 'dispatch;
	}
	// 832C98AC: 552A103A  slwi r10, r9, 2
	// 832C98B0: 3B9CFFFF  addi r28, r28, -1
	ctx.r[28].s64 = ctx.r[28].s64 + -1;
	// 832C98B4: 394A0003  addi r10, r10, 3
	ctx.r[10].s64 = ctx.r[10].s64 + 3;
	// 832C98B8: 995C0000  stb r10, 0(r28)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[28].u32.wrapping_add(0 as u32), ctx.r[10].u8 ) };
	// 832C98BC: 480000C4  b 0x832c9980
	pc = 0x832C9980; continue 'dispatch;
	// 832C98C0: 99280000  stb r9, 0(r8)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[9].u8 ) };
	// 832C98C4: 3BBD0001  addi r29, r29, 1
	ctx.r[29].s64 = ctx.r[29].s64 + 1;
	// 832C98C8: 39080001  addi r8, r8, 1
	ctx.r[8].s64 = ctx.r[8].s64 + 1;
	// 832C98CC: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C98D0: 409A0018  bne cr6, 0x832c98e8
	if !ctx.cr[6].eq {
	pc = 0x832C98E8; continue 'dispatch;
	}
	// 832C98D4: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C98D8: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C98DC: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C98E0: 5547F87E  srwi r7, r10, 1
	// 832C98E4: 48000010  b 0x832c98f4
	pc = 0x832C98F4; continue 'dispatch;
	// 832C98E8: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C98EC: 54E7F87E  srwi r7, r7, 1
	// 832C98F0: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C98F4: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C98F8: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C98FC: 7D5B00D0  neg r10, r27
	ctx.r[10].s64 = -ctx.r[27].s64;
	// 832C9900: 409A0008  bne cr6, 0x832c9908
	if !ctx.cr[6].eq {
	pc = 0x832C9908; continue 'dispatch;
	}
	// 832C9904: 7F6ADB78  mr r10, r27
	ctx.r[10].u64 = ctx.r[27].u64;
	// 832C9908: 7F1E2840  cmplw cr6, r30, r5
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[5].u32, &mut ctx.xer);
	// 832C990C: 7D4919AE  stbx r10, r9, r3
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[3].u32), ctx.r[10].u8) };
	// 832C9910: 3BDE0001  addi r30, r30, 1
	ctx.r[30].s64 = ctx.r[30].s64 + 1;
	// 832C9914: 419A0080  beq cr6, 0x832c9994
	if ctx.cr[6].eq {
	pc = 0x832C9994; continue 'dispatch;
	}
	// 832C9918: 48000068  b 0x832c9980
	pc = 0x832C9980; continue 'dispatch;
	// 832C991C: 5529F0BE  srwi r9, r9, 2
	// 832C9920: 3BBD0001  addi r29, r29, 1
	ctx.r[29].s64 = ctx.r[29].s64 + 1;
	// 832C9924: 99280000  stb r9, 0(r8)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[8].u32.wrapping_add(0 as u32), ctx.r[9].u8 ) };
	// 832C9928: 2B0B0000  cmplwi cr6, r11, 0
	ctx.cr[6].compare_u32(ctx.r[11].u32, 0 as u32, &mut ctx.xer);
	// 832C992C: 39080001  addi r8, r8, 1
	ctx.r[8].s64 = ctx.r[8].s64 + 1;
	// 832C9930: 409A0018  bne cr6, 0x832c9948
	if !ctx.cr[6].eq {
	pc = 0x832C9948; continue 'dispatch;
	}
	// 832C9934: 815F0000  lwz r10, 0(r31)
	ctx.r[10].u64 = unsafe { crate::rt::load_u32( base as *const u8, ctx.r[31].u32.wrapping_add(0 as u32) ) } as u64;
	// 832C9938: 3BFF0004  addi r31, r31, 4
	ctx.r[31].s64 = ctx.r[31].s64 + 4;
	// 832C993C: 3960001F  li r11, 0x1f
	ctx.r[11].s64 = 31;
	// 832C9940: 5547F87E  srwi r7, r10, 1
	// 832C9944: 48000010  b 0x832c9954
	pc = 0x832C9954; continue 'dispatch;
	// 832C9948: 7CEA3B78  mr r10, r7
	ctx.r[10].u64 = ctx.r[7].u64;
	// 832C994C: 54E7F87E  srwi r7, r7, 1
	// 832C9950: 396BFFFF  addi r11, r11, -1
	ctx.r[11].s64 = ctx.r[11].s64 + -1;
	// 832C9954: 554A07FE  clrlwi r10, r10, 0x1f
	ctx.r[10].u64 = ctx.r[10].u32 as u64 & 0x00000001u64;
	// 832C9958: 2B0A0000  cmplwi cr6, r10, 0
	ctx.cr[6].compare_u32(ctx.r[10].u32, 0 as u32, &mut ctx.xer);
	// 832C995C: 7D5B00D0  neg r10, r27
	ctx.r[10].s64 = -ctx.r[27].s64;
	// 832C9960: 409A0008  bne cr6, 0x832c9968
	if !ctx.cr[6].eq {
	pc = 0x832C9968; continue 'dispatch;
	}
	// 832C9964: 7F6ADB78  mr r10, r27
	ctx.r[10].u64 = ctx.r[27].u64;
	// 832C9968: 7F1E2840  cmplw cr6, r30, r5
	ctx.cr[6].compare_u32(ctx.r[30].u32, ctx.r[5].u32, &mut ctx.xer);
	// 832C996C: 7D4919AE  stbx r10, r9, r3
	unsafe { crate::rt::store_u8(base as *mut u8, ctx.r[9].u32.wrapping_add(ctx.r[3].u32), ctx.r[10].u8) };
	// 832C9970: 3BDE0001  addi r30, r30, 1
	ctx.r[30].s64 = ctx.r[30].s64 + 1;
	// 832C9974: 419A0020  beq cr6, 0x832c9994
	if ctx.cr[6].eq {
	pc = 0x832C9994; continue 'dispatch;
	}
	// 832C9978: 9B260000  stb r25, 0(r6)
	unsafe { crate::rt::store_u8( base as *mut u8, ctx.r[6].u32.wrapping_add(0 as u32), ctx.r[25].u8 ) };
	// 832C997C: 38C60001  addi r6, r6, 1
	ctx.r[6].s64 = ctx.r[6].s64 + 1;
	// 832C9980: 7F06D040  cmplw cr6, r6, r26
	ctx.cr[6].compare_u32(ctx.r[6].u32, ctx.r[26].u32, &mut ctx.xer);
	// 832C9984: 4198FC38  blt cr6, 0x832c95bc
	if ctx.cr[6].lt {
	pc = 0x832C95BC; continue 'dispatch;
	}
	// 832C9988: 7F7B0E70  srawi r27, r27, 1
	ctx.xer.ca = (ctx.r[27].s32 < 0) && ((ctx.r[27].u32 & ((1u32 << 1) - 1)) != 0);
	ctx.r[27].s64 = (ctx.r[27].s32 >> 1) as i64;
	// 832C998C: 3718FFFF  addic. r24, r24, -1
	ctx.xer.ca = (ctx.r[24].u32 > (!(-1 as u32)));
	ctx.r[24].s64 = ctx.r[24].s64 + -1;
	ctx.cr[0].compare_i32(ctx.r[24].s32, 0, &mut ctx.xer);
	// 832C9990: 4082FB98  bne 0x832c9528
	if !ctx.cr[0].eq {
	pc = 0x832C9528; continue 'dispatch;
	}
	// 832C9994: 93E40004  stw r31, 4(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(4 as u32), ctx.r[31].u32 ) };
	// 832C9998: 90E40000  stw r7, 0(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(0 as u32), ctx.r[7].u32 ) };
	// 832C999C: 91640008  stw r11, 8(r4)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[4].u32.wrapping_add(8 as u32), ctx.r[11].u32 ) };
	// 832C99A0: 4B9DFA9C  b 0x82ca943c
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA943C);
	return;
}

pub fn sub_832C99A8(ctx: &mut crate::recompiler::ppc_context::PPCContext, base: *mut u8) {
    let mut ea: u32 = 0;
    // ---- function 0x832C99A8 size=1684
	// 832C99A8: 7D8802A6  mflr r12
	ctx.r[12].u64 = ctx.lr;
	// 832C99AC: 4B9DFA25  bl 0x82ca93d0
	ctx.lr = 0x832C99B0;
	crate::recompiler::externs::call(&mut ctx, base, 0x82CA93D0);
	// 832C99B0: 9421FEB0  stwu r1, -0x150(r1)
	ea = ctx.r[1].u32.wrapping_add(-336 as u32);
	unsafe { crate::rt::store_u32(base as *mut u8, ea, ctx.r[1].u32) };
	ctx.r[1].u32 = ea;
	// 832C99B4: 1000038C  vspltisw v0, 0
	for i in 0..4 {
		ctx.v[0].u32[i] = 0;
	}
	// 832C99B8: 7D1D4378  mr r29, r8
	ctx.r[29].u64 = ctx.r[8].u64;
	// 832C99BC: 39610070  addi r11, r1, 0x70
	ctx.r[11].s64 = ctx.r[1].s64 + 112;
	// 832C99C0: 39410080  addi r10, r1, 0x80
	ctx.r[10].s64 = ctx.r[1].s64 + 128;
	// 832C99C4: 93A10064  stw r29, 0x64(r1)
	unsafe { crate::rt::store_u32( base as *mut u8, ctx.r[1].u32.wrapping_add(100 as u32), ctx.r[29].u32 ) };
	// 832C99C8: 39210090  addi r9, r1, 0x90
	ctx.r[9].s64 = ctx.r[1].s64 + 144;
	// 832C99CC: 390100A0  addi r8, r1, 0xa0
	ctx.r[8].s64 = ctx.r[1].s64 + 160;
	// 832C99D0: 7C9C2378  mr r28, r4
	ctx.r[28].u64 = ctx.r[4].u64;
	// 832C99D4: 7C7E1B78  mr r30, r3
	ctx.r[30].u64 = ctx.r[3].u64;
}

